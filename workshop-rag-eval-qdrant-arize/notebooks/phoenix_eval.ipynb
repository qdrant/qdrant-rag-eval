{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe1eaf7d-96f2-4f00-8827-beda9bed0baa",
   "metadata": {},
   "source": [
    "### **0. Import the relevant packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94a40759-5a50-4b24-a5d1-2bb043d2da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Setup projects\n",
    "SIMPLE_RAG_PROJECT = \"simple-rag\"\n",
    "os.environ[\"PHOENIX_PROJECT_NAME\"] = SIMPLE_RAG_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "924ac3f9-3a1b-4d8d-9f38-6515f1a43501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "import nest_asyncio\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from llama_index.core import set_global_handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4749de6-8022-474a-811c-7960dfee386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()  # needed for concurrent evals in notebook environments\n",
    "pd.set_option(\"display.max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f4b83f-8400-46eb-827d-dd064ba3a319",
   "metadata": {},
   "source": [
    "### **1. Launch Phoenix**\n",
    "You can run Phoenix in the background to collect trace data emitted by any LlamaIndex application that has been instrumented with the OpenInferenceTraceCallbackHandler. Phoenix supports LlamaIndex's one-click observability which will automatically instrument your LlamaIndex application! You can consult our integration guide for a more detailed explanation of how to instrument your LlamaIndex application.\n",
    "\n",
    "Launch Phoenix and follow the instructions in the cell output to open the Phoenix UI (the UI should be empty because we have yet to run the LlamaIndex application)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74976fcc-5e86-441a-b5ce-5f840d605999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üì∫ To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "session = px.launch_app()\n",
    "set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c86587-137a-4fd4-9a1f-d1c6309308f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∫ Opening a view to the Phoenix app. The app is running at http://localhost:6006/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://localhost:6006/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x33585fc10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can view the above data in the UI\n",
    "session.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de6b21-f9a6-46f8-89db-8d5f4fa460ef",
   "metadata": {},
   "source": [
    "### **2. Run Your Query Engine and View Your Traces in Phoenix**\n",
    "\n",
    "We've compiled a list of commonly asked questions about Qdrant. Let's download the sample queries and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc065884-7fad-4092-b94a-c3aa6c5a5c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the Eval dataset\n",
    "qdrant_qa = load_dataset(\"atitaarora/qdrant_docs_qna_ragas\", split=\"train\")\n",
    "qdrant_qa_question = qdrant_qa.select_columns(['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d34ec3a9-f9ae-4125-8548-9db3665c879a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the purpose of oversampling in Qdrant search process?',\n",
       " 'How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?',\n",
       " 'What is the difference between regular and neural search?',\n",
       " 'How can I use Qdrant as a vector store in Langchain Go?',\n",
       " 'How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?',\n",
       " 'Why do we still need keyword search?',\n",
       " 'What principles did Qdrant follow while designing benchmarks for vector search engines?',\n",
       " 'What models does Qdrant support for embedding generation?',\n",
       " 'How can you parallelize the upload of a large dataset using shards in Qdrant?',\n",
       " 'What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_qa_question['question'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecda53d-2564-4b86-ab44-e4cd2040e840",
   "metadata": {},
   "source": [
    "### *This example uses a QdrantVectorStore and uses the previously generated collection to work fully connected with Qdrant but you can use whatever LlamaIndex application you like.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5bf79dc-8aed-41a2-ba9c-78c9af3f25b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8d118c0-e3a8-45ab-8047-e2678024e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"qdrant_docs_arize_dense\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb35c4b0-aefb-455c-b881-fdeec9da8283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = QdrantClient(\n",
    "#    location=\":memory:\",\n",
    "# )\n",
    "\n",
    "##Uncomment below to connect to Qdrant Cloud\n",
    "client = QdrantClient(\n",
    "    os.environ.get(\"QDRANT_URL\"), \n",
    "    api_key=os.environ.get(\"QDRANT_API_KEY\"),\n",
    ")\n",
    "\n",
    "## Uncomment below to connect to local Qdrant\n",
    "#client = qdrant_client.QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13e8ae30-d706-4ab8-9a27-7b6d302c1479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[CollectionDescription(name='qdrant_docs_arize_dense')])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a277969-5b19-465a-9e66-abb34069f3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "from llama_index.core.indices.vector_store import VectorIndexRetriever\n",
    "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, storage_context=storage_context)\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    vector_store_query_mode=VectorStoreQueryMode.DEFAULT,\n",
    "    similarity_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1afd1d06-ce25-45ef-9844-8f3d7b5ebb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:03<00:00,  2.88it/s]\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "for query in tqdm(qdrant_qa_question['question'][:10]):\n",
    "    try:\n",
    "      query_engine.query(query)\n",
    "      #retriever.retrieve(query)\n",
    "    except Exception as e:\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c4643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
