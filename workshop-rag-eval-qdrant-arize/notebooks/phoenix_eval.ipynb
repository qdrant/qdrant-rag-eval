{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe1eaf7d-96f2-4f00-8827-beda9bed0baa",
   "metadata": {},
   "source": [
    "### **0. Import the relevant packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a40759-5a50-4b24-a5d1-2bb043d2da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Setup projects\n",
    "SIMPLE_RAG_PROJECT = \"simple-rag\"\n",
    "os.environ[\"PHOENIX_PROJECT_NAME\"] = SIMPLE_RAG_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ac3f9-3a1b-4d8d-9f38-6515f1a43501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import phoenix as px\n",
    "import nest_asyncio\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from phoenix.evals import (\n",
    "    HallucinationEvaluator, OpenAIModel, QAEvaluator,\n",
    "    RelevanceEvaluator, run_evals\n",
    ")\n",
    "from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n",
    "from phoenix.trace import DocumentEvaluations, SpanEvaluations\n",
    "from tqdm import tqdm\n",
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import set_global_handler, Settings\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import PointStruct\n",
    "\n",
    "nest_asyncio.apply()  # needed for concurrent evals in notebook environments\n",
    "pd.set_option(\"display.max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f4b83f-8400-46eb-827d-dd064ba3a319",
   "metadata": {},
   "source": [
    "### **1. Launch Phoenix**\n",
    "You can run Phoenix in the background to collect trace data emitted by any LlamaIndex application that has been instrumented with the OpenInferenceTraceCallbackHandler. Phoenix supports LlamaIndex's one-click observability which will automatically instrument your LlamaIndex application! You can consult our integration guide for a more detailed explanation of how to instrument your LlamaIndex application.\n",
    "\n",
    "Launch Phoenix and follow the instructions in the cell output to open the Phoenix UI (the UI should be empty because we have yet to run the LlamaIndex application)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74976fcc-5e86-441a-b5ce-5f840d605999",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = px.launch_app()\n",
    "set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c86587-137a-4fd4-9a1f-d1c6309308f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can view the above data in the UI\n",
    "px.active_session().view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de6b21-f9a6-46f8-89db-8d5f4fa460ef",
   "metadata": {},
   "source": [
    "### **2. Run Your Query Engine and View Your Traces in Phoenix**\n",
    "\n",
    "We've compiled a list of commonly asked questions about Qdrant. Let's download the sample queries and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc065884-7fad-4092-b94a-c3aa6c5a5c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the Eval dataset\n",
    "qdrant_qa = load_dataset(\"atitaarora/qdrant_docs_qna_ragas\", split=\"train\")\n",
    "qdrant_qa_question = qdrant_qa.select_columns(['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34ec3a9-f9ae-4125-8548-9db3665c879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_qa_question['question'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecda53d-2564-4b86-ab44-e4cd2040e840",
   "metadata": {},
   "source": [
    "### *This example uses a QdrantVectorStore and uses the previously generated collection to work fully connected with Qdrant but you can use whatever LlamaIndex application you like.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf79dc-8aed-41a2-ba9c-78c9af3f25b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d118c0-e3a8-45ab-8047-e2678024e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"qdrant_docs_arize_dense\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb35c4b0-aefb-455c-b881-fdeec9da8283",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\n",
    "    os.environ.get(\"QDRANT_URL\"), \n",
    "    api_key=os.environ.get(\"QDRANT_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e8ae30-d706-4ab8-9a27-7b6d302c1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a277969-5b19-465a-9e66-abb34069f3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "from llama_index.core.indices.vector_store import VectorIndexRetriever\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    vector_store_query_mode=VectorStoreQueryMode.DEFAULT,\n",
    "    similarity_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd1d06-ce25-45ef-9844-8f3d7b5ebb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "for query in tqdm(qdrant_qa_question['question'][:10]):\n",
    "    try:\n",
    "      query_engine.query(query)\n",
    "    except Exception as e:\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004aa586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
