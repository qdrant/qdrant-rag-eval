{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c23702c6-4059-4612-a04b-b49a30d2ad87",
   "metadata": {},
   "source": [
    "### **0. Import relevant packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689a9650-85e3-41b0-b92c-d7c808b71309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import pandas as pd\n",
    "import qdrant_client\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from typing import Optional, List, Tuple\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import PointStruct\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from llama_index.core import (\n",
    "    StorageContext\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3a0308-f753-47b2-8fb5-5af7bbde441f",
   "metadata": {},
   "source": [
    "### **1.  Retrieve the documents / dataset to be used**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e869ae8e-ea7a-4a84-825f-55c20744ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# If the dataset is gated/private, make sure you have run huggingface-cli login\n",
    "dataset = load_dataset(\"atitaarora/qdrant_doc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33098945-8e7c-4c4a-9aba-9f0bd2938ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62de9ada-8966-4acd-85b0-480b792052c2",
   "metadata": {},
   "source": [
    "### **2.  Definition of global chunk properties and chunk processing**\n",
    "Processing each document with desired **TEXT_SPLITTER_ALGO , CHUNK_SIZE , CHUNK_OVERLAP** etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e464f0ad-9096-417b-8f29-1bf701f8df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global config for chunk processing\n",
    "CHUNK_SIZE = 512 #1000\n",
    "CHUNK_OVERLAP = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a935db-68be-4b90-ad7b-13646e38dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "langchain_docs = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in tqdm(dataset)\n",
    "]\n",
    "\n",
    "# could showcase another variation of processed documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab102f-4f0a-4888-9f29-5a8e78366fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ab08a1-a723-45db-a371-9f03968bf63c",
   "metadata": {},
   "source": [
    "### **3. Process dataset as langchain (or llamaindex) document for further processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e20a28-bffc-4cf0-b2c7-48be1d9c84bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting Langchain document chunks above into Llamaindex Document for ingestion\n",
    "from llama_index.core import Document\n",
    "documents = [\n",
    "        Document.from_langchain_format(doc)\n",
    "        for doc in docs_processed\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e902e0fd-318a-4a7a-9f0a-0fff89d72527",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee87686-b16a-4d03-a070-70c45a164042",
   "metadata": {},
   "source": [
    "### **4. Setting up Qdrant and collection**\n",
    "\n",
    "We first set up the qdrant client and then create a collection so that our data may be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d295d5-b04f-445c-9ce1-3fa29fe897ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22207a55-ea5a-4028-b866-ca603b63d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##Uncomment to initialise qdrant client in memory\n",
    "#client = qdrant_client.QdrantClient(\n",
    "#    location=\":memory:\",\n",
    "#)\n",
    "\n",
    "##Uncomment below to connect to Qdrant Cloud\n",
    "client = QdrantClient(\n",
    "    os.environ.get(\"QDRANT_URL\"), \n",
    "    api_key=os.environ.get(\"QDRANT_API_KEY\"),\n",
    ")\n",
    "\n",
    "## Uncomment below to connect to local Qdrant\n",
    "#client = qdrant_client.QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac7bd9-b921-460e-8aea-25f19dbfe099",
   "metadata": {},
   "outputs": [],
   "source": [
    "## General Collection level operations\n",
    "\n",
    "## Get information about existing collections\n",
    "client.get_collections()\n",
    "\n",
    "## Get information about specific collection\n",
    "#collection_info = client.get_collection(COLLECTION_NAME)\n",
    "#print(collection_info)\n",
    "\n",
    "## Deleting collection , if need be\n",
    "#client.delete_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0956fb-37e3-438a-b1cc-a77c66c82732",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"qdrant_docs_arize_dense\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec9eac0-0a9a-4cb5-80d4-51b035136ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declaring the intended Embedding Model with Fastembed\n",
    "from fastembed.embedding import TextEmbedding\n",
    "\n",
    "pd.DataFrame(TextEmbedding.list_supported_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94273d1b-1b73-44f3-aad4-c31bc3b4e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Initilising embedding model\n",
    "embedding_model = TextEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "## For custom model supported by Fastembed\n",
    "#embedding_model = TextEmbedding(model_name=\"BAAI/bge-small-en\", max_length=512)\n",
    "\n",
    "## Using Default Model - BAAI/bge-small-en-v1.5\n",
    "embedding_model.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caf5104-04cd-469e-9098-bd9848a79f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_index\n",
    "from llama_index.core import Settings\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "## Uncomment it if you'll like to use FastEmbed instead of OpenAI\n",
    "## For the complete list of supported models ,\n",
    "##please check https://qdrant.github.io/fastembed/examples/Supported_Models/\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "##Uncomment if using FastEmbed\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "## Uncomment it if you'll like to use OpenAI Embeddings instead of FastEmbed\n",
    "#Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "Settings.llm = OpenAI(model=\"gpt-4-1106-preview\", temperature=0.0)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8bccba-b0b2-490b-993f-cb0398531b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.count(collection_name=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ab31f7-84c6-4e72-8acd-ef7bfd624e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "from llama_index.core.indices.vector_store import VectorIndexRetriever\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    vector_store_query_mode=VectorStoreQueryMode.DEFAULT,\n",
    "    similarity_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cd7441-cc19-4735-8cbb-cc22d7e3202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retriever.retrieve(\"What is quantization?\")\n",
    "for i, node in enumerate(response):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
