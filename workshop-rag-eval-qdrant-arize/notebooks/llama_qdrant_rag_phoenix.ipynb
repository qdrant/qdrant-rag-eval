{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmb4fjQvQmFT"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/qdrant_arize.png\" width=\"500\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Tuning a RAG Pipeline using Qdrant and Arize Phoenix</h1>\n",
    "\n",
    "‚ÑπÔ∏è This notebook requires an OpenAI API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Import Relevant Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jCyVyMs-JrWS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Setup projects\n",
    "SIMPLE_RAG_PROJECT = \"simple-rag\"\n",
    "HYBRID_RAG_PROJECT = \"hybrid-rag\"\n",
    "os.environ[\"PHOENIX_PROJECT_NAME\"] = SIMPLE_RAG_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NNJI9dP6GeUE"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import ssl\n",
    "import time\n",
    "import urllib\n",
    "from getpass import getpass\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import certifi\n",
    "import nest_asyncio\n",
    "import openai\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from llama_index.core import (\n",
    "    ServiceContext, StorageContext, download_loader,\n",
    "    load_index_from_storage, set_global_handler\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.graph_stores.simple import SimpleGraphStore\n",
    "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from phoenix.evals import (\n",
    "    HallucinationEvaluator, OpenAIModel, QAEvaluator,\n",
    "    RelevanceEvaluator, run_evals\n",
    ")\n",
    "from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n",
    "from phoenix.trace import DocumentEvaluations, SpanEvaluations\n",
    "from tqdm import tqdm\n",
    "\n",
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import PointStruct\n",
    "\n",
    "nest_asyncio.apply()  # needed for concurrent evals in notebook environments\n",
    "pd.set_option(\"display.max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hnr-p5J6jyEl"
   },
   "source": [
    "### **2. Launch Phoenix**\n",
    "You can run Phoenix in the background to collect trace data emitted by any LlamaIndex application that has been instrumented with the OpenInferenceTraceCallbackHandler. Phoenix supports LlamaIndex's one-click observability which will automatically instrument your LlamaIndex application! You can consult our integration guide for a more detailed explanation of how to instrument your LlamaIndex application.\n",
    "\n",
    "Launch Phoenix and follow the instructions in the cell output to open the Phoenix UI (the UI should be empty because we have yet to run the LlamaIndex application)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "zYfgwhXvZcOV",
    "outputId": "3599030d-8ba2-4f8a-c8ec-a0d5b576015b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üì∫ To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "session = px.launch_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9n105Mckm1f"
   },
   "source": [
    "Be sure to enable phoenix as your global handler for tracing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1Ym-djTOkjxQ"
   },
   "outputs": [],
   "source": [
    "set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z15BcuTcj6Cp"
   },
   "source": [
    "### **3. Setup your openai key and retrieve the documents to be used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7EsFBezzcGbG",
    "outputId": "158775a0-8f34-4201-88c5-b221fe40f883"
   },
   "outputs": [],
   "source": [
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Retrieve the documents / dataset to be used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "c3a80fdcb9944e62ba5b51838fef5ef4",
      "af7d2403f4504d49a027d935b13d3f62",
      "79335cc0bf85458ba630ced71d706b2e",
      "43e63c6009c14d9cb4eee3ea177d1c7d",
      "dce488c3561e4d7fa737379553ec0b3b",
      "6ff26418624f4662871a69796f67427a",
      "d5e93af9e5644c419bc653af2057e285",
      "251b1e790043469393aae0c8ebec3d77",
      "602df92e53334c65a2cc171301c67e46",
      "8f997d5f3659421981252a05845dd236",
      "af7527216b9f4240b36024d07ac5c507",
      "02ca3a5d43e343d5b756fca172bc1822",
      "f1e6788ea4344553adbe66202caa656c",
      "5a3d5d83788446028f4de4afbd262f3e",
      "bf06bcbdbff3485abb3895dc54256e00",
      "1ff9ec98314944bdb86d4fda42a4c88b",
      "f9e47bbf438740a2a224c5f7d913e502",
      "c30b902e82de486eb0c10a3e64687b44",
      "4cc3f02b7bb44a87b8d50dd2f98edce7",
      "e07726709c7748eeb708dbe5c37f9770",
      "8115b0b37d164080b58368be3fadbaa0",
      "0d229e81fcb246cf99dd919d06cf5905",
      "eecec2323e294d6fb4ee89f5937c241c",
      "ac507e1d00b446df9379fa58f39418fc",
      "eb848aa889474b118c49a7cdac509d9b",
      "37a8add4c6a242a49ea09ebef29cfdd9",
      "b44f529954e944729b7446a4d610ac1d",
      "c85db136dc2843a58e7aa648bbf9f9fe",
      "5cbb12c59ec948ba8d66dc3812d5c846",
      "adc2166e9b7d4ebdb5dd5415fd3e2ec5",
      "90f264886e864ddabdb6eddbecf6d75a",
      "b993aeba0d0f4021b4ae31b8378af903",
      "cfbcd04f19374fe6b8ab9802f6891612"
     ]
    },
    "id": "iisWzELAzYca",
    "outputId": "751b3676-2a8f-4497-de7e-85ee3e9ec199"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# If the dataset is gated/private, make sure you have run huggingface-cli login\n",
    "dataset = load_dataset(\"atitaarora/qdrant_doc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t5jYTnnlza6b",
    "outputId": "2894f142-0d41-42b8-b75d-63850ad8a2db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description='', citation='', homepage='', license='', features={'text': Value(dtype='string', id=None), 'source': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='csv', dataset_name='qdrant_doc', config_name='default', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=1767967, num_examples=240, shard_lengths=None, dataset_name='qdrant_doc')}, download_checksums={'hf://datasets/atitaarora/qdrant_doc@8d859890840f65337c38e96d660b81b1441bbecd/documents.csv': {'num_bytes': 1777260, 'checksum': None}}, download_size=1777260, post_processing_size=None, dataset_size=1767967, size_in_bytes=3545227)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Definition of global chunk properties and chunk processing**\n",
    "Processing each document with desired **TEXT_SPLITTER_ALGO , CHUNK_SIZE , CHUNK_OVERLAP** etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global config for chunk processing\n",
    "CHUNK_SIZE = 512 #1000\n",
    "CHUNK_OVERLAP = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Process dataset as langchain (or llamaindex) document for further processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qWTlffFW6Art",
    "outputId": "2685adf0-2965-4c2d-878f-67eee384d65c"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from llama_index.core import Document\n",
    "\n",
    "## Split and process the document chunks from the given dataset\n",
    "\n",
    "def process_document_chunks(dataset,chunk_size,chunk_overlap):\n",
    "    langchain_docs = [\n",
    "        LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "        for doc in tqdm(dataset)\n",
    "    ]\n",
    "\n",
    "    # could showcase another variation of processed documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        add_start_index=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in langchain_docs:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    ## Converting Langchain document chunks above into Llamaindex Document for ingestion\n",
    "    llama_documents = [\n",
    "        Document.from_langchain_format(doc)\n",
    "        for doc in docs_processed\n",
    "    ]\n",
    "    return llama_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IbWBhR_661SX",
    "outputId": "79b17993-361d-47e1-ec14-f6f6dfd6949c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [00:00<00:00, 8416.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4431"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = process_document_chunks(dataset, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr5DrAtMkbL-"
   },
   "source": [
    "### **7. Setting up Qdrant and Collection**\n",
    "\n",
    "We first set up the qdrant client and then create a collection so that our data may be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "QYir5ueWcb8r"
   },
   "outputs": [],
   "source": [
    "##Uncomment to initialise qdrant client in memory\n",
    "#client = qdrant_client.QdrantClient(\n",
    "#    location=\":memory:\",\n",
    "#)\n",
    "\n",
    "##Uncomment below to connect to Qdrant Cloud\n",
    "client = QdrantClient(\n",
    "    os.environ.get(\"QDRANT_URL\"), \n",
    "    api_key=os.environ.get(\"QDRANT_API_KEY\"),\n",
    ")\n",
    "\n",
    "## Uncomment below to connect to local Qdrant\n",
    "#client = qdrant_client.QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collection Name \n",
    "COLLECTION_NAME = \"qdrant_docs_arize_dense\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z-IUMQAT76Ep",
    "outputId": "88207dc6-c291-4d76-c8e9-ea6f180ed02a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[CollectionDescription(name='qdrant_docs_arize_hybrid')])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## General Collection level operations\n",
    "\n",
    "## Get information about existing collections \n",
    "client.get_collections()\n",
    "\n",
    "## Get information about specific collection\n",
    "#collection_info = client.get_collection(COLLECTION_NAME)\n",
    "#print(collection_info)\n",
    "\n",
    "## Deleting collection, if need be\n",
    "#client.delete_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-16 17:55:12.207\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mfastembed.embedding\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[33m\u001b[1mDefaultEmbedding, FlagEmbedding, JinaEmbedding are deprecated.Use from fastembed import TextEmbedding instead.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>dim</th>\n",
       "      <th>description</th>\n",
       "      <th>size_in_GB</th>\n",
       "      <th>sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BAAI/bge-base-en</td>\n",
       "      <td>768</td>\n",
       "      <td>Base English model</td>\n",
       "      <td>0.420</td>\n",
       "      <td>{'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en.tar.gz'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BAAI/bge-base-en-v1.5</td>\n",
       "      <td>768</td>\n",
       "      <td>Base English model, v1.5</td>\n",
       "      <td>0.210</td>\n",
       "      <td>{'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en-v1.5.tar.gz', 'hf': 'qdrant/bge-base-en-v1.5-onnx-q'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAAI/bge-large-en-v1.5</td>\n",
       "      <td>1024</td>\n",
       "      <td>Large English model, v1.5</td>\n",
       "      <td>1.200</td>\n",
       "      <td>{'hf': 'qdrant/bge-large-en-v1.5-onnx'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BAAI/bge-small-en</td>\n",
       "      <td>384</td>\n",
       "      <td>Fast English model</td>\n",
       "      <td>0.130</td>\n",
       "      <td>{'url': 'https://storage.googleapis.com/qdrant-fastembed/BAAI-bge-small-en.tar.gz'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>384</td>\n",
       "      <td>Fast and Default English model</td>\n",
       "      <td>0.067</td>\n",
       "      <td>{'hf': 'qdrant/bge-small-en-v1.5-onnx-q'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BAAI/bge-small-zh-v1.5</td>\n",
       "      <td>512</td>\n",
       "      <td>Fast and recommended Chinese model</td>\n",
       "      <td>0.090</td>\n",
       "      <td>{'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>384</td>\n",
       "      <td>Sentence Transformer model, MiniLM-L6-v2</td>\n",
       "      <td>0.090</td>\n",
       "      <td>{'url': 'https://storage.googleapis.com/qdrant-fastembed/sentence-transformers-all-MiniLM-L6-v2.tar.gz', 'hf': 'qdrant/all-MiniLM-L6-v2-onnx'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>384</td>\n",
       "      <td>Sentence Transformer model, paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>0.220</td>\n",
       "      <td>{'hf': 'qdrant/paraphrase-multilingual-MiniLM-L12-v2-onnx-Q'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nomic-ai/nomic-embed-text-v1</td>\n",
       "      <td>768</td>\n",
       "      <td>8192 context length english model</td>\n",
       "      <td>0.520</td>\n",
       "      <td>{'hf': 'nomic-ai/nomic-embed-text-v1'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nomic-ai/nomic-embed-text-v1.5</td>\n",
       "      <td>768</td>\n",
       "      <td>8192 context length english model</td>\n",
       "      <td>0.520</td>\n",
       "      <td>{'hf': 'nomic-ai/nomic-embed-text-v1.5'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>thenlper/gte-large</td>\n",
       "      <td>1024</td>\n",
       "      <td>Large general text embeddings model</td>\n",
       "      <td>1.200</td>\n",
       "      <td>{'hf': 'qdrant/gte-large-onnx'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mixedbread-ai/mxbai-embed-large-v1</td>\n",
       "      <td>1024</td>\n",
       "      <td>MixedBread Base sentence embedding model, does well on MTEB</td>\n",
       "      <td>0.640</td>\n",
       "      <td>{'hf': 'mixedbread-ai/mxbai-embed-large-v1'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>intfloat/multilingual-e5-large</td>\n",
       "      <td>1024</td>\n",
       "      <td>Multilingual model, e5-large. Recommend using this model for non-English languages</td>\n",
       "      <td>2.240</td>\n",
       "      <td>{'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-multilingual-e5-large.tar.gz', 'hf': 'qdrant/multilingual-e5-large-onnx'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sentence-transformers/paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>768</td>\n",
       "      <td>Sentence-transformers model for tasks like clustering or semantic search</td>\n",
       "      <td>1.000</td>\n",
       "      <td>{'hf': 'xenova/paraphrase-multilingual-mpnet-base-v2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jinaai/jina-embeddings-v2-base-en</td>\n",
       "      <td>768</td>\n",
       "      <td>English embedding model supporting 8192 sequence length</td>\n",
       "      <td>0.520</td>\n",
       "      <td>{'hf': 'xenova/jina-embeddings-v2-base-en'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>jinaai/jina-embeddings-v2-small-en</td>\n",
       "      <td>512</td>\n",
       "      <td>English embedding model supporting 8192 sequence length</td>\n",
       "      <td>0.120</td>\n",
       "      <td>{'hf': 'xenova/jina-embeddings-v2-small-en'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          model   dim  \\\n",
       "0                                              BAAI/bge-base-en   768   \n",
       "1                                         BAAI/bge-base-en-v1.5   768   \n",
       "2                                        BAAI/bge-large-en-v1.5  1024   \n",
       "3                                             BAAI/bge-small-en   384   \n",
       "4                                        BAAI/bge-small-en-v1.5   384   \n",
       "5                                        BAAI/bge-small-zh-v1.5   512   \n",
       "6                        sentence-transformers/all-MiniLM-L6-v2   384   \n",
       "7   sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2   384   \n",
       "8                                  nomic-ai/nomic-embed-text-v1   768   \n",
       "9                                nomic-ai/nomic-embed-text-v1.5   768   \n",
       "10                                           thenlper/gte-large  1024   \n",
       "11                           mixedbread-ai/mxbai-embed-large-v1  1024   \n",
       "12                               intfloat/multilingual-e5-large  1024   \n",
       "13  sentence-transformers/paraphrase-multilingual-mpnet-base-v2   768   \n",
       "14                            jinaai/jina-embeddings-v2-base-en   768   \n",
       "15                           jinaai/jina-embeddings-v2-small-en   512   \n",
       "\n",
       "                                                                           description  \\\n",
       "0                                                                   Base English model   \n",
       "1                                                             Base English model, v1.5   \n",
       "2                                                            Large English model, v1.5   \n",
       "3                                                                   Fast English model   \n",
       "4                                                       Fast and Default English model   \n",
       "5                                                   Fast and recommended Chinese model   \n",
       "6                                             Sentence Transformer model, MiniLM-L6-v2   \n",
       "7                    Sentence Transformer model, paraphrase-multilingual-MiniLM-L12-v2   \n",
       "8                                                    8192 context length english model   \n",
       "9                                                    8192 context length english model   \n",
       "10                                                 Large general text embeddings model   \n",
       "11                         MixedBread Base sentence embedding model, does well on MTEB   \n",
       "12  Multilingual model, e5-large. Recommend using this model for non-English languages   \n",
       "13            Sentence-transformers model for tasks like clustering or semantic search   \n",
       "14                             English embedding model supporting 8192 sequence length   \n",
       "15                             English embedding model supporting 8192 sequence length   \n",
       "\n",
       "    size_in_GB  \\\n",
       "0        0.420   \n",
       "1        0.210   \n",
       "2        1.200   \n",
       "3        0.130   \n",
       "4        0.067   \n",
       "5        0.090   \n",
       "6        0.090   \n",
       "7        0.220   \n",
       "8        0.520   \n",
       "9        0.520   \n",
       "10       1.200   \n",
       "11       0.640   \n",
       "12       2.240   \n",
       "13       1.000   \n",
       "14       0.520   \n",
       "15       0.120   \n",
       "\n",
       "                                                                                                                                           sources  \n",
       "0                                                               {'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en.tar.gz'}  \n",
       "1                  {'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en-v1.5.tar.gz', 'hf': 'qdrant/bge-base-en-v1.5-onnx-q'}  \n",
       "2                                                                                                          {'hf': 'qdrant/bge-large-en-v1.5-onnx'}  \n",
       "3                                                              {'url': 'https://storage.googleapis.com/qdrant-fastembed/BAAI-bge-small-en.tar.gz'}  \n",
       "4                                                                                                        {'hf': 'qdrant/bge-small-en-v1.5-onnx-q'}  \n",
       "5                                                         {'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz'}  \n",
       "6   {'url': 'https://storage.googleapis.com/qdrant-fastembed/sentence-transformers-all-MiniLM-L6-v2.tar.gz', 'hf': 'qdrant/all-MiniLM-L6-v2-onnx'}  \n",
       "7                                                                                    {'hf': 'qdrant/paraphrase-multilingual-MiniLM-L12-v2-onnx-Q'}  \n",
       "8                                                                                                           {'hf': 'nomic-ai/nomic-embed-text-v1'}  \n",
       "9                                                                                                         {'hf': 'nomic-ai/nomic-embed-text-v1.5'}  \n",
       "10                                                                                                                 {'hf': 'qdrant/gte-large-onnx'}  \n",
       "11                                                                                                    {'hf': 'mixedbread-ai/mxbai-embed-large-v1'}  \n",
       "12         {'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-multilingual-e5-large.tar.gz', 'hf': 'qdrant/multilingual-e5-large-onnx'}  \n",
       "13                                                                                          {'hf': 'xenova/paraphrase-multilingual-mpnet-base-v2'}  \n",
       "14                                                                                                     {'hf': 'xenova/jina-embeddings-v2-base-en'}  \n",
       "15                                                                                                    {'hf': 'xenova/jina-embeddings-v2-small-en'}  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Declaring the intended Embedding Model with Fastembed\n",
    "from fastembed.embedding import TextEmbedding\n",
    "\n",
    "pd.DataFrame(TextEmbedding.list_supported_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. Document Embedding processing and Ingestion**\n",
    "\n",
    "This example uses a `QdrantVectorStore` and creates a new collection to work fully connected with Qdrant but you can use whatever LlamaIndex application you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "aa8db2c8310b4f9582f2002adf3ffa1b",
      "b84c10312dbe486fb3b21c7c5e23d603",
      "178bbf06a9ae4b52baa5b731c0c81590",
      "22c94d8ddc9d4561930044518630bb0b",
      "acf3351a0fba41558f97945fbc6b84c7",
      "1f3117c63e8d4558b801a33e9b5c548a",
      "997305a8d871401ba6a1e805dc1ca045",
      "003c1412566441b2a0fd878aabed07a6",
      "cf1c5baa009c467a8c76a1a78d6987b1",
      "18ee1349c9394dab81163d5de23a04a8",
      "21e41bb05ed34b02b2e9e1c2d7d56145",
      "c762c26d4f2e4159b714ad55bbd195eb",
      "b51b292bc0b341de8fd94707e7fa2c0c",
      "f10f2f679d7a4ab8988601cd59aa4b70",
      "d5baee84346c4dc29ffa4fd2fe6495c6",
      "0a8c4c23041641308b625e929790a55e",
      "d8749c8627c44757aa8703b45421af2d",
      "9f88771edbfd468cb315a54b69eb7226",
      "af77bbc36b844afa960381389111bccf",
      "b3dc75c3ec7a48ceaff045c81f250bfd",
      "280b4a0048134a8aaf523deead4e3aef",
      "869c8dd95cf7452b841f7203e0e6b8c5",
      "1ad21fe66fac4742beb7c17ba8f8733f",
      "1677f41c03a74a9f8ac55c49a9c12799",
      "5b8040a4238f41128006185a858db466",
      "7fbfaf28e7134c638b3e4d16e3cb3e6c",
      "1f1e8f2a35b645eb91e96a847cf2856b",
      "bda9ff3e5071497c933f31a2dd44b449",
      "b6686121f3084e1a8c6e0399a8a72675",
      "503e9200b403425b8c0379b0e74b01fe",
      "6f6e8b35568945dc85ba3e47f7b0f3d7",
      "c47e7205eb6540a8a5c51120cd222fe3",
      "ea74a3a2d06a4c61b85db3c7552535a1",
      "fa26918ac71440c1803c82c939fbc79f",
      "366e10ebb46a45508d837c8ead93124c",
      "44cb1b5c1c0a446f9d308d93d33ae4f8",
      "fca056b08fc3427a977bc3717a020443",
      "7e8c241b51af4963aa78d91e6e78136e",
      "6a222b53b0b2448a97b23e2b782a37b5",
      "99f0a6ea567247639e7b17af1e1df9cf",
      "50d006eafd294bbab3339c2bece90673",
      "b21c626648d643d4972c76a094391134",
      "d44a32f577204732a7f4ce767d8e83f5",
      "6b115bdeaeb547e6b96a5a440b2fe3ef"
     ]
    },
    "id": "DTjwoobJcJO3",
    "outputId": "1dcd8c41-4066-4130-f20f-dabc2a1f31d5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a127856eaffc47d9b26e2c09005ecc6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Initializing the space to work with llama-index and related settings\n",
    "import llama_index\n",
    "from llama_index.core import Settings\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from phoenix.trace import suppress_tracing\n",
    "## Uncomment it if you'd like to use FastEmbed instead of OpenAI\n",
    "## For the complete list of supported models,\n",
    "##please check https://qdrant.github.io/fastembed/examples/Supported_Models/\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "##Uncomment if using FastEmbed\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "## Uncomment it if you'd like to use OpenAI Embeddings instead of FastEmbed\n",
    "#Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4-1106-preview\", temperature=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06da2b2743c54f6da20647fc80865682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/4431 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57ddb218f854a499db32a17924204c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225314e7e6554e139ab90dff70c65950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c64955d5a04ac2877da7d6332596a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Note: Indexing block is to be used when you're creating a new collection \n",
    "## If using an existing collection, Skip this block and execute the next one instead.\n",
    "\n",
    "from phoenix.trace import suppress_tracing\n",
    "with suppress_tracing():\n",
    "  dense_vector_index = VectorStoreIndex.from_documents(\n",
    "      documents,\n",
    "      storage_context=storage_context,\n",
    "      show_progress=True\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8a. Connecting to existing Collection**\n",
    "\n",
    "This example uses a `QdrantVectorStore` and uses the previously generated collection to work fully connected with Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: Execute this block when using an existing collection\n",
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "from llama_index.core.indices.vector_store import VectorIndexRetriever\n",
    "dense_vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)\n",
    "dense_vector_index = VectorStoreIndex.from_vector_store(vector_store=dense_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52PEPokb8FlO",
    "outputId": "1cfa3ea9-09e1-4fdd-eed4-2aa8764ea4a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountResult(count=4431)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sanity Check : Count the total number of documents processed equals number of documents in the collection\n",
    "client.count(collection_name=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nxhA_6plBSL"
   },
   "source": [
    "### **9.Running an example query and printing out the response.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Zlep2wVsf1tR"
   },
   "outputs": [],
   "source": [
    "##Initialise retriever to interact with the Qdrant collection\n",
    "dense_retriever = VectorIndexRetriever(\n",
    "    index=dense_vector_index,\n",
    "    vector_store_query_mode=VectorStoreQueryMode.DEFAULT,\n",
    "    similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXE8pRjjgSse",
    "outputId": "6207af91-b168-4a42-c1de-832246c4a001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Such segments, for example, are created as copy-on-write segments during optimization itself.\n",
      "\n",
      "\n",
      "\n",
      "It is also essential to have at least one small segment that Qdrant will use to store frequently updated data.\n",
      "\n",
      "On the other hand, too many small segments lead to suboptimal search performance.\n",
      "\n",
      "\n",
      "\n",
      "There is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.\n",
      "\n",
      "2 ---\n",
      "\n",
      "title: Optimizer\n",
      "\n",
      "weight: 70\n",
      "\n",
      "aliases:\n",
      "\n",
      "  - ../optimizer\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "# Optimizer\n",
      "\n",
      "\n",
      "\n",
      "It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\n",
      "\n",
      "\n",
      "\n",
      "Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## let's try out a sample query using our dense retriever\n",
    "response = dense_retriever.retrieve(\"What is a Merge Optimizer?\")\n",
    "for i, node in enumerate(response):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f7LrVhxuL613",
    "outputId": "397cb8ec-9b3a-44b1-84db-68d75b3bd655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∫ Opening a view to the Phoenix app. The app is running at http://localhost:6006/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://localhost:6006/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2d3bed390>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can view the above data in the UI\n",
    "px.active_session().view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfOh7f9VlJMX"
   },
   "source": [
    "### **10. Run Your Query Engine and View Your Traces in Phoenix**\n",
    "\n",
    "We've compiled a list of the baseline questions about Qdrant. Let's download the sample queries and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "01d4ff90aac943ff83bfe49b70c783ba",
      "8d9844dfef3f414aa750e6200a62bfd3",
      "57d8cc8ff7f047c19301f0793367b2ae",
      "4b23a2eb73b6435384686ced47f9c7f6",
      "6fc5bf4070d94eccbdcb4fad4247c758",
      "218ea3901010405a95033a6cb632250a",
      "8ef997a7dbf54d3d9cfa8147e1611cbd",
      "72ce212a2baa494a8a9ae439c7c3e742",
      "f61b254bd9e24480b0176b43c1c7f47e",
      "d9fa5a2efb1142c9abd47fe46f05cee6",
      "56dc39917eec4eba8194bbe0a3e91de1",
      "c95d4cfffece443182faa3e2a3e48dda",
      "df997aad597e4c1ab23501d12383e39b",
      "a1c3867b1d404a948a0edd0e6bc9a1b8",
      "0c90d55a6f6d4d3f8c72bd0ba19687ba",
      "1e846da2f0c44e099d24c350e0b0eb1f",
      "924c8fe0a9cb4feab39d31ec358089b8",
      "cff10cfd18704a9cb7c9b4d4ba5ccaef",
      "ff9f4b2c32234207acb5221babe9c061",
      "d51b6ff6e830489f8e1e100bf1f1ae98",
      "be68eca0c7b640abada072a3cc3cd042",
      "6eadc3109208449eb9cb7ad5b6a5bb46",
      "1057e11184c44ea38154a2ceabd90198",
      "4722676ae36d4749918f5edaac48b182",
      "a2041da6fb5f499191f405fa891c0907",
      "5c26103d1d89466c9d22960bb347db39",
      "0685ef29e4c54e2e8d5c424508ebfe82",
      "ba3888c2a5954fb29293f41577a5bcae",
      "f7b5096a4b944d62a281ee927a1ce2e4",
      "54aa6aff06214fe4baa3e621ec306c04",
      "66588b1f23bb421db7dbc90b2298a845",
      "4ada07ba27b14c1a8c071343cc21c9c4",
      "98d7cd89900f44a79893e7d8d6b8b0c7"
     ]
    },
    "id": "uFLVc1gkivfp",
    "outputId": "f2e8533d-1524-4365-e150-03d4b8ae811f"
   },
   "outputs": [],
   "source": [
    "## Loading the Eval dataset\n",
    "from datasets import load_dataset\n",
    "qdrant_qa = load_dataset(\"atitaarora/qdrant_doc_qna\", split=\"train\")\n",
    "qdrant_qa_question = qdrant_qa.select_columns(['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "obJfFvhLjs86",
    "outputId": "e634866e-4ca5-491b-e457-feda9afa9d04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is vaccum optimizer ?',\n",
       " 'Tell me about ‚Äòalways_ram‚Äô parameter?',\n",
       " 'What is difference between scalar and product quantization?',\n",
       " 'What is ‚Äòbest_score‚Äô strategy?',\n",
       " 'How does oversampling helps?',\n",
       " 'What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?',\n",
       " 'What is the purpose of ef_construct in HNSW ?',\n",
       " 'How do you use ‚Äòordering‚Äô parameter?',\n",
       " 'What is significance of ‚Äòon_disk_payload‚Äô setting?',\n",
       " 'What is the impact of ‚Äòwrite_consistency_factor‚Äô ?']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_qa_question['question'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzQwf65z3yrz",
    "outputId": "225075d8-855e-47d6-cfc6-0f71d2feedd8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                        | 0/10 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:15<00:00,  7.59s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "#define response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "#assemble query engine for dense retriever\n",
    "dense_query_engine = RetrieverQueryEngine(\n",
    "                     retriever=dense_retriever,\n",
    "                     response_synthesizer=response_synthesizer,)\n",
    "#query_engine = index.as_query_engine()\n",
    "for query in tqdm(qdrant_qa_question['question'][:10]):\n",
    "    try:\n",
    "      dense_query_engine.query(query)\n",
    "    except Exception as e:\n",
    "      pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDox2bLJlYO1"
   },
   "source": [
    "Check the Phoenix UI as your queries run. Your traces should appear in real time.\n",
    "\n",
    "Open the Phoenix UI with the link below if you haven't already and click through the queries to better understand how the query engine is performing. For each trace you will see a break\n",
    "\n",
    "Phoenix can be used to understand and troubleshoot your by surfacing:\n",
    " - **Application latency** - highlighting slow invocations of LLMs, Retrievers, etc.\n",
    " - **Token Usage** - Displays the breakdown of token usage with LLMs to surface up your most expensive LLM calls\n",
    " - **Runtime Exceptions** - Critical runtime exceptions such as rate-limiting are captured as exception events.\n",
    " - **Retrieved Documents** - view all the documents retrieved during a retriever call and the score and order in which they were returned\n",
    " - **Embeddings** - view the embedding text used for retrieval and the underlying embedding model\n",
    "LLM Parameters - view the parameters used when calling out to an LLM to debug things like temperature and the system prompts\n",
    " - **Prompt Templates** - Figure out what prompt template is used during the prompting step and what variables were used.\n",
    " - **Tool Descriptions** - view the description and function signature of the tools your LLM has been given access to\n",
    " - **LLM Function Calls** - if using OpenAI or other a model with function calls, you can view the function selection and function messages in the input messages to the LLM.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/RAG_trace_details.png\" alt=\"Trace Details View on Phoenix\" style=\"width:100%; height:auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "F-fyd_qP31Xf",
    "outputId": "b33bc66c-f2f1-4363-b14a-e96b85e0f5d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Open the Phoenix UI if you haven't already: http://localhost:6006/\n"
     ]
    }
   ],
   "source": [
    "print(f\"üöÄ Open the Phoenix UI if you haven't already: {session.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_M7evWcldWL"
   },
   "source": [
    "### **11. Export and Evaluate Your Trace Data**\n",
    "You can export your trace data as a pandas dataframe for further analysis and evaluation.\n",
    "\n",
    "In this case, we will export our retriever spans into two separate dataframes:\n",
    "\n",
    "queries_df, in which the retrieved documents for each query are concatenated into a single column, retrieved_documents_df, in which each retrieved document is \"exploded\" into its own row to enable the evaluation of each query-document pair in isolation. This will enable us to compute multiple kinds of evaluations, including:\n",
    "\n",
    "relevance: Are the retrieved documents grounded in the response? Q&A correctness: Are your application's responses grounded in the retrieved context? hallucinations: Is your application making up false information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "gQzwROt6JAf7"
   },
   "outputs": [],
   "source": [
    "queries_df = get_qa_with_reference(px.Client())\n",
    "retrieved_documents_df = get_retrieved_documents(px.Client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14fcacd026e9f882</th>\n",
       "      <td>What is the impact of ‚Äòwrite_consistency_factor‚Äô ?</td>\n",
       "      <td>The `write_consistency_factor` impacts the reliability and consistency of write operations in a distributed system. When this factor is increased, it requires more replicas to acknowledge a write operation before it is considered successful and a response is sent to the client. This makes write operations more tolerant to network partitions, as it ensures that a larger number of replicas have the write data before proceeding. However, it also means that a higher number of active replicas are needed to perform write operations, which could potentially lead to higher latency or even failures in write operations if the required number of replicas is not available.</td>\n",
       "      <td>- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.\\n\\n### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1a847e1aa16e4e15</th>\n",
       "      <td>What is significance of ‚Äòon_disk_payload‚Äô setting?</td>\n",
       "      <td>The `on_disk_payload` setting is significant because it determines where payload data is stored. When set to `true`, it ensures that payload data is stored only on disk, rather than in RAM. This can be particularly useful for managing memory usage effectively when dealing with large payloads, as it helps to limit the amount of RAM required by not loading large payload data into memory.</td>\n",
       "      <td>* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\\n\\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a449f21559141637</th>\n",
       "      <td>How do you use ‚Äòordering‚Äô parameter?</td>\n",
       "      <td>The `ordering` parameter can be used with update and delete operations to ensure that these operations are executed in the same order on all replicas. To use this parameter, you would include it in your request to Qdrant. For example, when making an HTTP PUT request to update points in a collection, you would append `?ordering=strong` to the URL, like this:\\n\\n```\\nPUT /collections/{collection_name}/points?ordering=strong\\n```\\n\\nIn the body of the request, you would include the details of the points you want to update, such as their IDs, payloads, and vectors.\\n\\nWhen using the `ordering` parameter, Qdrant will route the operation to the leader replica of the shard and will not respond to the client until the operation has been completed. This ensures that all replicas process the update or delete operations in the same order, which is particularly useful for avoiding data inconsistencies when there are concurrent updates to the same documents.</td>\n",
       "      <td>- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents\\n\\n```http\\n\\nPUT /collections/{collection_name}/points?ordering=strong\\n\\n{\\n\\n    \"batch\": {\\n\\n        \"ids\": [1, 2, 3],\\n\\n        \"payloads\": [\\n\\n            {\"color\": \"red\"},\\n\\n            {\"color\": \"green\"},\\n\\n            {\"color\": \"blue\"}\\n\\n        ],\\n\\n        \"vectors\": [\\n\\n            [0.9, 0.1, 0.1],\\n\\n            [0.1, 0.9, 0.1],\\n\\n            [0.1, 0.1, 0.9]\\n\\n        ]\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nclient.upsert(\\n\\n    collection_name=\"{collection_name}\",\\n\\n    points=models.Batch(\\n\\n        ids=[1, 2, 3],</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313e98c658beac38</th>\n",
       "      <td>What is the purpose of ef_construct in HNSW ?</td>\n",
       "      <td>The `ef_construct` parameter in HNSW is the number of neighbours to consider during the index building. The larger the value of `ef_construct`, the higher the precision of the search, but this also results in longer indexing time.</td>\n",
       "      <td>(\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),\\n\\nThe larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \\n\\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\\n\\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5734d6ff741beaee</th>\n",
       "      <td>What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?</td>\n",
       "      <td>The purpose of the `createPayloadIndex` function (which could be referred to as `CreatePayloadIndexAsync` in an asynchronous context) is to create an index on a payload field within a collection in a database. This function specifies the field to be indexed and defines the schema for the index, including the type of data, tokenizer, minimum and maximum token length, and whether the text should be converted to lowercase. This indexing facilitates efficient querying and retrieval of data based on the specified field.</td>\n",
       "      <td>client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};\\n\\n},\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"https://your-application-name.fly.dev/.well-known/logo.png\",\\n\\n  \"contact_email\": \"email@domain.com\",\\n\\n  \"legal_info_url\": \"email@domain.com\"\\n\\n}\\n\\n```\\n\\n\\n\\nThat was the last step before running the final command. The command that will deploy \\n\\nthe application on the server:\\n\\n\\n\\n```bash\\n\\nflyctl deploy\\n\\n```</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10259e2e2f83a1b3</th>\n",
       "      <td>How does oversampling helps?</td>\n",
       "      <td>Oversampling helps in two distinct ways:\\n\\n1. It improves the accuracy and performance of similarity search algorithms by allowing for significant compression of high-dimensional vectors in memory, while compensating for accuracy loss by re-scoring additional points with the original vectors.\\n\\n2. It equalizes the representation of classes in the training dataset, which enables more fair and accurate modeling of real-world scenarios.</td>\n",
       "      <td>### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.\\n\\noversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7f854af390f89fcb</th>\n",
       "      <td>What is ‚Äòbest_score‚Äô strategy?</td>\n",
       "      <td>The 'best_score' strategy is a method used to find similar vectors by comparing each candidate against every example to identify the ones that are closer to a positive example while avoiding those closer to a negative one. This strategy involves selecting the best positive and best negative scores for each candidate, and then calculating the final score using a step formula. If the best positive score is greater than the best negative score, the final score is the best positive score. Otherwise, the final score is the negative of the best negative score squared. The performance of this strategy is affected linearly by the number of examples used.</td>\n",
       "      <td>This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.\\n\\nThe way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\\n\\n\\n\\n```rust\\n\\nlet score = if best_positive_score &gt; best_negative_score {\\n\\n    best_positive_score;\\n\\n} else {\\n\\n    -(best_negative_score * best_negative_score);\\n\\n};\\n\\n```\\n\\n\\n\\n&lt;aside role=\"alert\"&gt;\\n\\nThe performance of &lt;code&gt;best_score&lt;/code&gt; strategy will be linearly impacted by the amount of examples.\\n\\n&lt;/as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a9d2001e9ca7f6a2</th>\n",
       "      <td>What is difference between scalar and product quantization?</td>\n",
       "      <td>Scalar quantization is a compression technique that reduces the number of bits used to represent each component of a vector. It involves converting the original floating-point representations of vector components into a lower bit representation, such as converting 32-bit floating numbers into 8-bit unsigned integers.\\n\\nProduct quantization, on the other hand, is a different approach to compressing high-dimensional vectors. It involves dividing the vector into smaller sub-vectors and quantizing each sub-vector separately. This method is not as friendly to SIMD (Single Instruction, Multiple Data) operations, which can make distance calculations slower compared to scalar quantization. Additionally, product quantization typically results in a loss of accuracy, and therefore, it is recommended for use with high-dimensional vectors where this trade-off is acceptable.</td>\n",
       "      <td>But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method\\n\\n*Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -&gt; uint8` conversion for each vector component.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a70081ec7e95ccc3</th>\n",
       "      <td>Tell me about ‚Äòalways_ram‚Äô parameter?</td>\n",
       "      <td>The `always_ram` parameter is a configuration option that determines whether quantized vectors should be always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, in certain setups, keeping quantized vectors in RAM can speed up the search process. If you want to store quantized vectors in RAM, you can set `always_ram` to `true`. This setting can be particularly useful if you experience a significant decrease in search quality and need to ensure faster search performance.</td>\n",
       "      <td>\"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\\n\\nIt might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52a8df108660be63</th>\n",
       "      <td>What is vaccum optimizer ?</td>\n",
       "      <td>The term \"vaccum optimizer\" does not appear directly in the provided context. However, the context does mention an \"optimizer_config\" with various parameters that could be related to the optimization process of a system. One of the parameters within this configuration is \"vacuum_min_vector_number,\" which suggests a threshold for a certain operation that could be part of an optimization or maintenance routine, possibly related to data storage or indexing. Without additional context, it's not possible to provide a detailed explanation of a \"vaccum optimizer.\"</td>\n",
       "      <td>return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.\\n\\n},\\n\\n            \"optimizer_config\": {\\n\\n                \"deleted_threshold\": 0.2,\\n\\n                \"vacuum_min_vector_number\": 1000,\\n\\n                \"default_segment_number\": 0,\\n\\n                \"max_segment_size\": null,\\n\\n                \"memmap_threshold\": null,\\n\\n                \"indexing_threshold\": 20000,\\n\\n                \"flush_interval_sec\": 5,\\n\\n                \"max_optimization_threads\": 1\\n\\n            },\\n\\n            \"wal_config\": {\\n\\n                \"wal_capacity_mb\": 32,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a6aa9e13c32a83c7</th>\n",
       "      <td>What is a Merge Optimizer?</td>\n",
       "      <td>None</td>\n",
       "      <td>Such segments, for example, are created as copy-on-write segments during optimization itself.\\n\\n\\n\\nIt is also essential to have at least one small segment that Qdrant will use to store frequently updated data.\\n\\nOn the other hand, too many small segments lead to suboptimal search performance.\\n\\n\\n\\nThere is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.\\n\\n---\\n\\ntitle: Optimizer\\n\\nweight: 70\\n\\naliases:\\n\\n  - ../optimizer\\n\\n---\\n\\n\\n\\n# Optimizer\\n\\n\\n\\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\\n\\n\\n\\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        input  \\\n",
       "context.span_id                                                                 \n",
       "14fcacd026e9f882           What is the impact of ‚Äòwrite_consistency_factor‚Äô ?   \n",
       "1a847e1aa16e4e15           What is significance of ‚Äòon_disk_payload‚Äô setting?   \n",
       "a449f21559141637                         How do you use ‚Äòordering‚Äô parameter?   \n",
       "313e98c658beac38                What is the purpose of ef_construct in HNSW ?   \n",
       "5734d6ff741beaee            What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?   \n",
       "10259e2e2f83a1b3                                 How does oversampling helps?   \n",
       "7f854af390f89fcb                               What is ‚Äòbest_score‚Äô strategy?   \n",
       "a9d2001e9ca7f6a2  What is difference between scalar and product quantization?   \n",
       "a70081ec7e95ccc3                        Tell me about ‚Äòalways_ram‚Äô parameter?   \n",
       "52a8df108660be63                                   What is vaccum optimizer ?   \n",
       "a6aa9e13c32a83c7                                   What is a Merge Optimizer?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           output  \\\n",
       "context.span_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "14fcacd026e9f882                                                                                                                                                                                                                                                                                                    The `write_consistency_factor` impacts the reliability and consistency of write operations in a distributed system. When this factor is increased, it requires more replicas to acknowledge a write operation before it is considered successful and a response is sent to the client. This makes write operations more tolerant to network partitions, as it ensures that a larger number of replicas have the write data before proceeding. However, it also means that a higher number of active replicas are needed to perform write operations, which could potentially lead to higher latency or even failures in write operations if the required number of replicas is not available.   \n",
       "1a847e1aa16e4e15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The `on_disk_payload` setting is significant because it determines where payload data is stored. When set to `true`, it ensures that payload data is stored only on disk, rather than in RAM. This can be particularly useful for managing memory usage effectively when dealing with large payloads, as it helps to limit the amount of RAM required by not loading large payload data into memory.   \n",
       "a449f21559141637  The `ordering` parameter can be used with update and delete operations to ensure that these operations are executed in the same order on all replicas. To use this parameter, you would include it in your request to Qdrant. For example, when making an HTTP PUT request to update points in a collection, you would append `?ordering=strong` to the URL, like this:\\n\\n```\\nPUT /collections/{collection_name}/points?ordering=strong\\n```\\n\\nIn the body of the request, you would include the details of the points you want to update, such as their IDs, payloads, and vectors.\\n\\nWhen using the `ordering` parameter, Qdrant will route the operation to the leader replica of the shard and will not respond to the client until the operation has been completed. This ensures that all replicas process the update or delete operations in the same order, which is particularly useful for avoiding data inconsistencies when there are concurrent updates to the same documents.   \n",
       "313e98c658beac38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The `ef_construct` parameter in HNSW is the number of neighbours to consider during the index building. The larger the value of `ef_construct`, the higher the precision of the search, but this also results in longer indexing time.   \n",
       "5734d6ff741beaee                                                                                                                                                                                                                                                                                                                                                                                                                                                         The purpose of the `createPayloadIndex` function (which could be referred to as `CreatePayloadIndexAsync` in an asynchronous context) is to create an index on a payload field within a collection in a database. This function specifies the field to be indexed and defines the schema for the index, including the type of data, tokenizer, minimum and maximum token length, and whether the text should be converted to lowercase. This indexing facilitates efficient querying and retrieval of data based on the specified field.   \n",
       "10259e2e2f83a1b3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Oversampling helps in two distinct ways:\\n\\n1. It improves the accuracy and performance of similarity search algorithms by allowing for significant compression of high-dimensional vectors in memory, while compensating for accuracy loss by re-scoring additional points with the original vectors.\\n\\n2. It equalizes the representation of classes in the training dataset, which enables more fair and accurate modeling of real-world scenarios.   \n",
       "7f854af390f89fcb                                                                                                                                                                                                                                                                                                                   The 'best_score' strategy is a method used to find similar vectors by comparing each candidate against every example to identify the ones that are closer to a positive example while avoiding those closer to a negative one. This strategy involves selecting the best positive and best negative scores for each candidate, and then calculating the final score using a step formula. If the best positive score is greater than the best negative score, the final score is the best positive score. Otherwise, the final score is the negative of the best negative score squared. The performance of this strategy is affected linearly by the number of examples used.   \n",
       "a9d2001e9ca7f6a2                                                                                       Scalar quantization is a compression technique that reduces the number of bits used to represent each component of a vector. It involves converting the original floating-point representations of vector components into a lower bit representation, such as converting 32-bit floating numbers into 8-bit unsigned integers.\\n\\nProduct quantization, on the other hand, is a different approach to compressing high-dimensional vectors. It involves dividing the vector into smaller sub-vectors and quantizing each sub-vector separately. This method is not as friendly to SIMD (Single Instruction, Multiple Data) operations, which can make distance calculations slower compared to scalar quantization. Additionally, product quantization typically results in a loss of accuracy, and therefore, it is recommended for use with high-dimensional vectors where this trade-off is acceptable.   \n",
       "a70081ec7e95ccc3                                                                                                                                                                                                                                                                                                                                                                                                                                        The `always_ram` parameter is a configuration option that determines whether quantized vectors should be always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, in certain setups, keeping quantized vectors in RAM can speed up the search process. If you want to store quantized vectors in RAM, you can set `always_ram` to `true`. This setting can be particularly useful if you experience a significant decrease in search quality and need to ensure faster search performance.   \n",
       "52a8df108660be63                                                                                                                                                                                                                                                                                                                                                                                                              The term \"vaccum optimizer\" does not appear directly in the provided context. However, the context does mention an \"optimizer_config\" with various parameters that could be related to the optimization process of a system. One of the parameters within this configuration is \"vacuum_min_vector_number,\" which suggests a threshold for a certain operation that could be part of an optimization or maintenance routine, possibly related to data storage or indexing. Without additional context, it's not possible to provide a detailed explanation of a \"vaccum optimizer.\"   \n",
       "a6aa9e13c32a83c7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             None   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                reference  \n",
       "context.span_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "14fcacd026e9f882                                                                                                                                                      - `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.\\n\\n### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python  \n",
       "1a847e1aa16e4e15                                                                                                                * `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\\n\\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.  \n",
       "a449f21559141637                                                  - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents\\n\\n```http\\n\\nPUT /collections/{collection_name}/points?ordering=strong\\n\\n{\\n\\n    \"batch\": {\\n\\n        \"ids\": [1, 2, 3],\\n\\n        \"payloads\": [\\n\\n            {\"color\": \"red\"},\\n\\n            {\"color\": \"green\"},\\n\\n            {\"color\": \"blue\"}\\n\\n        ],\\n\\n        \"vectors\": [\\n\\n            [0.9, 0.1, 0.1],\\n\\n            [0.1, 0.9, 0.1],\\n\\n            [0.1, 0.1, 0.9]\\n\\n        ]\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nclient.upsert(\\n\\n    collection_name=\"{collection_name}\",\\n\\n    points=models.Batch(\\n\\n        ids=[1, 2, 3],  \n",
       "313e98c658beac38                                                                             (\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),\\n\\nThe larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \\n\\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\\n\\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and  \n",
       "5734d6ff741beaee    client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};\\n\\n},\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"https://your-application-name.fly.dev/.well-known/logo.png\",\\n\\n  \"contact_email\": \"email@domain.com\",\\n\\n  \"legal_info_url\": \"email@domain.com\"\\n\\n}\\n\\n```\\n\\n\\n\\nThat was the last step before running the final command. The command that will deploy \\n\\nthe application on the server:\\n\\n\\n\\n```bash\\n\\nflyctl deploy\\n\\n```  \n",
       "10259e2e2f83a1b3                                                                                                                                                                                                                                                                                                                                                                                                                                                    ### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.\\n\\noversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.  \n",
       "7f854af390f89fcb  This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.\\n\\nThe way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\\n\\n\\n\\n```rust\\n\\nlet score = if best_positive_score > best_negative_score {\\n\\n    best_positive_score;\\n\\n} else {\\n\\n    -(best_negative_score * best_negative_score);\\n\\n};\\n\\n```\\n\\n\\n\\n<aside role=\"alert\">\\n\\nThe performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.\\n\\n</as...  \n",
       "a9d2001e9ca7f6a2                                       But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method\\n\\n*Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -> uint8` conversion for each vector component.  \n",
       "a70081ec7e95ccc3  \"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\\n\\nIt might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quant...  \n",
       "52a8df108660be63                                                                                       return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.\\n\\n},\\n\\n            \"optimizer_config\": {\\n\\n                \"deleted_threshold\": 0.2,\\n\\n                \"vacuum_min_vector_number\": 1000,\\n\\n                \"default_segment_number\": 0,\\n\\n                \"max_segment_size\": null,\\n\\n                \"memmap_threshold\": null,\\n\\n                \"indexing_threshold\": 20000,\\n\\n                \"flush_interval_sec\": 5,\\n\\n                \"max_optimization_threads\": 1\\n\\n            },\\n\\n            \"wal_config\": {\\n\\n                \"wal_capacity_mb\": 32,  \n",
       "a6aa9e13c32a83c7                                                                  Such segments, for example, are created as copy-on-write segments during optimization itself.\\n\\n\\n\\nIt is also essential to have at least one small segment that Qdrant will use to store frequently updated data.\\n\\nOn the other hand, too many small segments lead to suboptimal search performance.\\n\\n\\n\\nThere is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.\\n\\n---\\n\\ntitle: Optimizer\\n\\nweight: 70\\n\\naliases:\\n\\n  - ../optimizer\\n\\n---\\n\\n\\n\\n# Optimizer\\n\\n\\n\\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\\n\\n\\n\\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>input</th>\n",
       "      <th>reference</th>\n",
       "      <th>document_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th>document_position</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">7cab8e400cabe8e7</th>\n",
       "      <th>0</th>\n",
       "      <td>2920c32bcf94359d7b2906641d8f2a10</td>\n",
       "      <td>What is the impact of ‚Äòwrite_consistency_factor‚Äô ?</td>\n",
       "      <td>- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.</td>\n",
       "      <td>0.832443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2920c32bcf94359d7b2906641d8f2a10</td>\n",
       "      <td>What is the impact of ‚Äòwrite_consistency_factor‚Äô ?</td>\n",
       "      <td>### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python</td>\n",
       "      <td>0.825855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">36d7d3481d63eb2f</th>\n",
       "      <th>0</th>\n",
       "      <td>7817c0d936f861fa95d57d5965f6f831</td>\n",
       "      <td>What is significance of ‚Äòon_disk_payload‚Äô setting?</td>\n",
       "      <td>* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).</td>\n",
       "      <td>0.821666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7817c0d936f861fa95d57d5965f6f831</td>\n",
       "      <td>What is significance of ‚Äòon_disk_payload‚Äô setting?</td>\n",
       "      <td>The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.</td>\n",
       "      <td>0.787703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4f34361dbde57463</th>\n",
       "      <th>0</th>\n",
       "      <td>daa61b1e45237715bbd8eafcd09d6c2c</td>\n",
       "      <td>How do you use ‚Äòordering‚Äô parameter?</td>\n",
       "      <td>- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents</td>\n",
       "      <td>0.770652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daa61b1e45237715bbd8eafcd09d6c2c</td>\n",
       "      <td>How do you use ‚Äòordering‚Äô parameter?</td>\n",
       "      <td>```http\\n\\nPUT /collections/{collection_name}/points?ordering=strong\\n\\n{\\n\\n    \"batch\": {\\n\\n        \"ids\": [1, 2, 3],\\n\\n        \"payloads\": [\\n\\n            {\"color\": \"red\"},\\n\\n            {\"color\": \"green\"},\\n\\n            {\"color\": \"blue\"}\\n\\n        ],\\n\\n        \"vectors\": [\\n\\n            [0.9, 0.1, 0.1],\\n\\n            [0.1, 0.9, 0.1],\\n\\n            [0.1, 0.1, 0.9]\\n\\n        ]\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nclient.upsert(\\n\\n    collection_name=\"{collection_name}\",\\n\\n    points=models.Batch(\\n\\n        ids=[1, 2, 3],</td>\n",
       "      <td>0.740061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bfd965ae080f81ce</th>\n",
       "      <th>0</th>\n",
       "      <td>2e0440e73984c19e901106346153c857</td>\n",
       "      <td>What is the purpose of ef_construct in HNSW ?</td>\n",
       "      <td>(\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),</td>\n",
       "      <td>0.787150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2e0440e73984c19e901106346153c857</td>\n",
       "      <td>What is the purpose of ef_construct in HNSW ?</td>\n",
       "      <td>The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \\n\\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\\n\\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and</td>\n",
       "      <td>0.767757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">e67bf6d53c9b6a51</th>\n",
       "      <th>0</th>\n",
       "      <td>c96de30824656d78982669c4dcfc97ec</td>\n",
       "      <td>What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?</td>\n",
       "      <td>client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};</td>\n",
       "      <td>0.717572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c96de30824656d78982669c4dcfc97ec</td>\n",
       "      <td>What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?</td>\n",
       "      <td>},\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"https://your-application-name.fly.dev/.well-known/logo.png\",\\n\\n  \"contact_email\": \"email@domain.com\",\\n\\n  \"legal_info_url\": \"email@domain.com\"\\n\\n}\\n\\n```\\n\\n\\n\\nThat was the last step before running the final command. The command that will deploy \\n\\nthe application on the server:\\n\\n\\n\\n```bash\\n\\nflyctl deploy\\n\\n```</td>\n",
       "      <td>0.698218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">8f7a4e52a6e6d701</th>\n",
       "      <th>0</th>\n",
       "      <td>8ef88b9b258b5b5f0701af4d013d9148</td>\n",
       "      <td>How does oversampling helps?</td>\n",
       "      <td>### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.</td>\n",
       "      <td>0.819349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8ef88b9b258b5b5f0701af4d013d9148</td>\n",
       "      <td>How does oversampling helps?</td>\n",
       "      <td>oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.</td>\n",
       "      <td>0.815198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0991009617cb49c3</th>\n",
       "      <th>0</th>\n",
       "      <td>8432e0f2be7a3362c793486df97e83a8</td>\n",
       "      <td>What is ‚Äòbest_score‚Äô strategy?</td>\n",
       "      <td>This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.</td>\n",
       "      <td>0.825645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8432e0f2be7a3362c793486df97e83a8</td>\n",
       "      <td>What is ‚Äòbest_score‚Äô strategy?</td>\n",
       "      <td>The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\\n\\n\\n\\n```rust\\n\\nlet score = if best_positive_score &gt; best_negative_score {\\n\\n    best_positive_score;\\n\\n} else {\\n\\n    -(best_negative_score * best_negative_score);\\n\\n};\\n\\n```\\n\\n\\n\\n&lt;aside role=\"alert\"&gt;\\n\\nThe performance of &lt;code&gt;best_score&lt;/code&gt; strategy will be linearly impacted by the amount of examples.\\n\\n&lt;/aside&gt;</td>\n",
       "      <td>0.808645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5fd88bb374ffe79d</th>\n",
       "      <th>0</th>\n",
       "      <td>b925efbd5bba561c6311573daca658cb</td>\n",
       "      <td>What is difference between scalar and product quantization?</td>\n",
       "      <td>But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method</td>\n",
       "      <td>0.857293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b925efbd5bba561c6311573daca658cb</td>\n",
       "      <td>What is difference between scalar and product quantization?</td>\n",
       "      <td>*Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -&gt; uint8` conversion for each vector component.</td>\n",
       "      <td>0.848822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">628b03885d84d954</th>\n",
       "      <th>0</th>\n",
       "      <td>c88fa99870933175ce0729f6de90ae87</td>\n",
       "      <td>Tell me about ‚Äòalways_ram‚Äô parameter?</td>\n",
       "      <td>\"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\</td>\n",
       "      <td>0.777294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c88fa99870933175ce0729f6de90ae87</td>\n",
       "      <td>Tell me about ‚Äòalways_ram‚Äô parameter?</td>\n",
       "      <td>It might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quantization</td>\n",
       "      <td>0.765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">ae05246a212b081b</th>\n",
       "      <th>0</th>\n",
       "      <td>dd3dd56b00b59dd886c77503ac9e866c</td>\n",
       "      <td>What is vaccum optimizer ?</td>\n",
       "      <td>return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.</td>\n",
       "      <td>0.718610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dd3dd56b00b59dd886c77503ac9e866c</td>\n",
       "      <td>What is vaccum optimizer ?</td>\n",
       "      <td>},\\n\\n            \"optimizer_config\": {\\n\\n                \"deleted_threshold\": 0.2,\\n\\n                \"vacuum_min_vector_number\": 1000,\\n\\n                \"default_segment_number\": 0,\\n\\n                \"max_segment_size\": null,\\n\\n                \"memmap_threshold\": null,\\n\\n                \"indexing_threshold\": 20000,\\n\\n                \"flush_interval_sec\": 5,\\n\\n                \"max_optimization_threads\": 1\\n\\n            },\\n\\n            \"wal_config\": {\\n\\n                \"wal_capacity_mb\": 32,</td>\n",
       "      <td>0.707060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">a6aa9e13c32a83c7</th>\n",
       "      <th>0</th>\n",
       "      <td>1c0ed28883f197e130197e0d9dfce1c0</td>\n",
       "      <td>What is a Merge Optimizer?</td>\n",
       "      <td>Such segments, for example, are created as copy-on-write segments during optimization itself.\\n\\n\\n\\nIt is also essential to have at least one small segment that Qdrant will use to store frequently updated data.\\n\\nOn the other hand, too many small segments lead to suboptimal search performance.\\n\\n\\n\\nThere is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.</td>\n",
       "      <td>0.768700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1c0ed28883f197e130197e0d9dfce1c0</td>\n",
       "      <td>What is a Merge Optimizer?</td>\n",
       "      <td>---\\n\\ntitle: Optimizer\\n\\nweight: 70\\n\\naliases:\\n\\n  - ../optimizer\\n\\n---\\n\\n\\n\\n# Optimizer\\n\\n\\n\\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\\n\\n\\n\\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).</td>\n",
       "      <td>0.725430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    context.trace_id  \\\n",
       "context.span_id  document_position                                     \n",
       "7cab8e400cabe8e7 0                  2920c32bcf94359d7b2906641d8f2a10   \n",
       "                 1                  2920c32bcf94359d7b2906641d8f2a10   \n",
       "36d7d3481d63eb2f 0                  7817c0d936f861fa95d57d5965f6f831   \n",
       "                 1                  7817c0d936f861fa95d57d5965f6f831   \n",
       "4f34361dbde57463 0                  daa61b1e45237715bbd8eafcd09d6c2c   \n",
       "                 1                  daa61b1e45237715bbd8eafcd09d6c2c   \n",
       "bfd965ae080f81ce 0                  2e0440e73984c19e901106346153c857   \n",
       "                 1                  2e0440e73984c19e901106346153c857   \n",
       "e67bf6d53c9b6a51 0                  c96de30824656d78982669c4dcfc97ec   \n",
       "                 1                  c96de30824656d78982669c4dcfc97ec   \n",
       "8f7a4e52a6e6d701 0                  8ef88b9b258b5b5f0701af4d013d9148   \n",
       "                 1                  8ef88b9b258b5b5f0701af4d013d9148   \n",
       "0991009617cb49c3 0                  8432e0f2be7a3362c793486df97e83a8   \n",
       "                 1                  8432e0f2be7a3362c793486df97e83a8   \n",
       "5fd88bb374ffe79d 0                  b925efbd5bba561c6311573daca658cb   \n",
       "                 1                  b925efbd5bba561c6311573daca658cb   \n",
       "628b03885d84d954 0                  c88fa99870933175ce0729f6de90ae87   \n",
       "                 1                  c88fa99870933175ce0729f6de90ae87   \n",
       "ae05246a212b081b 0                  dd3dd56b00b59dd886c77503ac9e866c   \n",
       "                 1                  dd3dd56b00b59dd886c77503ac9e866c   \n",
       "a6aa9e13c32a83c7 0                  1c0ed28883f197e130197e0d9dfce1c0   \n",
       "                 1                  1c0ed28883f197e130197e0d9dfce1c0   \n",
       "\n",
       "                                                                                          input  \\\n",
       "context.span_id  document_position                                                                \n",
       "7cab8e400cabe8e7 0                           What is the impact of ‚Äòwrite_consistency_factor‚Äô ?   \n",
       "                 1                           What is the impact of ‚Äòwrite_consistency_factor‚Äô ?   \n",
       "36d7d3481d63eb2f 0                           What is significance of ‚Äòon_disk_payload‚Äô setting?   \n",
       "                 1                           What is significance of ‚Äòon_disk_payload‚Äô setting?   \n",
       "4f34361dbde57463 0                                         How do you use ‚Äòordering‚Äô parameter?   \n",
       "                 1                                         How do you use ‚Äòordering‚Äô parameter?   \n",
       "bfd965ae080f81ce 0                                What is the purpose of ef_construct in HNSW ?   \n",
       "                 1                                What is the purpose of ef_construct in HNSW ?   \n",
       "e67bf6d53c9b6a51 0                            What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?   \n",
       "                 1                            What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?   \n",
       "8f7a4e52a6e6d701 0                                                 How does oversampling helps?   \n",
       "                 1                                                 How does oversampling helps?   \n",
       "0991009617cb49c3 0                                               What is ‚Äòbest_score‚Äô strategy?   \n",
       "                 1                                               What is ‚Äòbest_score‚Äô strategy?   \n",
       "5fd88bb374ffe79d 0                  What is difference between scalar and product quantization?   \n",
       "                 1                  What is difference between scalar and product quantization?   \n",
       "628b03885d84d954 0                                        Tell me about ‚Äòalways_ram‚Äô parameter?   \n",
       "                 1                                        Tell me about ‚Äòalways_ram‚Äô parameter?   \n",
       "ae05246a212b081b 0                                                   What is vaccum optimizer ?   \n",
       "                 1                                                   What is vaccum optimizer ?   \n",
       "a6aa9e13c32a83c7 0                                                   What is a Merge Optimizer?   \n",
       "                 1                                                   What is a Merge Optimizer?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             reference  \\\n",
       "context.span_id  document_position                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "7cab8e400cabe8e7 0                                                                                                                                                                                                                                                       - `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.   \n",
       "                 1                                  ### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python   \n",
       "36d7d3481d63eb2f 0                                                                                                  * `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).   \n",
       "                 1                                                                                                                                                 The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.   \n",
       "4f34361dbde57463 0                                                                                                                                                                   - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents   \n",
       "                 1                  ```http\\n\\nPUT /collections/{collection_name}/points?ordering=strong\\n\\n{\\n\\n    \"batch\": {\\n\\n        \"ids\": [1, 2, 3],\\n\\n        \"payloads\": [\\n\\n            {\"color\": \"red\"},\\n\\n            {\"color\": \"green\"},\\n\\n            {\"color\": \"blue\"}\\n\\n        ],\\n\\n        \"vectors\": [\\n\\n            [0.9, 0.1, 0.1],\\n\\n            [0.1, 0.9, 0.1],\\n\\n            [0.1, 0.1, 0.9]\\n\\n        ]\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nclient.upsert(\\n\\n    collection_name=\"{collection_name}\",\\n\\n    points=models.Batch(\\n\\n        ids=[1, 2, 3],   \n",
       "bfd965ae080f81ce 0                                                                  (\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),   \n",
       "                 1                                                                                                                                              The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \\n\\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\\n\\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and   \n",
       "e67bf6d53c9b6a51 0                                                                                   client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};   \n",
       "                 1                                                    },\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"https://your-application-name.fly.dev/.well-known/logo.png\",\\n\\n  \"contact_email\": \"email@domain.com\",\\n\\n  \"legal_info_url\": \"email@domain.com\"\\n\\n}\\n\\n```\\n\\n\\n\\nThat was the last step before running the final command. The command that will deploy \\n\\nthe application on the server:\\n\\n\\n\\n```bash\\n\\nflyctl deploy\\n\\n```   \n",
       "8f7a4e52a6e6d701 0                                                                                                                                                            ### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.   \n",
       "                 1                                                                                                                                                                                                                                                                                                                                                                                                                           oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.   \n",
       "0991009617cb49c3 0                                                                                  This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.   \n",
       "                 1                                                  The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\\n\\n\\n\\n```rust\\n\\nlet score = if best_positive_score > best_negative_score {\\n\\n    best_positive_score;\\n\\n} else {\\n\\n    -(best_negative_score * best_negative_score);\\n\\n};\\n\\n```\\n\\n\\n\\n<aside role=\"alert\">\\n\\nThe performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.\\n\\n</aside>   \n",
       "5fd88bb374ffe79d 0                                                                                           But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method   \n",
       "                 1                                                                               *Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -> uint8` conversion for each vector component.   \n",
       "628b03885d84d954 0                                                                       \"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\   \n",
       "                 1                                                          It might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quantization   \n",
       "ae05246a212b081b 0                                                                                                                                                                 return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.   \n",
       "                 1                                                         },\\n\\n            \"optimizer_config\": {\\n\\n                \"deleted_threshold\": 0.2,\\n\\n                \"vacuum_min_vector_number\": 1000,\\n\\n                \"default_segment_number\": 0,\\n\\n                \"max_segment_size\": null,\\n\\n                \"memmap_threshold\": null,\\n\\n                \"indexing_threshold\": 20000,\\n\\n                \"flush_interval_sec\": 5,\\n\\n                \"max_optimization_threads\": 1\\n\\n            },\\n\\n            \"wal_config\": {\\n\\n                \"wal_capacity_mb\": 32,   \n",
       "a6aa9e13c32a83c7 0                                                                                                                             Such segments, for example, are created as copy-on-write segments during optimization itself.\\n\\n\\n\\nIt is also essential to have at least one small segment that Qdrant will use to store frequently updated data.\\n\\nOn the other hand, too many small segments lead to suboptimal search performance.\\n\\n\\n\\nThere is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.   \n",
       "                 1                                                                        ---\\n\\ntitle: Optimizer\\n\\nweight: 70\\n\\naliases:\\n\\n  - ../optimizer\\n\\n---\\n\\n\\n\\n# Optimizer\\n\\n\\n\\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\\n\\n\\n\\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).   \n",
       "\n",
       "                                    document_score  \n",
       "context.span_id  document_position                  \n",
       "7cab8e400cabe8e7 0                        0.832443  \n",
       "                 1                        0.825855  \n",
       "36d7d3481d63eb2f 0                        0.821666  \n",
       "                 1                        0.787703  \n",
       "4f34361dbde57463 0                        0.770652  \n",
       "                 1                        0.740061  \n",
       "bfd965ae080f81ce 0                        0.787150  \n",
       "                 1                        0.767757  \n",
       "e67bf6d53c9b6a51 0                        0.717572  \n",
       "                 1                        0.698218  \n",
       "8f7a4e52a6e6d701 0                        0.819349  \n",
       "                 1                        0.815198  \n",
       "0991009617cb49c3 0                        0.825645  \n",
       "                 1                        0.808645  \n",
       "5fd88bb374ffe79d 0                        0.857293  \n",
       "                 1                        0.848822  \n",
       "628b03885d84d954 0                        0.777294  \n",
       "                 1                        0.765625  \n",
       "ae05246a212b081b 0                        0.718610  \n",
       "                 1                        0.707060  \n",
       "a6aa9e13c32a83c7 0                        0.768700  \n",
       "                 1                        0.725430  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_documents_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **12. Define your evaluation model and your evaluators**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4XwhUYRlkdP"
   },
   "source": [
    "Next, define your evaluation model and your evaluators.\n",
    "\n",
    "Evaluators are built on top of language models and prompt the LLM to assess the quality of responses, the relevance of retrieved documents, etc., and provide a quality signal even in the absence of human-labeled data. Pick an evaluator type and instantiate it with the language model you want to use to perform evaluations using our battle-tested evaluation templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "7b8b3e1c571a4345bc2ec4eb00f0967e",
      "a74d6dce715f4494874de92852b56c09",
      "a5d80d3579ca481a98310bd08b8c6918",
      "f07a59b5bc7a4f56baa892ca864d488c",
      "84fd76a737334831a886755affb50886",
      "5c65981c04f34589a36c5c8bb4195f2b",
      "10d5d30b4fd14f3ba45da1794607c5e0",
      "ce0efd7352cd4539bda8706ebbc146fd",
      "128b89e88c684401818304e8410b6c33",
      "e7d4e009f37a4d798dbc23b39ed7c0b1",
      "d772ecb46ca54adbbb84749fe07ab735",
      "68ac22f645934827a9627e3559067ba1",
      "f3c5e97c2ac94cafa20b130f1f45ce67",
      "b505d5e7978d4b19b59dc8f3fe71e795",
      "080a6d0c05144698a9c7366e54bb39b0",
      "e69da775e4df4ed8a5f5ed1348278ab8",
      "77d79ac84c8c4d8393fcdb7f304b4bf3",
      "43dfd4070e324858974c08fbcb8055fd",
      "38d701e482684695b97bd73bac78ccfc",
      "e7d21aa7bce145b78e834897b158f9b0",
      "65f86e93b0fe48ddbaf2cabc9975c68b",
      "8e261a6071cd41a6afd4377c9ead98ca"
     ]
    },
    "id": "SnJSWjOkJGpE",
    "outputId": "b7119003-dd01-4a37-da4f-ed2bf3f130d9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88744727f145426a84e2ff73face1d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/22 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0419cd6fe3234bfe88ba2a5e4f361259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/22 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model = OpenAIModel(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    ")\n",
    "hallucination_evaluator = HallucinationEvaluator(eval_model)\n",
    "qa_correctness_evaluator = QAEvaluator(eval_model)\n",
    "relevance_evaluator = RelevanceEvaluator(eval_model)\n",
    "\n",
    "hallucination_eval_df, qa_correctness_eval_df = run_evals(\n",
    "    dataframe=queries_df,\n",
    "    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n",
    "    provide_explanation=True,\n",
    ")\n",
    "relevance_eval_df = run_evals(\n",
    "    dataframe=retrieved_documents_df,\n",
    "    evaluators=[relevance_evaluator],\n",
    "    provide_explanation=True,\n",
    ")[0]\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df),\n",
    "    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval_df),\n",
    ")\n",
    "px.Client().log_evaluations(DocumentEvaluations(eval_name=\"Relevance\", dataframe=relevance_eval_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOWquDbAlm4P"
   },
   "source": [
    "Your evaluations should now appear as annotations on the appropriate spans in Phoenix.\n",
    "\n",
    "![A view of the Phoenix UI with evaluation annotations](https://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/evals/traces_with_evaluation_annotations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5yek2mtlrRA"
   },
   "source": [
    "### **13. Let's try Hybrid search now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a new collection to store your hybrid emebeddings\n",
    "COLLECTION_NAME_HYBRID = \"qdrant_docs_arize_hybrid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reprocess documents with different settings if needed \n",
    "#documents = process_document_chunks(dataset , CHUNK_SIZE , CHUNK_OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'prithvida/Splade_PP_en_v1',\n",
       "  'vocab_size': 30522,\n",
       "  'description': 'Misspelled version of the model. Retained for backward compatibility. Independent Implementation of SPLADE++ Model for English',\n",
       "  'size_in_GB': 0.532,\n",
       "  'sources': {'hf': 'Qdrant/SPLADE_PP_en_v1'}},\n",
       " {'model': 'prithivida/Splade_PP_en_v1',\n",
       "  'vocab_size': 30522,\n",
       "  'description': 'Independent Implementation of SPLADE++ Model for English',\n",
       "  'size_in_GB': 0.532,\n",
       "  'sources': {'hf': 'Qdrant/SPLADE_PP_en_v1'}}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##List of supported sparse vector models\n",
    "from fastembed.sparse.sparse_text_embedding import SparseTextEmbedding\n",
    "SparseTextEmbedding.list_supported_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **14. Ingest Sparse and Dense vectors into Qdrant**\n",
    "\n",
    "Ingest sparse and dense vectors into Qdrant Collection.\n",
    "We are using Splade++ model for Sparse Vector Model and default Fastembed model - bge-small-en-1.5 for dense embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9711ebdf874902bb008c00c35d951d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59dddf9765e49c0839ac7345d1fd358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Initializing the space to work with llama-index and related settings\n",
    "import llama_index\n",
    "from llama_index.core import Settings\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from fastembed.sparse.sparse_text_embedding import SparseTextEmbedding, SparseEmbedding\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from typing import List, Tuple\n",
    "\n",
    "sparse_model_name = \"prithivida/Splade_PP_en_v1\"\n",
    "\n",
    "# This triggers the model download\n",
    "sparse_model = SparseTextEmbedding(model_name=sparse_model_name, batch_size=32)\n",
    "\n",
    "## Computing sparse vectors\n",
    "def compute_sparse_vectors(\n",
    "    texts: List[str],\n",
    "    ) -> Tuple[List[List[int]], List[List[float]]]:\n",
    "    indices, values = [], []\n",
    "    for embedding in sparse_model.embed(texts):\n",
    "        indices.append(embedding.indices.tolist())\n",
    "        values.append(embedding.values.tolist())\n",
    "    return indices, values\n",
    "\n",
    "## Creating a vector store with Hybrid search enabled\n",
    "hybrid_vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME_HYBRID,\n",
    "    enable_hybrid=True,\n",
    "    sparse_doc_fn=compute_sparse_vectors,\n",
    "    sparse_query_fn=compute_sparse_vectors)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=hybrid_vector_store)\n",
    "\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note : Ingesting sparse and dense vectors into Qdrant collection\n",
    "## This block is to be used when you're creating a new collection if using an existing collection,\n",
    "## Skip this block and execute the next one instead.\n",
    "from phoenix.trace import suppress_tracing\n",
    "with suppress_tracing():\n",
    "    hybrid_vector_index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        storage_context=storage_context,\n",
    "        show_progress=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: Execute this block when using an existing collection\n",
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "from llama_index.core.indices.vector_store import VectorIndexRetriever\n",
    "\n",
    "hybrid_vector_index = VectorStoreIndex.from_vector_store(vector_store=hybrid_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionInfo(status=<CollectionStatus.GREEN: 'green'>, optimizer_status=<OptimizersStatusOneOf.OK: 'ok'>, vectors_count=8862, indexed_vectors_count=4429, points_count=4431, segments_count=2, config=CollectionConfig(params=CollectionParams(vectors={'text-dense': VectorParams(size=384, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None)}, shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors={'text-sparse': SparseVectorParams(index=SparseIndexParams(full_scan_threshold=None, on_disk=None))}), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None), payload_schema={})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## collection level operations\n",
    "client.get_collection(COLLECTION_NAME_HYBRID)\n",
    "#client.delete_collection(COLLECTION_NAME_HYBRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountResult(count=4431)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check the number of documents matches the expected number of document chunks \n",
    "client.count(collection_name=COLLECTION_NAME_HYBRID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **15. Hybrid Search with Qdrant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Such segments, for example, are created as copy-on-write segments during optimization itself.\n",
      "\n",
      "\n",
      "\n",
      "It is also essential to have at least one small segment that Qdrant will use to store frequently updated data.\n",
      "\n",
      "On the other hand, too many small segments lead to suboptimal search performance.\n",
      "\n",
      "\n",
      "\n",
      "There is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.\n",
      "2 The criteria for starting the optimizer are defined in the configuration file.\n",
      "\n",
      "\n",
      "\n",
      "Here is an example of parameter values:\n",
      "\n",
      "\n",
      "\n",
      "```yaml\n",
      "\n",
      "storage:\n",
      "\n",
      "  optimizers:\n",
      "\n",
      "    # If the number of segments exceeds this value, the optimizer will merge the smallest segments.\n",
      "\n",
      "    max_segment_number: 5\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "## Indexing Optimizer\n",
      "\n",
      "\n",
      "\n",
      "Qdrant allows you to choose the type of indexes and data storage methods used depending on the number of records.\n"
     ]
    }
   ],
   "source": [
    "## Before trying Hybrid search , lets try Sparse Vector Search Retriever \n",
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "from llama_index.core.indices.vector_store import VectorIndexRetriever\n",
    "\n",
    "sparse_retriever = VectorIndexRetriever(\n",
    "    index=hybrid_vector_index,\n",
    "    vector_store_query_mode=VectorStoreQueryMode.SPARSE,\n",
    "    sparse_top_k=2,\n",
    ")\n",
    "\n",
    "## Pure sparse vector search\n",
    "nodes = sparse_retriever.retrieve(\"What is a Merge Optimizer?\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's try Hybrid Search Retriever now using the 'alpha' parameter that controls the weightage between\n",
    "## the dense and sparse vector search scores.\n",
    "# NOTE: For hybrid search (0 for sparse search, 1 for dense search)\n",
    "\n",
    "hybrid_retriever = VectorIndexRetriever(\n",
    "    index=hybrid_vector_index,\n",
    "    vector_store_query_mode=VectorStoreQueryMode.HYBRID,\n",
    "    sparse_top_k=1,\n",
    "    similarity_top_k=2,\n",
    "    alpha=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Such segments, for example, are created as copy-on-write segments during optimization itself.\n",
      "\n",
      "\n",
      "\n",
      "It is also essential to have at least one small segment that Qdrant will use to store frequently updated data.\n",
      "\n",
      "On the other hand, too many small segments lead to suboptimal search performance.\n",
      "\n",
      "\n",
      "\n",
      "There is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.\n",
      "2 ---\n",
      "\n",
      "title: Optimizer\n",
      "\n",
      "weight: 70\n",
      "\n",
      "aliases:\n",
      "\n",
      "  - ../optimizer\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "# Optimizer\n",
      "\n",
      "\n",
      "\n",
      "It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\n",
      "\n",
      "\n",
      "\n",
      "Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).\n"
     ]
    }
   ],
   "source": [
    "## Let's try hybrid retriever \n",
    "nodes = hybrid_retriever.retrieve(\"What is a Merge Optimizer?\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We shouldn't be modifying the alpha parameter after the retriever has been created\n",
    "# but that's the easiest way to show the effect of the parameter\n",
    "#hybrid_retriever._alpha = 0.1\n",
    "#hybrid_retriever._alpha = 0.9\n",
    "\n",
    "#nodes = hybrid_retriever.retrieve(\"What is merge optimizer?\")\n",
    "#for i, node in enumerate(nodes):\n",
    "#    print(i + 1, node.text, end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "#define response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "#assemble query engine for hybrid retriever\n",
    "hybrid_query_engine = RetrieverQueryEngine(\n",
    "                        retriever=hybrid_retriever,\n",
    "                        response_synthesizer=response_synthesizer,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **16. Re-Run Your Query Engine and View Your Traces in Phoenix**\n",
    "\n",
    "Let's rerun the list of the baseline questions about Qdrant on the Hybrid Retriever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                                                        | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                                                   | 1/10 [00:05<00:47,  5.26s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                      | 2/10 [00:12<00:52,  6.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                         | 3/10 [00:21<00:51,  7.36s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                            | 4/10 [00:29<00:47,  7.95s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                | 5/10 [00:35<00:35,  7.14s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                   | 6/10 [00:42<00:27,  6.90s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                      | 7/10 [00:45<00:17,  5.70s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                         | 8/10 [00:53<00:12,  6.48s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè            | 9/10 [00:58<00:06,  6.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:05<00:00,  6.55s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "## Switching phoenix project space\n",
    "from phoenix.trace import using_project\n",
    "\n",
    "# Switch project to run evals\n",
    "with using_project(HYBRID_RAG_PROJECT):\n",
    "# All spans created within this context will be associated with the `HYBRID_RAG_PROJECT` project.\n",
    "\n",
    "    ##Reuse the previously loaded dataset `qdrant_qa_question`\n",
    "    \n",
    "    for query in tqdm(qdrant_qa_question['question'][:10]):\n",
    "        try:\n",
    "          hybrid_query_engine.query(query)\n",
    "        except Exception as e:\n",
    "          pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Open the Phoenix UI if you haven't already: http://localhost:6006/\n"
     ]
    }
   ],
   "source": [
    "print(f\"üöÄ Open the Phoenix UI if you haven't already: {session.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Switching phoenix project space\n",
    "from phoenix.trace import using_project\n",
    "\n",
    "\n",
    "queries_df_hybrid = get_qa_with_reference(px.Client(), project_name=HYBRID_RAG_PROJECT)\n",
    "retrieved_documents_df_hybrid = get_retrieved_documents(px.Client(), project_name=HYBRID_RAG_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5c23a162c061df49</th>\n",
       "      <td>What is the impact of ‚Äòwrite_consistency_factor‚Äô ?</td>\n",
       "      <td>The impact of the `write_consistency_factor` is that it defines the number of replicas that must acknowledge a write operation before the system responds to the client. If this value is increased, write operations will be more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active for write operations to be performed successfully.</td>\n",
       "      <td>### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\n- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37c98aebc845e8be</th>\n",
       "      <td>What is significance of ‚Äòon_disk_payload‚Äô setting?</td>\n",
       "      <td>The `on_disk_payload` setting is significant because it determines where payload data is stored. When set to `true`, it ensures that payload data is stored only on disk, rather than in RAM. This can be particularly useful for managing memory usage effectively when dealing with large payloads, as it helps to limit the amount of RAM required by not loading large payload values into memory.</td>\n",
       "      <td>* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\\n\\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f805dcdab7f8a853</th>\n",
       "      <td>How do you use ‚Äòordering‚Äô parameter?</td>\n",
       "      <td>The `ordering` parameter can be used with update and delete operations to ensure that these operations are executed in the same order on all replicas. When this option is utilized, the operation is routed to the leader replica of the shard, and the system waits for a response from the leader before responding to the client. This is particularly useful for avoiding data inconsistencies that might arise from concurrent updates or deletions of the same documents.</td>\n",
       "      <td>Since the `filter` parameter is specified, the search is performed only among those points that satisfy the filter condition.\\n\\nSee details of possible filters and their work in the [filtering](../filtering) section.\\n\\n\\n\\nExample result of this API would be\\n\\n\\n\\n```json\\n\\n{\\n\\n  \"result\": [\\n\\n    { \"id\": 10, \"score\": 0.81 },\\n\\n    { \"id\": 14, \"score\": 0.75 },\\n\\n    { \"id\": 11, \"score\": 0.73 }\\n\\n  ],\\n\\n  \"status\": \"ok\",\\n\\n  \"time\": 0.001\\n\\n}\\n\\n```\\n\\n\\n\\nThe `result` contains ordered by `score` list of found point ids.\\n\\n- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f18bd2ff51730c94</th>\n",
       "      <td>What is the purpose of ef_construct in HNSW ?</td>\n",
       "      <td>The purpose of `ef_construct` in HNSW is to specify the search range when building the index.</td>\n",
       "      <td>In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.\\n\\n\\n\\nThe corresponding parameters could be configured in the configuration file:\\n\\n\\n\\n```yaml\\n\\nstorage:\\n\\n  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually\\n\\n  hnsw_index:\\n\\n    # Number of edges per node in the index graph.\\n\\n(\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aa11f9fef7241d75</th>\n",
       "      <td>What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?</td>\n",
       "      <td>The purpose of `CreatePayloadIndexAsync` is to create an index on a specific field within a collection, which in this case is `group_id`. This indexing operation is designed to optimize the search queries that filter on the `group_id` field, making them faster by efficiently locating the nearest neighbors within each group. However, it is noted that global search requests that do not use the `group_id` filter may experience slower performance because they would require scanning across all groups.</td>\n",
       "      <td>await client.CreatePayloadIndexAsync(collectionName: \"{collection_name}\", fieldName: \"group_id\");\\n\\n```\\n\\n\\n\\n## Limitations\\n\\n\\n\\nOne downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.\\n\\nclient.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ee2bf20b47a300fb</th>\n",
       "      <td>How does oversampling helps?</td>\n",
       "      <td>Oversampling helps in two distinct ways. In the context of training datasets, it helps to equalize the representation of classes, which facilitates more fair and accurate modeling of real-world scenarios. In the context of quantization for similarity search algorithms, oversampling allows for significant compression of high-dimensional vectors in memory while compensating for the accuracy loss by re-scoring additional points with the original vectors.</td>\n",
       "      <td>oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.\\n\\n### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aab780d492b0df42</th>\n",
       "      <td>What is ‚Äòbest_score‚Äô strategy?</td>\n",
       "      <td>The 'best_score' strategy is a method introduced in version 1.6.0 for finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. Unlike strategies that rely on averages, 'best_score' does not use a single query vector. Instead, it calculates the distance between a traversed point and each positive and negative example separately during each step of HNSW graph traversal. This approach is more flexible and allows for the passing of just negative samples. It employs a more sophisticated algorithm to determine the best score at every step.</td>\n",
       "      <td>### The new hotness - Best score\\n\\n\\n\\nThe new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative \\n\\nsamples and uses a slightly more sophisticated algorithm under the hood.\\n\\n\\n\\nThe best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point \\n\\nand every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a\\n\\nThis is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7bf4346a4098eada</th>\n",
       "      <td>What is difference between scalar and product quantization?</td>\n",
       "      <td>Scalar Quantization and Product Quantization are both techniques used to compress data, but they differ in their approach and outcomes. Scalar Quantization compresses data by quantizing each component of the vector independently, which can be more straightforward and SIMD-friendly, leading to faster computations. On the other hand, Product Quantization achieves a higher compression rate by dividing the vector into sub-vectors and quantizing these sub-vectors independently. However, this method can result in a loss of accuracy and may not be as fast as Scalar Quantization when it comes to in-RAM search speed due to its non-SIMD-friendly nature of distance calculations.</td>\n",
       "      <td>&lt;/tr&gt;\\n\\n   &lt;/tbody&gt;\\n\\n&lt;/table&gt;\\n\\n\\n\\nIt turns out that in some cases, Product Quantization may not only reduce the memory usage, \\n\\nbut also the search time.\\n\\n\\n\\n## Good practices\\n\\n\\n\\nCompared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\\n\\n\\n\\nProduct Quantization tends to be favored in certain specific scenarios:\\n\\nBut there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6ff81acee6114830</th>\n",
       "      <td>Tell me about ‚Äòalways_ram‚Äô parameter?</td>\n",
       "      <td>The `always_ram` parameter is a setting that determines whether quantized vectors should be kept cached in RAM at all times. When set to `true`, it ensures that quantized vectors are stored in RAM, which can speed up the search process. By default, quantized vectors are not always kept in RAM; they are loaded as needed, similar to the original vectors. However, if you are experiencing a significant decrease in search quality or want to improve search performance, you might consider setting `always_ram` to `true` to keep the quantized vectors in RAM.</td>\n",
       "      <td>It might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quantization\\n\\n\"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f4257b93b605915d</th>\n",
       "      <td>What is vaccum optimizer ?</td>\n",
       "      <td>The context provided does not contain any information about a \"vacuum optimizer.\" It is possible that you are referring to a different concept or there might be a typo in your query. If you are looking for information on a specific feature or concept related to optimizers, please provide the correct term or additional context.</td>\n",
       "      <td>So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan.\\n\\n\\n\\nThe Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached.\\n\\n\\n\\nThe criteria for starting the optimizer are defined in the configuration file.\\n\\n\\n\\nHere is an example of parameter values:\\n\\n\\n\\n```yaml\\n\\nstorage:\\n\\n  optimizers:\\n\\n    # Maximum size (in kilobytes) of vectors to store in-memory per segment.\\n\\nreturn optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        input  \\\n",
       "context.span_id                                                                 \n",
       "5c23a162c061df49           What is the impact of ‚Äòwrite_consistency_factor‚Äô ?   \n",
       "37c98aebc845e8be           What is significance of ‚Äòon_disk_payload‚Äô setting?   \n",
       "f805dcdab7f8a853                         How do you use ‚Äòordering‚Äô parameter?   \n",
       "f18bd2ff51730c94                What is the purpose of ef_construct in HNSW ?   \n",
       "aa11f9fef7241d75            What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?   \n",
       "ee2bf20b47a300fb                                 How does oversampling helps?   \n",
       "aab780d492b0df42                               What is ‚Äòbest_score‚Äô strategy?   \n",
       "7bf4346a4098eada  What is difference between scalar and product quantization?   \n",
       "6ff81acee6114830                        Tell me about ‚Äòalways_ram‚Äô parameter?   \n",
       "f4257b93b605915d                                   What is vaccum optimizer ?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                output  \\\n",
       "context.span_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "5c23a162c061df49                                                                                                                                                                                                                                                                                  The impact of the `write_consistency_factor` is that it defines the number of replicas that must acknowledge a write operation before the system responds to the client. If this value is increased, write operations will be more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active for write operations to be performed successfully.   \n",
       "37c98aebc845e8be                                                                                                                                                                                                                                                                                                The `on_disk_payload` setting is significant because it determines where payload data is stored. When set to `true`, it ensures that payload data is stored only on disk, rather than in RAM. This can be particularly useful for managing memory usage effectively when dealing with large payloads, as it helps to limit the amount of RAM required by not loading large payload values into memory.   \n",
       "f805dcdab7f8a853                                                                                                                                                                                                                      The `ordering` parameter can be used with update and delete operations to ensure that these operations are executed in the same order on all replicas. When this option is utilized, the operation is routed to the leader replica of the shard, and the system waits for a response from the leader before responding to the client. This is particularly useful for avoiding data inconsistencies that might arise from concurrent updates or deletions of the same documents.   \n",
       "f18bd2ff51730c94                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The purpose of `ef_construct` in HNSW is to specify the search range when building the index.   \n",
       "aa11f9fef7241d75                                                                                                                                                                                 The purpose of `CreatePayloadIndexAsync` is to create an index on a specific field within a collection, which in this case is `group_id`. This indexing operation is designed to optimize the search queries that filter on the `group_id` field, making them faster by efficiently locating the nearest neighbors within each group. However, it is noted that global search requests that do not use the `group_id` filter may experience slower performance because they would require scanning across all groups.   \n",
       "ee2bf20b47a300fb                                                                                                                                                                                                                               Oversampling helps in two distinct ways. In the context of training datasets, it helps to equalize the representation of classes, which facilitates more fair and accurate modeling of real-world scenarios. In the context of quantization for similarity search algorithms, oversampling allows for significant compression of high-dimensional vectors in memory while compensating for the accuracy loss by re-scoring additional points with the original vectors.   \n",
       "aab780d492b0df42                                                         The 'best_score' strategy is a method introduced in version 1.6.0 for finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. Unlike strategies that rely on averages, 'best_score' does not use a single query vector. Instead, it calculates the distance between a traversed point and each positive and negative example separately during each step of HNSW graph traversal. This approach is more flexible and allows for the passing of just negative samples. It employs a more sophisticated algorithm to determine the best score at every step.   \n",
       "7bf4346a4098eada  Scalar Quantization and Product Quantization are both techniques used to compress data, but they differ in their approach and outcomes. Scalar Quantization compresses data by quantizing each component of the vector independently, which can be more straightforward and SIMD-friendly, leading to faster computations. On the other hand, Product Quantization achieves a higher compression rate by dividing the vector into sub-vectors and quantizing these sub-vectors independently. However, this method can result in a loss of accuracy and may not be as fast as Scalar Quantization when it comes to in-RAM search speed due to its non-SIMD-friendly nature of distance calculations.   \n",
       "6ff81acee6114830                                                                                                                           The `always_ram` parameter is a setting that determines whether quantized vectors should be kept cached in RAM at all times. When set to `true`, it ensures that quantized vectors are stored in RAM, which can speed up the search process. By default, quantized vectors are not always kept in RAM; they are loaded as needed, similar to the original vectors. However, if you are experiencing a significant decrease in search quality or want to improve search performance, you might consider setting `always_ram` to `true` to keep the quantized vectors in RAM.   \n",
       "f4257b93b605915d                                                                                                                                                                                                                                                                                                                                                              The context provided does not contain any information about a \"vacuum optimizer.\" It is possible that you are referring to a different concept or there might be a typo in your query. If you are looking for information on a specific feature or concept related to optimizers, please provide the correct term or additional context.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                reference  \n",
       "context.span_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "5c23a162c061df49                                                                                                                                                      ### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\n- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.  \n",
       "37c98aebc845e8be                                                                                                                * `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\\n\\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.  \n",
       "f805dcdab7f8a853                                                           Since the `filter` parameter is specified, the search is performed only among those points that satisfy the filter condition.\\n\\nSee details of possible filters and their work in the [filtering](../filtering) section.\\n\\n\\n\\nExample result of this API would be\\n\\n\\n\\n```json\\n\\n{\\n\\n  \"result\": [\\n\\n    { \"id\": 10, \"score\": 0.81 },\\n\\n    { \"id\": 14, \"score\": 0.75 },\\n\\n    { \"id\": 11, \"score\": 0.73 }\\n\\n  ],\\n\\n  \"status\": \"ok\",\\n\\n  \"time\": 0.001\\n\\n}\\n\\n```\\n\\n\\n\\nThe `result` contains ordered by `score` list of found point ids.\\n\\n- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents  \n",
       "f18bd2ff51730c94  In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.\\n\\n\\n\\nThe corresponding parameters could be configured in the configuration file:\\n\\n\\n\\n```yaml\\n\\nstorage:\\n\\n  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually\\n\\n  hnsw_index:\\n\\n    # Number of edges per node in the index graph.\\n\\n(\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n    ...  \n",
       "aa11f9fef7241d75                                                                                                                                                                                                       await client.CreatePayloadIndexAsync(collectionName: \"{collection_name}\", fieldName: \"group_id\");\\n\\n```\\n\\n\\n\\n## Limitations\\n\\n\\n\\nOne downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.\\n\\nclient.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};  \n",
       "ee2bf20b47a300fb                                                                                                                                                                                                                                                                                                                                                                                                                                                    oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.\\n\\n### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.  \n",
       "aab780d492b0df42   ### The new hotness - Best score\\n\\n\\n\\nThe new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative \\n\\nsamples and uses a slightly more sophisticated algorithm under the hood.\\n\\n\\n\\nThe best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point \\n\\nand every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a\\n\\nThis is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.  \n",
       "7bf4346a4098eada                                                                   </tr>\\n\\n   </tbody>\\n\\n</table>\\n\\n\\n\\nIt turns out that in some cases, Product Quantization may not only reduce the memory usage, \\n\\nbut also the search time.\\n\\n\\n\\n## Good practices\\n\\n\\n\\nCompared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\\n\\n\\n\\nProduct Quantization tends to be favored in certain specific scenarios:\\n\\nBut there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method  \n",
       "6ff81acee6114830  It might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quantization\\n\\n\"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collec...  \n",
       "f4257b93b605915d                                                                       So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan.\\n\\n\\n\\nThe Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached.\\n\\n\\n\\nThe criteria for starting the optimizer are defined in the configuration file.\\n\\n\\n\\nHere is an example of parameter values:\\n\\n\\n\\n```yaml\\n\\nstorage:\\n\\n  optimizers:\\n\\n    # Maximum size (in kilobytes) of vectors to store in-memory per segment.\\n\\nreturn optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_df_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>input</th>\n",
       "      <th>reference</th>\n",
       "      <th>document_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th>document_position</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">483939ac09cf8106</th>\n",
       "      <th>0</th>\n",
       "      <td>05c01218d18e182252b4fefa2e82684d</td>\n",
       "      <td>What is the impact of ‚Äòwrite_consistency_factor‚Äô ?</td>\n",
       "      <td>### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python</td>\n",
       "      <td>19.420935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05c01218d18e182252b4fefa2e82684d</td>\n",
       "      <td>What is the impact of ‚Äòwrite_consistency_factor‚Äô ?</td>\n",
       "      <td>- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0a46933e8bfdf38d</th>\n",
       "      <th>0</th>\n",
       "      <td>663f1d1540de82a7ad575e205b5ff803</td>\n",
       "      <td>What is significance of ‚Äòon_disk_payload‚Äô setting?</td>\n",
       "      <td>* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).</td>\n",
       "      <td>12.349620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>663f1d1540de82a7ad575e205b5ff803</td>\n",
       "      <td>What is significance of ‚Äòon_disk_payload‚Äô setting?</td>\n",
       "      <td>The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bc1217192fe1f4d8</th>\n",
       "      <th>0</th>\n",
       "      <td>5bea862dbb13dea05e5797a69d530e35</td>\n",
       "      <td>How do you use ‚Äòordering‚Äô parameter?</td>\n",
       "      <td>Since the `filter` parameter is specified, the search is performed only among those points that satisfy the filter condition.\\n\\nSee details of possible filters and their work in the [filtering](../filtering) section.\\n\\n\\n\\nExample result of this API would be\\n\\n\\n\\n```json\\n\\n{\\n\\n  \"result\": [\\n\\n    { \"id\": 10, \"score\": 0.81 },\\n\\n    { \"id\": 14, \"score\": 0.75 },\\n\\n    { \"id\": 11, \"score\": 0.73 }\\n\\n  ],\\n\\n  \"status\": \"ok\",\\n\\n  \"time\": 0.001\\n\\n}\\n\\n```\\n\\n\\n\\nThe `result` contains ordered by `score` list of found point ids.</td>\n",
       "      <td>11.016777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5bea862dbb13dea05e5797a69d530e35</td>\n",
       "      <td>How do you use ‚Äòordering‚Äô parameter?</td>\n",
       "      <td>- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">6f3cfcf870268b23</th>\n",
       "      <th>0</th>\n",
       "      <td>851c0526ccef268680c9818a4dfebea3</td>\n",
       "      <td>What is the purpose of ef_construct in HNSW ?</td>\n",
       "      <td>In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.\\n\\n\\n\\nThe corresponding parameters could be configured in the configuration file:\\n\\n\\n\\n```yaml\\n\\nstorage:\\n\\n  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually\\n\\n  hnsw_index:\\n\\n    # Number of edges per node in the index graph.</td>\n",
       "      <td>15.599039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>851c0526ccef268680c9818a4dfebea3</td>\n",
       "      <td>What is the purpose of ef_construct in HNSW ?</td>\n",
       "      <td>(\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">40a17423fd3bb548</th>\n",
       "      <th>0</th>\n",
       "      <td>4d1a0bfa050b72d8ea801b5182d0ba1e</td>\n",
       "      <td>What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?</td>\n",
       "      <td>await client.CreatePayloadIndexAsync(collectionName: \"{collection_name}\", fieldName: \"group_id\");\\n\\n```\\n\\n\\n\\n## Limitations\\n\\n\\n\\nOne downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.</td>\n",
       "      <td>15.631700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4d1a0bfa050b72d8ea801b5182d0ba1e</td>\n",
       "      <td>What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?</td>\n",
       "      <td>client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">84aa942596f6421d</th>\n",
       "      <th>0</th>\n",
       "      <td>81f3b7adae989865c40765a1cfa27a0b</td>\n",
       "      <td>How does oversampling helps?</td>\n",
       "      <td>oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.</td>\n",
       "      <td>17.026328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81f3b7adae989865c40765a1cfa27a0b</td>\n",
       "      <td>How does oversampling helps?</td>\n",
       "      <td>### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5f4fa5b8af90db9d</th>\n",
       "      <th>0</th>\n",
       "      <td>6227fd1001c0461e3b5d07b797f01f04</td>\n",
       "      <td>What is ‚Äòbest_score‚Äô strategy?</td>\n",
       "      <td>### The new hotness - Best score\\n\\n\\n\\nThe new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative \\n\\nsamples and uses a slightly more sophisticated algorithm under the hood.\\n\\n\\n\\nThe best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point \\n\\nand every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a</td>\n",
       "      <td>19.246193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6227fd1001c0461e3b5d07b797f01f04</td>\n",
       "      <td>What is ‚Äòbest_score‚Äô strategy?</td>\n",
       "      <td>This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1d478027476745f8</th>\n",
       "      <th>0</th>\n",
       "      <td>ee0e5c425f5e0ebb1663351af27ed8c8</td>\n",
       "      <td>What is difference between scalar and product quantization?</td>\n",
       "      <td>&lt;/tr&gt;\\n\\n   &lt;/tbody&gt;\\n\\n&lt;/table&gt;\\n\\n\\n\\nIt turns out that in some cases, Product Quantization may not only reduce the memory usage, \\n\\nbut also the search time.\\n\\n\\n\\n## Good practices\\n\\n\\n\\nCompared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\\n\\n\\n\\nProduct Quantization tends to be favored in certain specific scenarios:</td>\n",
       "      <td>15.455185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ee0e5c425f5e0ebb1663351af27ed8c8</td>\n",
       "      <td>What is difference between scalar and product quantization?</td>\n",
       "      <td>But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">e5c9458cc34a3ed9</th>\n",
       "      <th>0</th>\n",
       "      <td>5c10258cdd80bd339fe26149f978a774</td>\n",
       "      <td>Tell me about ‚Äòalways_ram‚Äô parameter?</td>\n",
       "      <td>It might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quantization</td>\n",
       "      <td>13.652024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5c10258cdd80bd339fe26149f978a774</td>\n",
       "      <td>Tell me about ‚Äòalways_ram‚Äô parameter?</td>\n",
       "      <td>\"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">fc2a1e3793fa4f5e</th>\n",
       "      <th>0</th>\n",
       "      <td>5cfbe060851ce9e9b6f916883017ce76</td>\n",
       "      <td>What is vaccum optimizer ?</td>\n",
       "      <td>So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan.\\n\\n\\n\\nThe Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached.\\n\\n\\n\\nThe criteria for starting the optimizer are defined in the configuration file.\\n\\n\\n\\nHere is an example of parameter values:\\n\\n\\n\\n```yaml\\n\\nstorage:\\n\\n  optimizers:\\n\\n    # Maximum size (in kilobytes) of vectors to store in-memory per segment.</td>\n",
       "      <td>10.012674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5cfbe060851ce9e9b6f916883017ce76</td>\n",
       "      <td>What is vaccum optimizer ?</td>\n",
       "      <td>return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    context.trace_id  \\\n",
       "context.span_id  document_position                                     \n",
       "483939ac09cf8106 0                  05c01218d18e182252b4fefa2e82684d   \n",
       "                 1                  05c01218d18e182252b4fefa2e82684d   \n",
       "0a46933e8bfdf38d 0                  663f1d1540de82a7ad575e205b5ff803   \n",
       "                 1                  663f1d1540de82a7ad575e205b5ff803   \n",
       "bc1217192fe1f4d8 0                  5bea862dbb13dea05e5797a69d530e35   \n",
       "                 1                  5bea862dbb13dea05e5797a69d530e35   \n",
       "6f3cfcf870268b23 0                  851c0526ccef268680c9818a4dfebea3   \n",
       "                 1                  851c0526ccef268680c9818a4dfebea3   \n",
       "40a17423fd3bb548 0                  4d1a0bfa050b72d8ea801b5182d0ba1e   \n",
       "                 1                  4d1a0bfa050b72d8ea801b5182d0ba1e   \n",
       "84aa942596f6421d 0                  81f3b7adae989865c40765a1cfa27a0b   \n",
       "                 1                  81f3b7adae989865c40765a1cfa27a0b   \n",
       "5f4fa5b8af90db9d 0                  6227fd1001c0461e3b5d07b797f01f04   \n",
       "                 1                  6227fd1001c0461e3b5d07b797f01f04   \n",
       "1d478027476745f8 0                  ee0e5c425f5e0ebb1663351af27ed8c8   \n",
       "                 1                  ee0e5c425f5e0ebb1663351af27ed8c8   \n",
       "e5c9458cc34a3ed9 0                  5c10258cdd80bd339fe26149f978a774   \n",
       "                 1                  5c10258cdd80bd339fe26149f978a774   \n",
       "fc2a1e3793fa4f5e 0                  5cfbe060851ce9e9b6f916883017ce76   \n",
       "                 1                  5cfbe060851ce9e9b6f916883017ce76   \n",
       "\n",
       "                                                                                          input  \\\n",
       "context.span_id  document_position                                                                \n",
       "483939ac09cf8106 0                           What is the impact of ‚Äòwrite_consistency_factor‚Äô ?   \n",
       "                 1                           What is the impact of ‚Äòwrite_consistency_factor‚Äô ?   \n",
       "0a46933e8bfdf38d 0                           What is significance of ‚Äòon_disk_payload‚Äô setting?   \n",
       "                 1                           What is significance of ‚Äòon_disk_payload‚Äô setting?   \n",
       "bc1217192fe1f4d8 0                                         How do you use ‚Äòordering‚Äô parameter?   \n",
       "                 1                                         How do you use ‚Äòordering‚Äô parameter?   \n",
       "6f3cfcf870268b23 0                                What is the purpose of ef_construct in HNSW ?   \n",
       "                 1                                What is the purpose of ef_construct in HNSW ?   \n",
       "40a17423fd3bb548 0                            What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?   \n",
       "                 1                            What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?   \n",
       "84aa942596f6421d 0                                                 How does oversampling helps?   \n",
       "                 1                                                 How does oversampling helps?   \n",
       "5f4fa5b8af90db9d 0                                               What is ‚Äòbest_score‚Äô strategy?   \n",
       "                 1                                               What is ‚Äòbest_score‚Äô strategy?   \n",
       "1d478027476745f8 0                  What is difference between scalar and product quantization?   \n",
       "                 1                  What is difference between scalar and product quantization?   \n",
       "e5c9458cc34a3ed9 0                                        Tell me about ‚Äòalways_ram‚Äô parameter?   \n",
       "                 1                                        Tell me about ‚Äòalways_ram‚Äô parameter?   \n",
       "fc2a1e3793fa4f5e 0                                                   What is vaccum optimizer ?   \n",
       "                 1                                                   What is vaccum optimizer ?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    reference  \\\n",
       "context.span_id  document_position                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "483939ac09cf8106 0                         ### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python   \n",
       "                 1                                                                                                                                                                                                                                              - `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.   \n",
       "0a46933e8bfdf38d 0                                                                                         * `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).   \n",
       "                 1                                                                                                                                        The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.   \n",
       "bc1217192fe1f4d8 0                  Since the `filter` parameter is specified, the search is performed only among those points that satisfy the filter condition.\\n\\nSee details of possible filters and their work in the [filtering](../filtering) section.\\n\\n\\n\\nExample result of this API would be\\n\\n\\n\\n```json\\n\\n{\\n\\n  \"result\": [\\n\\n    { \"id\": 10, \"score\": 0.81 },\\n\\n    { \"id\": 14, \"score\": 0.75 },\\n\\n    { \"id\": 11, \"score\": 0.73 }\\n\\n  ],\\n\\n  \"status\": \"ok\",\\n\\n  \"time\": 0.001\\n\\n}\\n\\n```\\n\\n\\n\\nThe `result` contains ordered by `score` list of found point ids.   \n",
       "                 1                                                                                                                                                          - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents   \n",
       "6f3cfcf870268b23 0                                   In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.\\n\\n\\n\\nThe corresponding parameters could be configured in the configuration file:\\n\\n\\n\\n```yaml\\n\\nstorage:\\n\\n  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually\\n\\n  hnsw_index:\\n\\n    # Number of edges per node in the index graph.   \n",
       "                 1                                                         (\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),   \n",
       "40a17423fd3bb548 0                                                                                                                                                                                                                                              await client.CreatePayloadIndexAsync(collectionName: \"{collection_name}\", fieldName: \"group_id\");\\n\\n```\\n\\n\\n\\n## Limitations\\n\\n\\n\\nOne downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.   \n",
       "                 1                                                                          client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};   \n",
       "84aa942596f6421d 0                                                                                                                                                                                                                                                                                                                                                                                                                  oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.   \n",
       "                 1                                                                                                                                                   ### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.   \n",
       "5f4fa5b8af90db9d 0                                           ### The new hotness - Best score\\n\\n\\n\\nThe new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative \\n\\nsamples and uses a slightly more sophisticated algorithm under the hood.\\n\\n\\n\\nThe best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point \\n\\nand every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a   \n",
       "                 1                                                                         This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.   \n",
       "1d478027476745f8 0                                                                                                  </tr>\\n\\n   </tbody>\\n\\n</table>\\n\\n\\n\\nIt turns out that in some cases, Product Quantization may not only reduce the memory usage, \\n\\nbut also the search time.\\n\\n\\n\\n## Good practices\\n\\n\\n\\nCompared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\\n\\n\\n\\nProduct Quantization tends to be favored in certain specific scenarios:   \n",
       "                 1                                                                                  But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method   \n",
       "e5c9458cc34a3ed9 0                                                 It might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quantization   \n",
       "                 1                                                              \"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\   \n",
       "fc2a1e3793fa4f5e 0                                So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan.\\n\\n\\n\\nThe Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached.\\n\\n\\n\\nThe criteria for starting the optimizer are defined in the configuration file.\\n\\n\\n\\nHere is an example of parameter values:\\n\\n\\n\\n```yaml\\n\\nstorage:\\n\\n  optimizers:\\n\\n    # Maximum size (in kilobytes) of vectors to store in-memory per segment.   \n",
       "                 1                                                                                                                                                        return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.   \n",
       "\n",
       "                                    document_score  \n",
       "context.span_id  document_position                  \n",
       "483939ac09cf8106 0                       19.420935  \n",
       "                 1                        0.100000  \n",
       "0a46933e8bfdf38d 0                       12.349620  \n",
       "                 1                        0.000000  \n",
       "bc1217192fe1f4d8 0                       11.016777  \n",
       "                 1                        0.100000  \n",
       "6f3cfcf870268b23 0                       15.599039  \n",
       "                 1                        0.100000  \n",
       "40a17423fd3bb548 0                       15.631700  \n",
       "                 1                        0.100000  \n",
       "84aa942596f6421d 0                       17.026328  \n",
       "                 1                        0.100000  \n",
       "5f4fa5b8af90db9d 0                       19.246193  \n",
       "                 1                        0.100000  \n",
       "1d478027476745f8 0                       15.455185  \n",
       "                 1                        0.100000  \n",
       "e5c9458cc34a3ed9 0                       13.652024  \n",
       "                 1                        0.100000  \n",
       "fc2a1e3793fa4f5e 0                       10.012674  \n",
       "                 1                        0.100000  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_documents_df_hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **17. Define your evaluation model and your evaluators for Hybrid Search**\n",
    "\n",
    "Next, define your evaluation model and your evaluators.\n",
    "\n",
    "Evaluators are built on top of language models and prompt the LLM to assess the quality of responses, the relevance of retrieved documents, etc., and provide a quality signal even in the absence of human-labeled data. Pick an evaluator type and instantiate it with the language model you want to use to perform evaluations using our battle-tested evaluation templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61739f7fe8f49d39f88bd0e13aa9b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/20 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8b2280651f46079ddca9ad0e365c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/20 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# all spans created within this context will be associated with the `HYBRID_RAG_PROJECT` project.\n",
    "eval_model = OpenAIModel(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    ")\n",
    "hallucination_evaluator = HallucinationEvaluator(eval_model)\n",
    "qa_correctness_evaluator = QAEvaluator(eval_model)\n",
    "relevance_evaluator = RelevanceEvaluator(eval_model)\n",
    "\n",
    "hallucination_eval_df_hybrid, qa_correctness_eval_df_hybrid = run_evals(\n",
    "    dataframe=queries_df_hybrid,\n",
    "    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n",
    "    provide_explanation=True,\n",
    ")\n",
    "relevance_eval_df_hybrid = run_evals(\n",
    "    dataframe=retrieved_documents_df_hybrid,\n",
    "    evaluators=[relevance_evaluator],\n",
    "    provide_explanation=True,\n",
    ")[0]\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df_hybrid),\n",
    "    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval_df_hybrid),\n",
    "    project_name=HYBRID_RAG_PROJECT,\n",
    ")\n",
    "px.Client().log_evaluations(DocumentEvaluations(eval_name=\"Relevance\", dataframe=relevance_eval_df_hybrid),\n",
    "                            project_name=HYBRID_RAG_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "003c1412566441b2a0fd878aabed07a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "01d4ff90aac943ff83bfe49b70c783ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8d9844dfef3f414aa750e6200a62bfd3",
       "IPY_MODEL_57d8cc8ff7f047c19301f0793367b2ae",
       "IPY_MODEL_4b23a2eb73b6435384686ced47f9c7f6"
      ],
      "layout": "IPY_MODEL_6fc5bf4070d94eccbdcb4fad4247c758"
     }
    },
    "02ca3a5d43e343d5b756fca172bc1822": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f1e6788ea4344553adbe66202caa656c",
       "IPY_MODEL_5a3d5d83788446028f4de4afbd262f3e",
       "IPY_MODEL_bf06bcbdbff3485abb3895dc54256e00"
      ],
      "layout": "IPY_MODEL_1ff9ec98314944bdb86d4fda42a4c88b"
     }
    },
    "0685ef29e4c54e2e8d5c424508ebfe82": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "080a6d0c05144698a9c7366e54bb39b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65f86e93b0fe48ddbaf2cabc9975c68b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8e261a6071cd41a6afd4377c9ead98ca",
      "value": "‚Äá35/35‚Äá(100.0%)‚Äá|‚Äá‚è≥‚Äá00:37&lt;00:00‚Äá|‚Äá‚Äá1.72it/s"
     }
    },
    "0a8c4c23041641308b625e929790a55e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c90d55a6f6d4d3f8c72bd0ba19687ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be68eca0c7b640abada072a3cc3cd042",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6eadc3109208449eb9cb7ad5b6a5bb46",
      "value": "‚Äá125k/125k‚Äá[00:01&lt;00:00,‚Äá84.2kB/s]"
     }
    },
    "0d229e81fcb246cf99dd919d06cf5905": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1057e11184c44ea38154a2ceabd90198": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4722676ae36d4749918f5edaac48b182",
       "IPY_MODEL_a2041da6fb5f499191f405fa891c0907",
       "IPY_MODEL_5c26103d1d89466c9d22960bb347db39"
      ],
      "layout": "IPY_MODEL_0685ef29e4c54e2e8d5c424508ebfe82"
     }
    },
    "10d5d30b4fd14f3ba45da1794607c5e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "128b89e88c684401818304e8410b6c33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1677f41c03a74a9f8ac55c49a9c12799": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bda9ff3e5071497c933f31a2dd44b449",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_b6686121f3084e1a8c6e0399a8a72675",
      "value": "Generating‚Äáembeddings:‚Äá100%"
     }
    },
    "178bbf06a9ae4b52baa5b731c0c81590": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_003c1412566441b2a0fd878aabed07a6",
      "max": 4431,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cf1c5baa009c467a8c76a1a78d6987b1",
      "value": 4431
     }
    },
    "18ee1349c9394dab81163d5de23a04a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ad21fe66fac4742beb7c17ba8f8733f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1677f41c03a74a9f8ac55c49a9c12799",
       "IPY_MODEL_5b8040a4238f41128006185a858db466",
       "IPY_MODEL_7fbfaf28e7134c638b3e4d16e3cb3e6c"
      ],
      "layout": "IPY_MODEL_1f1e8f2a35b645eb91e96a847cf2856b"
     }
    },
    "1e846da2f0c44e099d24c350e0b0eb1f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f1e8f2a35b645eb91e96a847cf2856b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f3117c63e8d4558b801a33e9b5c548a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ff9ec98314944bdb86d4fda42a4c88b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "218ea3901010405a95033a6cb632250a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21e41bb05ed34b02b2e9e1c2d7d56145": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "22c94d8ddc9d4561930044518630bb0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18ee1349c9394dab81163d5de23a04a8",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_21e41bb05ed34b02b2e9e1c2d7d56145",
      "value": "‚Äá4431/4431‚Äá[00:02&lt;00:00,‚Äá2155.10it/s]"
     }
    },
    "251b1e790043469393aae0c8ebec3d77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "280b4a0048134a8aaf523deead4e3aef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "366e10ebb46a45508d837c8ead93124c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a222b53b0b2448a97b23e2b782a37b5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_99f0a6ea567247639e7b17af1e1df9cf",
      "value": "Generating‚Äáembeddings:‚Äá100%"
     }
    },
    "37a8add4c6a242a49ea09ebef29cfdd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b993aeba0d0f4021b4ae31b8378af903",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_cfbcd04f19374fe6b8ab9802f6891612",
      "value": "‚Äá240/0‚Äá[00:00&lt;00:00,‚Äá2042.22‚Äáexamples/s]"
     }
    },
    "38d701e482684695b97bd73bac78ccfc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43dfd4070e324858974c08fbcb8055fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "43e63c6009c14d9cb4eee3ea177d1c7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f997d5f3659421981252a05845dd236",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_af7527216b9f4240b36024d07ac5c507",
      "value": "‚Äá43.0/43.0‚Äá[00:00&lt;00:00,‚Äá946B/s]"
     }
    },
    "44cb1b5c1c0a446f9d308d93d33ae4f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50d006eafd294bbab3339c2bece90673",
      "max": 335,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b21c626648d643d4972c76a094391134",
      "value": 335
     }
    },
    "4722676ae36d4749918f5edaac48b182": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba3888c2a5954fb29293f41577a5bcae",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f7b5096a4b944d62a281ee927a1ce2e4",
      "value": "Generating‚Äátrain‚Äásplit:‚Äá"
     }
    },
    "4ada07ba27b14c1a8c071343cc21c9c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b23a2eb73b6435384686ced47f9c7f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9fa5a2efb1142c9abd47fe46f05cee6",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_56dc39917eec4eba8194bbe0a3e91de1",
      "value": "‚Äá43.0/43.0‚Äá[00:00&lt;00:00,‚Äá1.71kB/s]"
     }
    },
    "4cc3f02b7bb44a87b8d50dd2f98edce7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "503e9200b403425b8c0379b0e74b01fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50d006eafd294bbab3339c2bece90673": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54aa6aff06214fe4baa3e621ec306c04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "56dc39917eec4eba8194bbe0a3e91de1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57d8cc8ff7f047c19301f0793367b2ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72ce212a2baa494a8a9ae439c7c3e742",
      "max": 43,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f61b254bd9e24480b0176b43c1c7f47e",
      "value": 43
     }
    },
    "5a3d5d83788446028f4de4afbd262f3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cc3f02b7bb44a87b8d50dd2f98edce7",
      "max": 1777260,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e07726709c7748eeb708dbe5c37f9770",
      "value": 1777260
     }
    },
    "5b8040a4238f41128006185a858db466": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_503e9200b403425b8c0379b0e74b01fe",
      "max": 2048,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6f6e8b35568945dc85ba3e47f7b0f3d7",
      "value": 2048
     }
    },
    "5c26103d1d89466c9d22960bb347db39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ada07ba27b14c1a8c071343cc21c9c4",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_98d7cd89900f44a79893e7d8d6b8b0c7",
      "value": "‚Äá81/0‚Äá[00:00&lt;00:00,‚Äá1674.70‚Äáexamples/s]"
     }
    },
    "5c65981c04f34589a36c5c8bb4195f2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5cbb12c59ec948ba8d66dc3812d5c846": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "602df92e53334c65a2cc171301c67e46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "65f86e93b0fe48ddbaf2cabc9975c68b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66588b1f23bb421db7dbc90b2298a845": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "68ac22f645934827a9627e3559067ba1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f3c5e97c2ac94cafa20b130f1f45ce67",
       "IPY_MODEL_b505d5e7978d4b19b59dc8f3fe71e795",
       "IPY_MODEL_080a6d0c05144698a9c7366e54bb39b0"
      ],
      "layout": "IPY_MODEL_e69da775e4df4ed8a5f5ed1348278ab8"
     }
    },
    "6a222b53b0b2448a97b23e2b782a37b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b115bdeaeb547e6b96a5a440b2fe3ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6eadc3109208449eb9cb7ad5b6a5bb46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6f6e8b35568945dc85ba3e47f7b0f3d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6fc5bf4070d94eccbdcb4fad4247c758": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ff26418624f4662871a69796f67427a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72ce212a2baa494a8a9ae439c7c3e742": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77d79ac84c8c4d8393fcdb7f304b4bf3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79335cc0bf85458ba630ced71d706b2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_251b1e790043469393aae0c8ebec3d77",
      "max": 43,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_602df92e53334c65a2cc171301c67e46",
      "value": 43
     }
    },
    "7b8b3e1c571a4345bc2ec4eb00f0967e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a74d6dce715f4494874de92852b56c09",
       "IPY_MODEL_a5d80d3579ca481a98310bd08b8c6918",
       "IPY_MODEL_f07a59b5bc7a4f56baa892ca864d488c"
      ],
      "layout": "IPY_MODEL_84fd76a737334831a886755affb50886"
     }
    },
    "7e8c241b51af4963aa78d91e6e78136e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7fbfaf28e7134c638b3e4d16e3cb3e6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c47e7205eb6540a8a5c51120cd222fe3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_ea74a3a2d06a4c61b85db3c7552535a1",
      "value": "‚Äá2048/2048‚Äá[00:20&lt;00:00,‚Äá120.27it/s]"
     }
    },
    "8115b0b37d164080b58368be3fadbaa0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84fd76a737334831a886755affb50886": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "869c8dd95cf7452b841f7203e0e6b8c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d9844dfef3f414aa750e6200a62bfd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_218ea3901010405a95033a6cb632250a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8ef997a7dbf54d3d9cfa8147e1611cbd",
      "value": "Downloading‚Äáreadme:‚Äá100%"
     }
    },
    "8e261a6071cd41a6afd4377c9ead98ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8ef997a7dbf54d3d9cfa8147e1611cbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f997d5f3659421981252a05845dd236": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90f264886e864ddabdb6eddbecf6d75a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "924c8fe0a9cb4feab39d31ec358089b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98d7cd89900f44a79893e7d8d6b8b0c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "997305a8d871401ba6a1e805dc1ca045": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "99f0a6ea567247639e7b17af1e1df9cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f88771edbfd468cb315a54b69eb7226": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1c3867b1d404a948a0edd0e6bc9a1b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff9f4b2c32234207acb5221babe9c061",
      "max": 124978,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d51b6ff6e830489f8e1e100bf1f1ae98",
      "value": 124978
     }
    },
    "a2041da6fb5f499191f405fa891c0907": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54aa6aff06214fe4baa3e621ec306c04",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_66588b1f23bb421db7dbc90b2298a845",
      "value": 1
     }
    },
    "a5d80d3579ca481a98310bd08b8c6918": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce0efd7352cd4539bda8706ebbc146fd",
      "max": 22,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_128b89e88c684401818304e8410b6c33",
      "value": 22
     }
    },
    "a74d6dce715f4494874de92852b56c09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c65981c04f34589a36c5c8bb4195f2b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_10d5d30b4fd14f3ba45da1794607c5e0",
      "value": "run_evals‚Äá"
     }
    },
    "aa8db2c8310b4f9582f2002adf3ffa1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b84c10312dbe486fb3b21c7c5e23d603",
       "IPY_MODEL_178bbf06a9ae4b52baa5b731c0c81590",
       "IPY_MODEL_22c94d8ddc9d4561930044518630bb0b"
      ],
      "layout": "IPY_MODEL_acf3351a0fba41558f97945fbc6b84c7"
     }
    },
    "ac507e1d00b446df9379fa58f39418fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c85db136dc2843a58e7aa648bbf9f9fe",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5cbb12c59ec948ba8d66dc3812d5c846",
      "value": "Generating‚Äátrain‚Äásplit:‚Äá"
     }
    },
    "acf3351a0fba41558f97945fbc6b84c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "adc2166e9b7d4ebdb5dd5415fd3e2ec5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "af7527216b9f4240b36024d07ac5c507": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af77bbc36b844afa960381389111bccf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af7d2403f4504d49a027d935b13d3f62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ff26418624f4662871a69796f67427a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d5e93af9e5644c419bc653af2057e285",
      "value": "Downloading‚Äáreadme:‚Äá100%"
     }
    },
    "b21c626648d643d4972c76a094391134": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b3dc75c3ec7a48ceaff045c81f250bfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b44f529954e944729b7446a4d610ac1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b505d5e7978d4b19b59dc8f3fe71e795": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38d701e482684695b97bd73bac78ccfc",
      "max": 35,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e7d21aa7bce145b78e834897b158f9b0",
      "value": 35
     }
    },
    "b51b292bc0b341de8fd94707e7fa2c0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8749c8627c44757aa8703b45421af2d",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_9f88771edbfd468cb315a54b69eb7226",
      "value": "Generating‚Äáembeddings:‚Äá100%"
     }
    },
    "b6686121f3084e1a8c6e0399a8a72675": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b84c10312dbe486fb3b21c7c5e23d603": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f3117c63e8d4558b801a33e9b5c548a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_997305a8d871401ba6a1e805dc1ca045",
      "value": "Parsing‚Äánodes:‚Äá100%"
     }
    },
    "b993aeba0d0f4021b4ae31b8378af903": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba3888c2a5954fb29293f41577a5bcae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bda9ff3e5071497c933f31a2dd44b449": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be68eca0c7b640abada072a3cc3cd042": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf06bcbdbff3485abb3895dc54256e00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8115b0b37d164080b58368be3fadbaa0",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0d229e81fcb246cf99dd919d06cf5905",
      "value": "‚Äá1.78M/1.78M‚Äá[00:00&lt;00:00,‚Äá4.56MB/s]"
     }
    },
    "c30b902e82de486eb0c10a3e64687b44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3a80fdcb9944e62ba5b51838fef5ef4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_af7d2403f4504d49a027d935b13d3f62",
       "IPY_MODEL_79335cc0bf85458ba630ced71d706b2e",
       "IPY_MODEL_43e63c6009c14d9cb4eee3ea177d1c7d"
      ],
      "layout": "IPY_MODEL_dce488c3561e4d7fa737379553ec0b3b"
     }
    },
    "c47e7205eb6540a8a5c51120cd222fe3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c762c26d4f2e4159b714ad55bbd195eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b51b292bc0b341de8fd94707e7fa2c0c",
       "IPY_MODEL_f10f2f679d7a4ab8988601cd59aa4b70",
       "IPY_MODEL_d5baee84346c4dc29ffa4fd2fe6495c6"
      ],
      "layout": "IPY_MODEL_0a8c4c23041641308b625e929790a55e"
     }
    },
    "c85db136dc2843a58e7aa648bbf9f9fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c95d4cfffece443182faa3e2a3e48dda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df997aad597e4c1ab23501d12383e39b",
       "IPY_MODEL_a1c3867b1d404a948a0edd0e6bc9a1b8",
       "IPY_MODEL_0c90d55a6f6d4d3f8c72bd0ba19687ba"
      ],
      "layout": "IPY_MODEL_1e846da2f0c44e099d24c350e0b0eb1f"
     }
    },
    "ce0efd7352cd4539bda8706ebbc146fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf1c5baa009c467a8c76a1a78d6987b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cfbcd04f19374fe6b8ab9802f6891612": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cff10cfd18704a9cb7c9b4d4ba5ccaef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d44a32f577204732a7f4ce767d8e83f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d51b6ff6e830489f8e1e100bf1f1ae98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d5baee84346c4dc29ffa4fd2fe6495c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_280b4a0048134a8aaf523deead4e3aef",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_869c8dd95cf7452b841f7203e0e6b8c5",
      "value": "‚Äá2048/2048‚Äá[00:19&lt;00:00,‚Äá119.05it/s]"
     }
    },
    "d5e93af9e5644c419bc653af2057e285": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d772ecb46ca54adbbb84749fe07ab735": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8749c8627c44757aa8703b45421af2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9fa5a2efb1142c9abd47fe46f05cee6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dce488c3561e4d7fa737379553ec0b3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df997aad597e4c1ab23501d12383e39b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_924c8fe0a9cb4feab39d31ec358089b8",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_cff10cfd18704a9cb7c9b4d4ba5ccaef",
      "value": "Downloading‚Äádata:‚Äá100%"
     }
    },
    "e07726709c7748eeb708dbe5c37f9770": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e69da775e4df4ed8a5f5ed1348278ab8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7d21aa7bce145b78e834897b158f9b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e7d4e009f37a4d798dbc23b39ed7c0b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea74a3a2d06a4c61b85db3c7552535a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb848aa889474b118c49a7cdac509d9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adc2166e9b7d4ebdb5dd5415fd3e2ec5",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_90f264886e864ddabdb6eddbecf6d75a",
      "value": 1
     }
    },
    "eecec2323e294d6fb4ee89f5937c241c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ac507e1d00b446df9379fa58f39418fc",
       "IPY_MODEL_eb848aa889474b118c49a7cdac509d9b",
       "IPY_MODEL_37a8add4c6a242a49ea09ebef29cfdd9"
      ],
      "layout": "IPY_MODEL_b44f529954e944729b7446a4d610ac1d"
     }
    },
    "f07a59b5bc7a4f56baa892ca864d488c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7d4e009f37a4d798dbc23b39ed7c0b1",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d772ecb46ca54adbbb84749fe07ab735",
      "value": "‚Äá22/22‚Äá(100.0%)‚Äá|‚Äá‚è≥‚Äá00:52&lt;00:00‚Äá|‚Äá‚Äá2.31it/s"
     }
    },
    "f10f2f679d7a4ab8988601cd59aa4b70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af77bbc36b844afa960381389111bccf",
      "max": 2048,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b3dc75c3ec7a48ceaff045c81f250bfd",
      "value": 2048
     }
    },
    "f1e6788ea4344553adbe66202caa656c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9e47bbf438740a2a224c5f7d913e502",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c30b902e82de486eb0c10a3e64687b44",
      "value": "Downloading‚Äádata:‚Äá100%"
     }
    },
    "f3c5e97c2ac94cafa20b130f1f45ce67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77d79ac84c8c4d8393fcdb7f304b4bf3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_43dfd4070e324858974c08fbcb8055fd",
      "value": "run_evals‚Äá"
     }
    },
    "f61b254bd9e24480b0176b43c1c7f47e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f7b5096a4b944d62a281ee927a1ce2e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9e47bbf438740a2a224c5f7d913e502": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa26918ac71440c1803c82c939fbc79f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_366e10ebb46a45508d837c8ead93124c",
       "IPY_MODEL_44cb1b5c1c0a446f9d308d93d33ae4f8",
       "IPY_MODEL_fca056b08fc3427a977bc3717a020443"
      ],
      "layout": "IPY_MODEL_7e8c241b51af4963aa78d91e6e78136e"
     }
    },
    "fca056b08fc3427a977bc3717a020443": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d44a32f577204732a7f4ce767d8e83f5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6b115bdeaeb547e6b96a5a440b2fe3ef",
      "value": "‚Äá335/335‚Äá[00:05&lt;00:00,‚Äá59.78it/s]"
     }
    },
    "ff9f4b2c32234207acb5221babe9c061": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
