{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmb4fjQvQmFT"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/qdrant_arize.png\" width=\"500\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Tuning a RAG Pipeline using Qdrant and Arize Phoenix</h1>\n",
    "\n",
    "‚ÑπÔ∏è This notebook requires an OpenAI API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Import Relevant Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jCyVyMs-JrWS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Setup projects\n",
    "SIMPLE_RAG_PROJECT = \"simple-rag\"\n",
    "HYBRID_RAG_PROJECT = \"hybrid-rag\"\n",
    "os.environ[\"PHOENIX_PROJECT_NAME\"] = SIMPLE_RAG_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NNJI9dP6GeUE"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import ssl\n",
    "import time\n",
    "import urllib\n",
    "from getpass import getpass\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import certifi\n",
    "import nest_asyncio\n",
    "import openai\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from llama_index.core import (\n",
    "    ServiceContext, StorageContext, download_loader,\n",
    "    load_index_from_storage, set_global_handler\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.graph_stores.simple import SimpleGraphStore\n",
    "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from phoenix.evals import (\n",
    "    HallucinationEvaluator, OpenAIModel, QAEvaluator,\n",
    "    RelevanceEvaluator, run_evals\n",
    ")\n",
    "from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n",
    "from phoenix.trace import DocumentEvaluations, SpanEvaluations\n",
    "from tqdm import tqdm\n",
    "\n",
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import PointStruct\n",
    "\n",
    "nest_asyncio.apply()  # needed for concurrent evals in notebook environments\n",
    "pd.set_option(\"display.max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hnr-p5J6jyEl"
   },
   "source": [
    "### **2. Launch Phoenix**\n",
    "You can run Phoenix in the background to collect trace data emitted by any LlamaIndex application that has been instrumented with the OpenInferenceTraceCallbackHandler. Phoenix supports LlamaIndex's one-click observability which will automatically instrument your LlamaIndex application! You can consult our integration guide for a more detailed explanation of how to instrument your LlamaIndex application.\n",
    "\n",
    "Launch Phoenix and follow the instructions in the cell output to open the Phoenix UI (the UI should be empty because we have yet to run the LlamaIndex application)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "zYfgwhXvZcOV",
    "outputId": "3599030d-8ba2-4f8a-c8ec-a0d5b576015b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üì∫ To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "session = px.launch_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9n105Mckm1f"
   },
   "source": [
    "Be sure to enable phoenix as your global handler for tracing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1Ym-djTOkjxQ"
   },
   "outputs": [],
   "source": [
    "set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z15BcuTcj6Cp"
   },
   "source": [
    "### **3. Setup your openai key and retrieve the documents to be used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7EsFBezzcGbG",
    "outputId": "158775a0-8f34-4201-88c5-b221fe40f883"
   },
   "outputs": [],
   "source": [
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Retrieve the documents / dataset to be used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "c3a80fdcb9944e62ba5b51838fef5ef4",
      "af7d2403f4504d49a027d935b13d3f62",
      "79335cc0bf85458ba630ced71d706b2e",
      "43e63c6009c14d9cb4eee3ea177d1c7d",
      "dce488c3561e4d7fa737379553ec0b3b",
      "6ff26418624f4662871a69796f67427a",
      "d5e93af9e5644c419bc653af2057e285",
      "251b1e790043469393aae0c8ebec3d77",
      "602df92e53334c65a2cc171301c67e46",
      "8f997d5f3659421981252a05845dd236",
      "af7527216b9f4240b36024d07ac5c507",
      "02ca3a5d43e343d5b756fca172bc1822",
      "f1e6788ea4344553adbe66202caa656c",
      "5a3d5d83788446028f4de4afbd262f3e",
      "bf06bcbdbff3485abb3895dc54256e00",
      "1ff9ec98314944bdb86d4fda42a4c88b",
      "f9e47bbf438740a2a224c5f7d913e502",
      "c30b902e82de486eb0c10a3e64687b44",
      "4cc3f02b7bb44a87b8d50dd2f98edce7",
      "e07726709c7748eeb708dbe5c37f9770",
      "8115b0b37d164080b58368be3fadbaa0",
      "0d229e81fcb246cf99dd919d06cf5905",
      "eecec2323e294d6fb4ee89f5937c241c",
      "ac507e1d00b446df9379fa58f39418fc",
      "eb848aa889474b118c49a7cdac509d9b",
      "37a8add4c6a242a49ea09ebef29cfdd9",
      "b44f529954e944729b7446a4d610ac1d",
      "c85db136dc2843a58e7aa648bbf9f9fe",
      "5cbb12c59ec948ba8d66dc3812d5c846",
      "adc2166e9b7d4ebdb5dd5415fd3e2ec5",
      "90f264886e864ddabdb6eddbecf6d75a",
      "b993aeba0d0f4021b4ae31b8378af903",
      "cfbcd04f19374fe6b8ab9802f6891612"
     ]
    },
    "id": "iisWzELAzYca",
    "outputId": "751b3676-2a8f-4497-de7e-85ee3e9ec199"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# If the dataset is gated/private, make sure you have run huggingface-cli login\n",
    "dataset = load_dataset(\"atitaarora/qdrant_doc\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t5jYTnnlza6b",
    "outputId": "2894f142-0d41-42b8-b75d-63850ad8a2db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description='', citation='', homepage='', license='', features={'text': Value(dtype='string', id=None), 'source': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='csv', dataset_name='qdrant_doc', config_name='default', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=1767967, num_examples=240, shard_lengths=None, dataset_name='qdrant_doc')}, download_checksums={'hf://datasets/atitaarora/qdrant_doc@8d859890840f65337c38e96d660b81b1441bbecd/documents.csv': {'num_bytes': 1777260, 'checksum': None}}, download_size=1777260, post_processing_size=None, dataset_size=1767967, size_in_bytes=3545227)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Definition of global chunk properties and chunk processing**\n",
    "Processing each document with desired **TEXT_SPLITTER_ALGO , CHUNK_SIZE , CHUNK_OVERLAP** etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global config for chunk processing\n",
    "CHUNK_SIZE = 512 #1000\n",
    "CHUNK_OVERLAP = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Process dataset as langchain (or llamaindex) document for further processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qWTlffFW6Art",
    "outputId": "2685adf0-2965-4c2d-878f-67eee384d65c"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from llama_index.core import Document\n",
    "\n",
    "## Split and process the document chunks from the given dataset\n",
    "\n",
    "def process_document_chunks(dataset,chunk_size,chunk_overlap):\n",
    "    langchain_docs = [\n",
    "        LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "        for doc in tqdm(dataset)\n",
    "    ]\n",
    "\n",
    "    # could showcase another variation of processed documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        add_start_index=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in langchain_docs:\n",
    "        docs_processed += text_splitter.split_documents([doc])\n",
    "\n",
    "    ## Converting Langchain document chunks above into Llamaindex Document for ingestion\n",
    "    llama_documents = [\n",
    "        Document.from_langchain_format(doc)\n",
    "        for doc in docs_processed\n",
    "    ]\n",
    "    return llama_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IbWBhR_661SX",
    "outputId": "79b17993-361d-47e1-ec14-f6f6dfd6949c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 240/240 [00:00<00:00, 14744.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4431"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = process_document_chunks(dataset, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr5DrAtMkbL-"
   },
   "source": [
    "### **7. Setting up Qdrant and Collection**\n",
    "\n",
    "We first set up the qdrant client and then create a collection so that our data may be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QYir5ueWcb8r"
   },
   "outputs": [],
   "source": [
    "##Uncomment to initialise qdrant client in memory\n",
    "#client = qdrant_client.QdrantClient(\n",
    "#    location=\":memory:\",\n",
    "#)\n",
    "\n",
    "##Uncomment below to connect to Qdrant Cloud\n",
    "client = QdrantClient(\n",
    "    os.environ.get(\"QDRANT_URL\"), \n",
    "    api_key=os.environ.get(\"QDRANT_API_KEY\"),\n",
    ")\n",
    "\n",
    "## Uncomment below to connect to local Qdrant\n",
    "#client = qdrant_client.QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collection Name \n",
    "COLLECTION_NAME = \"qdrant_docs_arize_dense\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z-IUMQAT76Ep",
    "outputId": "88207dc6-c291-4d76-c8e9-ea6f180ed02a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=4431 indexed_vectors_count=0 points_count=4431 segments_count=2 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=384, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None) payload_schema={}\n"
     ]
    }
   ],
   "source": [
    "## General Collection level operations\n",
    "\n",
    "## Get information about existing collections \n",
    "#client.get_collections()\n",
    "\n",
    "## Get information about specific collection\n",
    "collection_info = client.get_collection(COLLECTION_NAME)\n",
    "print(collection_info)\n",
    "\n",
    "## Deleting collection, if need be\n",
    "#client.delete_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-04-16 17:55:12.207\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mfastembed.embedding\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[33m\u001b[1mDefaultEmbedding, FlagEmbedding, JinaEmbedding are deprecated.Use from fastembed import TextEmbedding instead.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>dim</th>\n",
       "      <th>description</th>\n",
       "      <th>size_in_GB</th>\n",
       "      <th>sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BAAI/bge-base-en</td>\n",
       "      <td>768</td>\n",
       "      <td>Base English model</td>\n",
       "      <td>0.420</td>\n",
       "      <td>{'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en.tar.gz'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BAAI/bge-base-en-v1.5</td>\n",
       "      <td>768</td>\n",
       "      <td>Base English model, v1.5</td>\n",
       "      <td>0.210</td>\n",
       "      <td>{'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en-v1.5.tar.gz', 'hf': 'qdrant/bge-base-en-v1.5-onnx-q'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAAI/bge-large-en-v1.5</td>\n",
       "      <td>1024</td>\n",
       "      <td>Large English model, v1.5</td>\n",
       "      <td>1.200</td>\n",
       "      <td>{'hf': 'qdrant/bge-large-en-v1.5-onnx'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BAAI/bge-small-en</td>\n",
       "      <td>384</td>\n",
       "      <td>Fast English model</td>\n",
       "      <td>0.130</td>\n",
       "      <td>{'url': 'https://storage.googleapis.com/qdrant-fastembed/BAAI-bge-small-en.tar.gz'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BAAI/bge-small-en-v1.5</td>\n",
       "      <td>384</td>\n",
       "      <td>Fast and Default English model</td>\n",
       "      <td>0.067</td>\n",
       "      <td>{'hf': 'qdrant/bge-small-en-v1.5-onnx-q'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BAAI/bge-small-zh-v1.5</td>\n",
       "      <td>512</td>\n",
       "      <td>Fast and recommended Chinese model</td>\n",
       "      <td>0.090</td>\n",
       "      <td>{'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>384</td>\n",
       "      <td>Sentence Transformer model, MiniLM-L6-v2</td>\n",
       "      <td>0.090</td>\n",
       "      <td>{'url': 'https://storage.googleapis.com/qdrant-fastembed/sentence-transformers-all-MiniLM-L6-v2.tar.gz', 'hf': 'qdrant/all-MiniLM-L6-v2-onnx'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>384</td>\n",
       "      <td>Sentence Transformer model, paraphrase-multilingual-MiniLM-L12-v2</td>\n",
       "      <td>0.220</td>\n",
       "      <td>{'hf': 'qdrant/paraphrase-multilingual-MiniLM-L12-v2-onnx-Q'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nomic-ai/nomic-embed-text-v1</td>\n",
       "      <td>768</td>\n",
       "      <td>8192 context length english model</td>\n",
       "      <td>0.520</td>\n",
       "      <td>{'hf': 'nomic-ai/nomic-embed-text-v1'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nomic-ai/nomic-embed-text-v1.5</td>\n",
       "      <td>768</td>\n",
       "      <td>8192 context length english model</td>\n",
       "      <td>0.520</td>\n",
       "      <td>{'hf': 'nomic-ai/nomic-embed-text-v1.5'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>thenlper/gte-large</td>\n",
       "      <td>1024</td>\n",
       "      <td>Large general text embeddings model</td>\n",
       "      <td>1.200</td>\n",
       "      <td>{'hf': 'qdrant/gte-large-onnx'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mixedbread-ai/mxbai-embed-large-v1</td>\n",
       "      <td>1024</td>\n",
       "      <td>MixedBread Base sentence embedding model, does well on MTEB</td>\n",
       "      <td>0.640</td>\n",
       "      <td>{'hf': 'mixedbread-ai/mxbai-embed-large-v1'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>intfloat/multilingual-e5-large</td>\n",
       "      <td>1024</td>\n",
       "      <td>Multilingual model, e5-large. Recommend using this model for non-English languages</td>\n",
       "      <td>2.240</td>\n",
       "      <td>{'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-multilingual-e5-large.tar.gz', 'hf': 'qdrant/multilingual-e5-large-onnx'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sentence-transformers/paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>768</td>\n",
       "      <td>Sentence-transformers model for tasks like clustering or semantic search</td>\n",
       "      <td>1.000</td>\n",
       "      <td>{'hf': 'xenova/paraphrase-multilingual-mpnet-base-v2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jinaai/jina-embeddings-v2-base-en</td>\n",
       "      <td>768</td>\n",
       "      <td>English embedding model supporting 8192 sequence length</td>\n",
       "      <td>0.520</td>\n",
       "      <td>{'hf': 'xenova/jina-embeddings-v2-base-en'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>jinaai/jina-embeddings-v2-small-en</td>\n",
       "      <td>512</td>\n",
       "      <td>English embedding model supporting 8192 sequence length</td>\n",
       "      <td>0.120</td>\n",
       "      <td>{'hf': 'xenova/jina-embeddings-v2-small-en'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          model   dim  \\\n",
       "0                                              BAAI/bge-base-en   768   \n",
       "1                                         BAAI/bge-base-en-v1.5   768   \n",
       "2                                        BAAI/bge-large-en-v1.5  1024   \n",
       "3                                             BAAI/bge-small-en   384   \n",
       "4                                        BAAI/bge-small-en-v1.5   384   \n",
       "5                                        BAAI/bge-small-zh-v1.5   512   \n",
       "6                        sentence-transformers/all-MiniLM-L6-v2   384   \n",
       "7   sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2   384   \n",
       "8                                  nomic-ai/nomic-embed-text-v1   768   \n",
       "9                                nomic-ai/nomic-embed-text-v1.5   768   \n",
       "10                                           thenlper/gte-large  1024   \n",
       "11                           mixedbread-ai/mxbai-embed-large-v1  1024   \n",
       "12                               intfloat/multilingual-e5-large  1024   \n",
       "13  sentence-transformers/paraphrase-multilingual-mpnet-base-v2   768   \n",
       "14                            jinaai/jina-embeddings-v2-base-en   768   \n",
       "15                           jinaai/jina-embeddings-v2-small-en   512   \n",
       "\n",
       "                                                                           description  \\\n",
       "0                                                                   Base English model   \n",
       "1                                                             Base English model, v1.5   \n",
       "2                                                            Large English model, v1.5   \n",
       "3                                                                   Fast English model   \n",
       "4                                                       Fast and Default English model   \n",
       "5                                                   Fast and recommended Chinese model   \n",
       "6                                             Sentence Transformer model, MiniLM-L6-v2   \n",
       "7                    Sentence Transformer model, paraphrase-multilingual-MiniLM-L12-v2   \n",
       "8                                                    8192 context length english model   \n",
       "9                                                    8192 context length english model   \n",
       "10                                                 Large general text embeddings model   \n",
       "11                         MixedBread Base sentence embedding model, does well on MTEB   \n",
       "12  Multilingual model, e5-large. Recommend using this model for non-English languages   \n",
       "13            Sentence-transformers model for tasks like clustering or semantic search   \n",
       "14                             English embedding model supporting 8192 sequence length   \n",
       "15                             English embedding model supporting 8192 sequence length   \n",
       "\n",
       "    size_in_GB  \\\n",
       "0        0.420   \n",
       "1        0.210   \n",
       "2        1.200   \n",
       "3        0.130   \n",
       "4        0.067   \n",
       "5        0.090   \n",
       "6        0.090   \n",
       "7        0.220   \n",
       "8        0.520   \n",
       "9        0.520   \n",
       "10       1.200   \n",
       "11       0.640   \n",
       "12       2.240   \n",
       "13       1.000   \n",
       "14       0.520   \n",
       "15       0.120   \n",
       "\n",
       "                                                                                                                                           sources  \n",
       "0                                                               {'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en.tar.gz'}  \n",
       "1                  {'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-base-en-v1.5.tar.gz', 'hf': 'qdrant/bge-base-en-v1.5-onnx-q'}  \n",
       "2                                                                                                          {'hf': 'qdrant/bge-large-en-v1.5-onnx'}  \n",
       "3                                                              {'url': 'https://storage.googleapis.com/qdrant-fastembed/BAAI-bge-small-en.tar.gz'}  \n",
       "4                                                                                                        {'hf': 'qdrant/bge-small-en-v1.5-onnx-q'}  \n",
       "5                                                         {'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz'}  \n",
       "6   {'url': 'https://storage.googleapis.com/qdrant-fastembed/sentence-transformers-all-MiniLM-L6-v2.tar.gz', 'hf': 'qdrant/all-MiniLM-L6-v2-onnx'}  \n",
       "7                                                                                    {'hf': 'qdrant/paraphrase-multilingual-MiniLM-L12-v2-onnx-Q'}  \n",
       "8                                                                                                           {'hf': 'nomic-ai/nomic-embed-text-v1'}  \n",
       "9                                                                                                         {'hf': 'nomic-ai/nomic-embed-text-v1.5'}  \n",
       "10                                                                                                                 {'hf': 'qdrant/gte-large-onnx'}  \n",
       "11                                                                                                    {'hf': 'mixedbread-ai/mxbai-embed-large-v1'}  \n",
       "12         {'url': 'https://storage.googleapis.com/qdrant-fastembed/fast-multilingual-e5-large.tar.gz', 'hf': 'qdrant/multilingual-e5-large-onnx'}  \n",
       "13                                                                                          {'hf': 'xenova/paraphrase-multilingual-mpnet-base-v2'}  \n",
       "14                                                                                                     {'hf': 'xenova/jina-embeddings-v2-base-en'}  \n",
       "15                                                                                                    {'hf': 'xenova/jina-embeddings-v2-small-en'}  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Declaring the intended Embedding Model with Fastembed\n",
    "from fastembed.embedding import TextEmbedding\n",
    "\n",
    "pd.DataFrame(TextEmbedding.list_supported_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. Document Embedding processing and Ingestion**\n",
    "\n",
    "This example uses a `QdrantVectorStore` and creates a new collection to work fully connected with Qdrant but you can use whatever LlamaIndex application you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "aa8db2c8310b4f9582f2002adf3ffa1b",
      "b84c10312dbe486fb3b21c7c5e23d603",
      "178bbf06a9ae4b52baa5b731c0c81590",
      "22c94d8ddc9d4561930044518630bb0b",
      "acf3351a0fba41558f97945fbc6b84c7",
      "1f3117c63e8d4558b801a33e9b5c548a",
      "997305a8d871401ba6a1e805dc1ca045",
      "003c1412566441b2a0fd878aabed07a6",
      "cf1c5baa009c467a8c76a1a78d6987b1",
      "18ee1349c9394dab81163d5de23a04a8",
      "21e41bb05ed34b02b2e9e1c2d7d56145",
      "c762c26d4f2e4159b714ad55bbd195eb",
      "b51b292bc0b341de8fd94707e7fa2c0c",
      "f10f2f679d7a4ab8988601cd59aa4b70",
      "d5baee84346c4dc29ffa4fd2fe6495c6",
      "0a8c4c23041641308b625e929790a55e",
      "d8749c8627c44757aa8703b45421af2d",
      "9f88771edbfd468cb315a54b69eb7226",
      "af77bbc36b844afa960381389111bccf",
      "b3dc75c3ec7a48ceaff045c81f250bfd",
      "280b4a0048134a8aaf523deead4e3aef",
      "869c8dd95cf7452b841f7203e0e6b8c5",
      "1ad21fe66fac4742beb7c17ba8f8733f",
      "1677f41c03a74a9f8ac55c49a9c12799",
      "5b8040a4238f41128006185a858db466",
      "7fbfaf28e7134c638b3e4d16e3cb3e6c",
      "1f1e8f2a35b645eb91e96a847cf2856b",
      "bda9ff3e5071497c933f31a2dd44b449",
      "b6686121f3084e1a8c6e0399a8a72675",
      "503e9200b403425b8c0379b0e74b01fe",
      "6f6e8b35568945dc85ba3e47f7b0f3d7",
      "c47e7205eb6540a8a5c51120cd222fe3",
      "ea74a3a2d06a4c61b85db3c7552535a1",
      "fa26918ac71440c1803c82c939fbc79f",
      "366e10ebb46a45508d837c8ead93124c",
      "44cb1b5c1c0a446f9d308d93d33ae4f8",
      "fca056b08fc3427a977bc3717a020443",
      "7e8c241b51af4963aa78d91e6e78136e",
      "6a222b53b0b2448a97b23e2b782a37b5",
      "99f0a6ea567247639e7b17af1e1df9cf",
      "50d006eafd294bbab3339c2bece90673",
      "b21c626648d643d4972c76a094391134",
      "d44a32f577204732a7f4ce767d8e83f5",
      "6b115bdeaeb547e6b96a5a440b2fe3ef"
     ]
    },
    "id": "DTjwoobJcJO3",
    "outputId": "1dcd8c41-4066-4130-f20f-dabc2a1f31d5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258c65156a574057a186958b5ce9f4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Initializing the space to work with llama-index and related settings\n",
    "import llama_index\n",
    "from llama_index.core import Settings\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from phoenix.trace import suppress_tracing\n",
    "## Uncomment it if you'd like to use FastEmbed instead of OpenAI\n",
    "## For the complete list of supported models,\n",
    "##please check https://qdrant.github.io/fastembed/examples/Supported_Models/\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "##Uncomment if using FastEmbed\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "## Uncomment it if you'd like to use OpenAI Embeddings instead of FastEmbed\n",
    "#Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4-1106-preview\", temperature=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: This block is to be used when you're creating a new collection if using an existing collection,\n",
    "## Skip this block and execute the next one instead.\n",
    "\n",
    "from phoenix.trace import suppress_tracing\n",
    "with suppress_tracing():\n",
    "  index = VectorStoreIndex.from_documents(\n",
    "      documents,\n",
    "      storage_context=storage_context,\n",
    "      show_progress=True\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8a. Connecting to existing Collection**\n",
    "\n",
    "This example uses a `QdrantVectorStore` and uses the previously generated collection to work fully connected with Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: Execute this block when using an existing collection\n",
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "from llama_index.core.indices.vector_store import VectorIndexRetriever\n",
    "dense_vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)\n",
    "dense_vector_index = VectorStoreIndex.from_vector_store(vector_store=dense_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52PEPokb8FlO",
    "outputId": "1cfa3ea9-09e1-4fdd-eed4-2aa8764ea4a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountResult(count=4431)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sanity Check : Count the total number of documents processed equals number of documents in the collection\n",
    "client.count(collection_name=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nxhA_6plBSL"
   },
   "source": [
    "### **9.Running an example query and printing out the response.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Zlep2wVsf1tR"
   },
   "outputs": [],
   "source": [
    "##Initialise retriever to interact with the Qdrant collection\n",
    "dense_retriever = VectorIndexRetriever(\n",
    "    index=dense_vector_index,\n",
    "    vector_store_query_mode=VectorStoreQueryMode.DEFAULT,\n",
    "    similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXE8pRjjgSse",
    "outputId": "6207af91-b168-4a42-c1de-832246c4a001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Such segments, for example, are created as copy-on-write segments during optimization itself.\n",
      "\n",
      "\n",
      "\n",
      "It is also essential to have at least one small segment that Qdrant will use to store frequently updated data.\n",
      "\n",
      "On the other hand, too many small segments lead to suboptimal search performance.\n",
      "\n",
      "\n",
      "\n",
      "There is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.\n",
      "\n",
      "2 ---\n",
      "\n",
      "title: Optimizer\n",
      "\n",
      "weight: 70\n",
      "\n",
      "aliases:\n",
      "\n",
      "  - ../optimizer\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "# Optimizer\n",
      "\n",
      "\n",
      "\n",
      "It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\n",
      "\n",
      "\n",
      "\n",
      "Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = dense_retriever.retrieve(\"What is a Merge Optimizer?\")\n",
    "for i, node in enumerate(response):\n",
    "    print(i + 1, node.text, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f7LrVhxuL613",
    "outputId": "397cb8ec-9b3a-44b1-84db-68d75b3bd655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∫ Opening a view to the Phoenix app. The app is running at http://localhost:6006/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://localhost:6006/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x136903e80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can view the above data in the UI\n",
    "px.active_session().view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfOh7f9VlJMX"
   },
   "source": [
    "### **10. Run Your Query Engine and View Your Traces in Phoenix**\n",
    "\n",
    "We've compiled a list of the baseline questions about Qdrant. Let's download the sample queries and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "01d4ff90aac943ff83bfe49b70c783ba",
      "8d9844dfef3f414aa750e6200a62bfd3",
      "57d8cc8ff7f047c19301f0793367b2ae",
      "4b23a2eb73b6435384686ced47f9c7f6",
      "6fc5bf4070d94eccbdcb4fad4247c758",
      "218ea3901010405a95033a6cb632250a",
      "8ef997a7dbf54d3d9cfa8147e1611cbd",
      "72ce212a2baa494a8a9ae439c7c3e742",
      "f61b254bd9e24480b0176b43c1c7f47e",
      "d9fa5a2efb1142c9abd47fe46f05cee6",
      "56dc39917eec4eba8194bbe0a3e91de1",
      "c95d4cfffece443182faa3e2a3e48dda",
      "df997aad597e4c1ab23501d12383e39b",
      "a1c3867b1d404a948a0edd0e6bc9a1b8",
      "0c90d55a6f6d4d3f8c72bd0ba19687ba",
      "1e846da2f0c44e099d24c350e0b0eb1f",
      "924c8fe0a9cb4feab39d31ec358089b8",
      "cff10cfd18704a9cb7c9b4d4ba5ccaef",
      "ff9f4b2c32234207acb5221babe9c061",
      "d51b6ff6e830489f8e1e100bf1f1ae98",
      "be68eca0c7b640abada072a3cc3cd042",
      "6eadc3109208449eb9cb7ad5b6a5bb46",
      "1057e11184c44ea38154a2ceabd90198",
      "4722676ae36d4749918f5edaac48b182",
      "a2041da6fb5f499191f405fa891c0907",
      "5c26103d1d89466c9d22960bb347db39",
      "0685ef29e4c54e2e8d5c424508ebfe82",
      "ba3888c2a5954fb29293f41577a5bcae",
      "f7b5096a4b944d62a281ee927a1ce2e4",
      "54aa6aff06214fe4baa3e621ec306c04",
      "66588b1f23bb421db7dbc90b2298a845",
      "4ada07ba27b14c1a8c071343cc21c9c4",
      "98d7cd89900f44a79893e7d8d6b8b0c7"
     ]
    },
    "id": "uFLVc1gkivfp",
    "outputId": "f2e8533d-1524-4365-e150-03d4b8ae811f"
   },
   "outputs": [],
   "source": [
    "## Loading the Eval dataset\n",
    "from datasets import load_dataset\n",
    "qdrant_qa = load_dataset(\"atitaarora/qdrant_doc_qna\", split=\"train\")\n",
    "qdrant_qa_question = qdrant_qa.select_columns(['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "obJfFvhLjs86",
    "outputId": "e634866e-4ca5-491b-e457-feda9afa9d04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is vaccum optimizer ?',\n",
       " 'Tell me about ‚Äòalways_ram‚Äô parameter?',\n",
       " 'What is difference between scalar and product quantization?',\n",
       " 'What is ‚Äòbest_score‚Äô strategy?',\n",
       " 'How does oversampling helps?',\n",
       " 'What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?',\n",
       " 'What is the purpose of ef_construct in HNSW ?',\n",
       " 'How do you use ‚Äòordering‚Äô parameter?',\n",
       " 'What is significance of ‚Äòon_disk_payload‚Äô setting?',\n",
       " 'What is the impact of ‚Äòwrite_consistency_factor‚Äô ?']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_qa_question['question'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzQwf65z3yrz",
    "outputId": "225075d8-855e-47d6-cfc6-0f71d2feedd8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                              | 0/10 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:34<00:00,  9.50s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "#define response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "#assemble query engine for dense retriever\n",
    "dense_query_engine = RetrieverQueryEngine(\n",
    "                     retriever=dense_retriever,\n",
    "                     response_synthesizer=response_synthesizer,)\n",
    "#query_engine = index.as_query_engine()\n",
    "for query in tqdm(qdrant_qa_question['question'][:10]):\n",
    "    try:\n",
    "      dense_query_engine.query(query)\n",
    "    except Exception as e:\n",
    "      pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDox2bLJlYO1"
   },
   "source": [
    "Check the Phoenix UI as your queries run. Your traces should appear in real time.\n",
    "\n",
    "Open the Phoenix UI with the link below if you haven't already and click through the queries to better understand how the query engine is performing. For each trace you will see a break\n",
    "\n",
    "Phoenix can be used to understand and troubleshoot your by surfacing:\n",
    " - **Application latency** - highlighting slow invocations of LLMs, Retrievers, etc.\n",
    " - **Token Usage** - Displays the breakdown of token usage with LLMs to surface up your most expensive LLM calls\n",
    " - **Runtime Exceptions** - Critical runtime exceptions such as rate-limiting are captured as exception events.\n",
    " - **Retrieved Documents** - view all the documents retrieved during a retriever call and the score and order in which they were returned\n",
    " - **Embeddings** - view the embedding text used for retrieval and the underlying embedding model\n",
    "LLM Parameters - view the parameters used when calling out to an LLM to debug things like temperature and the system prompts\n",
    " - **Prompt Templates** - Figure out what prompt template is used during the prompting step and what variables were used.\n",
    " - **Tool Descriptions** - view the description and function signature of the tools your LLM has been given access to\n",
    " - **LLM Function Calls** - if using OpenAI or other a model with function calls, you can view the function selection and function messages in the input messages to the LLM.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/RAG_trace_details.png\" alt=\"Trace Details View on Phoenix\" style=\"width:100%; height:auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "F-fyd_qP31Xf",
    "outputId": "b33bc66c-f2f1-4363-b14a-e96b85e0f5d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Open the Phoenix UI if you haven't already: http://localhost:6006/\n"
     ]
    }
   ],
   "source": [
    "print(f\"üöÄ Open the Phoenix UI if you haven't already: {session.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_M7evWcldWL"
   },
   "source": [
    "### **11. Export and Evaluate Your Trace Data**\n",
    "You can export your trace data as a pandas dataframe for further analysis and evaluation.\n",
    "\n",
    "In this case, we will export our retriever spans into two separate dataframes:\n",
    "\n",
    "queries_df, in which the retrieved documents for each query are concatenated into a single column, retrieved_documents_df, in which each retrieved document is \"exploded\" into its own row to enable the evaluation of each query-document pair in isolation. This will enable us to compute multiple kinds of evaluations, including:\n",
    "\n",
    "relevance: Are the retrieved documents grounded in the response? Q&A correctness: Are your application's responses grounded in the retrieved context? hallucinations: Is your application making up false information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "gQzwROt6JAf7"
   },
   "outputs": [],
   "source": [
    "queries_df = get_qa_with_reference(px.Client())\n",
    "retrieved_documents_df = get_retrieved_documents(px.Client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4be4a0e549dfd38e</th>\n",
       "      <td>What is the impact of ‚Äòwrite_consistency_factor‚Äô ?</td>\n",
       "      <td>The impact of the `write_consistency_factor` is that it determines the number of replicas that need to acknowledge a write operation before the operation is considered successful and a response is sent to the client. If you increase the `write_consistency_factor`, write operations will be more resilient to network partitions within the cluster, as more replicas are required to confirm the write. However, this also means that a larger number of active replicas are necessary to complete write operations, which could impact performance if not enough replicas are available.</td>\n",
       "      <td>- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.\\n\\n### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6193cc1ce104ae10</th>\n",
       "      <td>What is significance of ‚Äòon_disk_payload‚Äô setting?</td>\n",
       "      <td>The `on_disk_payload` setting is significant because it determines where payload data is stored. When set to `true`, it ensures that payload data is stored only on disk, rather than in RAM. This can be particularly useful for managing memory usage effectively when dealing with large payloads, as it helps to limit the amount of RAM required by not loading large payload values into memory.</td>\n",
       "      <td>* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\\n\\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d098e9274285a741</th>\n",
       "      <td>How do you use ‚Äòordering‚Äô parameter?</td>\n",
       "      <td>The `ordering` parameter can be used with update and delete operations to ensure that these operations are executed in the same order on all replicas. To use this parameter, you would include it in your request to Qdrant. For example, when sending an HTTP PUT request to update points in a collection, you would append `?ordering=strong` to the endpoint URL, like so:\\n\\n```\\nPUT /collections/{collection_name}/points?ordering=strong\\n```\\n\\nIn the body of the request, you would include the details of the points you want to update, such as their `ids`, `payloads`, and `vectors`.\\n\\nWhen using the `ordering` parameter, Qdrant will route the operation to the leader replica of the shard and will not respond to the client until the operation has been completed. This ensures that all replicas process the update or delete operations in the same order, which is particularly useful for avoiding data inconsistencies when there are concurrent updates to the same documents.</td>\n",
       "      <td>- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents\\n\\n```http\\n\\nPUT /collections/{collection_name}/points?ordering=strong\\n\\n{\\n\\n    \"batch\": {\\n\\n        \"ids\": [1, 2, 3],\\n\\n        \"payloads\": [\\n\\n            {\"color\": \"red\"},\\n\\n            {\"color\": \"green\"},\\n\\n            {\"color\": \"blue\"}\\n\\n        ],\\n\\n        \"vectors\": [\\n\\n            [0.9, 0.1, 0.1],\\n\\n            [0.1, 0.9, 0.1],\\n\\n            [0.1, 0.1, 0.9]\\n\\n        ]\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nclient.upsert(\\n\\n    collection_name=\"{collection_name}\",\\n\\n    points=models.Batch(\\n\\n        ids=[1, 2, 3],</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d53da8c284ebe696</th>\n",
       "      <td>What is the purpose of ef_construct in HNSW ?</td>\n",
       "      <td>The `ef_construct` parameter in HNSW is the number of neighbors to consider during the index building. The larger the value of `ef_construct`, the higher the precision of the search, but this also results in longer indexing time.</td>\n",
       "      <td>(\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),\\n\\nThe larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \\n\\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\\n\\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9fa2c0203b87f325</th>\n",
       "      <td>What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?</td>\n",
       "      <td>The purpose of the `createPayloadIndex` function (which could be referred to as `CreatePayloadIndexAsync` in an asynchronous context) is to create an index for a specific field within a collection in a database. This index is designed to optimize the performance of queries that filter or sort based on the indexed field. The parameters within the function specify the name of the field to index, as well as the schema for the index, which includes the type of data (text), the tokenizer (word), the minimum and maximum token length, and whether to convert the text to lowercase. This indexing function is likely part of a database or search engine that supports text search capabilities.</td>\n",
       "      <td>client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};\\n\\n},\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"https://your-application-name.fly.dev/.well-known/logo.png\",\\n\\n  \"contact_email\": \"email@domain.com\",\\n\\n  \"legal_info_url\": \"email@domain.com\"\\n\\n}\\n\\n```\\n\\n\\n\\nThat was the last step before running the final command. The command that will deploy \\n\\nthe application on the server:\\n\\n\\n\\n```bash\\n\\nflyctl deploy\\n\\n```</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195aab08906d6ec6</th>\n",
       "      <td>How does oversampling helps?</td>\n",
       "      <td>Oversampling helps in two distinct ways:\\n\\n1. It improves the accuracy and performance of similarity search algorithms by allowing for significant compression of high-dimensional vectors in memory, while compensating for accuracy loss by re-scoring additional points with the original vectors.\\n\\n2. It helps equalize the representation of classes in the training dataset, which enables more fair and accurate modeling of real-world scenarios.</td>\n",
       "      <td>### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.\\n\\noversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e0588735613566ef</th>\n",
       "      <td>What is ‚Äòbest_score‚Äô strategy?</td>\n",
       "      <td>The `best_score` strategy is a method used to find similar vectors by comparing each candidate against every example to identify the ones that are closer to a positive example and further from a negative one. The strategy selects the best positive and best negative scores for each candidate, and the final score is determined using a step formula. If the best positive score is greater than the best negative score, the final score is the best positive score. Otherwise, the final score is the negative of the square of the best negative score. This strategy was introduced in version 1.6.0 and its performance is linearly impacted by the number of examples used.</td>\n",
       "      <td>This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.\\n\\nThe way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\\n\\n\\n\\n```rust\\n\\nlet score = if best_positive_score &gt; best_negative_score {\\n\\n    best_positive_score;\\n\\n} else {\\n\\n    -(best_negative_score * best_negative_score);\\n\\n};\\n\\n```\\n\\n\\n\\n&lt;aside role=\"alert\"&gt;\\n\\nThe performance of &lt;code&gt;best_score&lt;/code&gt; strategy will be linearly impacted by the amount of examples.\\n\\n&lt;/as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c5abb62fcafa683c</th>\n",
       "      <td>What is difference between scalar and product quantization?</td>\n",
       "      <td>Scalar quantization is a compression technique that reduces the number of bits used to represent each component of a vector. For example, it can convert 32-bit floating-point numbers into 8-bit unsigned integers for each component of the vector. This method is straightforward and can be SIMD-friendly, which means it can be processed quickly and efficiently on modern CPUs.\\n\\nProduct quantization, on the other hand, is also a compression technique used for high-dimensional vectors, but it works differently. It divides the vector into smaller sub-vectors and quantizes each sub-vector independently. While this method can be effective for compressing high-dimensional data, it is not as SIMD-friendly as scalar quantization, which can make it slower. Additionally, product quantization typically results in a loss of accuracy compared to scalar quantization.</td>\n",
       "      <td>But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method\\n\\n*Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -&gt; uint8` conversion for each vector component.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e9a877067bbc9023</th>\n",
       "      <td>Tell me about ‚Äòalways_ram‚Äô parameter?</td>\n",
       "      <td>The `always_ram` parameter is a configuration option that determines whether quantized vectors should be always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, if there is a need to speed up the search process, you can set `always_ram` to `true`. This will ensure that quantized vectors are stored in RAM, potentially improving search performance at the cost of increased memory usage.</td>\n",
       "      <td>\"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\\n\\nIt might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05dc44e6e637a2bc</th>\n",
       "      <td>What is vaccum optimizer ?</td>\n",
       "      <td>The term \"vacuum optimizer\" does not appear directly in the provided context. However, the context does mention an \"optimizer_config\" with various parameters that could be related to the optimization process of a system. One of the parameters within this configuration is \"vacuum_min_vector_number,\" which suggests a threshold for a certain operation that might be part of an optimization or maintenance routine, possibly to do with data storage or memory management. The specific function or definition of a \"vacuum optimizer\" cannot be determined from the given context.</td>\n",
       "      <td>return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.\\n\\n},\\n\\n            \"optimizer_config\": {\\n\\n                \"deleted_threshold\": 0.2,\\n\\n                \"vacuum_min_vector_number\": 1000,\\n\\n                \"default_segment_number\": 0,\\n\\n                \"max_segment_size\": null,\\n\\n                \"memmap_threshold\": null,\\n\\n                \"indexing_threshold\": 20000,\\n\\n                \"flush_interval_sec\": 5,\\n\\n                \"max_optimization_threads\": 1\\n\\n            },\\n\\n            \"wal_config\": {\\n\\n                \"wal_capacity_mb\": 32,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4f4702cda07f8d1e</th>\n",
       "      <td>What is a Merge Optimizer?</td>\n",
       "      <td>None</td>\n",
       "      <td>Such segments, for example, are created as copy-on-write segments during optimization itself.\\n\\n\\n\\nIt is also essential to have at least one small segment that Qdrant will use to store frequently updated data.\\n\\nOn the other hand, too many small segments lead to suboptimal search performance.\\n\\n\\n\\nThere is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.\\n\\n---\\n\\ntitle: Optimizer\\n\\nweight: 70\\n\\naliases:\\n\\n  - ../optimizer\\n\\n---\\n\\n\\n\\n# Optimizer\\n\\n\\n\\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\\n\\n\\n\\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        input  \\\n",
       "context.span_id                                                                 \n",
       "4be4a0e549dfd38e           What is the impact of ‚Äòwrite_consistency_factor‚Äô ?   \n",
       "6193cc1ce104ae10           What is significance of ‚Äòon_disk_payload‚Äô setting?   \n",
       "d098e9274285a741                         How do you use ‚Äòordering‚Äô parameter?   \n",
       "d53da8c284ebe696                What is the purpose of ef_construct in HNSW ?   \n",
       "9fa2c0203b87f325            What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?   \n",
       "195aab08906d6ec6                                 How does oversampling helps?   \n",
       "e0588735613566ef                               What is ‚Äòbest_score‚Äô strategy?   \n",
       "c5abb62fcafa683c  What is difference between scalar and product quantization?   \n",
       "e9a877067bbc9023                        Tell me about ‚Äòalways_ram‚Äô parameter?   \n",
       "05dc44e6e637a2bc                                   What is vaccum optimizer ?   \n",
       "4f4702cda07f8d1e                                   What is a Merge Optimizer?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         output  \\\n",
       "context.span_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "4be4a0e549dfd38e                                                                                                                                                                                                                                                                                                                                                                                                               The impact of the `write_consistency_factor` is that it determines the number of replicas that need to acknowledge a write operation before the operation is considered successful and a response is sent to the client. If you increase the `write_consistency_factor`, write operations will be more resilient to network partitions within the cluster, as more replicas are required to confirm the write. However, this also means that a larger number of active replicas are necessary to complete write operations, which could impact performance if not enough replicas are available.   \n",
       "6193cc1ce104ae10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The `on_disk_payload` setting is significant because it determines where payload data is stored. When set to `true`, it ensures that payload data is stored only on disk, rather than in RAM. This can be particularly useful for managing memory usage effectively when dealing with large payloads, as it helps to limit the amount of RAM required by not loading large payload values into memory.   \n",
       "d098e9274285a741  The `ordering` parameter can be used with update and delete operations to ensure that these operations are executed in the same order on all replicas. To use this parameter, you would include it in your request to Qdrant. For example, when sending an HTTP PUT request to update points in a collection, you would append `?ordering=strong` to the endpoint URL, like so:\\n\\n```\\nPUT /collections/{collection_name}/points?ordering=strong\\n```\\n\\nIn the body of the request, you would include the details of the points you want to update, such as their `ids`, `payloads`, and `vectors`.\\n\\nWhen using the `ordering` parameter, Qdrant will route the operation to the leader replica of the shard and will not respond to the client until the operation has been completed. This ensures that all replicas process the update or delete operations in the same order, which is particularly useful for avoiding data inconsistencies when there are concurrent updates to the same documents.   \n",
       "d53da8c284ebe696                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The `ef_construct` parameter in HNSW is the number of neighbors to consider during the index building. The larger the value of `ef_construct`, the higher the precision of the search, but this also results in longer indexing time.   \n",
       "9fa2c0203b87f325                                                                                                                                                                                                                                                                                               The purpose of the `createPayloadIndex` function (which could be referred to as `CreatePayloadIndexAsync` in an asynchronous context) is to create an index for a specific field within a collection in a database. This index is designed to optimize the performance of queries that filter or sort based on the indexed field. The parameters within the function specify the name of the field to index, as well as the schema for the index, which includes the type of data (text), the tokenizer (word), the minimum and maximum token length, and whether to convert the text to lowercase. This indexing function is likely part of a database or search engine that supports text search capabilities.   \n",
       "195aab08906d6ec6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Oversampling helps in two distinct ways:\\n\\n1. It improves the accuracy and performance of similarity search algorithms by allowing for significant compression of high-dimensional vectors in memory, while compensating for accuracy loss by re-scoring additional points with the original vectors.\\n\\n2. It helps equalize the representation of classes in the training dataset, which enables more fair and accurate modeling of real-world scenarios.   \n",
       "e0588735613566ef                                                                                                                                                                                                                                                                                                                       The `best_score` strategy is a method used to find similar vectors by comparing each candidate against every example to identify the ones that are closer to a positive example and further from a negative one. The strategy selects the best positive and best negative scores for each candidate, and the final score is determined using a step formula. If the best positive score is greater than the best negative score, the final score is the best positive score. Otherwise, the final score is the negative of the square of the best negative score. This strategy was introduced in version 1.6.0 and its performance is linearly impacted by the number of examples used.   \n",
       "c5abb62fcafa683c                                                                                                                 Scalar quantization is a compression technique that reduces the number of bits used to represent each component of a vector. For example, it can convert 32-bit floating-point numbers into 8-bit unsigned integers for each component of the vector. This method is straightforward and can be SIMD-friendly, which means it can be processed quickly and efficiently on modern CPUs.\\n\\nProduct quantization, on the other hand, is also a compression technique used for high-dimensional vectors, but it works differently. It divides the vector into smaller sub-vectors and quantizes each sub-vector independently. While this method can be effective for compressing high-dimensional data, it is not as SIMD-friendly as scalar quantization, which can make it slower. Additionally, product quantization typically results in a loss of accuracy compared to scalar quantization.   \n",
       "e9a877067bbc9023                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The `always_ram` parameter is a configuration option that determines whether quantized vectors should be always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, if there is a need to speed up the search process, you can set `always_ram` to `true`. This will ensure that quantized vectors are stored in RAM, potentially improving search performance at the cost of increased memory usage.   \n",
       "05dc44e6e637a2bc                                                                                                                                                                                                                                                                                                                                                                                                                   The term \"vacuum optimizer\" does not appear directly in the provided context. However, the context does mention an \"optimizer_config\" with various parameters that could be related to the optimization process of a system. One of the parameters within this configuration is \"vacuum_min_vector_number,\" which suggests a threshold for a certain operation that might be part of an optimization or maintenance routine, possibly to do with data storage or memory management. The specific function or definition of a \"vacuum optimizer\" cannot be determined from the given context.   \n",
       "4f4702cda07f8d1e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           None   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                reference  \n",
       "context.span_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "4be4a0e549dfd38e                                                                                                                                                      - `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.\\n\\n### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python  \n",
       "6193cc1ce104ae10                                                                                                                * `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\\n\\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.  \n",
       "d098e9274285a741                                                  - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents\\n\\n```http\\n\\nPUT /collections/{collection_name}/points?ordering=strong\\n\\n{\\n\\n    \"batch\": {\\n\\n        \"ids\": [1, 2, 3],\\n\\n        \"payloads\": [\\n\\n            {\"color\": \"red\"},\\n\\n            {\"color\": \"green\"},\\n\\n            {\"color\": \"blue\"}\\n\\n        ],\\n\\n        \"vectors\": [\\n\\n            [0.9, 0.1, 0.1],\\n\\n            [0.1, 0.9, 0.1],\\n\\n            [0.1, 0.1, 0.9]\\n\\n        ]\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nclient.upsert(\\n\\n    collection_name=\"{collection_name}\",\\n\\n    points=models.Batch(\\n\\n        ids=[1, 2, 3],  \n",
       "d53da8c284ebe696                                                                             (\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),\\n\\nThe larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \\n\\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\\n\\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and  \n",
       "9fa2c0203b87f325    client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};\\n\\n},\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"https://your-application-name.fly.dev/.well-known/logo.png\",\\n\\n  \"contact_email\": \"email@domain.com\",\\n\\n  \"legal_info_url\": \"email@domain.com\"\\n\\n}\\n\\n```\\n\\n\\n\\nThat was the last step before running the final command. The command that will deploy \\n\\nthe application on the server:\\n\\n\\n\\n```bash\\n\\nflyctl deploy\\n\\n```  \n",
       "195aab08906d6ec6                                                                                                                                                                                                                                                                                                                                                                                                                                                    ### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.\\n\\noversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.  \n",
       "e0588735613566ef  This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.\\n\\nThe way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\\n\\n\\n\\n```rust\\n\\nlet score = if best_positive_score > best_negative_score {\\n\\n    best_positive_score;\\n\\n} else {\\n\\n    -(best_negative_score * best_negative_score);\\n\\n};\\n\\n```\\n\\n\\n\\n<aside role=\"alert\">\\n\\nThe performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.\\n\\n</as...  \n",
       "c5abb62fcafa683c                                       But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method\\n\\n*Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -> uint8` conversion for each vector component.  \n",
       "e9a877067bbc9023  \"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\\n\\nIt might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quant...  \n",
       "05dc44e6e637a2bc                                                                                       return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.\\n\\n},\\n\\n            \"optimizer_config\": {\\n\\n                \"deleted_threshold\": 0.2,\\n\\n                \"vacuum_min_vector_number\": 1000,\\n\\n                \"default_segment_number\": 0,\\n\\n                \"max_segment_size\": null,\\n\\n                \"memmap_threshold\": null,\\n\\n                \"indexing_threshold\": 20000,\\n\\n                \"flush_interval_sec\": 5,\\n\\n                \"max_optimization_threads\": 1\\n\\n            },\\n\\n            \"wal_config\": {\\n\\n                \"wal_capacity_mb\": 32,  \n",
       "4f4702cda07f8d1e                                                                  Such segments, for example, are created as copy-on-write segments during optimization itself.\\n\\n\\n\\nIt is also essential to have at least one small segment that Qdrant will use to store frequently updated data.\\n\\nOn the other hand, too many small segments lead to suboptimal search performance.\\n\\n\\n\\nThere is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.\\n\\n---\\n\\ntitle: Optimizer\\n\\nweight: 70\\n\\naliases:\\n\\n  - ../optimizer\\n\\n---\\n\\n\\n\\n# Optimizer\\n\\n\\n\\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\\n\\n\\n\\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>input</th>\n",
       "      <th>reference</th>\n",
       "      <th>document_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th>document_position</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">9e34835f2eb24ce3</th>\n",
       "      <th>0</th>\n",
       "      <td>29c792aba1b2eddff4a04b063a76f132</td>\n",
       "      <td>What is the impact of ‚Äòwrite_consistency_factor‚Äô ?</td>\n",
       "      <td>- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.</td>\n",
       "      <td>0.832443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29c792aba1b2eddff4a04b063a76f132</td>\n",
       "      <td>What is the impact of ‚Äòwrite_consistency_factor‚Äô ?</td>\n",
       "      <td>### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python</td>\n",
       "      <td>0.825855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">872c9bb60ec178ba</th>\n",
       "      <th>0</th>\n",
       "      <td>eb2462da211358528bd53cf86e1f6b07</td>\n",
       "      <td>What is significance of ‚Äòon_disk_payload‚Äô setting?</td>\n",
       "      <td>* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).</td>\n",
       "      <td>0.821666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eb2462da211358528bd53cf86e1f6b07</td>\n",
       "      <td>What is significance of ‚Äòon_disk_payload‚Äô setting?</td>\n",
       "      <td>The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.</td>\n",
       "      <td>0.787703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">68719e00f8cedcb3</th>\n",
       "      <th>0</th>\n",
       "      <td>f86058035955e1abcc66c7bbf10182c5</td>\n",
       "      <td>How do you use ‚Äòordering‚Äô parameter?</td>\n",
       "      <td>- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents</td>\n",
       "      <td>0.770652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f86058035955e1abcc66c7bbf10182c5</td>\n",
       "      <td>How do you use ‚Äòordering‚Äô parameter?</td>\n",
       "      <td>```http\\n\\nPUT /collections/{collection_name}/points?ordering=strong\\n\\n{\\n\\n    \"batch\": {\\n\\n        \"ids\": [1, 2, 3],\\n\\n        \"payloads\": [\\n\\n            {\"color\": \"red\"},\\n\\n            {\"color\": \"green\"},\\n\\n            {\"color\": \"blue\"}\\n\\n        ],\\n\\n        \"vectors\": [\\n\\n            [0.9, 0.1, 0.1],\\n\\n            [0.1, 0.9, 0.1],\\n\\n            [0.1, 0.1, 0.9]\\n\\n        ]\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nclient.upsert(\\n\\n    collection_name=\"{collection_name}\",\\n\\n    points=models.Batch(\\n\\n        ids=[1, 2, 3],</td>\n",
       "      <td>0.740061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4f2ee291ba438e0d</th>\n",
       "      <th>0</th>\n",
       "      <td>d40ae7a595f4597684e4563503d99496</td>\n",
       "      <td>What is the purpose of ef_construct in HNSW ?</td>\n",
       "      <td>(\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),</td>\n",
       "      <td>0.787150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d40ae7a595f4597684e4563503d99496</td>\n",
       "      <td>What is the purpose of ef_construct in HNSW ?</td>\n",
       "      <td>The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \\n\\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\\n\\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and</td>\n",
       "      <td>0.767757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">36b617edce8cdf95</th>\n",
       "      <th>0</th>\n",
       "      <td>456d93957118f3fa1d65854bf4d4b323</td>\n",
       "      <td>What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?</td>\n",
       "      <td>client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};</td>\n",
       "      <td>0.717572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>456d93957118f3fa1d65854bf4d4b323</td>\n",
       "      <td>What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?</td>\n",
       "      <td>},\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"https://your-application-name.fly.dev/.well-known/logo.png\",\\n\\n  \"contact_email\": \"email@domain.com\",\\n\\n  \"legal_info_url\": \"email@domain.com\"\\n\\n}\\n\\n```\\n\\n\\n\\nThat was the last step before running the final command. The command that will deploy \\n\\nthe application on the server:\\n\\n\\n\\n```bash\\n\\nflyctl deploy\\n\\n```</td>\n",
       "      <td>0.698218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">e0ce5df0639da7e0</th>\n",
       "      <th>0</th>\n",
       "      <td>c310be3ea4f1e94d32e014e7146f4cf6</td>\n",
       "      <td>How does oversampling helps?</td>\n",
       "      <td>### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.</td>\n",
       "      <td>0.819349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c310be3ea4f1e94d32e014e7146f4cf6</td>\n",
       "      <td>How does oversampling helps?</td>\n",
       "      <td>oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.</td>\n",
       "      <td>0.815198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">7bec38e24822f4a1</th>\n",
       "      <th>0</th>\n",
       "      <td>910f3039488a45e54b773c2a7a94bfaf</td>\n",
       "      <td>What is ‚Äòbest_score‚Äô strategy?</td>\n",
       "      <td>This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.</td>\n",
       "      <td>0.825645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>910f3039488a45e54b773c2a7a94bfaf</td>\n",
       "      <td>What is ‚Äòbest_score‚Äô strategy?</td>\n",
       "      <td>The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\\n\\n\\n\\n```rust\\n\\nlet score = if best_positive_score &gt; best_negative_score {\\n\\n    best_positive_score;\\n\\n} else {\\n\\n    -(best_negative_score * best_negative_score);\\n\\n};\\n\\n```\\n\\n\\n\\n&lt;aside role=\"alert\"&gt;\\n\\nThe performance of &lt;code&gt;best_score&lt;/code&gt; strategy will be linearly impacted by the amount of examples.\\n\\n&lt;/aside&gt;</td>\n",
       "      <td>0.808645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">7b8df6462aa62481</th>\n",
       "      <th>0</th>\n",
       "      <td>615635ff7ff1acca024464a0f4534855</td>\n",
       "      <td>What is difference between scalar and product quantization?</td>\n",
       "      <td>But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method</td>\n",
       "      <td>0.857293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>615635ff7ff1acca024464a0f4534855</td>\n",
       "      <td>What is difference between scalar and product quantization?</td>\n",
       "      <td>*Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -&gt; uint8` conversion for each vector component.</td>\n",
       "      <td>0.848822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4533c4ac5d100017</th>\n",
       "      <th>0</th>\n",
       "      <td>c4e0ce343a4c3c8eed8b8b67ec36b02a</td>\n",
       "      <td>Tell me about ‚Äòalways_ram‚Äô parameter?</td>\n",
       "      <td>\"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\</td>\n",
       "      <td>0.777294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c4e0ce343a4c3c8eed8b8b67ec36b02a</td>\n",
       "      <td>Tell me about ‚Äòalways_ram‚Äô parameter?</td>\n",
       "      <td>It might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quantization</td>\n",
       "      <td>0.765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">914d79177f58ddde</th>\n",
       "      <th>0</th>\n",
       "      <td>7a985f950d57264c96360a5547842953</td>\n",
       "      <td>What is vaccum optimizer ?</td>\n",
       "      <td>return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.</td>\n",
       "      <td>0.718610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7a985f950d57264c96360a5547842953</td>\n",
       "      <td>What is vaccum optimizer ?</td>\n",
       "      <td>},\\n\\n            \"optimizer_config\": {\\n\\n                \"deleted_threshold\": 0.2,\\n\\n                \"vacuum_min_vector_number\": 1000,\\n\\n                \"default_segment_number\": 0,\\n\\n                \"max_segment_size\": null,\\n\\n                \"memmap_threshold\": null,\\n\\n                \"indexing_threshold\": 20000,\\n\\n                \"flush_interval_sec\": 5,\\n\\n                \"max_optimization_threads\": 1\\n\\n            },\\n\\n            \"wal_config\": {\\n\\n                \"wal_capacity_mb\": 32,</td>\n",
       "      <td>0.707060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4f4702cda07f8d1e</th>\n",
       "      <th>0</th>\n",
       "      <td>7e111679fab354963807fc2827700afb</td>\n",
       "      <td>What is a Merge Optimizer?</td>\n",
       "      <td>Such segments, for example, are created as copy-on-write segments during optimization itself.\\n\\n\\n\\nIt is also essential to have at least one small segment that Qdrant will use to store frequently updated data.\\n\\nOn the other hand, too many small segments lead to suboptimal search performance.\\n\\n\\n\\nThere is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.</td>\n",
       "      <td>0.768700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7e111679fab354963807fc2827700afb</td>\n",
       "      <td>What is a Merge Optimizer?</td>\n",
       "      <td>---\\n\\ntitle: Optimizer\\n\\nweight: 70\\n\\naliases:\\n\\n  - ../optimizer\\n\\n---\\n\\n\\n\\n# Optimizer\\n\\n\\n\\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\\n\\n\\n\\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).</td>\n",
       "      <td>0.725430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    context.trace_id  \\\n",
       "context.span_id  document_position                                     \n",
       "9e34835f2eb24ce3 0                  29c792aba1b2eddff4a04b063a76f132   \n",
       "                 1                  29c792aba1b2eddff4a04b063a76f132   \n",
       "872c9bb60ec178ba 0                  eb2462da211358528bd53cf86e1f6b07   \n",
       "                 1                  eb2462da211358528bd53cf86e1f6b07   \n",
       "68719e00f8cedcb3 0                  f86058035955e1abcc66c7bbf10182c5   \n",
       "                 1                  f86058035955e1abcc66c7bbf10182c5   \n",
       "4f2ee291ba438e0d 0                  d40ae7a595f4597684e4563503d99496   \n",
       "                 1                  d40ae7a595f4597684e4563503d99496   \n",
       "36b617edce8cdf95 0                  456d93957118f3fa1d65854bf4d4b323   \n",
       "                 1                  456d93957118f3fa1d65854bf4d4b323   \n",
       "e0ce5df0639da7e0 0                  c310be3ea4f1e94d32e014e7146f4cf6   \n",
       "                 1                  c310be3ea4f1e94d32e014e7146f4cf6   \n",
       "7bec38e24822f4a1 0                  910f3039488a45e54b773c2a7a94bfaf   \n",
       "                 1                  910f3039488a45e54b773c2a7a94bfaf   \n",
       "7b8df6462aa62481 0                  615635ff7ff1acca024464a0f4534855   \n",
       "                 1                  615635ff7ff1acca024464a0f4534855   \n",
       "4533c4ac5d100017 0                  c4e0ce343a4c3c8eed8b8b67ec36b02a   \n",
       "                 1                  c4e0ce343a4c3c8eed8b8b67ec36b02a   \n",
       "914d79177f58ddde 0                  7a985f950d57264c96360a5547842953   \n",
       "                 1                  7a985f950d57264c96360a5547842953   \n",
       "4f4702cda07f8d1e 0                  7e111679fab354963807fc2827700afb   \n",
       "                 1                  7e111679fab354963807fc2827700afb   \n",
       "\n",
       "                                                                                          input  \\\n",
       "context.span_id  document_position                                                                \n",
       "9e34835f2eb24ce3 0                           What is the impact of ‚Äòwrite_consistency_factor‚Äô ?   \n",
       "                 1                           What is the impact of ‚Äòwrite_consistency_factor‚Äô ?   \n",
       "872c9bb60ec178ba 0                           What is significance of ‚Äòon_disk_payload‚Äô setting?   \n",
       "                 1                           What is significance of ‚Äòon_disk_payload‚Äô setting?   \n",
       "68719e00f8cedcb3 0                                         How do you use ‚Äòordering‚Äô parameter?   \n",
       "                 1                                         How do you use ‚Äòordering‚Äô parameter?   \n",
       "4f2ee291ba438e0d 0                                What is the purpose of ef_construct in HNSW ?   \n",
       "                 1                                What is the purpose of ef_construct in HNSW ?   \n",
       "36b617edce8cdf95 0                            What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?   \n",
       "                 1                            What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?   \n",
       "e0ce5df0639da7e0 0                                                 How does oversampling helps?   \n",
       "                 1                                                 How does oversampling helps?   \n",
       "7bec38e24822f4a1 0                                               What is ‚Äòbest_score‚Äô strategy?   \n",
       "                 1                                               What is ‚Äòbest_score‚Äô strategy?   \n",
       "7b8df6462aa62481 0                  What is difference between scalar and product quantization?   \n",
       "                 1                  What is difference between scalar and product quantization?   \n",
       "4533c4ac5d100017 0                                        Tell me about ‚Äòalways_ram‚Äô parameter?   \n",
       "                 1                                        Tell me about ‚Äòalways_ram‚Äô parameter?   \n",
       "914d79177f58ddde 0                                                   What is vaccum optimizer ?   \n",
       "                 1                                                   What is vaccum optimizer ?   \n",
       "4f4702cda07f8d1e 0                                                   What is a Merge Optimizer?   \n",
       "                 1                                                   What is a Merge Optimizer?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             reference  \\\n",
       "context.span_id  document_position                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "9e34835f2eb24ce3 0                                                                                                                                                                                                                                                       - `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.   \n",
       "                 1                                  ### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python   \n",
       "872c9bb60ec178ba 0                                                                                                  * `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).   \n",
       "                 1                                                                                                                                                 The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.   \n",
       "68719e00f8cedcb3 0                                                                                                                                                                   - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents   \n",
       "                 1                  ```http\\n\\nPUT /collections/{collection_name}/points?ordering=strong\\n\\n{\\n\\n    \"batch\": {\\n\\n        \"ids\": [1, 2, 3],\\n\\n        \"payloads\": [\\n\\n            {\"color\": \"red\"},\\n\\n            {\"color\": \"green\"},\\n\\n            {\"color\": \"blue\"}\\n\\n        ],\\n\\n        \"vectors\": [\\n\\n            [0.9, 0.1, 0.1],\\n\\n            [0.1, 0.9, 0.1],\\n\\n            [0.1, 0.1, 0.9]\\n\\n        ]\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nclient.upsert(\\n\\n    collection_name=\"{collection_name}\",\\n\\n    points=models.Batch(\\n\\n        ids=[1, 2, 3],   \n",
       "4f2ee291ba438e0d 0                                                                  (\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),   \n",
       "                 1                                                                                                                                              The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \\n\\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\\n\\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and   \n",
       "36b617edce8cdf95 0                                                                                   client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};   \n",
       "                 1                                                    },\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"https://your-application-name.fly.dev/.well-known/logo.png\",\\n\\n  \"contact_email\": \"email@domain.com\",\\n\\n  \"legal_info_url\": \"email@domain.com\"\\n\\n}\\n\\n```\\n\\n\\n\\nThat was the last step before running the final command. The command that will deploy \\n\\nthe application on the server:\\n\\n\\n\\n```bash\\n\\nflyctl deploy\\n\\n```   \n",
       "e0ce5df0639da7e0 0                                                                                                                                                            ### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.   \n",
       "                 1                                                                                                                                                                                                                                                                                                                                                                                                                           oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.   \n",
       "7bec38e24822f4a1 0                                                                                  This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.   \n",
       "                 1                                                  The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\\n\\n\\n\\n```rust\\n\\nlet score = if best_positive_score > best_negative_score {\\n\\n    best_positive_score;\\n\\n} else {\\n\\n    -(best_negative_score * best_negative_score);\\n\\n};\\n\\n```\\n\\n\\n\\n<aside role=\"alert\">\\n\\nThe performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.\\n\\n</aside>   \n",
       "7b8df6462aa62481 0                                                                                           But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method   \n",
       "                 1                                                                               *Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -> uint8` conversion for each vector component.   \n",
       "4533c4ac5d100017 0                                                                       \"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\   \n",
       "                 1                                                          It might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quantization   \n",
       "914d79177f58ddde 0                                                                                                                                                                 return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.   \n",
       "                 1                                                         },\\n\\n            \"optimizer_config\": {\\n\\n                \"deleted_threshold\": 0.2,\\n\\n                \"vacuum_min_vector_number\": 1000,\\n\\n                \"default_segment_number\": 0,\\n\\n                \"max_segment_size\": null,\\n\\n                \"memmap_threshold\": null,\\n\\n                \"indexing_threshold\": 20000,\\n\\n                \"flush_interval_sec\": 5,\\n\\n                \"max_optimization_threads\": 1\\n\\n            },\\n\\n            \"wal_config\": {\\n\\n                \"wal_capacity_mb\": 32,   \n",
       "4f4702cda07f8d1e 0                                                                                                                             Such segments, for example, are created as copy-on-write segments during optimization itself.\\n\\n\\n\\nIt is also essential to have at least one small segment that Qdrant will use to store frequently updated data.\\n\\nOn the other hand, too many small segments lead to suboptimal search performance.\\n\\n\\n\\nThere is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.   \n",
       "                 1                                                                        ---\\n\\ntitle: Optimizer\\n\\nweight: 70\\n\\naliases:\\n\\n  - ../optimizer\\n\\n---\\n\\n\\n\\n# Optimizer\\n\\n\\n\\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\\n\\n\\n\\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).   \n",
       "\n",
       "                                    document_score  \n",
       "context.span_id  document_position                  \n",
       "9e34835f2eb24ce3 0                        0.832443  \n",
       "                 1                        0.825855  \n",
       "872c9bb60ec178ba 0                        0.821666  \n",
       "                 1                        0.787703  \n",
       "68719e00f8cedcb3 0                        0.770652  \n",
       "                 1                        0.740061  \n",
       "4f2ee291ba438e0d 0                        0.787150  \n",
       "                 1                        0.767757  \n",
       "36b617edce8cdf95 0                        0.717572  \n",
       "                 1                        0.698218  \n",
       "e0ce5df0639da7e0 0                        0.819349  \n",
       "                 1                        0.815198  \n",
       "7bec38e24822f4a1 0                        0.825645  \n",
       "                 1                        0.808645  \n",
       "7b8df6462aa62481 0                        0.857293  \n",
       "                 1                        0.848822  \n",
       "4533c4ac5d100017 0                        0.777294  \n",
       "                 1                        0.765625  \n",
       "914d79177f58ddde 0                        0.718610  \n",
       "                 1                        0.707060  \n",
       "4f4702cda07f8d1e 0                        0.768700  \n",
       "                 1                        0.725430  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_documents_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **12. Define your evaluation model and your evaluators**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4XwhUYRlkdP"
   },
   "source": [
    "Next, define your evaluation model and your evaluators.\n",
    "\n",
    "Evaluators are built on top of language models and prompt the LLM to assess the quality of responses, the relevance of retrieved documents, etc., and provide a quality signal even in the absence of human-labeled data. Pick an evaluator type and instantiate it with the language model you want to use to perform evaluations using our battle-tested evaluation templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "7b8b3e1c571a4345bc2ec4eb00f0967e",
      "a74d6dce715f4494874de92852b56c09",
      "a5d80d3579ca481a98310bd08b8c6918",
      "f07a59b5bc7a4f56baa892ca864d488c",
      "84fd76a737334831a886755affb50886",
      "5c65981c04f34589a36c5c8bb4195f2b",
      "10d5d30b4fd14f3ba45da1794607c5e0",
      "ce0efd7352cd4539bda8706ebbc146fd",
      "128b89e88c684401818304e8410b6c33",
      "e7d4e009f37a4d798dbc23b39ed7c0b1",
      "d772ecb46ca54adbbb84749fe07ab735",
      "68ac22f645934827a9627e3559067ba1",
      "f3c5e97c2ac94cafa20b130f1f45ce67",
      "b505d5e7978d4b19b59dc8f3fe71e795",
      "080a6d0c05144698a9c7366e54bb39b0",
      "e69da775e4df4ed8a5f5ed1348278ab8",
      "77d79ac84c8c4d8393fcdb7f304b4bf3",
      "43dfd4070e324858974c08fbcb8055fd",
      "38d701e482684695b97bd73bac78ccfc",
      "e7d21aa7bce145b78e834897b158f9b0",
      "65f86e93b0fe48ddbaf2cabc9975c68b",
      "8e261a6071cd41a6afd4377c9ead98ca"
     ]
    },
    "id": "SnJSWjOkJGpE",
    "outputId": "b7119003-dd01-4a37-da4f-ed2bf3f130d9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c360feed2d4fa58a955cb58e1f6683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/22 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e408b54c43a3430785eb999670bc84fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/22 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model = OpenAIModel(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    ")\n",
    "hallucination_evaluator = HallucinationEvaluator(eval_model)\n",
    "qa_correctness_evaluator = QAEvaluator(eval_model)\n",
    "relevance_evaluator = RelevanceEvaluator(eval_model)\n",
    "\n",
    "hallucination_eval_df, qa_correctness_eval_df = run_evals(\n",
    "    dataframe=queries_df,\n",
    "    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n",
    "    provide_explanation=True,\n",
    ")\n",
    "relevance_eval_df = run_evals(\n",
    "    dataframe=retrieved_documents_df,\n",
    "    evaluators=[relevance_evaluator],\n",
    "    provide_explanation=True,\n",
    ")[0]\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df),\n",
    "    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval_df),\n",
    ")\n",
    "px.Client().log_evaluations(DocumentEvaluations(eval_name=\"Relevance\", dataframe=relevance_eval_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOWquDbAlm4P"
   },
   "source": [
    "Your evaluations should now appear as annotations on the appropriate spans in Phoenix.\n",
    "\n",
    "![A view of the Phoenix UI with evaluation annotations](https://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/evals/traces_with_evaluation_annotations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5yek2mtlrRA"
   },
   "source": [
    "### **13. Let's try Hybrid search now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a new collection to store your hybrid emebeddings\n",
    "COLLECTION_NAME_HYBRID = \"qdrant_docs_arize_hybrid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reprocess documents with different settings if needed \n",
    "#documents = process_document_chunks(dataset , CHUNK_SIZE , CHUNK_OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'prithvida/Splade_PP_en_v1',\n",
       "  'vocab_size': 30522,\n",
       "  'description': 'Misspelled version of the model. Retained for backward compatibility. Independent Implementation of SPLADE++ Model for English',\n",
       "  'size_in_GB': 0.532,\n",
       "  'sources': {'hf': 'Qdrant/SPLADE_PP_en_v1'}},\n",
       " {'model': 'prithivida/Splade_PP_en_v1',\n",
       "  'vocab_size': 30522,\n",
       "  'description': 'Independent Implementation of SPLADE++ Model for English',\n",
       "  'size_in_GB': 0.532,\n",
       "  'sources': {'hf': 'Qdrant/SPLADE_PP_en_v1'}}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##List of supported sparse vector models\n",
    "from fastembed.sparse.sparse_text_embedding import SparseTextEmbedding\n",
    "SparseTextEmbedding.list_supported_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **14. Ingest Sparse and Dense vectors into Qdrant**\n",
    "\n",
    "Ingest sparse and dense vectors into Qdrant Collection.\n",
    "We are using Splade++ model for Sparse Vector Model and default Fastembed model - bge-small-en-1.5 for dense embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4933ecec9b4d485ca824ec2f75579fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400864fd14c44e65b39a7ca6f2ea92cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Initializing the space to work with llama-index and related settings\n",
    "import llama_index\n",
    "from llama_index.core import Settings\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from fastembed.sparse.sparse_text_embedding import SparseTextEmbedding, SparseEmbedding\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from typing import List, Tuple\n",
    "\n",
    "sparse_model_name = \"prithivida/Splade_PP_en_v1\"\n",
    "\n",
    "# This triggers the model download\n",
    "sparse_model = SparseTextEmbedding(model_name=sparse_model_name, batch_size=32)\n",
    "\n",
    "## Computing sparse vectors\n",
    "def compute_sparse_vectors(\n",
    "    texts: List[str],\n",
    "    ) -> Tuple[List[List[int]], List[List[float]]]:\n",
    "    indices, values = [], []\n",
    "    for embedding in sparse_model.embed(texts):\n",
    "        indices.append(embedding.indices.tolist())\n",
    "        values.append(embedding.values.tolist())\n",
    "    return indices, values\n",
    "\n",
    "## Creating a vector store with Hybrid search enabled\n",
    "hybrid_vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME_HYBRID,\n",
    "    enable_hybrid=True,\n",
    "    sparse_doc_fn=compute_sparse_vectors,\n",
    "    sparse_query_fn=compute_sparse_vectors)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=hybrid_vector_store)\n",
    "\n",
    "Settings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note : Ingesting sparse and dense vectors into Qdrant collection\n",
    "## This block is to be used when you're creating a new collection if using an existing collection,\n",
    "## Skip this block and execute the next one instead.\n",
    "\n",
    "hybrid_vector_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: Execute this block when using an existing collection\n",
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "from llama_index.core.indices.vector_store import VectorIndexRetriever\n",
    "\n",
    "hybrid_vector_index = VectorStoreIndex.from_vector_store(vector_store=hybrid_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionInfo(status=<CollectionStatus.GREEN: 'green'>, optimizer_status=<OptimizersStatusOneOf.OK: 'ok'>, vectors_count=8862, indexed_vectors_count=4429, points_count=4431, segments_count=2, config=CollectionConfig(params=CollectionParams(vectors={'text-dense': VectorParams(size=384, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None)}, shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors={'text-sparse': SparseVectorParams(index=SparseIndexParams(full_scan_threshold=None, on_disk=None))}), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None), payload_schema={})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## collection level operations\n",
    "client.get_collection(COLLECTION_NAME_HYBRID)\n",
    "#client.delete_collection(COLLECTION_NAME_HYBRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountResult(count=4431)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check the number of documents matches the expected number of document chunks \n",
    "client.count(collection_name=COLLECTION_NAME_HYBRID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **15. Hybrid Search with Qdrant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Like many other databases, Qdrant does not delete entries immediately after a query.\n",
      "\n",
      "Instead, it marks records as deleted and ignores them for future queries.\n",
      "\n",
      "\n",
      "\n",
      "This strategy allows us to minimize disk access - one of the slowest operations.\n",
      "\n",
      "However, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.\n",
      "\n",
      "\n",
      "\n",
      "To avoid these adverse effects, Vacuum Optimizer is used.\n",
      "\n",
      "It is used if the segment has accumulated too many deleted records.\n",
      "2 In this case, the segment to be optimized remains readable for the time of the rebuild.\n",
      "\n",
      "\n",
      "\n",
      "![Segment optimization](/docs/optimization.svg)\n",
      "\n",
      "\n",
      "\n",
      "The availability is achieved by wrapping the segment into a proxy that transparently handles data changes.\n",
      "\n",
      "Changed data is placed in the copy-on-write segment, which has priority for retrieval and subsequent updates.\n",
      "\n",
      "\n",
      "\n",
      "## Vacuum Optimizer\n",
      "\n",
      "\n",
      "\n",
      "The simplest example of a case where you need to rebuild a segment repository is to remove points.\n"
     ]
    }
   ],
   "source": [
    "## Before trying Hybrid search , lets try Sparse Vector Search Retriever \n",
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "from llama_index.core.indices.vector_store import VectorIndexRetriever\n",
    "\n",
    "sparse_retriever = VectorIndexRetriever(\n",
    "    index=hybrid_vector_index,\n",
    "    vector_store_query_mode=VectorStoreQueryMode.SPARSE,\n",
    "    sparse_top_k=2,\n",
    ")\n",
    "\n",
    "## Pure sparse vector search\n",
    "nodes = sparse_retriever.retrieve(\"What is a Vacuum Optimizer?\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's try Hybrid Search Retriever now using the 'alpha' parameter that controls the weightage between\n",
    "## the dense and sparse vector search scores.\n",
    "# NOTE: For hybrid search (0 for sparse search, 1 for dense search)\n",
    "\n",
    "hybrid_retriever = VectorIndexRetriever(\n",
    "    index=hybrid_vector_index,\n",
    "    vector_store_query_mode=VectorStoreQueryMode.HYBRID,\n",
    "    sparse_top_k=1,\n",
    "    similarity_top_k=3,\n",
    "    alpha=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Like many other databases, Qdrant does not delete entries immediately after a query.\n",
      "\n",
      "Instead, it marks records as deleted and ignores them for future queries.\n",
      "\n",
      "\n",
      "\n",
      "This strategy allows us to minimize disk access - one of the slowest operations.\n",
      "\n",
      "However, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.\n",
      "\n",
      "\n",
      "\n",
      "To avoid these adverse effects, Vacuum Optimizer is used.\n",
      "\n",
      "It is used if the segment has accumulated too many deleted records.\n",
      "2 return optimizer\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "Caching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\n",
      "\n",
      "When it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\n",
      "\n",
      "It provides both a considerable speedup and less memory footprint.\n",
      "\n",
      "However, it is quite a bit versatile and has several knobs to tune.\n",
      "3 },\n",
      "\n",
      "            \"optimizer_config\": {\n",
      "\n",
      "                \"deleted_threshold\": 0.2,\n",
      "\n",
      "                \"vacuum_min_vector_number\": 1000,\n",
      "\n",
      "                \"default_segment_number\": 0,\n",
      "\n",
      "                \"max_segment_size\": null,\n",
      "\n",
      "                \"memmap_threshold\": null,\n",
      "\n",
      "                \"indexing_threshold\": 20000,\n",
      "\n",
      "                \"flush_interval_sec\": 5,\n",
      "\n",
      "                \"max_optimization_threads\": 1\n",
      "\n",
      "            },\n",
      "\n",
      "            \"wal_config\": {\n",
      "\n",
      "                \"wal_capacity_mb\": 32,\n"
     ]
    }
   ],
   "source": [
    "## Let's try hybrid retriever \n",
    "nodes = hybrid_retriever.retrieve(\"What is a Vacuum Optimizer?\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(i + 1, node.text, end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We shouldn't be modifying the alpha parameter after the retriever has been created\n",
    "# but that's the easiest way to show the effect of the parameter\n",
    "#hybrid_retriever._alpha = 0.1\n",
    "#hybrid_retriever._alpha = 0.9\n",
    "\n",
    "#nodes = hybrid_retriever.retrieve(\"What is merge optimizer?\")\n",
    "#for i, node in enumerate(nodes):\n",
    "#    print(i + 1, node.text, end=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "#define response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "#assemble query engine for hybrid retriever\n",
    "hybrid_query_engine = RetrieverQueryEngine(\n",
    "                        retriever=hybrid_retriever,\n",
    "                        response_synthesizer=response_synthesizer,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **16. Re-Run Your Query Engine and View Your Traces in Phoenix**\n",
    "\n",
    "Let's rerun the list of the baseline questions about Qdrant on the Hybrid Retriever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                                                              | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                                                        | 1/10 [00:08<01:19,  8.84s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                                           | 2/10 [00:17<01:07,  8.44s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                             | 3/10 [00:28<01:07,  9.68s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                | 4/10 [00:39<01:02, 10.48s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                   | 5/10 [00:47<00:47,  9.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                     | 6/10 [00:55<00:35,  8.92s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                        | 7/10 [01:00<00:22,  7.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 8/10 [01:18<00:21, 10.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå             | 9/10 [01:26<00:10, 10.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:35<00:00,  9.54s/it]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "## Switching phoenix project space\n",
    "from phoenix.trace import using_project\n",
    "\n",
    "# Switch project to run evals\n",
    "with using_project(HYBRID_RAG_PROJECT):\n",
    "# All spans created within this context will be associated with the `HYBRID_RAG_PROJECT` project.\n",
    "\n",
    "    ##Reuse the previously loaded dataset `qdrant_qa_question`\n",
    "    \n",
    "    for query in tqdm(qdrant_qa_question['question'][:10]):\n",
    "        try:\n",
    "          hybrid_query_engine.query(query)\n",
    "        except Exception as e:\n",
    "          pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Open the Phoenix UI if you haven't already: http://localhost:6006/\n"
     ]
    }
   ],
   "source": [
    "print(f\"üöÄ Open the Phoenix UI if you haven't already: {session.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Switching phoenix project space\n",
    "from phoenix.trace import using_project\n",
    "\n",
    "\n",
    "queries_df_hybrid = get_qa_with_reference(px.Client(), project_name=HYBRID_RAG_PROJECT)\n",
    "retrieved_documents_df_hybrid = get_retrieved_documents(px.Client(), project_name=HYBRID_RAG_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a78cf0e6c332e4e5</th>\n",
       "      <td>What is the impact of ‚Äòwrite_consistency_factor‚Äô ?</td>\n",
       "      <td>The `write_consistency_factor` impacts the reliability and consistency of write operations in a distributed system. When this factor is increased, write operations become more tolerant to network partitions, as a higher number of replicas must acknowledge the write before it is considered successful. This can enhance the fault tolerance of the system. However, it also means that a greater number of replicas need to be active and available for a write operation to succeed. If the number of active replicas falls below the `write_consistency_factor`, write operations will fail. This setting can be configured at the time of collection creation and is set to one by default.</td>\n",
       "      <td>### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\n- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.\\n\\nwriteConsistencyFactor: 2\\n\\n);\\n\\n```\\n\\n\\n\\nWrite operations will fail if the number of active replicas is less than the `write_consistency...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6cd1bdd6c96a8b3e</th>\n",
       "      <td>What is significance of ‚Äòon_disk_payload‚Äô setting?</td>\n",
       "      <td>The 'on_disk_payload' setting determines where the payload data is stored for a collection. When set to `true`, it indicates that the payload will not be kept in memory but will instead be stored on disk. This approach is beneficial for managing memory usage, as it can significantly reduce the amount of RAM required, particularly when dealing with large payload values. However, using on-disk storage for payload data may result in slightly increased response times since the data must be read from the disk each time it is requested. Despite this, payload values that are used for filtering and are indexed will still remain in RAM for quick access.</td>\n",
       "      <td>* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\\n\\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.\\n\\ntemp_path: null\\n\\n\\n\\n  # If true - point's payload will not be stored in memory.\\n\\n  # It will be re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cf8e2f4111865a53</th>\n",
       "      <td>How do you use ‚Äòordering‚Äô parameter?</td>\n",
       "      <td>The `ordering` parameter can be used with update and delete operations to ensure that these operations are executed in the same order on all replicas. To use this parameter, you would include it in your request to the API. For example, when performing an update operation, you would send an HTTP PUT request with the `ordering` parameter set to `strong` in the query string. This instructs the system to route the operation to the leader replica of the shard and wait for a response before responding to the client, which helps maintain data consistency in the event of concurrent updates to the same documents. Here is an example of how to use the `ordering` parameter in an HTTP request:\\n\\n```\\nPUT /collections/{collection_name}/points?ordering=strong\\n{\\n    \"batch\": {\\n        \"ids\": [1, 2, 3],\\n        \"payloads\": [\\n            {\"color\": \"red\"},\\n            {\"color\": \"green\"},\\n            {\"color\": \"blue\"}\\n        ],\\n        \"vectors\": [\\n            [0.9, 0.1, 0.1],\\n           ...</td>\n",
       "      <td>Since the `filter` parameter is specified, the search is performed only among those points that satisfy the filter condition.\\n\\nSee details of possible filters and their work in the [filtering](../filtering) section.\\n\\n\\n\\nExample result of this API would be\\n\\n\\n\\n```json\\n\\n{\\n\\n  \"result\": [\\n\\n    { \"id\": 10, \"score\": 0.81 },\\n\\n    { \"id\": 14, \"score\": 0.75 },\\n\\n    { \"id\": 11, \"score\": 0.73 }\\n\\n  ],\\n\\n  \"status\": \"ok\",\\n\\n  \"time\": 0.001\\n\\n}\\n\\n```\\n\\n\\n\\nThe `result` contains ordered by `score` list of found point ids.\\n\\n- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents\\n\\n```http\\n\\nPUT /collections/{collection_name}/poin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7e4b17165e91c3e8</th>\n",
       "      <td>What is the purpose of ef_construct in HNSW ?</td>\n",
       "      <td>The purpose of `ef_construct` in HNSW is to specify the number of neighbors to consider during the index building process. The larger the value of `ef_construct`, the higher the precision of the index, but this also results in longer indexing times.</td>\n",
       "      <td>In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.\\n\\n\\n\\nThe corresponding parameters could be configured in the configuration file:\\n\\n\\n\\n```yaml\\n\\nstorage:\\n\\n  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually\\n\\n  hnsw_index:\\n\\n    # Number of edges per node in the index graph.\\n\\n(\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5fb2cecc3b2b2d29</th>\n",
       "      <td>What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?</td>\n",
       "      <td>The purpose of `CreatePayloadIndexAsync` is to create an index on a specific field within a collection, which in this case is the `group_id` field. This indexing is intended to optimize the search and retrieval of documents based on the values of the indexed field, thereby improving the performance of queries that filter on this field. However, it is noted that while this approach speeds up queries that use the `group_id` filter, global requests that do not use this filter may be slower, as they would require scanning all groups to find the nearest neighbors.</td>\n",
       "      <td>await client.CreatePayloadIndexAsync(collectionName: \"{collection_name}\", fieldName: \"group_id\");\\n\\n```\\n\\n\\n\\n## Limitations\\n\\n\\n\\nOne downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.\\n\\nclient.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};\\n\\n},\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5aa21a981c82f77f</th>\n",
       "      <td>How does oversampling helps?</td>\n",
       "      <td>Oversampling helps by improving the accuracy of processes such as similarity search algorithms and modeling. In the context of similarity search algorithms, it allows for the compression of high-dimensional vectors in memory while compensating for accuracy loss by re-scoring additional points with the original vectors. This is achieved by setting a factor that multiplies the limit during the search, and then the results are re-scored with the original vector before selecting the top results. In the context of training datasets, oversampling helps to equalize the representation of classes, which enables more fair and accurate modeling of real-world scenarios.</td>\n",
       "      <td>oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.\\n\\n### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.\\n\\nIO-induced delays.\\n\\n\\n\\nOversampling is a new feature to improve accuracy at the cost of some\\n\\nperformance. It allows setting a factor, which is multiplied with the `limit`\\n\\nwhile doing the search. The results are then re-scored using the original vector\\n\\nand only then the top results up to the limit are selected.\\n\\n\\n\\n## Discussion\\n\\n\\n\\nLooking back, disk IO used to be very serialized; re-positioning read-write...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fbe4a08b57e5a424</th>\n",
       "      <td>What is ‚Äòbest_score‚Äô strategy?</td>\n",
       "      <td>The 'best_score' strategy is a method introduced in version 1.6.0 for finding similar vectors by comparing each candidate against every positive and negative example. It selects the best positive and best negative scores for each candidate, and the final score is determined using a step formula. If the best positive score is greater than the best negative score, the final score is the best positive score. Otherwise, the final score is the negative of the square of the best negative score. This strategy does not use a single query vector and is more flexible than average-based methods, allowing for the use of only negative samples if desired. The performance of the 'best_score' strategy is linearly impacted by the number of examples used.</td>\n",
       "      <td>### The new hotness - Best score\\n\\n\\n\\nThe new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative \\n\\nsamples and uses a slightly more sophisticated algorithm under the hood.\\n\\n\\n\\nThe best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point \\n\\nand every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a\\n\\nThis is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953892bae8ba9fe8</th>\n",
       "      <td>What is difference between scalar and product quantization?</td>\n",
       "      <td>Scalar quantization is a compression technique that reduces the number of bits used to represent each component of a vector. For example, it can convert 32-bit floating numbers into 8-bit unsigned integers for each vector component. This method is straightforward and can be SIMD-friendly, which means it can be optimized for fast computation on modern CPUs.\\n\\nProduct quantization, on the other hand, offers a higher compression rate compared to scalar quantization. It works by dividing the vector into smaller sub-vectors and quantizing each sub-vector independently. This method can lead to a reduction in both memory usage and search time in some cases. However, product quantization is not as SIMD-friendly, which can make it slower than scalar quantization. Additionally, it typically results in a loss of accuracy, and therefore, it is recommended for use with high-dimensional vectors where this trade-off is acceptable.</td>\n",
       "      <td>&lt;/tr&gt;\\n\\n   &lt;/tbody&gt;\\n\\n&lt;/table&gt;\\n\\n\\n\\nIt turns out that in some cases, Product Quantization may not only reduce the memory usage, \\n\\nbut also the search time.\\n\\n\\n\\n## Good practices\\n\\n\\n\\nCompared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\\n\\n\\n\\nProduct Quantization tends to be favored in certain specific scenarios:\\n\\nBut there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method\\n\\n*Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f31e166b59dfeb33</th>\n",
       "      <td>Tell me about ‚Äòalways_ram‚Äô parameter?</td>\n",
       "      <td>The `always_ram` parameter is a setting that determines whether quantized vectors should be kept cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. If you want to speed up the search process by keeping quantized vectors in RAM, you can set `always_ram` to `true`. This may be particularly useful in some setups where maintaining a high search speed is important and sufficient RAM is available. However, if `always_ram` is set to `false`, the system will not keep the quantized vectors in RAM, which might be preferable in scenarios where RAM usage needs to be minimized.</td>\n",
       "      <td>It might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quantization\\n\\n\"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a7f3f448d6df30fe</th>\n",
       "      <td>What is vaccum optimizer ?</td>\n",
       "      <td>The context provided does not explicitly define what a \"vacuum optimizer\" is. However, it does mention a \"vacuum_min_vector_number\" parameter within an \"optimizer_config\" section, which suggests that it could be related to a threshold setting for optimization processes in a data storage or database system. The \"vacuum_min_vector_number\" parameter is set to 1000, which might indicate the minimum number of vectors required before a vacuuming process is triggered to optimize the storage or clean up the data. Without additional context, a more detailed explanation cannot be provided.</td>\n",
       "      <td>So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan.\\n\\n\\n\\nThe Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached.\\n\\n\\n\\nThe criteria for starting the optimizer are defined in the configuration file.\\n\\n\\n\\nHere is an example of parameter values:\\n\\n\\n\\n```yaml\\n\\nstorage:\\n\\n  optimizers:\\n\\n    # Maximum size (in kilobytes) of vectors to store in-memory per segment.\\n\\nreturn optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.\\n\\n},\\n\\n            \"optimizer_config\": {\\n\\n                \"de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        input  \\\n",
       "context.span_id                                                                 \n",
       "a78cf0e6c332e4e5           What is the impact of ‚Äòwrite_consistency_factor‚Äô ?   \n",
       "6cd1bdd6c96a8b3e           What is significance of ‚Äòon_disk_payload‚Äô setting?   \n",
       "cf8e2f4111865a53                         How do you use ‚Äòordering‚Äô parameter?   \n",
       "7e4b17165e91c3e8                What is the purpose of ef_construct in HNSW ?   \n",
       "5fb2cecc3b2b2d29            What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?   \n",
       "5aa21a981c82f77f                                 How does oversampling helps?   \n",
       "fbe4a08b57e5a424                               What is ‚Äòbest_score‚Äô strategy?   \n",
       "953892bae8ba9fe8  What is difference between scalar and product quantization?   \n",
       "f31e166b59dfeb33                        Tell me about ‚Äòalways_ram‚Äô parameter?   \n",
       "a7f3f448d6df30fe                                   What is vaccum optimizer ?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   output  \\\n",
       "context.span_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "a78cf0e6c332e4e5                                                                                                                                                                                                                                                                                                                                    The `write_consistency_factor` impacts the reliability and consistency of write operations in a distributed system. When this factor is increased, write operations become more tolerant to network partitions, as a higher number of replicas must acknowledge the write before it is considered successful. This can enhance the fault tolerance of the system. However, it also means that a greater number of replicas need to be active and available for a write operation to succeed. If the number of active replicas falls below the `write_consistency_factor`, write operations will fail. This setting can be configured at the time of collection creation and is set to one by default.   \n",
       "6cd1bdd6c96a8b3e                                                                                                                                                                                                                                                                                                                                                             The 'on_disk_payload' setting determines where the payload data is stored for a collection. When set to `true`, it indicates that the payload will not be kept in memory but will instead be stored on disk. This approach is beneficial for managing memory usage, as it can significantly reduce the amount of RAM required, particularly when dealing with large payload values. However, using on-disk storage for payload data may result in slightly increased response times since the data must be read from the disk each time it is requested. Despite this, payload values that are used for filtering and are indexed will still remain in RAM for quick access.   \n",
       "cf8e2f4111865a53  The `ordering` parameter can be used with update and delete operations to ensure that these operations are executed in the same order on all replicas. To use this parameter, you would include it in your request to the API. For example, when performing an update operation, you would send an HTTP PUT request with the `ordering` parameter set to `strong` in the query string. This instructs the system to route the operation to the leader replica of the shard and wait for a response before responding to the client, which helps maintain data consistency in the event of concurrent updates to the same documents. Here is an example of how to use the `ordering` parameter in an HTTP request:\\n\\n```\\nPUT /collections/{collection_name}/points?ordering=strong\\n{\\n    \"batch\": {\\n        \"ids\": [1, 2, 3],\\n        \"payloads\": [\\n            {\"color\": \"red\"},\\n            {\"color\": \"green\"},\\n            {\"color\": \"blue\"}\\n        ],\\n        \"vectors\": [\\n            [0.9, 0.1, 0.1],\\n           ...   \n",
       "7e4b17165e91c3e8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The purpose of `ef_construct` in HNSW is to specify the number of neighbors to consider during the index building process. The larger the value of `ef_construct`, the higher the precision of the index, but this also results in longer indexing times.   \n",
       "5fb2cecc3b2b2d29                                                                                                                                                                                                                                                                                                                                                                                                                                                    The purpose of `CreatePayloadIndexAsync` is to create an index on a specific field within a collection, which in this case is the `group_id` field. This indexing is intended to optimize the search and retrieval of documents based on the values of the indexed field, thereby improving the performance of queries that filter on this field. However, it is noted that while this approach speeds up queries that use the `group_id` filter, global requests that do not use this filter may be slower, as they would require scanning all groups to find the nearest neighbors.   \n",
       "5aa21a981c82f77f                                                                                                                                                                                                                                                                                                                                               Oversampling helps by improving the accuracy of processes such as similarity search algorithms and modeling. In the context of similarity search algorithms, it allows for the compression of high-dimensional vectors in memory while compensating for accuracy loss by re-scoring additional points with the original vectors. This is achieved by setting a factor that multiplies the limit during the search, and then the results are re-scored with the original vector before selecting the top results. In the context of training datasets, oversampling helps to equalize the representation of classes, which enables more fair and accurate modeling of real-world scenarios.   \n",
       "fbe4a08b57e5a424                                                                                                                                                                                                                                                              The 'best_score' strategy is a method introduced in version 1.6.0 for finding similar vectors by comparing each candidate against every positive and negative example. It selects the best positive and best negative scores for each candidate, and the final score is determined using a step formula. If the best positive score is greater than the best negative score, the final score is the best positive score. Otherwise, the final score is the negative of the square of the best negative score. This strategy does not use a single query vector and is more flexible than average-based methods, allowing for the use of only negative samples if desired. The performance of the 'best_score' strategy is linearly impacted by the number of examples used.   \n",
       "953892bae8ba9fe8                                                                       Scalar quantization is a compression technique that reduces the number of bits used to represent each component of a vector. For example, it can convert 32-bit floating numbers into 8-bit unsigned integers for each vector component. This method is straightforward and can be SIMD-friendly, which means it can be optimized for fast computation on modern CPUs.\\n\\nProduct quantization, on the other hand, offers a higher compression rate compared to scalar quantization. It works by dividing the vector into smaller sub-vectors and quantizing each sub-vector independently. This method can lead to a reduction in both memory usage and search time in some cases. However, product quantization is not as SIMD-friendly, which can make it slower than scalar quantization. Additionally, it typically results in a loss of accuracy, and therefore, it is recommended for use with high-dimensional vectors where this trade-off is acceptable.   \n",
       "f31e166b59dfeb33                                                                                                                                                                                                                                                                                                                                                                                         The `always_ram` parameter is a setting that determines whether quantized vectors should be kept cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. If you want to speed up the search process by keeping quantized vectors in RAM, you can set `always_ram` to `true`. This may be particularly useful in some setups where maintaining a high search speed is important and sufficient RAM is available. However, if `always_ram` is set to `false`, the system will not keep the quantized vectors in RAM, which might be preferable in scenarios where RAM usage needs to be minimized.   \n",
       "a7f3f448d6df30fe                                                                                                                                                                                                                                                                                                                                                                                                                               The context provided does not explicitly define what a \"vacuum optimizer\" is. However, it does mention a \"vacuum_min_vector_number\" parameter within an \"optimizer_config\" section, which suggests that it could be related to a threshold setting for optimization processes in a data storage or database system. The \"vacuum_min_vector_number\" parameter is set to 1000, which might indicate the minimum number of vectors required before a vacuuming process is triggered to optimize the storage or clean up the data. Without additional context, a more detailed explanation cannot be provided.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                reference  \n",
       "context.span_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "a78cf0e6c332e4e5  ### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\n- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.\\n\\nwriteConsistencyFactor: 2\\n\\n);\\n\\n```\\n\\n\\n\\nWrite operations will fail if the number of active replicas is less than the `write_consistency...  \n",
       "6cd1bdd6c96a8b3e  * `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\\n\\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.\\n\\ntemp_path: null\\n\\n\\n\\n  # If true - point's payload will not be stored in memory.\\n\\n  # It will be re...  \n",
       "cf8e2f4111865a53  Since the `filter` parameter is specified, the search is performed only among those points that satisfy the filter condition.\\n\\nSee details of possible filters and their work in the [filtering](../filtering) section.\\n\\n\\n\\nExample result of this API would be\\n\\n\\n\\n```json\\n\\n{\\n\\n  \"result\": [\\n\\n    { \"id\": 10, \"score\": 0.81 },\\n\\n    { \"id\": 14, \"score\": 0.75 },\\n\\n    { \"id\": 11, \"score\": 0.73 }\\n\\n  ],\\n\\n  \"status\": \"ok\",\\n\\n  \"time\": 0.001\\n\\n}\\n\\n```\\n\\n\\n\\nThe `result` contains ordered by `score` list of found point ids.\\n\\n- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents\\n\\n```http\\n\\nPUT /collections/{collection_name}/poin...  \n",
       "7e4b17165e91c3e8  In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.\\n\\n\\n\\nThe corresponding parameters could be configured in the configuration file:\\n\\n\\n\\n```yaml\\n\\nstorage:\\n\\n  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually\\n\\n  hnsw_index:\\n\\n    # Number of edges per node in the index graph.\\n\\n(\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n    ...  \n",
       "5fb2cecc3b2b2d29  await client.CreatePayloadIndexAsync(collectionName: \"{collection_name}\", fieldName: \"group_id\");\\n\\n```\\n\\n\\n\\n## Limitations\\n\\n\\n\\nOne downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.\\n\\nclient.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};\\n\\n},\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"h...  \n",
       "5aa21a981c82f77f  oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.\\n\\n### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.\\n\\nIO-induced delays.\\n\\n\\n\\nOversampling is a new feature to improve accuracy at the cost of some\\n\\nperformance. It allows setting a factor, which is multiplied with the `limit`\\n\\nwhile doing the search. The results are then re-scored using the original vector\\n\\nand only then the top results up to the limit are selected.\\n\\n\\n\\n## Discussion\\n\\n\\n\\nLooking back, disk IO used to be very serialized; re-positioning read-write...  \n",
       "fbe4a08b57e5a424  ### The new hotness - Best score\\n\\n\\n\\nThe new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative \\n\\nsamples and uses a slightly more sophisticated algorithm under the hood.\\n\\n\\n\\nThe best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point \\n\\nand every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a\\n\\nThis is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative on...  \n",
       "953892bae8ba9fe8  </tr>\\n\\n   </tbody>\\n\\n</table>\\n\\n\\n\\nIt turns out that in some cases, Product Quantization may not only reduce the memory usage, \\n\\nbut also the search time.\\n\\n\\n\\n## Good practices\\n\\n\\n\\nCompared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\\n\\n\\n\\nProduct Quantization tends to be favored in certain specific scenarios:\\n\\nBut there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method\\n\\n*Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in th...  \n",
       "f31e166b59dfeb33  It might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quantization\\n\\n\"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collec...  \n",
       "a7f3f448d6df30fe  So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan.\\n\\n\\n\\nThe Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached.\\n\\n\\n\\nThe criteria for starting the optimizer are defined in the configuration file.\\n\\n\\n\\nHere is an example of parameter values:\\n\\n\\n\\n```yaml\\n\\nstorage:\\n\\n  optimizers:\\n\\n    # Maximum size (in kilobytes) of vectors to store in-memory per segment.\\n\\nreturn optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.\\n\\n},\\n\\n            \"optimizer_config\": {\\n\\n                \"de...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_df_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>input</th>\n",
       "      <th>reference</th>\n",
       "      <th>document_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th>document_position</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">73a070225f5a1fc5</th>\n",
       "      <th>0</th>\n",
       "      <td>1c39f6362c181574f7e25265496182c1</td>\n",
       "      <td>What is the impact of ‚Äòwrite_consistency_factor‚Äô ?</td>\n",
       "      <td>### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python</td>\n",
       "      <td>0.990056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1c39f6362c181574f7e25265496182c1</td>\n",
       "      <td>What is the impact of ‚Äòwrite_consistency_factor‚Äô ?</td>\n",
       "      <td>- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1c39f6362c181574f7e25265496182c1</td>\n",
       "      <td>What is the impact of ‚Äòwrite_consistency_factor‚Äô ?</td>\n",
       "      <td>writeConsistencyFactor: 2\\n\\n);\\n\\n```\\n\\n\\n\\nWrite operations will fail if the number of active replicas is less than the `write_consistency_factor`.\\n\\n\\n\\n### Read consistency\\n\\n\\n\\nRead `consistency` can be specified for most read requests and will ensure that the returned result\\n\\nis consistent across cluster nodes.\\n\\n\\n\\n- `all` will query all nodes and return points, which present on all of them\\n\\n- `majority` will query all nodes and return points, which present on the majority of them</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">12a4efe4893dcac9</th>\n",
       "      <th>0</th>\n",
       "      <td>8319101710762c740d3e520651cf8d89</td>\n",
       "      <td>What is significance of ‚Äòon_disk_payload‚Äô setting?</td>\n",
       "      <td>* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8319101710762c740d3e520651cf8d89</td>\n",
       "      <td>What is significance of ‚Äòon_disk_payload‚Äô setting?</td>\n",
       "      <td>The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.</td>\n",
       "      <td>0.050402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8319101710762c740d3e520651cf8d89</td>\n",
       "      <td>What is significance of ‚Äòon_disk_payload‚Äô setting?</td>\n",
       "      <td>temp_path: null\\n\\n\\n\\n  # If true - point's payload will not be stored in memory.\\n\\n  # It will be read from the disk every time it is requested.\\n\\n  # This setting saves RAM by (slightly) increasing the response time.\\n\\n  # Note: those payload values that are involved in filtering and are indexed - remain in RAM.\\n\\n  on_disk_payload: true\\n\\n\\n\\n  # Maximum number of concurrent updates to shard replicas\\n\\n  # If `null` - maximum concurrency is used.\\n\\n  update_concurrency: null</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">edf6979a8eab585a</th>\n",
       "      <th>0</th>\n",
       "      <td>5cae7e646a62de6c342e4d6c98bfd136</td>\n",
       "      <td>How do you use ‚Äòordering‚Äô parameter?</td>\n",
       "      <td>Since the `filter` parameter is specified, the search is performed only among those points that satisfy the filter condition.\\n\\nSee details of possible filters and their work in the [filtering](../filtering) section.\\n\\n\\n\\nExample result of this API would be\\n\\n\\n\\n```json\\n\\n{\\n\\n  \"result\": [\\n\\n    { \"id\": 10, \"score\": 0.81 },\\n\\n    { \"id\": 14, \"score\": 0.75 },\\n\\n    { \"id\": 11, \"score\": 0.73 }\\n\\n  ],\\n\\n  \"status\": \"ok\",\\n\\n  \"time\": 0.001\\n\\n}\\n\\n```\\n\\n\\n\\nThe `result` contains ordered by `score` list of found point ids.</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5cae7e646a62de6c342e4d6c98bfd136</td>\n",
       "      <td>How do you use ‚Äòordering‚Äô parameter?</td>\n",
       "      <td>- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5cae7e646a62de6c342e4d6c98bfd136</td>\n",
       "      <td>How do you use ‚Äòordering‚Äô parameter?</td>\n",
       "      <td>```http\\n\\nPUT /collections/{collection_name}/points?ordering=strong\\n\\n{\\n\\n    \"batch\": {\\n\\n        \"ids\": [1, 2, 3],\\n\\n        \"payloads\": [\\n\\n            {\"color\": \"red\"},\\n\\n            {\"color\": \"green\"},\\n\\n            {\"color\": \"blue\"}\\n\\n        ],\\n\\n        \"vectors\": [\\n\\n            [0.9, 0.1, 0.1],\\n\\n            [0.1, 0.9, 0.1],\\n\\n            [0.1, 0.1, 0.9]\\n\\n        ]\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nclient.upsert(\\n\\n    collection_name=\"{collection_name}\",\\n\\n    points=models.Batch(\\n\\n        ids=[1, 2, 3],</td>\n",
       "      <td>0.004290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">8899520008f40e42</th>\n",
       "      <th>0</th>\n",
       "      <td>ae397be1bffa24aa0c080d4f1a9c8e2a</td>\n",
       "      <td>What is the purpose of ef_construct in HNSW ?</td>\n",
       "      <td>In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.\\n\\n\\n\\nThe corresponding parameters could be configured in the configuration file:\\n\\n\\n\\n```yaml\\n\\nstorage:\\n\\n  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually\\n\\n  hnsw_index:\\n\\n    # Number of edges per node in the index graph.</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ae397be1bffa24aa0c080d4f1a9c8e2a</td>\n",
       "      <td>What is the purpose of ef_construct in HNSW ?</td>\n",
       "      <td>(\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ae397be1bffa24aa0c080d4f1a9c8e2a</td>\n",
       "      <td>What is the purpose of ef_construct in HNSW ?</td>\n",
       "      <td>The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \\n\\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\\n\\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and</td>\n",
       "      <td>0.044159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">25cdb70f3b96f3fb</th>\n",
       "      <th>0</th>\n",
       "      <td>260594de35afb5d7729788ae2296a314</td>\n",
       "      <td>What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?</td>\n",
       "      <td>await client.CreatePayloadIndexAsync(collectionName: \"{collection_name}\", fieldName: \"group_id\");\\n\\n```\\n\\n\\n\\n## Limitations\\n\\n\\n\\nOne downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>260594de35afb5d7729788ae2296a314</td>\n",
       "      <td>What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?</td>\n",
       "      <td>client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>260594de35afb5d7729788ae2296a314</td>\n",
       "      <td>What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?</td>\n",
       "      <td>},\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"https://your-application-name.fly.dev/.well-known/logo.png\",\\n\\n  \"contact_email\": \"email@domain.com\",\\n\\n  \"legal_info_url\": \"email@domain.com\"\\n\\n}\\n\\n```\\n\\n\\n\\nThat was the last step before running the final command. The command that will deploy \\n\\nthe application on the server:\\n\\n\\n\\n```bash\\n\\nflyctl deploy\\n\\n```</td>\n",
       "      <td>0.024248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0401f5cb6cc1fab1</th>\n",
       "      <th>0</th>\n",
       "      <td>ffeda93e7cca443ccd879ab7c2a3d0a1</td>\n",
       "      <td>How does oversampling helps?</td>\n",
       "      <td>oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.</td>\n",
       "      <td>0.920467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ffeda93e7cca443ccd879ab7c2a3d0a1</td>\n",
       "      <td>How does oversampling helps?</td>\n",
       "      <td>### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ffeda93e7cca443ccd879ab7c2a3d0a1</td>\n",
       "      <td>How does oversampling helps?</td>\n",
       "      <td>IO-induced delays.\\n\\n\\n\\nOversampling is a new feature to improve accuracy at the cost of some\\n\\nperformance. It allows setting a factor, which is multiplied with the `limit`\\n\\nwhile doing the search. The results are then re-scored using the original vector\\n\\nand only then the top results up to the limit are selected.\\n\\n\\n\\n## Discussion\\n\\n\\n\\nLooking back, disk IO used to be very serialized; re-positioning read-write\\n\\nheads on moving platter was a slow and messy business. So the system overhead</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2be45639e90c4e73</th>\n",
       "      <th>0</th>\n",
       "      <td>cd0be542e506598d4e8e6dd711e8f0b8</td>\n",
       "      <td>What is ‚Äòbest_score‚Äô strategy?</td>\n",
       "      <td>### The new hotness - Best score\\n\\n\\n\\nThe new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative \\n\\nsamples and uses a slightly more sophisticated algorithm under the hood.\\n\\n\\n\\nThe best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point \\n\\nand every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cd0be542e506598d4e8e6dd711e8f0b8</td>\n",
       "      <td>What is ‚Äòbest_score‚Äô strategy?</td>\n",
       "      <td>This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cd0be542e506598d4e8e6dd711e8f0b8</td>\n",
       "      <td>What is ‚Äòbest_score‚Äô strategy?</td>\n",
       "      <td>The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\\n\\n\\n\\n```rust\\n\\nlet score = if best_positive_score &gt; best_negative_score {\\n\\n    best_positive_score;\\n\\n} else {\\n\\n    -(best_negative_score * best_negative_score);\\n\\n};\\n\\n```\\n\\n\\n\\n&lt;aside role=\"alert\"&gt;\\n\\nThe performance of &lt;code&gt;best_score&lt;/code&gt; strategy will be linearly impacted by the amount of examples.\\n\\n&lt;/aside&gt;</td>\n",
       "      <td>0.056918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">f62a8dcec289dc9c</th>\n",
       "      <th>0</th>\n",
       "      <td>b1717c3bd977310c7840d48e9a55292d</td>\n",
       "      <td>What is difference between scalar and product quantization?</td>\n",
       "      <td>&lt;/tr&gt;\\n\\n   &lt;/tbody&gt;\\n\\n&lt;/table&gt;\\n\\n\\n\\nIt turns out that in some cases, Product Quantization may not only reduce the memory usage, \\n\\nbut also the search time.\\n\\n\\n\\n## Good practices\\n\\n\\n\\nCompared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\\n\\n\\n\\nProduct Quantization tends to be favored in certain specific scenarios:</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b1717c3bd977310c7840d48e9a55292d</td>\n",
       "      <td>What is difference between scalar and product quantization?</td>\n",
       "      <td>But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b1717c3bd977310c7840d48e9a55292d</td>\n",
       "      <td>What is difference between scalar and product quantization?</td>\n",
       "      <td>*Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -&gt; uint8` conversion for each vector component.</td>\n",
       "      <td>0.049068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">08a9fa1f6c796ad5</th>\n",
       "      <th>0</th>\n",
       "      <td>fb39f1e160120f444c2bf95a6252214b</td>\n",
       "      <td>Tell me about ‚Äòalways_ram‚Äô parameter?</td>\n",
       "      <td>It might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quantization</td>\n",
       "      <td>0.909791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fb39f1e160120f444c2bf95a6252214b</td>\n",
       "      <td>Tell me about ‚Äòalways_ram‚Äô parameter?</td>\n",
       "      <td>\"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fb39f1e160120f444c2bf95a6252214b</td>\n",
       "      <td>Tell me about ‚Äòalways_ram‚Äô parameter?</td>\n",
       "      <td>},\\n\\n    \"optimizers_config\": {\\n\\n        \"memmap_threshold\": 20000\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"always_ram\": true\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nfrom qdrant_client import QdrantClient\\n\\nfrom qdrant_client.http import models\\n\\n\\n\\nclient = QdrantClient(\"localhost\", port=6333)\\n\\n\\n\\nclient.create_collection(\\n\\n    collection_name=\"{collection_name}\",</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1b5780b94ce2ebc3</th>\n",
       "      <th>0</th>\n",
       "      <td>54b379d98a414efc103a4f89cc9ba0b3</td>\n",
       "      <td>What is vaccum optimizer ?</td>\n",
       "      <td>So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan.\\n\\n\\n\\nThe Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached.\\n\\n\\n\\nThe criteria for starting the optimizer are defined in the configuration file.\\n\\n\\n\\nHere is an example of parameter values:\\n\\n\\n\\n```yaml\\n\\nstorage:\\n\\n  optimizers:\\n\\n    # Maximum size (in kilobytes) of vectors to store in-memory per segment.</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54b379d98a414efc103a4f89cc9ba0b3</td>\n",
       "      <td>What is vaccum optimizer ?</td>\n",
       "      <td>return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54b379d98a414efc103a4f89cc9ba0b3</td>\n",
       "      <td>What is vaccum optimizer ?</td>\n",
       "      <td>},\\n\\n            \"optimizer_config\": {\\n\\n                \"deleted_threshold\": 0.2,\\n\\n                \"vacuum_min_vector_number\": 1000,\\n\\n                \"default_segment_number\": 0,\\n\\n                \"max_segment_size\": null,\\n\\n                \"memmap_threshold\": null,\\n\\n                \"indexing_threshold\": 20000,\\n\\n                \"flush_interval_sec\": 5,\\n\\n                \"max_optimization_threads\": 1\\n\\n            },\\n\\n            \"wal_config\": {\\n\\n                \"wal_capacity_mb\": 32,</td>\n",
       "      <td>0.037267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    context.trace_id  \\\n",
       "context.span_id  document_position                                     \n",
       "73a070225f5a1fc5 0                  1c39f6362c181574f7e25265496182c1   \n",
       "                 1                  1c39f6362c181574f7e25265496182c1   \n",
       "                 2                  1c39f6362c181574f7e25265496182c1   \n",
       "12a4efe4893dcac9 0                  8319101710762c740d3e520651cf8d89   \n",
       "                 1                  8319101710762c740d3e520651cf8d89   \n",
       "                 2                  8319101710762c740d3e520651cf8d89   \n",
       "edf6979a8eab585a 0                  5cae7e646a62de6c342e4d6c98bfd136   \n",
       "                 1                  5cae7e646a62de6c342e4d6c98bfd136   \n",
       "                 2                  5cae7e646a62de6c342e4d6c98bfd136   \n",
       "8899520008f40e42 0                  ae397be1bffa24aa0c080d4f1a9c8e2a   \n",
       "                 1                  ae397be1bffa24aa0c080d4f1a9c8e2a   \n",
       "                 2                  ae397be1bffa24aa0c080d4f1a9c8e2a   \n",
       "25cdb70f3b96f3fb 0                  260594de35afb5d7729788ae2296a314   \n",
       "                 1                  260594de35afb5d7729788ae2296a314   \n",
       "                 2                  260594de35afb5d7729788ae2296a314   \n",
       "0401f5cb6cc1fab1 0                  ffeda93e7cca443ccd879ab7c2a3d0a1   \n",
       "                 1                  ffeda93e7cca443ccd879ab7c2a3d0a1   \n",
       "                 2                  ffeda93e7cca443ccd879ab7c2a3d0a1   \n",
       "2be45639e90c4e73 0                  cd0be542e506598d4e8e6dd711e8f0b8   \n",
       "                 1                  cd0be542e506598d4e8e6dd711e8f0b8   \n",
       "                 2                  cd0be542e506598d4e8e6dd711e8f0b8   \n",
       "f62a8dcec289dc9c 0                  b1717c3bd977310c7840d48e9a55292d   \n",
       "                 1                  b1717c3bd977310c7840d48e9a55292d   \n",
       "                 2                  b1717c3bd977310c7840d48e9a55292d   \n",
       "08a9fa1f6c796ad5 0                  fb39f1e160120f444c2bf95a6252214b   \n",
       "                 1                  fb39f1e160120f444c2bf95a6252214b   \n",
       "                 2                  fb39f1e160120f444c2bf95a6252214b   \n",
       "1b5780b94ce2ebc3 0                  54b379d98a414efc103a4f89cc9ba0b3   \n",
       "                 1                  54b379d98a414efc103a4f89cc9ba0b3   \n",
       "                 2                  54b379d98a414efc103a4f89cc9ba0b3   \n",
       "\n",
       "                                                                                          input  \\\n",
       "context.span_id  document_position                                                                \n",
       "73a070225f5a1fc5 0                           What is the impact of ‚Äòwrite_consistency_factor‚Äô ?   \n",
       "                 1                           What is the impact of ‚Äòwrite_consistency_factor‚Äô ?   \n",
       "                 2                           What is the impact of ‚Äòwrite_consistency_factor‚Äô ?   \n",
       "12a4efe4893dcac9 0                           What is significance of ‚Äòon_disk_payload‚Äô setting?   \n",
       "                 1                           What is significance of ‚Äòon_disk_payload‚Äô setting?   \n",
       "                 2                           What is significance of ‚Äòon_disk_payload‚Äô setting?   \n",
       "edf6979a8eab585a 0                                         How do you use ‚Äòordering‚Äô parameter?   \n",
       "                 1                                         How do you use ‚Äòordering‚Äô parameter?   \n",
       "                 2                                         How do you use ‚Äòordering‚Äô parameter?   \n",
       "8899520008f40e42 0                                What is the purpose of ef_construct in HNSW ?   \n",
       "                 1                                What is the purpose of ef_construct in HNSW ?   \n",
       "                 2                                What is the purpose of ef_construct in HNSW ?   \n",
       "25cdb70f3b96f3fb 0                            What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?   \n",
       "                 1                            What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?   \n",
       "                 2                            What is the purpose of ‚ÄòCreatePayloadIndexAsync‚Äô?   \n",
       "0401f5cb6cc1fab1 0                                                 How does oversampling helps?   \n",
       "                 1                                                 How does oversampling helps?   \n",
       "                 2                                                 How does oversampling helps?   \n",
       "2be45639e90c4e73 0                                               What is ‚Äòbest_score‚Äô strategy?   \n",
       "                 1                                               What is ‚Äòbest_score‚Äô strategy?   \n",
       "                 2                                               What is ‚Äòbest_score‚Äô strategy?   \n",
       "f62a8dcec289dc9c 0                  What is difference between scalar and product quantization?   \n",
       "                 1                  What is difference between scalar and product quantization?   \n",
       "                 2                  What is difference between scalar and product quantization?   \n",
       "08a9fa1f6c796ad5 0                                        Tell me about ‚Äòalways_ram‚Äô parameter?   \n",
       "                 1                                        Tell me about ‚Äòalways_ram‚Äô parameter?   \n",
       "                 2                                        Tell me about ‚Äòalways_ram‚Äô parameter?   \n",
       "1b5780b94ce2ebc3 0                                                   What is vaccum optimizer ?   \n",
       "                 1                                                   What is vaccum optimizer ?   \n",
       "                 2                                                   What is vaccum optimizer ?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             reference  \\\n",
       "context.span_id  document_position                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "73a070225f5a1fc5 0                                  ### Write consistency factor\\n\\n\\n\\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\\n\\nIt can be configured at the collection's creation time.\\n\\n\\n\\n```http\\n\\nPUT /collections/{collection_name}\\n\\n{\\n\\n    \"vectors\": {\\n\\n        \"size\": 300,\\n\\n        \"distance\": \"Cosine\"\\n\\n    },\\n\\n    \"shard_number\": 6,\\n\\n    \"replication_factor\": 2,\\n\\n    \"write_consistency_factor\": 2,\\n\\n}\\n\\n```\\n\\n\\n\\n```python   \n",
       "                 1                                                                                                                                                                                                                                                       - `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.   \n",
       "                 2                                                              writeConsistencyFactor: 2\\n\\n);\\n\\n```\\n\\n\\n\\nWrite operations will fail if the number of active replicas is less than the `write_consistency_factor`.\\n\\n\\n\\n### Read consistency\\n\\n\\n\\nRead `consistency` can be specified for most read requests and will ensure that the returned result\\n\\nis consistent across cluster nodes.\\n\\n\\n\\n- `all` will query all nodes and return points, which present on all of them\\n\\n- `majority` will query all nodes and return points, which present on the majority of them   \n",
       "12a4efe4893dcac9 0                                                                                                  * `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\\n\\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\\n\\n\\n\\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).   \n",
       "                 1                                                                                                                                                 The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\\n\\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\\n\\n\\n\\nIn the case of large payload values, it might be better to use OnDisk payload storage.   \n",
       "                 2                                                                          temp_path: null\\n\\n\\n\\n  # If true - point's payload will not be stored in memory.\\n\\n  # It will be read from the disk every time it is requested.\\n\\n  # This setting saves RAM by (slightly) increasing the response time.\\n\\n  # Note: those payload values that are involved in filtering and are indexed - remain in RAM.\\n\\n  on_disk_payload: true\\n\\n\\n\\n  # Maximum number of concurrent updates to shard replicas\\n\\n  # If `null` - maximum concurrency is used.\\n\\n  update_concurrency: null   \n",
       "edf6979a8eab585a 0                           Since the `filter` parameter is specified, the search is performed only among those points that satisfy the filter condition.\\n\\nSee details of possible filters and their work in the [filtering](../filtering) section.\\n\\n\\n\\nExample result of this API would be\\n\\n\\n\\n```json\\n\\n{\\n\\n  \"result\": [\\n\\n    { \"id\": 10, \"score\": 0.81 },\\n\\n    { \"id\": 14, \"score\": 0.75 },\\n\\n    { \"id\": 11, \"score\": 0.73 }\\n\\n  ],\\n\\n  \"status\": \"ok\",\\n\\n  \"time\": 0.001\\n\\n}\\n\\n```\\n\\n\\n\\nThe `result` contains ordered by `score` list of found point ids.   \n",
       "                 1                                                                                                                                                                   - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents   \n",
       "                 2                  ```http\\n\\nPUT /collections/{collection_name}/points?ordering=strong\\n\\n{\\n\\n    \"batch\": {\\n\\n        \"ids\": [1, 2, 3],\\n\\n        \"payloads\": [\\n\\n            {\"color\": \"red\"},\\n\\n            {\"color\": \"green\"},\\n\\n            {\"color\": \"blue\"}\\n\\n        ],\\n\\n        \"vectors\": [\\n\\n            [0.9, 0.1, 0.1],\\n\\n            [0.1, 0.9, 0.1],\\n\\n            [0.1, 0.1, 0.9]\\n\\n        ]\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nclient.upsert(\\n\\n    collection_name=\"{collection_name}\",\\n\\n    points=models.Batch(\\n\\n        ids=[1, 2, 3],   \n",
       "8899520008f40e42 0                                            In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.\\n\\n\\n\\nThe corresponding parameters could be configured in the configuration file:\\n\\n\\n\\n```yaml\\n\\nstorage:\\n\\n  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually\\n\\n  hnsw_index:\\n\\n    # Number of edges per node in the index graph.   \n",
       "                 1                                                                  (\"my_vector\".into()),\\n\\n                        VectorParamsDiff {\\n\\n                            hnsw_config: Some(HnswConfigDiff {\\n\\n                                m: Some(32),\\n\\n                                ef_construct: Some(123),\\n\\n                                ..Default::default()\\n\\n                            }),\\n\\n                            ..Default::default()\\n\\n                        },\\n\\n                    )]),\\n\\n                },\\n\\n            )),\\n\\n        }),   \n",
       "                 2                                                                                                                                              The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \\n\\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\\n\\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and   \n",
       "25cdb70f3b96f3fb 0                                                                                                                                                                                                                                                       await client.CreatePayloadIndexAsync(collectionName: \"{collection_name}\", fieldName: \"group_id\");\\n\\n```\\n\\n\\n\\n## Limitations\\n\\n\\n\\nOne downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.   \n",
       "                 1                                                                                   client.createPayloadIndex(\"{collection_name}\", {\\n\\n  field_name: \"name_of_the_field_to_index\",\\n\\n  field_schema: {\\n\\n    type: \"text\",\\n\\n    tokenizer: \"word\",\\n\\n    min_token_len: 2,\\n\\n    max_token_len: 15,\\n\\n    lowercase: true,\\n\\n  },\\n\\n});\\n\\n```\\n\\n\\n\\n```rust\\n\\nuse qdrant_client::{\\n\\n    client::QdrantClient,\\n\\n    qdrant::{\\n\\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\\n\\n        TokenizerType,\\n\\n    },\\n\\n};   \n",
       "                 2                                                    },\\n\\n  \"api\": {\\n\\n    \"type\": \"openapi\",\\n\\n    \"url\": \"https://your-application-name.fly.dev/.well-known/openapi.yaml\",\\n\\n    \"has_user_authentication\": false\\n\\n  },\\n\\n  \"logo_url\": \"https://your-application-name.fly.dev/.well-known/logo.png\",\\n\\n  \"contact_email\": \"email@domain.com\",\\n\\n  \"legal_info_url\": \"email@domain.com\"\\n\\n}\\n\\n```\\n\\n\\n\\nThat was the last step before running the final command. The command that will deploy \\n\\nthe application on the server:\\n\\n\\n\\n```bash\\n\\nflyctl deploy\\n\\n```   \n",
       "0401f5cb6cc1fab1 0                                                                                                                                                                                                                                                                                                                                                                                                                           oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.   \n",
       "                 1                                                                                                                                                            ### Oversampling for quantization\\n\\n\\n\\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.   \n",
       "                 2                                                        IO-induced delays.\\n\\n\\n\\nOversampling is a new feature to improve accuracy at the cost of some\\n\\nperformance. It allows setting a factor, which is multiplied with the `limit`\\n\\nwhile doing the search. The results are then re-scored using the original vector\\n\\nand only then the top results up to the limit are selected.\\n\\n\\n\\n## Discussion\\n\\n\\n\\nLooking back, disk IO used to be very serialized; re-positioning read-write\\n\\nheads on moving platter was a slow and messy business. So the system overhead   \n",
       "2be45639e90c4e73 0                                                    ### The new hotness - Best score\\n\\n\\n\\nThe new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative \\n\\nsamples and uses a slightly more sophisticated algorithm under the hood.\\n\\n\\n\\nThe best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point \\n\\nand every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a   \n",
       "                 1                                                                                  This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `\"strategy\": \"average_vector\"` in the recommendation request.\\n\\n\\n\\n### Best score strategy\\n\\n\\n\\n*Available as of v1.6.0*\\n\\n\\n\\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.   \n",
       "                 2                                                  The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\\n\\n\\n\\n```rust\\n\\nlet score = if best_positive_score > best_negative_score {\\n\\n    best_positive_score;\\n\\n} else {\\n\\n    -(best_negative_score * best_negative_score);\\n\\n};\\n\\n```\\n\\n\\n\\n<aside role=\"alert\">\\n\\nThe performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.\\n\\n</aside>   \n",
       "f62a8dcec289dc9c 0                                                                                                           </tr>\\n\\n   </tbody>\\n\\n</table>\\n\\n\\n\\nIt turns out that in some cases, Product Quantization may not only reduce the memory usage, \\n\\nbut also the search time.\\n\\n\\n\\n## Good practices\\n\\n\\n\\nCompared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\\n\\n\\n\\nProduct Quantization tends to be favored in certain specific scenarios:   \n",
       "                 1                                                                                           But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\\n\\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\\n\\n\\n\\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\\n\\n\\n\\n## How to choose the right quantization method   \n",
       "                 2                                                                               *Available as of v1.1.0*\\n\\n\\n\\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\\n\\n\\n\\n\\n\\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\\n\\nIn other words, Qdrant performs `float32 -> uint8` conversion for each vector component.   \n",
       "08a9fa1f6c796ad5 0                                                          It might be worth tuning this parameter if you experience a significant decrease in search quality.\\n\\n\\n\\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\\n\\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\\n\\n\\n\\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\\n\\n\\n\\n### Setting up Binary Quantization   \n",
       "                 1                                                                       \"compression\": \"x32\",\\n\\n                    \"always_ram\": true\\n\\n                }\\n\\n            },\\n\\n            \"on_disk\": true\\n\\n        }\\n\\n    },\\n\\n    \"hnsw_config\": {\\n\\n        \"ef_construct\": 123\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"quantile\": 0.8,\\n\\n            \"always_ram\": false\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```bash\\n\\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\   \n",
       "                 2                                                                                                     },\\n\\n    \"optimizers_config\": {\\n\\n        \"memmap_threshold\": 20000\\n\\n    },\\n\\n    \"quantization_config\": {\\n\\n        \"scalar\": {\\n\\n            \"type\": \"int8\",\\n\\n            \"always_ram\": true\\n\\n        }\\n\\n    }\\n\\n}\\n\\n```\\n\\n\\n\\n```python\\n\\nfrom qdrant_client import QdrantClient\\n\\nfrom qdrant_client.http import models\\n\\n\\n\\nclient = QdrantClient(\"localhost\", port=6333)\\n\\n\\n\\nclient.create_collection(\\n\\n    collection_name=\"{collection_name}\",   \n",
       "1b5780b94ce2ebc3 0                                         So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan.\\n\\n\\n\\nThe Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached.\\n\\n\\n\\nThe criteria for starting the optimizer are defined in the configuration file.\\n\\n\\n\\nHere is an example of parameter values:\\n\\n\\n\\n```yaml\\n\\nstorage:\\n\\n  optimizers:\\n\\n    # Maximum size (in kilobytes) of vectors to store in-memory per segment.   \n",
       "                 1                                                                                                                                                                 return optimizer\\n\\n```\\n\\n\\n\\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\\n\\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\\n\\nIt provides both a considerable speedup and less memory footprint.\\n\\nHowever, it is quite a bit versatile and has several knobs to tune.   \n",
       "                 2                                                         },\\n\\n            \"optimizer_config\": {\\n\\n                \"deleted_threshold\": 0.2,\\n\\n                \"vacuum_min_vector_number\": 1000,\\n\\n                \"default_segment_number\": 0,\\n\\n                \"max_segment_size\": null,\\n\\n                \"memmap_threshold\": null,\\n\\n                \"indexing_threshold\": 20000,\\n\\n                \"flush_interval_sec\": 5,\\n\\n                \"max_optimization_threads\": 1\\n\\n            },\\n\\n            \"wal_config\": {\\n\\n                \"wal_capacity_mb\": 32,   \n",
       "\n",
       "                                    document_score  \n",
       "context.span_id  document_position                  \n",
       "73a070225f5a1fc5 0                        0.990056  \n",
       "                 1                        0.100000  \n",
       "                 2                        0.000000  \n",
       "12a4efe4893dcac9 0                        1.000000  \n",
       "                 1                        0.050402  \n",
       "                 2                        0.000000  \n",
       "edf6979a8eab585a 0                        0.900000  \n",
       "                 1                        0.100000  \n",
       "                 2                        0.004290  \n",
       "8899520008f40e42 0                        0.900000  \n",
       "                 1                        0.100000  \n",
       "                 2                        0.044159  \n",
       "25cdb70f3b96f3fb 0                        0.900000  \n",
       "                 1                        0.100000  \n",
       "                 2                        0.024248  \n",
       "0401f5cb6cc1fab1 0                        0.920467  \n",
       "                 1                        0.100000  \n",
       "                 2                        0.000000  \n",
       "2be45639e90c4e73 0                        0.900000  \n",
       "                 1                        0.100000  \n",
       "                 2                        0.056918  \n",
       "f62a8dcec289dc9c 0                        0.900000  \n",
       "                 1                        0.100000  \n",
       "                 2                        0.049068  \n",
       "08a9fa1f6c796ad5 0                        0.909791  \n",
       "                 1                        0.100000  \n",
       "                 2                        0.000000  \n",
       "1b5780b94ce2ebc3 0                        0.900000  \n",
       "                 1                        0.100000  \n",
       "                 2                        0.037267  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_documents_df_hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **17. Define your evaluation model and your evaluators for Hybrid Search**\n",
    "\n",
    "Next, define your evaluation model and your evaluators.\n",
    "\n",
    "Evaluators are built on top of language models and prompt the LLM to assess the quality of responses, the relevance of retrieved documents, etc., and provide a quality signal even in the absence of human-labeled data. Pick an evaluator type and instantiate it with the language model you want to use to perform evaluations using our battle-tested evaluation templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561897ab1090452da0916a4c48b2c981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/20 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b842c1f9b0640cab83e6d8755387185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/30 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# all spans created within this context will be associated with the `HYBRID_RAG_PROJECT` project.\n",
    "eval_model = OpenAIModel(\n",
    "    model=\"gpt-4-turbo-preview\",\n",
    ")\n",
    "hallucination_evaluator = HallucinationEvaluator(eval_model)\n",
    "qa_correctness_evaluator = QAEvaluator(eval_model)\n",
    "relevance_evaluator = RelevanceEvaluator(eval_model)\n",
    "\n",
    "hallucination_eval_df_hybrid, qa_correctness_eval_df_hybrid = run_evals(\n",
    "    dataframe=queries_df_hybrid,\n",
    "    evaluators=[hallucination_evaluator, qa_correctness_evaluator],\n",
    "    provide_explanation=True,\n",
    ")\n",
    "relevance_eval_df_hybrid = run_evals(\n",
    "    dataframe=retrieved_documents_df_hybrid,\n",
    "    evaluators=[relevance_evaluator],\n",
    "    provide_explanation=True,\n",
    ")[0]\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df_hybrid),\n",
    "    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval_df_hybrid),\n",
    "    project_name=HYBRID_RAG_PROJECT,\n",
    ")\n",
    "px.Client().log_evaluations(DocumentEvaluations(eval_name=\"Relevance\", dataframe=relevance_eval_df_hybrid),\n",
    "                            project_name=HYBRID_RAG_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "003c1412566441b2a0fd878aabed07a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "01d4ff90aac943ff83bfe49b70c783ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8d9844dfef3f414aa750e6200a62bfd3",
       "IPY_MODEL_57d8cc8ff7f047c19301f0793367b2ae",
       "IPY_MODEL_4b23a2eb73b6435384686ced47f9c7f6"
      ],
      "layout": "IPY_MODEL_6fc5bf4070d94eccbdcb4fad4247c758"
     }
    },
    "02ca3a5d43e343d5b756fca172bc1822": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f1e6788ea4344553adbe66202caa656c",
       "IPY_MODEL_5a3d5d83788446028f4de4afbd262f3e",
       "IPY_MODEL_bf06bcbdbff3485abb3895dc54256e00"
      ],
      "layout": "IPY_MODEL_1ff9ec98314944bdb86d4fda42a4c88b"
     }
    },
    "0685ef29e4c54e2e8d5c424508ebfe82": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "080a6d0c05144698a9c7366e54bb39b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65f86e93b0fe48ddbaf2cabc9975c68b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8e261a6071cd41a6afd4377c9ead98ca",
      "value": "‚Äá35/35‚Äá(100.0%)‚Äá|‚Äá‚è≥‚Äá00:37&lt;00:00‚Äá|‚Äá‚Äá1.72it/s"
     }
    },
    "0a8c4c23041641308b625e929790a55e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c90d55a6f6d4d3f8c72bd0ba19687ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be68eca0c7b640abada072a3cc3cd042",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6eadc3109208449eb9cb7ad5b6a5bb46",
      "value": "‚Äá125k/125k‚Äá[00:01&lt;00:00,‚Äá84.2kB/s]"
     }
    },
    "0d229e81fcb246cf99dd919d06cf5905": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1057e11184c44ea38154a2ceabd90198": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4722676ae36d4749918f5edaac48b182",
       "IPY_MODEL_a2041da6fb5f499191f405fa891c0907",
       "IPY_MODEL_5c26103d1d89466c9d22960bb347db39"
      ],
      "layout": "IPY_MODEL_0685ef29e4c54e2e8d5c424508ebfe82"
     }
    },
    "10d5d30b4fd14f3ba45da1794607c5e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "128b89e88c684401818304e8410b6c33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1677f41c03a74a9f8ac55c49a9c12799": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bda9ff3e5071497c933f31a2dd44b449",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_b6686121f3084e1a8c6e0399a8a72675",
      "value": "Generating‚Äáembeddings:‚Äá100%"
     }
    },
    "178bbf06a9ae4b52baa5b731c0c81590": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_003c1412566441b2a0fd878aabed07a6",
      "max": 4431,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cf1c5baa009c467a8c76a1a78d6987b1",
      "value": 4431
     }
    },
    "18ee1349c9394dab81163d5de23a04a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ad21fe66fac4742beb7c17ba8f8733f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1677f41c03a74a9f8ac55c49a9c12799",
       "IPY_MODEL_5b8040a4238f41128006185a858db466",
       "IPY_MODEL_7fbfaf28e7134c638b3e4d16e3cb3e6c"
      ],
      "layout": "IPY_MODEL_1f1e8f2a35b645eb91e96a847cf2856b"
     }
    },
    "1e846da2f0c44e099d24c350e0b0eb1f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f1e8f2a35b645eb91e96a847cf2856b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f3117c63e8d4558b801a33e9b5c548a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ff9ec98314944bdb86d4fda42a4c88b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "218ea3901010405a95033a6cb632250a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21e41bb05ed34b02b2e9e1c2d7d56145": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "22c94d8ddc9d4561930044518630bb0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18ee1349c9394dab81163d5de23a04a8",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_21e41bb05ed34b02b2e9e1c2d7d56145",
      "value": "‚Äá4431/4431‚Äá[00:02&lt;00:00,‚Äá2155.10it/s]"
     }
    },
    "251b1e790043469393aae0c8ebec3d77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "280b4a0048134a8aaf523deead4e3aef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "366e10ebb46a45508d837c8ead93124c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a222b53b0b2448a97b23e2b782a37b5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_99f0a6ea567247639e7b17af1e1df9cf",
      "value": "Generating‚Äáembeddings:‚Äá100%"
     }
    },
    "37a8add4c6a242a49ea09ebef29cfdd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b993aeba0d0f4021b4ae31b8378af903",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_cfbcd04f19374fe6b8ab9802f6891612",
      "value": "‚Äá240/0‚Äá[00:00&lt;00:00,‚Äá2042.22‚Äáexamples/s]"
     }
    },
    "38d701e482684695b97bd73bac78ccfc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43dfd4070e324858974c08fbcb8055fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "43e63c6009c14d9cb4eee3ea177d1c7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f997d5f3659421981252a05845dd236",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_af7527216b9f4240b36024d07ac5c507",
      "value": "‚Äá43.0/43.0‚Äá[00:00&lt;00:00,‚Äá946B/s]"
     }
    },
    "44cb1b5c1c0a446f9d308d93d33ae4f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50d006eafd294bbab3339c2bece90673",
      "max": 335,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b21c626648d643d4972c76a094391134",
      "value": 335
     }
    },
    "4722676ae36d4749918f5edaac48b182": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba3888c2a5954fb29293f41577a5bcae",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f7b5096a4b944d62a281ee927a1ce2e4",
      "value": "Generating‚Äátrain‚Äásplit:‚Äá"
     }
    },
    "4ada07ba27b14c1a8c071343cc21c9c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b23a2eb73b6435384686ced47f9c7f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9fa5a2efb1142c9abd47fe46f05cee6",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_56dc39917eec4eba8194bbe0a3e91de1",
      "value": "‚Äá43.0/43.0‚Äá[00:00&lt;00:00,‚Äá1.71kB/s]"
     }
    },
    "4cc3f02b7bb44a87b8d50dd2f98edce7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "503e9200b403425b8c0379b0e74b01fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50d006eafd294bbab3339c2bece90673": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54aa6aff06214fe4baa3e621ec306c04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "56dc39917eec4eba8194bbe0a3e91de1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57d8cc8ff7f047c19301f0793367b2ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72ce212a2baa494a8a9ae439c7c3e742",
      "max": 43,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f61b254bd9e24480b0176b43c1c7f47e",
      "value": 43
     }
    },
    "5a3d5d83788446028f4de4afbd262f3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cc3f02b7bb44a87b8d50dd2f98edce7",
      "max": 1777260,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e07726709c7748eeb708dbe5c37f9770",
      "value": 1777260
     }
    },
    "5b8040a4238f41128006185a858db466": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_503e9200b403425b8c0379b0e74b01fe",
      "max": 2048,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6f6e8b35568945dc85ba3e47f7b0f3d7",
      "value": 2048
     }
    },
    "5c26103d1d89466c9d22960bb347db39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ada07ba27b14c1a8c071343cc21c9c4",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_98d7cd89900f44a79893e7d8d6b8b0c7",
      "value": "‚Äá81/0‚Äá[00:00&lt;00:00,‚Äá1674.70‚Äáexamples/s]"
     }
    },
    "5c65981c04f34589a36c5c8bb4195f2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5cbb12c59ec948ba8d66dc3812d5c846": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "602df92e53334c65a2cc171301c67e46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "65f86e93b0fe48ddbaf2cabc9975c68b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66588b1f23bb421db7dbc90b2298a845": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "68ac22f645934827a9627e3559067ba1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f3c5e97c2ac94cafa20b130f1f45ce67",
       "IPY_MODEL_b505d5e7978d4b19b59dc8f3fe71e795",
       "IPY_MODEL_080a6d0c05144698a9c7366e54bb39b0"
      ],
      "layout": "IPY_MODEL_e69da775e4df4ed8a5f5ed1348278ab8"
     }
    },
    "6a222b53b0b2448a97b23e2b782a37b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b115bdeaeb547e6b96a5a440b2fe3ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6eadc3109208449eb9cb7ad5b6a5bb46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6f6e8b35568945dc85ba3e47f7b0f3d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6fc5bf4070d94eccbdcb4fad4247c758": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ff26418624f4662871a69796f67427a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72ce212a2baa494a8a9ae439c7c3e742": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77d79ac84c8c4d8393fcdb7f304b4bf3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79335cc0bf85458ba630ced71d706b2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_251b1e790043469393aae0c8ebec3d77",
      "max": 43,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_602df92e53334c65a2cc171301c67e46",
      "value": 43
     }
    },
    "7b8b3e1c571a4345bc2ec4eb00f0967e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a74d6dce715f4494874de92852b56c09",
       "IPY_MODEL_a5d80d3579ca481a98310bd08b8c6918",
       "IPY_MODEL_f07a59b5bc7a4f56baa892ca864d488c"
      ],
      "layout": "IPY_MODEL_84fd76a737334831a886755affb50886"
     }
    },
    "7e8c241b51af4963aa78d91e6e78136e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7fbfaf28e7134c638b3e4d16e3cb3e6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c47e7205eb6540a8a5c51120cd222fe3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_ea74a3a2d06a4c61b85db3c7552535a1",
      "value": "‚Äá2048/2048‚Äá[00:20&lt;00:00,‚Äá120.27it/s]"
     }
    },
    "8115b0b37d164080b58368be3fadbaa0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84fd76a737334831a886755affb50886": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "869c8dd95cf7452b841f7203e0e6b8c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d9844dfef3f414aa750e6200a62bfd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_218ea3901010405a95033a6cb632250a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8ef997a7dbf54d3d9cfa8147e1611cbd",
      "value": "Downloading‚Äáreadme:‚Äá100%"
     }
    },
    "8e261a6071cd41a6afd4377c9ead98ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8ef997a7dbf54d3d9cfa8147e1611cbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f997d5f3659421981252a05845dd236": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90f264886e864ddabdb6eddbecf6d75a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "924c8fe0a9cb4feab39d31ec358089b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98d7cd89900f44a79893e7d8d6b8b0c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "997305a8d871401ba6a1e805dc1ca045": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "99f0a6ea567247639e7b17af1e1df9cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f88771edbfd468cb315a54b69eb7226": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1c3867b1d404a948a0edd0e6bc9a1b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff9f4b2c32234207acb5221babe9c061",
      "max": 124978,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d51b6ff6e830489f8e1e100bf1f1ae98",
      "value": 124978
     }
    },
    "a2041da6fb5f499191f405fa891c0907": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54aa6aff06214fe4baa3e621ec306c04",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_66588b1f23bb421db7dbc90b2298a845",
      "value": 1
     }
    },
    "a5d80d3579ca481a98310bd08b8c6918": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce0efd7352cd4539bda8706ebbc146fd",
      "max": 22,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_128b89e88c684401818304e8410b6c33",
      "value": 22
     }
    },
    "a74d6dce715f4494874de92852b56c09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c65981c04f34589a36c5c8bb4195f2b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_10d5d30b4fd14f3ba45da1794607c5e0",
      "value": "run_evals‚Äá"
     }
    },
    "aa8db2c8310b4f9582f2002adf3ffa1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b84c10312dbe486fb3b21c7c5e23d603",
       "IPY_MODEL_178bbf06a9ae4b52baa5b731c0c81590",
       "IPY_MODEL_22c94d8ddc9d4561930044518630bb0b"
      ],
      "layout": "IPY_MODEL_acf3351a0fba41558f97945fbc6b84c7"
     }
    },
    "ac507e1d00b446df9379fa58f39418fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c85db136dc2843a58e7aa648bbf9f9fe",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5cbb12c59ec948ba8d66dc3812d5c846",
      "value": "Generating‚Äátrain‚Äásplit:‚Äá"
     }
    },
    "acf3351a0fba41558f97945fbc6b84c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "adc2166e9b7d4ebdb5dd5415fd3e2ec5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "af7527216b9f4240b36024d07ac5c507": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af77bbc36b844afa960381389111bccf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af7d2403f4504d49a027d935b13d3f62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ff26418624f4662871a69796f67427a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d5e93af9e5644c419bc653af2057e285",
      "value": "Downloading‚Äáreadme:‚Äá100%"
     }
    },
    "b21c626648d643d4972c76a094391134": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b3dc75c3ec7a48ceaff045c81f250bfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b44f529954e944729b7446a4d610ac1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b505d5e7978d4b19b59dc8f3fe71e795": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38d701e482684695b97bd73bac78ccfc",
      "max": 35,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e7d21aa7bce145b78e834897b158f9b0",
      "value": 35
     }
    },
    "b51b292bc0b341de8fd94707e7fa2c0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8749c8627c44757aa8703b45421af2d",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_9f88771edbfd468cb315a54b69eb7226",
      "value": "Generating‚Äáembeddings:‚Äá100%"
     }
    },
    "b6686121f3084e1a8c6e0399a8a72675": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b84c10312dbe486fb3b21c7c5e23d603": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f3117c63e8d4558b801a33e9b5c548a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_997305a8d871401ba6a1e805dc1ca045",
      "value": "Parsing‚Äánodes:‚Äá100%"
     }
    },
    "b993aeba0d0f4021b4ae31b8378af903": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba3888c2a5954fb29293f41577a5bcae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bda9ff3e5071497c933f31a2dd44b449": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be68eca0c7b640abada072a3cc3cd042": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf06bcbdbff3485abb3895dc54256e00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8115b0b37d164080b58368be3fadbaa0",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_0d229e81fcb246cf99dd919d06cf5905",
      "value": "‚Äá1.78M/1.78M‚Äá[00:00&lt;00:00,‚Äá4.56MB/s]"
     }
    },
    "c30b902e82de486eb0c10a3e64687b44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3a80fdcb9944e62ba5b51838fef5ef4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_af7d2403f4504d49a027d935b13d3f62",
       "IPY_MODEL_79335cc0bf85458ba630ced71d706b2e",
       "IPY_MODEL_43e63c6009c14d9cb4eee3ea177d1c7d"
      ],
      "layout": "IPY_MODEL_dce488c3561e4d7fa737379553ec0b3b"
     }
    },
    "c47e7205eb6540a8a5c51120cd222fe3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c762c26d4f2e4159b714ad55bbd195eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b51b292bc0b341de8fd94707e7fa2c0c",
       "IPY_MODEL_f10f2f679d7a4ab8988601cd59aa4b70",
       "IPY_MODEL_d5baee84346c4dc29ffa4fd2fe6495c6"
      ],
      "layout": "IPY_MODEL_0a8c4c23041641308b625e929790a55e"
     }
    },
    "c85db136dc2843a58e7aa648bbf9f9fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c95d4cfffece443182faa3e2a3e48dda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df997aad597e4c1ab23501d12383e39b",
       "IPY_MODEL_a1c3867b1d404a948a0edd0e6bc9a1b8",
       "IPY_MODEL_0c90d55a6f6d4d3f8c72bd0ba19687ba"
      ],
      "layout": "IPY_MODEL_1e846da2f0c44e099d24c350e0b0eb1f"
     }
    },
    "ce0efd7352cd4539bda8706ebbc146fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf1c5baa009c467a8c76a1a78d6987b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cfbcd04f19374fe6b8ab9802f6891612": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cff10cfd18704a9cb7c9b4d4ba5ccaef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d44a32f577204732a7f4ce767d8e83f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d51b6ff6e830489f8e1e100bf1f1ae98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d5baee84346c4dc29ffa4fd2fe6495c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_280b4a0048134a8aaf523deead4e3aef",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_869c8dd95cf7452b841f7203e0e6b8c5",
      "value": "‚Äá2048/2048‚Äá[00:19&lt;00:00,‚Äá119.05it/s]"
     }
    },
    "d5e93af9e5644c419bc653af2057e285": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d772ecb46ca54adbbb84749fe07ab735": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8749c8627c44757aa8703b45421af2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9fa5a2efb1142c9abd47fe46f05cee6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dce488c3561e4d7fa737379553ec0b3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df997aad597e4c1ab23501d12383e39b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_924c8fe0a9cb4feab39d31ec358089b8",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_cff10cfd18704a9cb7c9b4d4ba5ccaef",
      "value": "Downloading‚Äádata:‚Äá100%"
     }
    },
    "e07726709c7748eeb708dbe5c37f9770": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e69da775e4df4ed8a5f5ed1348278ab8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7d21aa7bce145b78e834897b158f9b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e7d4e009f37a4d798dbc23b39ed7c0b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea74a3a2d06a4c61b85db3c7552535a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb848aa889474b118c49a7cdac509d9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adc2166e9b7d4ebdb5dd5415fd3e2ec5",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_90f264886e864ddabdb6eddbecf6d75a",
      "value": 1
     }
    },
    "eecec2323e294d6fb4ee89f5937c241c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ac507e1d00b446df9379fa58f39418fc",
       "IPY_MODEL_eb848aa889474b118c49a7cdac509d9b",
       "IPY_MODEL_37a8add4c6a242a49ea09ebef29cfdd9"
      ],
      "layout": "IPY_MODEL_b44f529954e944729b7446a4d610ac1d"
     }
    },
    "f07a59b5bc7a4f56baa892ca864d488c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7d4e009f37a4d798dbc23b39ed7c0b1",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d772ecb46ca54adbbb84749fe07ab735",
      "value": "‚Äá22/22‚Äá(100.0%)‚Äá|‚Äá‚è≥‚Äá00:52&lt;00:00‚Äá|‚Äá‚Äá2.31it/s"
     }
    },
    "f10f2f679d7a4ab8988601cd59aa4b70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af77bbc36b844afa960381389111bccf",
      "max": 2048,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b3dc75c3ec7a48ceaff045c81f250bfd",
      "value": 2048
     }
    },
    "f1e6788ea4344553adbe66202caa656c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9e47bbf438740a2a224c5f7d913e502",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c30b902e82de486eb0c10a3e64687b44",
      "value": "Downloading‚Äádata:‚Äá100%"
     }
    },
    "f3c5e97c2ac94cafa20b130f1f45ce67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77d79ac84c8c4d8393fcdb7f304b4bf3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_43dfd4070e324858974c08fbcb8055fd",
      "value": "run_evals‚Äá"
     }
    },
    "f61b254bd9e24480b0176b43c1c7f47e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f7b5096a4b944d62a281ee927a1ce2e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9e47bbf438740a2a224c5f7d913e502": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa26918ac71440c1803c82c939fbc79f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_366e10ebb46a45508d837c8ead93124c",
       "IPY_MODEL_44cb1b5c1c0a446f9d308d93d33ae4f8",
       "IPY_MODEL_fca056b08fc3427a977bc3717a020443"
      ],
      "layout": "IPY_MODEL_7e8c241b51af4963aa78d91e6e78136e"
     }
    },
    "fca056b08fc3427a977bc3717a020443": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d44a32f577204732a7f4ce767d8e83f5",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6b115bdeaeb547e6b96a5a440b2fe3ef",
      "value": "‚Äá335/335‚Äá[00:05&lt;00:00,‚Äá59.78it/s]"
     }
    },
    "ff9f4b2c32234207acb5221babe9c061": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
