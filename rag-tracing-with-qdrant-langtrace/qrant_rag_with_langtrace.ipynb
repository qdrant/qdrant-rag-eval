{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG system with Qdrant and Langtrace\n",
        "\n",
        "This notebook demonstrates how to implement a Retrieval-Augmented Generation (RAG) system using Qdrant as the vector database and Langtrace for tracing operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preqs:\n",
        "* Sign-up for a free account on [Langtrace](https://langtrace.ai).\n",
        "* Create a project then generate an API key. Refer to [quickstart documentation](https://docs.langtrace.ai/quickstart) for help. \n",
        "* You will also need an OpenAI API Key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Install the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install -Uq qdrant-client openai fastembed langtrace-python-sdk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, let's import the necessary libraries and initialize our clients. Be sure to update your OpenAI API Key and Langtrace API Key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import openai\n",
        "from qdrant_client import QdrantClient\n",
        "from langtrace_python_sdk import langtrace, with_langtrace_root_span\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Initialize environment and clients\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_key\"  # Replace with your actual OpenAI API key\n",
        "langtrace.init(api_key='your_langtrace_api_key')  # Replace with your actual Langtrace API key\n",
        "qdrant_client = QdrantClient(\":memory:\") \n",
        "openai_client = openai.Client(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "print(\"Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementing RAG Components\n",
        "\n",
        "Now, let's implement the core components of our RAG system:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@with_langtrace_root_span(\"initialize_knowledge_base\")\n",
        "def initialize_knowledge_base(documents: List[str]) -> None:\n",
        "    qdrant_client.add(\n",
        "        collection_name=\"knowledge-base\",\n",
        "        documents=documents\n",
        "    )\n",
        "    print(f\"Knowledge base initialized with {len(documents)} documents \")\n",
        "\n",
        "@with_langtrace_root_span(\"query_vector_db\")\n",
        "def query_vector_db(question: str, n_points: int = 3) -> List[Dict[str, Any]]:\n",
        "    results = qdrant_client.query(\n",
        "        collection_name=\"knowledge-base\",\n",
        "        query_text=question,\n",
        "        limit=n_points,\n",
        "    )\n",
        "    print(f\"Vector DB queried, returned {len(results)} results\")\n",
        "    return results\n",
        "\n",
        "@with_langtrace_root_span(\"generate_llm_response\")\n",
        "def generate_llm_response(prompt: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
        "    completion = openai_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        timeout=10.0,\n",
        "    )\n",
        "    response = completion.choices[0].message.content\n",
        "    print(f\"LLM response generated\")\n",
        "    return response\n",
        "\n",
        "@with_langtrace_root_span(\"rag\")\n",
        "def rag(question: str, n_points: int = 3) -> str:\n",
        "    print(f\"Processing RAG for question: {question}\")\n",
        "    \n",
        "    results = query_vector_db(question, n_points)\n",
        "    context = \"\\n\".join([r.document for r in results]) if results else \"No relevant information found.\"\n",
        "    print(f\"Context retrieved\")\n",
        "    \n",
        "    metaprompt = f\"\"\"\n",
        "    You are a software architect.\n",
        "    Answer the following question using the provided context.\n",
        "    If you can't find the answer in the context, respond with \"I don't have enough information to answer this question.\"\n",
        "\n",
        "    Question: {question.strip()}\n",
        "\n",
        "    Context:\n",
        "    {context.strip()}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    print(f\"Prompt constructed\")\n",
        "    \n",
        "    answer = generate_llm_response(metaprompt)\n",
        "    print(f\"RAG completed, answer length: {len(answer)} characters\")\n",
        "    return answer\n",
        "\n",
        "print(\"RAG components defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initializing the Knowledge Base\n",
        "\n",
        "Let's initialize our knowledge base with some sample documents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents = [\n",
        "    \"Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\",\n",
        "    \"Docker helps developers build, share, and run applications anywhere â€” without tedious environment configuration or management.\",\n",
        "    \"PyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.\",\n",
        "    \"MySQL is an open-source relational database management system (RDBMS). A relational database organizes data into one or more data tables in which data may be related to each other; these relations help structure the data. SQL is a language that programmers use to create, modify and extract data from the relational database, as well as control user access to the database.\",\n",
        "    \"NGINX is a free, open-source, high-performance HTTP server and reverse proxy, as well as an IMAP/POP3 proxy server. NGINX is known for its high performance, stability, rich feature set, simple configuration, and low resource consumption.\",\n",
        "    \"FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.\",\n",
        "    \"SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.\",\n",
        "    \"The cron command-line utility is a job scheduler on Unix-like operating systems. Users who set up and maintain software environments use cron to schedule jobs (commands or shell scripts), also known as cron jobs, to run periodically at fixed times, dates, or intervals.\",\n",
        "]\n",
        "\n",
        "initialize_knowledge_base(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing the RAG System\n",
        "\n",
        "Now, let's test our RAG system with a few sample questions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def demonstrate_different_queries():\n",
        "    questions = [\n",
        "        \"What is Qdrant used for?\",\n",
        "        \"How does Docker help developers?\",\n",
        "        \"What is the purpose of MySQL?\",\n",
        "        \"Can you explain what FastAPI is?\",\n",
        "        \"What is FIFPRO Award?\"\n",
        "    ]\n",
        "    for question in questions:\n",
        "        try:\n",
        "            answer = rag(question)\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Answer: {answer}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question '{question}': {str(e)}\\n\")\n",
        "\n",
        "demonstrate_different_queries()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyzing Traces\n",
        "\n",
        "After running our RAG system, we can analyze the traces in the Langtrace dashboard. Here's what to look for:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "![Alt text](./assets/langtraceqdrant.png \"a trace view in Langtrace\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Analyzing traces:\")\n",
        "print(\"1. Check the Langtrace dashboard for a visual representation of the traces.\")\n",
        "print(\"2. Look for the 'rag' root span and its child spans to understand the flow.\")\n",
        "print(\"3. Examine the timing information shown for each operation to identify potential bottlenecks.\")\n",
        "print(\"4. Review any error messages printed to understand and address issues.\")\n",
        "print(\"5. Pay attention to questions like 'What is FIFPRO Award?' to see how the system handles queries without relevant information in the knowledge base.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
