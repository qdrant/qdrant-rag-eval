success,failure_reason,Completion Time (ms),content.id,content.input_text,content.answer,content.completion,content.context,content.documents,content.formatted_content,content.raw_content.input_text,content.raw_content.answer,content.raw_content.documents,content.raw_content.context,rouge1_precision,rouge1_recall,rouge1_fmeasure,rouge2_precision,rouge2_recall,rouge2_fmeasure,rougeL_precision,rougeL_recall,ROUGE-L,rougeLsum_precision,rougeLsum_recall,rougeLsum_fmeasure,BERT Sentence Similarity,bert_score_precision,bert_score_recall,BERTScore,Faithfulness,Context Relevance,Completion Verbosity,Verbosity Ratio,Chunk Relevance
True,,2493.138,a4c01a93,What is vaccum optimizer ?,"The Vacuum Optimizer is a feature in the Qdrant database designed to manage and improve system performance by removing obsolete data. In Qdrant, when records are deleted, they are not immediately removed from the database. Instead, they are marked as deleted and subsequently ignored in future queries. This approach helps minimize disk access but results in accumulated deleted records that can occupy valuable memory and degrade performance over time.
The Vacuum Optimizer addresses this issue by systematically clearing out these marked-as-deleted records once they reach a certain threshold. This threshold is configurable and includes conditions such as the minimum fraction of deleted vectors in a segment and the minimum number of vectors present in a segment. ","A vacuum optimizer is not explicitly mentioned in the provided context. However, based on the information provided about indexing optimization and storage optimization in the configuration file, it seems like the vacuum optimizer may refer to a component or process that helps optimize storage by implementing the enabling of indexes and memmap storage when a minimal amount of records is reached.

In general database management, a vacuum operation is often used to reclaim storage space occupied by obsolete or deleted data. It involves reorganizing the storage of the database to optimize performance and efficiency. So, a vacuum optimizer in this context could be a tool or process that helps optimize storage efficiency and performance by managing indexes and storage mechanisms based on certain criteria defined in the configuration file.","So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan.



The Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached.



The criteria for starting the optimizer are defined in the configuration file.



Here is an example of parameter values:



```yaml

storage:

  optimizers:

    # Maximum size (in kilobytes) of vectors to store in-memory per segment.
So it's a shorthand to say that they optimize CPU performance. I think the more correct way to look at it is that we use the CPU better. But let's talk about optimization or quantization, which we do here, right? So most of what we do is from optimum and the way optimum call set up is they call these levels. So you can basically go from let's say level zero, which is there are no optimizations to let's say 99 where there's a bunch of extra optimizations happening
. It uses only two most optimized CPU instructions ever. It's Pixor and Popcount. Instead of complicated float point subprocessor, you only need those tool. It works with any register you have, and it's very fast.
return optimizer

```



Caching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.

When it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.

It provides both a considerable speedup and less memory footprint.

However, it is quite a bit versatile and has several knobs to tune.
config: Some(Config::Params(VectorParams {

                size: 768,

                distance: Distance::Cosine.into(),

                ..Default::default()

            })),

        }),

        optimizers_config: Some(OptimizersConfigDiff {

            memmap_threshold: Some(20000),

            ..Default::default()

        }),

        hnsw_config: Some(HnswConfigDiff {

            on_disk: Some(true),

            ..Default::default()

        }),

        ..Default::default()","['So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan.\n\n\n\nThe Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached.\n\n\n\nThe criteria for starting the optimizer are defined in the configuration file.\n\n\n\nHere is an example of parameter values:\n\n\n\n```yaml\n\nstorage:\n\n  optimizers:\n\n    # Maximum size (in kilobytes) of vectors to store in-memory per segment.', ""So it's a shorthand to say that they optimize CPU performance. I think the more correct way to look at it is that we use the CPU better. But let's talk about optimization or quantization, which we do here, right? So most of what we do is from optimum and the way optimum call set up is they call these levels. So you can basically go from let's say level zero, which is there are no optimizations to let's say 99 where there's a bunch of extra optimizations happening"", "". It uses only two most optimized CPU instructions ever. It's Pixor and Popcount. Instead of complicated float point subprocessor, you only need those tool. It works with any register you have, and it's very fast."", 'return optimizer\n\n```\n\n\n\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\n\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\n\nIt provides both a considerable speedup and less memory footprint.\n\nHowever, it is quite a bit versatile and has several knobs to tune.', 'config: Some(Config::Params(VectorParams {\n\n                size: 768,\n\n                distance: Distance::Cosine.into(),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        optimizers_config: Some(OptimizersConfigDiff {\n\n            memmap_threshold: Some(20000),\n\n            ..Default::default()\n\n        }),\n\n        hnsw_config: Some(HnswConfigDiff {\n\n            on_disk: Some(true),\n\n            ..Default::default()\n\n        }),\n\n        ..Default::default()']","Question: What is vaccum optimizer ?\n\nContext: So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan.



The Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached.



The criteria for starting the optimizer are defined in the configuration file.



Here is an example of parameter values:



```yaml

storage:

optimizers:

# Maximum size (in kilobytes) of vectors to store in-memory per segment.
So it's a shorthand to say that they optimize CPU performance. I think the more correct way to look at it is that we use the CPU better. But let's talk about optimization or quantization, which we do here, right? So most of what we do is from optimum and the way optimum call set up is they call these levels. So you can basically go from let's say level zero, which is there are no optimizations to let's say 99 where there's a bunch of extra optimizations happening
. It uses only two most optimized CPU instructions ever. It's Pixor and Popcount. Instead of complicated float point subprocessor, you only need those tool. It works with any register you have, and it's very fast.
return optimizer

```



Caching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.

When it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.

It provides both a considerable speedup and less memory footprint.

However, it is quite a bit versatile and has several knobs to tune.
config: Some(Config::Params(VectorParams {

size: 768,

distance: Distance::Cosine.into(),

..Default::default()

})),

}),

optimizers_config: Some(OptimizersConfigDiff {

memmap_threshold: Some(20000),

..Default::default()

}),

hnsw_config: Some(HnswConfigDiff {

on_disk: Some(true),

..Default::default()

}),

..Default::default()\n\nAnswer:",What is vaccum optimizer ?,"The Vacuum Optimizer is a feature in the Qdrant database designed to manage and improve system performance by removing obsolete data. In Qdrant, when records are deleted, they are not immediately removed from the database. Instead, they are marked as deleted and subsequently ignored in future queries. This approach helps minimize disk access but results in accumulated deleted records that can occupy valuable memory and degrade performance over time.
The Vacuum Optimizer addresses this issue by systematically clearing out these marked-as-deleted records once they reach a certain threshold. This threshold is configurable and includes conditions such as the minimum fraction of deleted vectors in a segment and the minimum number of vectors present in a segment. ","['So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan.\n\n\n\nThe Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached.\n\n\n\nThe criteria for starting the optimizer are defined in the configuration file.\n\n\n\nHere is an example of parameter values:\n\n\n\n```yaml\n\nstorage:\n\n  optimizers:\n\n    # Maximum size (in kilobytes) of vectors to store in-memory per segment.', ""So it's a shorthand to say that they optimize CPU performance. I think the more correct way to look at it is that we use the CPU better. But let's talk about optimization or quantization, which we do here, right? So most of what we do is from optimum and the way optimum call set up is they call these levels. So you can basically go from let's say level zero, which is there are no optimizations to let's say 99 where there's a bunch of extra optimizations happening"", "". It uses only two most optimized CPU instructions ever. It's Pixor and Popcount. Instead of complicated float point subprocessor, you only need those tool. It works with any register you have, and it's very fast."", 'return optimizer\n\n```\n\n\n\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\n\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\n\nIt provides both a considerable speedup and less memory footprint.\n\nHowever, it is quite a bit versatile and has several knobs to tune.', 'config: Some(Config::Params(VectorParams {\n\n                size: 768,\n\n                distance: Distance::Cosine.into(),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        optimizers_config: Some(OptimizersConfigDiff {\n\n            memmap_threshold: Some(20000),\n\n            ..Default::default()\n\n        }),\n\n        hnsw_config: Some(HnswConfigDiff {\n\n            on_disk: Some(true),\n\n            ..Default::default()\n\n        }),\n\n        ..Default::default()']","So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan.



The Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached.



The criteria for starting the optimizer are defined in the configuration file.



Here is an example of parameter values:



```yaml

storage:

  optimizers:

    # Maximum size (in kilobytes) of vectors to store in-memory per segment.
So it's a shorthand to say that they optimize CPU performance. I think the more correct way to look at it is that we use the CPU better. But let's talk about optimization or quantization, which we do here, right? So most of what we do is from optimum and the way optimum call set up is they call these levels. So you can basically go from let's say level zero, which is there are no optimizations to let's say 99 where there's a bunch of extra optimizations happening
. It uses only two most optimized CPU instructions ever. It's Pixor and Popcount. Instead of complicated float point subprocessor, you only need those tool. It works with any register you have, and it's very fast.
return optimizer

```



Caching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.

When it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.

It provides both a considerable speedup and less memory footprint.

However, it is quite a bit versatile and has several knobs to tune.
config: Some(Config::Params(VectorParams {

                size: 768,

                distance: Distance::Cosine.into(),

                ..Default::default()

            })),

        }),

        optimizers_config: Some(OptimizersConfigDiff {

            memmap_threshold: Some(20000),

            ..Default::default()

        }),

        hnsw_config: Some(HnswConfigDiff {

            on_disk: Some(true),

            ..Default::default()

        }),

        ..Default::default()",0.34782608695652173,0.37037037037037035,0.35874439461883406,0.03508771929824561,0.037383177570093455,0.03619909502262443,0.16521739130434782,0.17592592592592593,0.17040358744394618,0.3130434782608696,0.3333333333333333,0.3228699551569507,0.678084135055542,0.5944650769233704,0.6012719869613647,0.5978491306304932,0.14902471303939824,0.06639085213343299,129,1.1120689655172413,0.2920795797845986
True,,2740.3900000000003,9b94127b,Tell me about ‘always_ram’ parameter?,"In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times.","The `always_ram` parameter is used in the context of quantization configuration in Qdrant. When set to `true`, it indicates that quantized vectors should always be cached in RAM. This means that the quantized vectors will be kept in memory for faster access. 

By default, quantized vectors are loaded in the same way as the original vectors. So, setting `always_ram` to `true` ensures that the quantized vectors are always available in RAM, which can improve search performance.

It's worth noting that this parameter only affects the precision of the search results and not the memory footprint. If you experience a decrease in search quality, you may consider tuning this parameter.

You can set the `always_ram` parameter in the quantization configuration when creating a collection or setting up quantization configurations in Qdrant.","always_ram: Some(true),

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CompressionRatio;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.ProductQuantization;
always_ram: Some(true),

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.BinaryQuantization;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.QuantizationConfig;
Using quantiles lower than `1.0` might be useful if there are outliers in your vector components.

This parameter only affects the resulting precision and not the memory footprint.

It might be worth tuning this parameter if you experience a significant decrease in search quality.



`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": false

        }

    }

}

```



```python

from qdrant_client import QdrantClient, models
..Default::default()

        }),

        quantization_config: Some(QuantizationConfig {

            quantization: Some(Quantization::Scalar(ScalarQuantization {

                r#type: QuantizationType::Int8.into(),

                always_ram: Some(true),

                ..Default::default()

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;","['always_ram: Some(true),\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.CompressionRatio;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.ProductQuantization;', 'always_ram: Some(true),\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.BinaryQuantization;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.QuantizationConfig;', 'Using quantiles lower than `1.0` might be useful if there are outliers in your vector components.\n\nThis parameter only affects the resulting precision and not the memory footprint.\n\nIt might be worth tuning this parameter if you experience a significant decrease in search quality.\n\n\n\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.', 'This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models', '..Default::default()\n\n        }),\n\n        quantization_config: Some(QuantizationConfig {\n\n            quantization: Some(Quantization::Scalar(ScalarQuantization {\n\n                r#type: QuantizationType::Int8.into(),\n\n                always_ram: Some(true),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;']","Question: Tell me about ‘always_ram’ parameter?\n\nContext: always_ram: Some(true),

})),

}),

..Default::default()

})

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CompressionRatio;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.ProductQuantization;
always_ram: Some(true),

})),

}),

..Default::default()

})

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.BinaryQuantization;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.QuantizationConfig;
Using quantiles lower than `1.0` might be useful if there are outliers in your vector components.

This parameter only affects the resulting precision and not the memory footprint.

It might be worth tuning this parameter if you experience a significant decrease in search quality.



`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""optimizers_config"": {

""memmap_threshold"": 20000

},

""quantization_config"": {

""scalar"": {

""type"": ""int8"",

""always_ram"": false

}

}

}

```



```python

from qdrant_client import QdrantClient, models
..Default::default()

}),

quantization_config: Some(QuantizationConfig {

quantization: Some(Quantization::Scalar(ScalarQuantization {

r#type: QuantizationType::Int8.into(),

always_ram: Some(true),

..Default::default()

})),

}),

..Default::default()

})

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;\n\nAnswer:",Tell me about ‘always_ram’ parameter?,"In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times.","['always_ram: Some(true),\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.CompressionRatio;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.ProductQuantization;', 'always_ram: Some(true),\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.BinaryQuantization;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.QuantizationConfig;', 'Using quantiles lower than `1.0` might be useful if there are outliers in your vector components.\n\nThis parameter only affects the resulting precision and not the memory footprint.\n\nIt might be worth tuning this parameter if you experience a significant decrease in search quality.\n\n\n\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.', 'This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models', '..Default::default()\n\n        }),\n\n        quantization_config: Some(QuantizationConfig {\n\n            quantization: Some(Quantization::Scalar(ScalarQuantization {\n\n                r#type: QuantizationType::Int8.into(),\n\n                always_ram: Some(true),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;']","always_ram: Some(true),

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CompressionRatio;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.ProductQuantization;
always_ram: Some(true),

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.BinaryQuantization;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.QuantizationConfig;
Using quantiles lower than `1.0` might be useful if there are outliers in your vector components.

This parameter only affects the resulting precision and not the memory footprint.

It might be worth tuning this parameter if you experience a significant decrease in search quality.



`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": false

        }

    }

}

```



```python

from qdrant_client import QdrantClient, models
..Default::default()

        }),

        quantization_config: Some(QuantizationConfig {

            quantization: Some(Quantization::Scalar(ScalarQuantization {

                r#type: QuantizationType::Int8.into(),

                always_ram: Some(true),

                ..Default::default()

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;",0.38016528925619836,0.8679245283018868,0.5287356321839081,0.25,0.5769230769230769,0.3488372093023256,0.2975206611570248,0.6792452830188679,0.41379310344827586,0.371900826446281,0.8490566037735849,0.5172413793103449,0.8971604704856873,0.6601020097732544,0.7766855359077454,0.7136638164520264,0.8839308511669515,0.7594885432627052,131,2.3392857142857144,0.6635476679570325
True,,4904.227,a7cf1e28,What is difference between scalar and product quantization?,"While both methods aim to reduce the memory footprint and potentially speed up operations, scalar quantization offers faster processing with SIMD-friendly operations and minimal accuracy loss, making it suitable for scenarios where speed is critical. Product quantization achieves higher compression rates and is better suited for handling very high-dimensional vectors where memory space is more of a concern than computation speed.","Scalar quantization and product quantization are two different techniques used in quantization for reducing memory usage in vector databases. Here are the key differences between them:

1. **Scalar Quantization:**
   - In scalar quantization, each element of the vector is quantized independently.
   - It offers a simpler and faster quantization process compared to product quantization.
   - Scalar quantization provides lower compression ratios compared to product quantization.
   - It is generally more accurate than product quantization.
   - Scalar quantization is suitable for scenarios where the speed of vector comparison is critical and the memory footprint is not the primary concern.

2. **Product Quantization:**
   - Product quantization involves dividing the vector into subspaces and quantizing each subspace independently.
   - It offers a better compression ratio compared to scalar quantization, leading to reduced memory usage.
   - Product quantization may result in a loss of accuracy due to the quantization of subspaces.
   - It is slower than scalar quantization due to the additional computations involved in quantizing subspaces and performing distance calculations.
   - Product quantization is recommended when memory optimization is a top priority and search speed is not critical.

In summary, scalar quantization is faster and more accurate but provides lower compression ratios, while product quantization offers better memory optimization at the cost of reduced accuracy and slower search speeds. The choice between the two quantization methods depends on the specific requirements of the application, such as the importance of accuracy, speed, and memory usage.","* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.



## Setting up Quantization in Qdrant



You can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.
<td>474 s</td>

      </tr>

   </tbody>

</table>



It turns out that in some cases, Product Quantization may not only reduce the memory usage, 

but also the search time.



## Good practices



Compared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.



Product Quantization tends to be favored in certain specific scenarios:
But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method
- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself

- Situations where the dimensionality of the original vectors is sufficiently high

- Cases where indexing speed is not a critical factor



In circumstances that do not align with the above, Scalar Quantization should be the preferred choice.



Qdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization)
weight: 4

author: Kacper Łukawski

author_link: https://medium.com/@lukawskikacper

date: 2023-05-30T09:45:00+02:00

draft: false

keywords:

  - vector search

  - product quantization

  - memory optimization

aliases: [ /articles/product_quantization/ ]

---



Qdrant 1.1.0 brought the support of [Scalar Quantization](/articles/scalar-quantization/),

a technique of reducing the memory footprint by even four times, by using `int8` to represent","['* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.\n\n\n\n## Setting up Quantization in Qdrant\n\n\n\nYou can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.', '<td>474 s</td>\n\n      </tr>\n\n   </tbody>\n\n</table>\n\n\n\nIt turns out that in some cases, Product Quantization may not only reduce the memory usage, \n\nbut also the search time.\n\n\n\n## Good practices\n\n\n\nCompared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\n\n\n\nProduct Quantization tends to be favored in certain specific scenarios:', 'But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\n\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\n\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n## How to choose the right quantization method', '- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself\n\n- Situations where the dimensionality of the original vectors is sufficiently high\n\n- Cases where indexing speed is not a critical factor\n\n\n\nIn circumstances that do not align with the above, Scalar Quantization should be the preferred choice.\n\n\n\nQdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization)', 'weight: 4\n\nauthor: Kacper Łukawski\n\nauthor_link: https://medium.com/@lukawskikacper\n\ndate: 2023-05-30T09:45:00+02:00\n\ndraft: false\n\nkeywords:\n\n  - vector search\n\n  - product quantization\n\n  - memory optimization\n\naliases: [ /articles/product_quantization/ ]\n\n---\n\n\n\nQdrant 1.1.0 brought the support of [Scalar Quantization](/articles/scalar-quantization/),\n\na technique of reducing the memory footprint by even four times, by using `int8` to represent']","Question: What is difference between scalar and product quantization?\n\nContext: * **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.



## Setting up Quantization in Qdrant



You can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.
<td>474 s</td>

</tr>

</tbody>

</table>



It turns out that in some cases, Product Quantization may not only reduce the memory usage, 

but also the search time.



## Good practices



Compared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.



Product Quantization tends to be favored in certain specific scenarios:
But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method
- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself

- Situations where the dimensionality of the original vectors is sufficiently high

- Cases where indexing speed is not a critical factor



In circumstances that do not align with the above, Scalar Quantization should be the preferred choice.



Qdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization)
weight: 4

author: Kacper Łukawski

author_link: https://medium.com/@lukawskikacper

date: 2023-05-30T09:45:00+02:00

draft: false

keywords:

- vector search

- product quantization

- memory optimization

aliases: [ /articles/product_quantization/ ]

---



Qdrant 1.1.0 brought the support of [Scalar Quantization](/articles/scalar-quantization/),

a technique of reducing the memory footprint by even four times, by using `int8` to represent\n\nAnswer:",What is difference between scalar and product quantization?,"While both methods aim to reduce the memory footprint and potentially speed up operations, scalar quantization offers faster processing with SIMD-friendly operations and minimal accuracy loss, making it suitable for scenarios where speed is critical. Product quantization achieves higher compression rates and is better suited for handling very high-dimensional vectors where memory space is more of a concern than computation speed.","['* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.\n\n\n\n## Setting up Quantization in Qdrant\n\n\n\nYou can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.', '<td>474 s</td>\n\n      </tr>\n\n   </tbody>\n\n</table>\n\n\n\nIt turns out that in some cases, Product Quantization may not only reduce the memory usage, \n\nbut also the search time.\n\n\n\n## Good practices\n\n\n\nCompared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\n\n\n\nProduct Quantization tends to be favored in certain specific scenarios:', 'But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\n\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\n\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n## How to choose the right quantization method', '- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself\n\n- Situations where the dimensionality of the original vectors is sufficiently high\n\n- Cases where indexing speed is not a critical factor\n\n\n\nIn circumstances that do not align with the above, Scalar Quantization should be the preferred choice.\n\n\n\nQdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization)', 'weight: 4\n\nauthor: Kacper Łukawski\n\nauthor_link: https://medium.com/@lukawskikacper\n\ndate: 2023-05-30T09:45:00+02:00\n\ndraft: false\n\nkeywords:\n\n  - vector search\n\n  - product quantization\n\n  - memory optimization\n\naliases: [ /articles/product_quantization/ ]\n\n---\n\n\n\nQdrant 1.1.0 brought the support of [Scalar Quantization](/articles/scalar-quantization/),\n\na technique of reducing the memory footprint by even four times, by using `int8` to represent']","* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.



## Setting up Quantization in Qdrant



You can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.
<td>474 s</td>

      </tr>

   </tbody>

</table>



It turns out that in some cases, Product Quantization may not only reduce the memory usage, 

but also the search time.



## Good practices



Compared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.



Product Quantization tends to be favored in certain specific scenarios:
But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method
- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself

- Situations where the dimensionality of the original vectors is sufficiently high

- Cases where indexing speed is not a critical factor



In circumstances that do not align with the above, Scalar Quantization should be the preferred choice.



Qdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization)
weight: 4

author: Kacper Łukawski

author_link: https://medium.com/@lukawskikacper

date: 2023-05-30T09:45:00+02:00

draft: false

keywords:

  - vector search

  - product quantization

  - memory optimization

aliases: [ /articles/product_quantization/ ]

---



Qdrant 1.1.0 brought the support of [Scalar Quantization](/articles/scalar-quantization/),

a technique of reducing the memory footprint by even four times, by using `int8` to represent",0.18779342723004694,0.6557377049180327,0.291970802919708,0.05660377358490566,0.2,0.08823529411764706,0.11737089201877934,0.4098360655737705,0.18248175182481752,0.1643192488262911,0.5737704918032787,0.25547445255474455,0.888197660446167,0.5113333463668823,0.597323477268219,0.5509936213493347,0.8122763569854821,0.949602481778129,231,3.7868852459016393,0.4384021602038928
True,,3959.075,602604d7,What is ‘best_score’ strategy?,"The `best_score` strategy is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided.","The 'best_score' strategy is a new approach introduced in version 1.6.0 of the recommendation system. It is based on the concept of finding similar vectors that are closer to a positive example while avoiding vectors that are closer to a negative example. This strategy calculates the best score at every step of the graph traversal in the HNSW (hierarchical navigable small world) algorithm. 

One significant feature of the 'best_score' strategy is that it can be used with only negative examples. This allows users to find the most dissimilar vectors to the ones provided, which can be helpful for identifying outliers in data or finding vectors that are very different from a given one. 

The 'best_score' strategy takes into account multiple vectors, unlike the previous strategy that only considered a single vector. By measuring each candidate against every example and selecting the best positive and best negative scores, this strategy increases the variety of results and provides richer recommendations. 

The calculation of the final score in the 'best_score' strategy involves comparing the best positive score with the negative score and applying a formula to determine the overall score. The performance of the 'best_score' strategy will be influenced by the number of examples provided, as it operates in a linear manner with respect to the amount of data.","### The new hotness - Best score



The new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative 

samples and uses a slightly more sophisticated algorithm under the hood.



The best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point 

and every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a
This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
#### Using only negative examples



A beneficial side-effect of `best_score` strategy is that you can use it with only negative examples. This will allow you to find the most dissimilar vectors to the ones you provide. This can be useful for finding outliers in your data, or for finding the most dissimilar vectors to a given one.



Combining negative-only examples with filtering can be a powerful tool for data exploration and cleaning.



### Multiple vectors



*Available as of v0.10.0*
##### One positive and one negative example



<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>



The more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there 

is just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making 

the variety richer.
The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:



```rust

let score = if best_positive_score > best_negative_score {

    best_positive_score;

} else {

    -(best_negative_score * best_negative_score);

};

```



<aside role=""alert"">

The performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.

</aside>","['### The new hotness - Best score\n\n\n\nThe new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative \n\nsamples and uses a slightly more sophisticated algorithm under the hood.\n\n\n\nThe best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point \n\nand every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a', 'This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.\n\n\n\n### Best score strategy\n\n\n\n*Available as of v1.6.0*\n\n\n\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.', '#### Using only negative examples\n\n\n\nA beneficial side-effect of `best_score` strategy is that you can use it with only negative examples. This will allow you to find the most dissimilar vectors to the ones you provide. This can be useful for finding outliers in your data, or for finding the most dissimilar vectors to a given one.\n\n\n\nCombining negative-only examples with filtering can be a powerful tool for data exploration and cleaning.\n\n\n\n### Multiple vectors\n\n\n\n*Available as of v0.10.0*', '##### One positive and one negative example\n\n\n\n<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>\n\n\n\nThe more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there \n\nis just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making \n\nthe variety richer.', 'The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\n\n\n\n```rust\n\nlet score = if best_positive_score > best_negative_score {\n\n    best_positive_score;\n\n} else {\n\n    -(best_negative_score * best_negative_score);\n\n};\n\n```\n\n\n\n<aside role=""alert"">\n\nThe performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.\n\n</aside>']","Question: What is ‘best_score’ strategy?\n\nContext: ### The new hotness - Best score



The new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative 

samples and uses a slightly more sophisticated algorithm under the hood.



The best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point 

and every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a
This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
#### Using only negative examples



A beneficial side-effect of `best_score` strategy is that you can use it with only negative examples. This will allow you to find the most dissimilar vectors to the ones you provide. This can be useful for finding outliers in your data, or for finding the most dissimilar vectors to a given one.



Combining negative-only examples with filtering can be a powerful tool for data exploration and cleaning.



### Multiple vectors



*Available as of v0.10.0*
##### One positive and one negative example



<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>



The more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there 

is just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making 

the variety richer.
The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:



```rust

let score = if best_positive_score > best_negative_score {

best_positive_score;

} else {

-(best_negative_score * best_negative_score);

};

```



<aside role=""alert"">

The performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.

</aside>\n\nAnswer:",What is ‘best_score’ strategy?,"The `best_score` strategy is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided.","['### The new hotness - Best score\n\n\n\nThe new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative \n\nsamples and uses a slightly more sophisticated algorithm under the hood.\n\n\n\nThe best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point \n\nand every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a', 'This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.\n\n\n\n### Best score strategy\n\n\n\n*Available as of v1.6.0*\n\n\n\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.', '#### Using only negative examples\n\n\n\nA beneficial side-effect of `best_score` strategy is that you can use it with only negative examples. This will allow you to find the most dissimilar vectors to the ones you provide. This can be useful for finding outliers in your data, or for finding the most dissimilar vectors to a given one.\n\n\n\nCombining negative-only examples with filtering can be a powerful tool for data exploration and cleaning.\n\n\n\n### Multiple vectors\n\n\n\n*Available as of v0.10.0*', '##### One positive and one negative example\n\n\n\n<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>\n\n\n\nThe more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there \n\nis just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making \n\nthe variety richer.', 'The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\n\n\n\n```rust\n\nlet score = if best_positive_score > best_negative_score {\n\n    best_positive_score;\n\n} else {\n\n    -(best_negative_score * best_negative_score);\n\n};\n\n```\n\n\n\n<aside role=""alert"">\n\nThe performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.\n\n</aside>']","### The new hotness - Best score



The new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative 

samples and uses a slightly more sophisticated algorithm under the hood.



The best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point 

and every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a
This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
#### Using only negative examples



A beneficial side-effect of `best_score` strategy is that you can use it with only negative examples. This will allow you to find the most dissimilar vectors to the ones you provide. This can be useful for finding outliers in your data, or for finding the most dissimilar vectors to a given one.



Combining negative-only examples with filtering can be a powerful tool for data exploration and cleaning.



### Multiple vectors



*Available as of v0.10.0*
##### One positive and one negative example



<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>



The more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there 

is just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making 

the variety richer.
The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:



```rust

let score = if best_positive_score > best_negative_score {

    best_positive_score;

} else {

    -(best_negative_score * best_negative_score);

};

```



<aside role=""alert"">

The performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.

</aside>",0.40932642487046633,0.7745098039215687,0.535593220338983,0.2552083333333333,0.48514851485148514,0.3344709897610922,0.31088082901554404,0.5882352941176471,0.4067796610169492,0.37823834196891193,0.7156862745098039,0.4949152542372881,0.6873433589935303,0.6498458385467529,0.7442771196365356,0.6938632726669312,0.9651381550227396,0.9555792281404137,216,1.8305084745762712,0.4402423447288047
True,,2092.3509999999997,ffcf4e03,How does oversampling helps?,"Defines how many extra vectors should be pre-selected using quantized index, and then re-scored using original vectors. For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring. Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.","Oversampling helps in improving the accuracy and performance of similarity search algorithms by allowing for the compression of high-dimensional vectors in memory while compensating for the accuracy loss by re-scoring additional points with the original vectors. This technique helps control the precision of the search in real-time, enhancing the search results by selecting vectors that are more accurate for the user. By oversampling, we can improve the accuracy of searches, although this may require more computational resources.","oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
- **Oversampling**: By oversampling, we can limit the loss of information inherent in quantization. This also helps to preserve the semantic richness of your OpenAI embeddings. We experimented with different oversampling factors, and identified the impact on the accuracy and efficiency of search. Spoiler: higher oversampling factors tend to improve the accuracy of searches. However, they usually require more computational resources.
### Oversampling for quantization



We are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.
Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user
IO-induced delays.



Oversampling is a new feature to improve accuracy at the cost of some

performance. It allows setting a factor, which is multiplied with the `limit`

while doing the search. The results are then re-scored using the original vector

and only then the top results up to the limit are selected.



## Discussion



Looking back, disk IO used to be very serialized; re-positioning read-write

heads on moving platter was a slow and messy business. So the system overhead","['oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.', '- **Oversampling**: By oversampling, we can limit the loss of information inherent in quantization. This also helps to preserve the semantic richness of your OpenAI embeddings. We experimented with different oversampling factors, and identified the impact on the accuracy and efficiency of search. Spoiler: higher oversampling factors tend to improve the accuracy of searches. However, they usually require more computational resources.', '### Oversampling for quantization\n\n\n\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.', 'Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user', 'IO-induced delays.\n\n\n\nOversampling is a new feature to improve accuracy at the cost of some\n\nperformance. It allows setting a factor, which is multiplied with the `limit`\n\nwhile doing the search. The results are then re-scored using the original vector\n\nand only then the top results up to the limit are selected.\n\n\n\n## Discussion\n\n\n\nLooking back, disk IO used to be very serialized; re-positioning read-write\n\nheads on moving platter was a slow and messy business. So the system overhead']","Question: How does oversampling helps?\n\nContext: oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
- **Oversampling**: By oversampling, we can limit the loss of information inherent in quantization. This also helps to preserve the semantic richness of your OpenAI embeddings. We experimented with different oversampling factors, and identified the impact on the accuracy and efficiency of search. Spoiler: higher oversampling factors tend to improve the accuracy of searches. However, they usually require more computational resources.
### Oversampling for quantization



We are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.
Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user
IO-induced delays.



Oversampling is a new feature to improve accuracy at the cost of some

performance. It allows setting a factor, which is multiplied with the `limit`

while doing the search. The results are then re-scored using the original vector

and only then the top results up to the limit are selected.



## Discussion



Looking back, disk IO used to be very serialized; re-positioning read-write

heads on moving platter was a slow and messy business. So the system overhead\n\nAnswer:",How does oversampling helps?,"Defines how many extra vectors should be pre-selected using quantized index, and then re-scored using original vectors. For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring. Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.","['oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.', '- **Oversampling**: By oversampling, we can limit the loss of information inherent in quantization. This also helps to preserve the semantic richness of your OpenAI embeddings. We experimented with different oversampling factors, and identified the impact on the accuracy and efficiency of search. Spoiler: higher oversampling factors tend to improve the accuracy of searches. However, they usually require more computational resources.', '### Oversampling for quantization\n\n\n\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.', 'Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user', 'IO-induced delays.\n\n\n\nOversampling is a new feature to improve accuracy at the cost of some\n\nperformance. It allows setting a factor, which is multiplied with the `limit`\n\nwhile doing the search. The results are then re-scored using the original vector\n\nand only then the top results up to the limit are selected.\n\n\n\n## Discussion\n\n\n\nLooking back, disk IO used to be very serialized; re-positioning read-write\n\nheads on moving platter was a slow and messy business. So the system overhead']","oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
- **Oversampling**: By oversampling, we can limit the loss of information inherent in quantization. This also helps to preserve the semantic richness of your OpenAI embeddings. We experimented with different oversampling factors, and identified the impact on the accuracy and efficiency of search. Spoiler: higher oversampling factors tend to improve the accuracy of searches. However, they usually require more computational resources.
### Oversampling for quantization



We are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.
Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user
IO-induced delays.



Oversampling is a new feature to improve accuracy at the cost of some

performance. It allows setting a factor, which is multiplied with the `limit`

while doing the search. The results are then re-scored using the original vector

and only then the top results up to the limit are selected.



## Discussion



Looking back, disk IO used to be very serialized; re-positioning read-write

heads on moving platter was a slow and messy business. So the system overhead",0.2112676056338028,0.22058823529411764,0.21582733812949642,0.02857142857142857,0.029850746268656716,0.029197080291970802,0.11267605633802817,0.11764705882352941,0.11510791366906475,0.2112676056338028,0.22058823529411764,0.21582733812949642,0.5317639112472534,0.5822427272796631,0.5909034013748169,0.5865411162376404,0.9963274808251299,0.8044660037072996,77,1.203125,0.5190186834172293
True,,1890.457,7093ee2b,What is the purpose of ‘CreatePayloadIndexAsync’?,CreatePayloadIndexAsync is a method in the Qdrant library that enables the creation of a payload index in Qdrant. A payload index is a data structure designed to store supplemental information about the data stored in Qdrant. This method requires a Qdrant instance and a payload index name as input parameters.,"The purpose of `CreatePayloadIndexAsync` is to create an index for a specific field in a collection for efficient querying and searching. In the provided code snippets, it is used to create a payload index for a specified field within a collection using the Qdrant client library. By creating an index for a field, it allows for faster retrieval of relevant data when querying the collection based on that field.","client

    .createPayloadIndexAsync(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        PayloadSchemaType.Keyword,

        null,

        null,

        null,

        null)

    .get();

```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");

```



Available field types are:
.get();

```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""group_id"");

```



## Limitations



One downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.
FieldType::Keyword,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createPayloadIndexAsync(

        ""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null)

    .get();

```
const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createPayloadIndex(""{collection_name}"", {

  field_name: ""name_of_the_field_to_index"",

  field_schema: {

    type: ""text"",

    tokenizer: ""word"",

    min_token_len: 2,

    max_token_len: 15,

    lowercase: true,

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,

        TokenizerType,
""type"": ""user_http"",

    ""authorization_type"": ""bearer""

  },

  ""api"": {

    ""type"": ""openapi"",

    ""url"": ""https://your-application-name.fly.dev/.well-known/openapi.yaml"",

    ""has_user_authentication"": false

  },

  ""logo_url"": ""https://your-application-name.fly.dev/.well-known/logo.png"",

  ""contact_email"": ""email@domain.com"",

  ""legal_info_url"": ""email@domain.com""

}

```



That was the last step before running the final command. The command that will deploy","['client\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        PayloadSchemaType.Keyword,\n\n        null,\n\n        null,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");\n\n```\n\n\n\nAvailable field types are:', '.get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""group_id"");\n\n```\n\n\n\n## Limitations\n\n\n\nOne downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.', 'FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null)\n\n    .get();\n\n```', 'const client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createPayloadIndex(""{collection_name}"", {\n\n  field_name: ""name_of_the_field_to_index"",\n\n  field_schema: {\n\n    type: ""text"",\n\n    tokenizer: ""word"",\n\n    min_token_len: 2,\n\n    max_token_len: 15,\n\n    lowercase: true,\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\n\n        TokenizerType,', '""type"": ""user_http"",\n\n    ""authorization_type"": ""bearer""\n\n  },\n\n  ""api"": {\n\n    ""type"": ""openapi"",\n\n    ""url"": ""https://your-application-name.fly.dev/.well-known/openapi.yaml"",\n\n    ""has_user_authentication"": false\n\n  },\n\n  ""logo_url"": ""https://your-application-name.fly.dev/.well-known/logo.png"",\n\n  ""contact_email"": ""email@domain.com"",\n\n  ""legal_info_url"": ""email@domain.com""\n\n}\n\n```\n\n\n\nThat was the last step before running the final command. The command that will deploy']","Question: What is the purpose of ‘CreatePayloadIndexAsync’?\n\nContext: client

.createPayloadIndexAsync(

""{collection_name}"",

""name_of_the_field_to_index"",

PayloadSchemaType.Keyword,

null,

null,

null,

null)

.get();

```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");

```



Available field types are:
.get();

```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""group_id"");

```



## Limitations



One downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.
FieldType::Keyword,

None,

None,

)

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createPayloadIndexAsync(

""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null)

.get();

```
const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createPayloadIndex(""{collection_name}"", {

field_name: ""name_of_the_field_to_index"",

field_schema: {

type: ""text"",

tokenizer: ""word"",

min_token_len: 2,

max_token_len: 15,

lowercase: true,

},

});

```



```rust

use qdrant_client::{

client::QdrantClient,

qdrant::{

payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,

TokenizerType,
""type"": ""user_http"",

""authorization_type"": ""bearer""

},

""api"": {

""type"": ""openapi"",

""url"": ""https://your-application-name.fly.dev/.well-known/openapi.yaml"",

""has_user_authentication"": false

},

""logo_url"": ""https://your-application-name.fly.dev/.well-known/logo.png"",

""contact_email"": ""email@domain.com"",

""legal_info_url"": ""email@domain.com""

}

```



That was the last step before running the final command. The command that will deploy\n\nAnswer:",What is the purpose of ‘CreatePayloadIndexAsync’?,CreatePayloadIndexAsync is a method in the Qdrant library that enables the creation of a payload index in Qdrant. A payload index is a data structure designed to store supplemental information about the data stored in Qdrant. This method requires a Qdrant instance and a payload index name as input parameters.,"['client\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        PayloadSchemaType.Keyword,\n\n        null,\n\n        null,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");\n\n```\n\n\n\nAvailable field types are:', '.get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""group_id"");\n\n```\n\n\n\n## Limitations\n\n\n\nOne downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.', 'FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null)\n\n    .get();\n\n```', 'const client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createPayloadIndex(""{collection_name}"", {\n\n  field_name: ""name_of_the_field_to_index"",\n\n  field_schema: {\n\n    type: ""text"",\n\n    tokenizer: ""word"",\n\n    min_token_len: 2,\n\n    max_token_len: 15,\n\n    lowercase: true,\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\n\n        TokenizerType,', '""type"": ""user_http"",\n\n    ""authorization_type"": ""bearer""\n\n  },\n\n  ""api"": {\n\n    ""type"": ""openapi"",\n\n    ""url"": ""https://your-application-name.fly.dev/.well-known/openapi.yaml"",\n\n    ""has_user_authentication"": false\n\n  },\n\n  ""logo_url"": ""https://your-application-name.fly.dev/.well-known/logo.png"",\n\n  ""contact_email"": ""email@domain.com"",\n\n  ""legal_info_url"": ""email@domain.com""\n\n}\n\n```\n\n\n\nThat was the last step before running the final command. The command that will deploy']","client

    .createPayloadIndexAsync(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        PayloadSchemaType.Keyword,

        null,

        null,

        null,

        null)

    .get();

```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");

```



Available field types are:
.get();

```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""group_id"");

```



## Limitations



One downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.
FieldType::Keyword,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createPayloadIndexAsync(

        ""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null)

    .get();

```
const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createPayloadIndex(""{collection_name}"", {

  field_name: ""name_of_the_field_to_index"",

  field_schema: {

    type: ""text"",

    tokenizer: ""word"",

    min_token_len: 2,

    max_token_len: 15,

    lowercase: true,

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,

        TokenizerType,
""type"": ""user_http"",

    ""authorization_type"": ""bearer""

  },

  ""api"": {

    ""type"": ""openapi"",

    ""url"": ""https://your-application-name.fly.dev/.well-known/openapi.yaml"",

    ""has_user_authentication"": false

  },

  ""logo_url"": ""https://your-application-name.fly.dev/.well-known/logo.png"",

  ""contact_email"": ""email@domain.com"",

  ""legal_info_url"": ""email@domain.com""

}

```



That was the last step before running the final command. The command that will deploy",0.2807017543859649,0.3902439024390244,0.326530612244898,0.03571428571428571,0.05,0.041666666666666664,0.14035087719298245,0.1951219512195122,0.163265306122449,0.24561403508771928,0.34146341463414637,0.28571428571428575,0.8190537691116333,0.6325597763061523,0.6574411392211914,0.6447604894638062,0.5394308070341747,0.6059502859910328,69,1.38,0.2746623273906794
True,,1739.056,a15ae520,What is the purpose of ef_construct in HNSW ?,"In HNSW algorithm the ef_construct parameter is the number of neighbours to consider during the index building. The larger the value, the higher the precision, but the longer the indexing time. The default values of this parameters 100","The purpose of `ef_construct` in HNSW is to specify the number of neighbors to consider during the index building process. A higher value of `ef_construct` can lead to higher precision in search results, but it also increases the indexing time. The default value for `ef_construct` is set to 100, but it can be adjusted based on the trade-off between search precision and indexing time that best suits your requirements.","In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.



The corresponding parameters could be configured in the configuration file:



```yaml

storage:

  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually

  hnsw_index:

    # Number of edges per node in the index graph.
# in this case full-scan search should be preferred by query planner and additional indexing is not required.

    # Note: 1Kb = 1 vector of size 256

    full_scan_threshold: 10000



```



And so in the process of creating a [collection](../collections). The `ef` parameter is configured during [the search](../search) and by default is equal to `ef_construct`.



HNSW is chosen for several reasons.
The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of 

neighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.

The default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and
""m"": 32,

                ""ef_construct"": 123

            },

            ""quantization_config"": {

                ""product"": {

                    ""compression"": ""x32"",

                    ""always_ram"": true

                }

            },

            ""on_disk"": true

        }

    },

    ""hnsw_config"": {

        ""ef_construct"": 123

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""quantile"": 0.8,
Rishabh Bhardwaj:

So should I go with the slide first or should I answer your question first?



Demetrios:

Probably answer the question so we don't get too far off track, and then we can hit up your slides. And the slides, I'm sure, will prompt many other questions from my side and the audience's side.



Rishabh Bhardwaj:

So, for HNSW configuration, we have specified the value of M, which is, I think, basically the layers as 64, and the value for EF construct is 256.



Demetrios:","['In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.\n\n\n\nThe corresponding parameters could be configured in the configuration file:\n\n\n\n```yaml\n\nstorage:\n\n  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually\n\n  hnsw_index:\n\n    # Number of edges per node in the index graph.', '# in this case full-scan search should be preferred by query planner and additional indexing is not required.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    full_scan_threshold: 10000\n\n\n\n```\n\n\n\nAnd so in the process of creating a [collection](../collections). The `ef` parameter is configured during [the search](../search) and by default is equal to `ef_construct`.\n\n\n\nHNSW is chosen for several reasons.', ""The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \n\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\n\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and"", '""m"": 32,\n\n                ""ef_construct"": 123\n\n            },\n\n            ""quantization_config"": {\n\n                ""product"": {\n\n                    ""compression"": ""x32"",\n\n                    ""always_ram"": true\n\n                }\n\n            },\n\n            ""on_disk"": true\n\n        }\n\n    },\n\n    ""hnsw_config"": {\n\n        ""ef_construct"": 123\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""quantile"": 0.8,', ""Rishabh Bhardwaj:\n\nSo should I go with the slide first or should I answer your question first?\n\n\n\nDemetrios:\n\nProbably answer the question so we don't get too far off track, and then we can hit up your slides. And the slides, I'm sure, will prompt many other questions from my side and the audience's side.\n\n\n\nRishabh Bhardwaj:\n\nSo, for HNSW configuration, we have specified the value of M, which is, I think, basically the layers as 64, and the value for EF construct is 256.\n\n\n\nDemetrios:""]","Question: What is the purpose of ef_construct in HNSW ?\n\nContext: In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.



The corresponding parameters could be configured in the configuration file:



```yaml

storage:

# Default parameters of HNSW Index. Could be overridden for each collection or named vector individually

hnsw_index:

# Number of edges per node in the index graph.
# in this case full-scan search should be preferred by query planner and additional indexing is not required.

# Note: 1Kb = 1 vector of size 256

full_scan_threshold: 10000



```



And so in the process of creating a [collection](../collections). The `ef` parameter is configured during [the search](../search) and by default is equal to `ef_construct`.



HNSW is chosen for several reasons.
The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of 

neighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.

The default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and
""m"": 32,

""ef_construct"": 123

},

""quantization_config"": {

""product"": {

""compression"": ""x32"",

""always_ram"": true

}

},

""on_disk"": true

}

},

""hnsw_config"": {

""ef_construct"": 123

},

""quantization_config"": {

""scalar"": {

""type"": ""int8"",

""quantile"": 0.8,
Rishabh Bhardwaj:

So should I go with the slide first or should I answer your question first?



Demetrios:

Probably answer the question so we don't get too far off track, and then we can hit up your slides. And the slides, I'm sure, will prompt many other questions from my side and the audience's side.



Rishabh Bhardwaj:

So, for HNSW configuration, we have specified the value of M, which is, I think, basically the layers as 64, and the value for EF construct is 256.



Demetrios:\n\nAnswer:",What is the purpose of ef_construct in HNSW ?,"In HNSW algorithm the ef_construct parameter is the number of neighbours to consider during the index building. The larger the value, the higher the precision, but the longer the indexing time. The default values of this parameters 100","['In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.\n\n\n\nThe corresponding parameters could be configured in the configuration file:\n\n\n\n```yaml\n\nstorage:\n\n  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually\n\n  hnsw_index:\n\n    # Number of edges per node in the index graph.', '# in this case full-scan search should be preferred by query planner and additional indexing is not required.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    full_scan_threshold: 10000\n\n\n\n```\n\n\n\nAnd so in the process of creating a [collection](../collections). The `ef` parameter is configured during [the search](../search) and by default is equal to `ef_construct`.\n\n\n\nHNSW is chosen for several reasons.', ""The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \n\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\n\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and"", '""m"": 32,\n\n                ""ef_construct"": 123\n\n            },\n\n            ""quantization_config"": {\n\n                ""product"": {\n\n                    ""compression"": ""x32"",\n\n                    ""always_ram"": true\n\n                }\n\n            },\n\n            ""on_disk"": true\n\n        }\n\n    },\n\n    ""hnsw_config"": {\n\n        ""ef_construct"": 123\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""quantile"": 0.8,', ""Rishabh Bhardwaj:\n\nSo should I go with the slide first or should I answer your question first?\n\n\n\nDemetrios:\n\nProbably answer the question so we don't get too far off track, and then we can hit up your slides. And the slides, I'm sure, will prompt many other questions from my side and the audience's side.\n\n\n\nRishabh Bhardwaj:\n\nSo, for HNSW configuration, we have specified the value of M, which is, I think, basically the layers as 64, and the value for EF construct is 256.\n\n\n\nDemetrios:""]","In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.



The corresponding parameters could be configured in the configuration file:



```yaml

storage:

  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually

  hnsw_index:

    # Number of edges per node in the index graph.
# in this case full-scan search should be preferred by query planner and additional indexing is not required.

    # Note: 1Kb = 1 vector of size 256

    full_scan_threshold: 10000



```



And so in the process of creating a [collection](../collections). The `ef` parameter is configured during [the search](../search) and by default is equal to `ef_construct`.



HNSW is chosen for several reasons.
The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of 

neighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.

The default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and
""m"": 32,

                ""ef_construct"": 123

            },

            ""quantization_config"": {

                ""product"": {

                    ""compression"": ""x32"",

                    ""always_ram"": true

                }

            },

            ""on_disk"": true

        }

    },

    ""hnsw_config"": {

        ""ef_construct"": 123

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""quantile"": 0.8,
Rishabh Bhardwaj:

So should I go with the slide first or should I answer your question first?



Demetrios:

Probably answer the question so we don't get too far off track, and then we can hit up your slides. And the slides, I'm sure, will prompt many other questions from my side and the audience's side.



Rishabh Bhardwaj:

So, for HNSW configuration, we have specified the value of M, which is, I think, basically the layers as 64, and the value for EF construct is 256.



Demetrios:",0.3333333333333333,0.7586206896551724,0.4631578947368421,0.18461538461538463,0.42857142857142855,0.25806451612903225,0.2878787878787879,0.6551724137931034,0.4,0.3333333333333333,0.7586206896551724,0.4631578947368421,0.8863860368728638,0.6319847702980042,0.7417197227478027,0.6824693083763123,0.910399728997921,0.408019964893659,69,1.8157894736842106,0.3507996756953123
True,,8515.588,9ff09934,How do you use ‘ordering’ parameter?,"Write ordering can be specified for any write request to serialize it through a single “leader” node, which ensures that all write operations (issued with the same ordering) are performed and observed sequentially. It is of 3 types weak , medium and strong and is used in python with additional param ordering=models.WriteOrdering.STRONG to upsert request.","To use the `ordering` parameter in Qdrant, you can specify it in your API requests when performing update and delete operations. The `ordering` parameter ensures that these operations are executed in the same order on all replicas, thereby helping to maintain data consistency in case of concurrent updates to the same documents.

When you set the `ordering` parameter, Qdrant will route the operation to the leader replica of the shard and wait for a response before responding to the client. This synchronization helps prevent data inconsistency issues that may arise from simultaneous updates.

In the provided example code snippet, the `ordering` parameter is set to ""strong"" in the `client.upsert` function call. This indicates that the operation should be executed with strong ordering guarantees, ensuring consistency across replicas.

Here is an example of how the `ordering` parameter is used in a Python API request:

```python
client.upsert(""{collection_name}"", {
    batch: {
        ids: [1, 2, 3],
        payloads: [{ color: ""red"" }, { color: ""green"" }, { color: ""blue"" }],
        vectors: [
            [0.9, 0.1, 0.1],
            [0.1, 0.9, 0.1],
            [0.1, 0.1, 0.9],
        ],
    },
    ordering: ""strong"",
});
```

In this request, the `client.upsert` function is used to update or insert batches of data into the specified collection. The `ordering` parameter is set to ""strong"" to ensure strong ordering for the operation.

By utilizing the `ordering` parameter in your API requests, you can maintain data consistency and avoid conflicts that may occur during concurrent updates on distributed replicas.","- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
Since the `filter` parameter is specified, the search is performed only among those points that satisfy the filter condition.

See details of possible filters and their work in the [filtering](../filtering) section.



Example result of this API would be



```json

{

  ""result"": [

    { ""id"": 10, ""score"": 0.81 },

    { ""id"": 14, ""score"": 0.75 },

    { ""id"": 11, ""score"": 0.73 }

  ],

  ""status"": ""ok"",

  ""time"": 0.001

}

```



The `result` contains ordered by `score` list of found point ids.
{""color"": ""blue""},

        ],

        vectors=[

            [0.9, 0.1, 0.1],

            [0.1, 0.9, 0.1],

            [0.1, 0.1, 0.9],

        ],

    ),

    ordering=""strong"",

)

```



```typescript

client.upsert(""{collection_name}"", {

  batch: {

    ids: [1, 2, 3],

    payloads: [{ color: ""red"" }, { color: ""green"" }, { color: ""blue"" }],

    vectors: [

      [0.9, 0.1, 0.1],

      [0.1, 0.9, 0.1],

      [0.1, 0.1, 0.9],

    ],

  },

  ordering: ""strong"",

});

```
);

```



The rule of thumb to set the memmap threshold parameter is simple:



- if you have a balanced use scenario - set memmap threshold the same as `indexing_threshold` (default is 20000). In this case the optimizer will not make any extra runs and will optimize all thresholds at once.
```http

POST /collections/{collection_name}/points/payload

{

    ""payload"": {

        ""property1"": ""string"",

        ""property2"": ""string""

    },

    ""filter"": {

        ""must"": [

            {

                ""key"": ""color"",

                ""match"": {

                    ""value"": ""red""

                }

            }

        ]

    }

}

```



```python

client.set_payload(

    collection_name=""{collection_name}"",

    payload={

        ""property1"": ""string"",","['- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', 'Since the `filter` parameter is specified, the search is performed only among those points that satisfy the filter condition.\n\nSee details of possible filters and their work in the [filtering](../filtering) section.\n\n\n\nExample result of this API would be\n\n\n\n```json\n\n{\n\n  ""result"": [\n\n    { ""id"": 10, ""score"": 0.81 },\n\n    { ""id"": 14, ""score"": 0.75 },\n\n    { ""id"": 11, ""score"": 0.73 }\n\n  ],\n\n  ""status"": ""ok"",\n\n  ""time"": 0.001\n\n}\n\n```\n\n\n\nThe `result` contains ordered by `score` list of found point ids.', '{""color"": ""blue""},\n\n        ],\n\n        vectors=[\n\n            [0.9, 0.1, 0.1],\n\n            [0.1, 0.9, 0.1],\n\n            [0.1, 0.1, 0.9],\n\n        ],\n\n    ),\n\n    ordering=""strong"",\n\n)\n\n```\n\n\n\n```typescript\n\nclient.upsert(""{collection_name}"", {\n\n  batch: {\n\n    ids: [1, 2, 3],\n\n    payloads: [{ color: ""red"" }, { color: ""green"" }, { color: ""blue"" }],\n\n    vectors: [\n\n      [0.9, 0.1, 0.1],\n\n      [0.1, 0.9, 0.1],\n\n      [0.1, 0.1, 0.9],\n\n    ],\n\n  },\n\n  ordering: ""strong"",\n\n});\n\n```', ');\n\n```\n\n\n\nThe rule of thumb to set the memmap threshold parameter is simple:\n\n\n\n- if you have a balanced use scenario - set memmap threshold the same as `indexing_threshold` (default is 20000). In this case the optimizer will not make any extra runs and will optimize all thresholds at once.', '```http\n\nPOST /collections/{collection_name}/points/payload\n\n{\n\n    ""payload"": {\n\n        ""property1"": ""string"",\n\n        ""property2"": ""string""\n\n    },\n\n    ""filter"": {\n\n        ""must"": [\n\n            {\n\n                ""key"": ""color"",\n\n                ""match"": {\n\n                    ""value"": ""red""\n\n                }\n\n            }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.set_payload(\n\n    collection_name=""{collection_name}"",\n\n    payload={\n\n        ""property1"": ""string"",']","Question: How do you use ‘ordering’ parameter?\n\nContext: - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
Since the `filter` parameter is specified, the search is performed only among those points that satisfy the filter condition.

See details of possible filters and their work in the [filtering](../filtering) section.



Example result of this API would be



```json

{

""result"": [

{ ""id"": 10, ""score"": 0.81 },

{ ""id"": 14, ""score"": 0.75 },

{ ""id"": 11, ""score"": 0.73 }

],

""status"": ""ok"",

""time"": 0.001

}

```



The `result` contains ordered by `score` list of found point ids.
{""color"": ""blue""},

],

vectors=[

[0.9, 0.1, 0.1],

[0.1, 0.9, 0.1],

[0.1, 0.1, 0.9],

],

),

ordering=""strong"",

)

```



```typescript

client.upsert(""{collection_name}"", {

batch: {

ids: [1, 2, 3],

payloads: [{ color: ""red"" }, { color: ""green"" }, { color: ""blue"" }],

vectors: [

[0.9, 0.1, 0.1],

[0.1, 0.9, 0.1],

[0.1, 0.1, 0.9],

],

},

ordering: ""strong"",

});

```
);

```



The rule of thumb to set the memmap threshold parameter is simple:



- if you have a balanced use scenario - set memmap threshold the same as `indexing_threshold` (default is 20000). In this case the optimizer will not make any extra runs and will optimize all thresholds at once.
```http

POST /collections/{collection_name}/points/payload

{

""payload"": {

""property1"": ""string"",

""property2"": ""string""

},

""filter"": {

""must"": [

{

""key"": ""color"",

""match"": {

""value"": ""red""

}

}

]

}

}

```



```python

client.set_payload(

collection_name=""{collection_name}"",

payload={

""property1"": ""string"",\n\nAnswer:",How do you use ‘ordering’ parameter?,"Write ordering can be specified for any write request to serialize it through a single “leader” node, which ensures that all write operations (issued with the same ordering) are performed and observed sequentially. It is of 3 types weak , medium and strong and is used in python with additional param ordering=models.WriteOrdering.STRONG to upsert request.","['- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', 'Since the `filter` parameter is specified, the search is performed only among those points that satisfy the filter condition.\n\nSee details of possible filters and their work in the [filtering](../filtering) section.\n\n\n\nExample result of this API would be\n\n\n\n```json\n\n{\n\n  ""result"": [\n\n    { ""id"": 10, ""score"": 0.81 },\n\n    { ""id"": 14, ""score"": 0.75 },\n\n    { ""id"": 11, ""score"": 0.73 }\n\n  ],\n\n  ""status"": ""ok"",\n\n  ""time"": 0.001\n\n}\n\n```\n\n\n\nThe `result` contains ordered by `score` list of found point ids.', '{""color"": ""blue""},\n\n        ],\n\n        vectors=[\n\n            [0.9, 0.1, 0.1],\n\n            [0.1, 0.9, 0.1],\n\n            [0.1, 0.1, 0.9],\n\n        ],\n\n    ),\n\n    ordering=""strong"",\n\n)\n\n```\n\n\n\n```typescript\n\nclient.upsert(""{collection_name}"", {\n\n  batch: {\n\n    ids: [1, 2, 3],\n\n    payloads: [{ color: ""red"" }, { color: ""green"" }, { color: ""blue"" }],\n\n    vectors: [\n\n      [0.9, 0.1, 0.1],\n\n      [0.1, 0.9, 0.1],\n\n      [0.1, 0.1, 0.9],\n\n    ],\n\n  },\n\n  ordering: ""strong"",\n\n});\n\n```', ');\n\n```\n\n\n\nThe rule of thumb to set the memmap threshold parameter is simple:\n\n\n\n- if you have a balanced use scenario - set memmap threshold the same as `indexing_threshold` (default is 20000). In this case the optimizer will not make any extra runs and will optimize all thresholds at once.', '```http\n\nPOST /collections/{collection_name}/points/payload\n\n{\n\n    ""payload"": {\n\n        ""property1"": ""string"",\n\n        ""property2"": ""string""\n\n    },\n\n    ""filter"": {\n\n        ""must"": [\n\n            {\n\n                ""key"": ""color"",\n\n                ""match"": {\n\n                    ""value"": ""red""\n\n                }\n\n            }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.set_payload(\n\n    collection_name=""{collection_name}"",\n\n    payload={\n\n        ""property1"": ""string"",']","- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
Since the `filter` parameter is specified, the search is performed only among those points that satisfy the filter condition.

See details of possible filters and their work in the [filtering](../filtering) section.



Example result of this API would be



```json

{

  ""result"": [

    { ""id"": 10, ""score"": 0.81 },

    { ""id"": 14, ""score"": 0.75 },

    { ""id"": 11, ""score"": 0.73 }

  ],

  ""status"": ""ok"",

  ""time"": 0.001

}

```



The `result` contains ordered by `score` list of found point ids.
{""color"": ""blue""},

        ],

        vectors=[

            [0.9, 0.1, 0.1],

            [0.1, 0.9, 0.1],

            [0.1, 0.1, 0.9],

        ],

    ),

    ordering=""strong"",

)

```



```typescript

client.upsert(""{collection_name}"", {

  batch: {

    ids: [1, 2, 3],

    payloads: [{ color: ""red"" }, { color: ""green"" }, { color: ""blue"" }],

    vectors: [

      [0.9, 0.1, 0.1],

      [0.1, 0.9, 0.1],

      [0.1, 0.1, 0.9],

    ],

  },

  ordering: ""strong"",

});

```
);

```



The rule of thumb to set the memmap threshold parameter is simple:



- if you have a balanced use scenario - set memmap threshold the same as `indexing_threshold` (default is 20000). In this case the optimizer will not make any extra runs and will optimize all thresholds at once.
```http

POST /collections/{collection_name}/points/payload

{

    ""payload"": {

        ""property1"": ""string"",

        ""property2"": ""string""

    },

    ""filter"": {

        ""must"": [

            {

                ""key"": ""color"",

                ""match"": {

                    ""value"": ""red""

                }

            }

        ]

    }

}

```



```python

client.set_payload(

    collection_name=""{collection_name}"",

    payload={

        ""property1"": ""string"",",0.1590909090909091,0.6363636363636364,0.2545454545454546,0.0273972602739726,0.1111111111111111,0.04395604395604396,0.09545454545454546,0.38181818181818183,0.15272727272727274,0.12727272727272726,0.509090909090909,0.20363636363636362,0.4479013979434967,0.5175435543060303,0.602545976638794,0.5568193793296814,0.8803933806096514,0.16236194968223572,229,4.089285714285714,0.514743565227655
True,,3187.47,64fee08f,What is significance of ‘on_disk_payload’ setting?,"The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes.","The significance of the `on_disk_payload` setting is that it defines where to store the payload data within a collection. When set to `true`, the payload data will be stored on disk only, rather than in memory. This setting can be useful for limiting RAM usage, especially when dealing with collections that have large payload data.

When `on_disk_payload` is set to `true`, it means that the point's payload will not be stored in memory. Instead, it will be read from the disk every time it is requested. This can help save RAM, although it may slightly increase the response time since the data needs to be fetched from disk.

It's important to note that only the payload values that are involved in filtering and are indexed will remain in RAM when `on_disk_payload` is set to `true`.

Overall, using `on_disk_payload` can be beneficial in scenarios where RAM usage needs to be limited, especially for collections with large payload data.","* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.

* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.
temp_path: null



  # If true - point's payload will not be stored in memory.

  # It will be read from the disk every time it is requested.

  # This setting saves RAM by (slightly) increasing the response time.

  # Note: those payload values that are involved in filtering and are indexed - remain in RAM.

  on_disk_payload: true



  # Maximum number of concurrent updates to shard replicas

  # If `null` - maximum concurrency is used.

  update_concurrency: null
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
InMemory payload storage is organized in the same way as in-memory vectors.

The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.

This type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.



In the case of large payload values, it might be better to use OnDisk payload storage.
""on_disk"": true

    }

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),

    hnsw_config=models.HnswConfigDiff(on_disk=True),

)

```



```typescript","['* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.\n\n* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\n\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.', ""temp_path: null\n\n\n\n  # If true - point's payload will not be stored in memory.\n\n  # It will be read from the disk every time it is requested.\n\n  # This setting saves RAM by (slightly) increasing the response time.\n\n  # Note: those payload values that are involved in filtering and are indexed - remain in RAM.\n\n  on_disk_payload: true\n\n\n\n  # Maximum number of concurrent updates to shard replicas\n\n  # If `null` - maximum concurrency is used.\n\n  update_concurrency: null"", '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', 'InMemory payload storage is organized in the same way as in-memory vectors.\n\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\n\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\n\n\n\nIn the case of large payload values, it might be better to use OnDisk payload storage.', '""on_disk"": true\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),\n\n    hnsw_config=models.HnswConfigDiff(on_disk=True),\n\n)\n\n```\n\n\n\n```typescript']","Question: What is significance of ‘on_disk_payload’ setting?\n\nContext: * `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.

* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.
temp_path: null



# If true - point's payload will not be stored in memory.

# It will be read from the disk every time it is requested.

# This setting saves RAM by (slightly) increasing the response time.

# Note: those payload values that are involved in filtering and are indexed - remain in RAM.

on_disk_payload: true



# Maximum number of concurrent updates to shard replicas

# If `null` - maximum concurrency is used.

update_concurrency: null
""on_disk_payload"": false

},

""hnsw_config"": {

""m"": 16,

""ef_construct"": 100,

""full_scan_threshold"": 10000,

""max_indexing_threads"": 0

},

""optimizer_config"": {

""deleted_threshold"": 0.2,

""vacuum_min_vector_number"": 1000,

""default_segment_number"": 0,

""max_segment_size"": null,
InMemory payload storage is organized in the same way as in-memory vectors.

The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.

This type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.



In the case of large payload values, it might be better to use OnDisk payload storage.
""on_disk"": true

}

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),

hnsw_config=models.HnswConfigDiff(on_disk=True),

)

```



```typescript\n\nAnswer:",What is significance of ‘on_disk_payload’ setting?,"The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes.","['* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.\n\n* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\n\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.', ""temp_path: null\n\n\n\n  # If true - point's payload will not be stored in memory.\n\n  # It will be read from the disk every time it is requested.\n\n  # This setting saves RAM by (slightly) increasing the response time.\n\n  # Note: those payload values that are involved in filtering and are indexed - remain in RAM.\n\n  on_disk_payload: true\n\n\n\n  # Maximum number of concurrent updates to shard replicas\n\n  # If `null` - maximum concurrency is used.\n\n  update_concurrency: null"", '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', 'InMemory payload storage is organized in the same way as in-memory vectors.\n\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\n\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\n\n\n\nIn the case of large payload values, it might be better to use OnDisk payload storage.', '""on_disk"": true\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),\n\n    hnsw_config=models.HnswConfigDiff(on_disk=True),\n\n)\n\n```\n\n\n\n```typescript']","* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.

* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.
temp_path: null



  # If true - point's payload will not be stored in memory.

  # It will be read from the disk every time it is requested.

  # This setting saves RAM by (slightly) increasing the response time.

  # Note: those payload values that are involved in filtering and are indexed - remain in RAM.

  on_disk_payload: true



  # Maximum number of concurrent updates to shard replicas

  # If `null` - maximum concurrency is used.

  update_concurrency: null
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
InMemory payload storage is organized in the same way as in-memory vectors.

The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.

This type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.



In the case of large payload values, it might be better to use OnDisk payload storage.
""on_disk"": true

    }

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),

    hnsw_config=models.HnswConfigDiff(on_disk=True),

)

```



```typescript",0.4394904458598726,0.7340425531914894,0.5498007968127491,0.28205128205128205,0.4731182795698925,0.3534136546184739,0.35668789808917195,0.5957446808510638,0.44621513944223107,0.4394904458598726,0.7340425531914894,0.5498007968127491,0.8574696183204651,0.6483508944511414,0.7110214233398438,0.678241491317749,0.9403638030053116,0.9006570316851139,159,1.606060606060606,0.4710657473354399
True,,2605.621,a9a81c9c,What is the impact of ‘write_consistency_factor’ ?,"The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully.","The impact of the `write_consistency_factor` is significant in distributed systems where data is replicated across multiple nodes. By setting the `write_consistency_factor` to a value greater than one, it ensures that a certain number of replicas must acknowledge a write operation before it is considered successful.

Increasing the `write_consistency_factor` makes the write operations more resilient to network partitions and ensures that data consistency is maintained even in the presence of node failures or network issues. However, it also means that a higher number of replicas need to be active and reachable in order to perform the write operations successfully.

In essence, the `write_consistency_factor` parameter plays a crucial role in balancing the trade-off between consistency and availability in distributed systems. By adjusting this factor, system administrators can tailor the level of consistency required for their specific use case while considering the impact on system performance and resilience.","- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

    ""write_consistency_factor"": 2,

}

```



```python
""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

    shard_number=6,

    replication_factor=2,

    write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

	shardNumber: 6,

	replicationFactor: 2,

	writeConsistencyFactor: 2

);

```



Write operations will fail if the number of active replicas is less than the `write_consistency_factor`.



### Read consistency



Read `consistency` can be specified for most read requests and will ensure that the returned result

is consistent across cluster nodes.
replication_factor: Some(2),

        write_consistency_factor: Some(2),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =","['- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.', '### Write consistency factor\n\n\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\n\nIt can be configured at the collection\'s creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python', '""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n\n    shard_number=6,\n\n    replication_factor=2,\n\n    write_consistency_factor=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', 'collectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\n\tshardNumber: 6,\n\n\treplicationFactor: 2,\n\n\twriteConsistencyFactor: 2\n\n);\n\n```\n\n\n\nWrite operations will fail if the number of active replicas is less than the `write_consistency_factor`.\n\n\n\n### Read consistency\n\n\n\nRead `consistency` can be specified for most read requests and will ensure that the returned result\n\nis consistent across cluster nodes.', 'replication_factor: Some(2),\n\n        write_consistency_factor: Some(2),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =']","Question: What is the impact of ‘write_consistency_factor’ ?\n\nContext: - `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 300,

""distance"": ""Cosine""

},

""shard_number"": 6,

""replication_factor"": 2,

""write_consistency_factor"": 2,

}

```



```python
""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

shard_number=6,

replication_factor=2,

write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
collectionName: ""{collection_name}"",

vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

shardNumber: 6,

replicationFactor: 2,

writeConsistencyFactor: 2

);

```



Write operations will fail if the number of active replicas is less than the `write_consistency_factor`.



### Read consistency



Read `consistency` can be specified for most read requests and will ensure that the returned result

is consistent across cluster nodes.
replication_factor: Some(2),

write_consistency_factor: Some(2),

..Default::default()

})

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =\n\nAnswer:",What is the impact of ‘write_consistency_factor’ ?,"The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully.","['- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.', '### Write consistency factor\n\n\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\n\nIt can be configured at the collection\'s creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python', '""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n\n    shard_number=6,\n\n    replication_factor=2,\n\n    write_consistency_factor=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', 'collectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\n\tshardNumber: 6,\n\n\treplicationFactor: 2,\n\n\twriteConsistencyFactor: 2\n\n);\n\n```\n\n\n\nWrite operations will fail if the number of active replicas is less than the `write_consistency_factor`.\n\n\n\n### Read consistency\n\n\n\nRead `consistency` can be specified for most read requests and will ensure that the returned result\n\nis consistent across cluster nodes.', 'replication_factor: Some(2),\n\n        write_consistency_factor: Some(2),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =']","- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

    ""write_consistency_factor"": 2,

}

```



```python
""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

    shard_number=6,

    replication_factor=2,

    write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

	shardNumber: 6,

	replicationFactor: 2,

	writeConsistencyFactor: 2

);

```



Write operations will fail if the number of active replicas is less than the `write_consistency_factor`.



### Read consistency



Read `consistency` can be specified for most read requests and will ensure that the returned result

is consistent across cluster nodes.
replication_factor: Some(2),

        write_consistency_factor: Some(2),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =",0.3333333333333333,0.8214285714285714,0.4742268041237113,0.2116788321167883,0.5272727272727272,0.3020833333333333,0.2826086956521739,0.6964285714285714,0.40206185567010305,0.32608695652173914,0.8035714285714286,0.46391752577319595,0.8645058274269104,0.6574887037277222,0.7773781418800354,0.7124247550964355,0.9291331268226107,0.9414428891032003,145,2.338709677419355,0.7434092079894886
True,,1828.379,4f029348,What is the purpose of oversampling in Qdrant search process?,Oversampling in Qdrant search process defines how many extra vectors should be pre-selected using quantized index and then re-scored using original vectors to improve search quality.,"The purpose of oversampling in the Qdrant search process is to improve the accuracy and performance of similarity search algorithms by compensating for the accuracy loss that may occur during the compression of high-dimensional vectors in memory. By oversampling, users are able to retrieve more vectors than actually needed from the quantized storage. These extra vectors are then rescored with the original vectors, providing a more precise score. This allows for a more controlled precision of the search in real time, ensuring that only the most relevant vectors are selected for the user.","### Oversampling and Rescoring



A distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.

This enables the best combination of speed, accuracy, and RAM usage.



Qdrant stores the original vectors, so it is possible to rescore the top-k results with

the original vectors after doing the neighbours search in quantized space. That obviously
### Oversampling for quantization



We are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.
Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user
1. Asynchronous I/O interface: Reduce overhead by managing I/O operations asynchronously, thus minimizing context switches.

2. Oversampling for Quantization: Improve the accuracy and performance of your queries while using Scalar or Product Quantization.

3. Grouping API lookup: Storage optimization method that lets you look for points in another collection using group ids.

4. Qdrant Web UI: A convenient dashboard to help you manage data stored in Qdrant.
```http

POST /collections/{collection_name}/points/search

{

    ""params"": {

        ""quantization"": {

            ""ignore"": false,

            ""rescore"": true,

            ""oversampling"": 2.0

        }

    },

    ""vector"": [0.2, 0.1, 0.9, 0.7],

    ""limit"": 10

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.search(

    collection_name=""{collection_name}"",","['### Oversampling and Rescoring\n\n\n\nA distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.\n\nThis enables the best combination of speed, accuracy, and RAM usage.\n\n\n\nQdrant stores the original vectors, so it is possible to rescore the top-k results with\n\nthe original vectors after doing the neighbours search in quantized space. That obviously', '### Oversampling for quantization\n\n\n\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.', 'Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user', '1. Asynchronous I/O interface: Reduce overhead by managing I/O operations asynchronously, thus minimizing context switches.\n\n2. Oversampling for Quantization: Improve the accuracy and performance of your queries while using Scalar or Product Quantization.\n\n3. Grouping API lookup: Storage optimization method that lets you look for points in another collection using group ids.\n\n4. Qdrant Web UI: A convenient dashboard to help you manage data stored in Qdrant.', '```http\n\nPOST /collections/{collection_name}/points/search\n\n{\n\n    ""params"": {\n\n        ""quantization"": {\n\n            ""ignore"": false,\n\n            ""rescore"": true,\n\n            ""oversampling"": 2.0\n\n        }\n\n    },\n\n    ""vector"": [0.2, 0.1, 0.9, 0.7],\n\n    ""limit"": 10\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.search(\n\n    collection_name=""{collection_name}"",']","Question: What is the purpose of oversampling in Qdrant search process?\n\nContext: ### Oversampling and Rescoring



A distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.

This enables the best combination of speed, accuracy, and RAM usage.



Qdrant stores the original vectors, so it is possible to rescore the top-k results with

the original vectors after doing the neighbours search in quantized space. That obviously
### Oversampling for quantization



We are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.
Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user
1. Asynchronous I/O interface: Reduce overhead by managing I/O operations asynchronously, thus minimizing context switches.

2. Oversampling for Quantization: Improve the accuracy and performance of your queries while using Scalar or Product Quantization.

3. Grouping API lookup: Storage optimization method that lets you look for points in another collection using group ids.

4. Qdrant Web UI: A convenient dashboard to help you manage data stored in Qdrant.
```http

POST /collections/{collection_name}/points/search

{

""params"": {

""quantization"": {

""ignore"": false,

""rescore"": true,

""oversampling"": 2.0

}

},

""vector"": [0.2, 0.1, 0.9, 0.7],

""limit"": 10

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.search(

collection_name=""{collection_name}"",\n\nAnswer:",What is the purpose of oversampling in Qdrant search process?,Oversampling in Qdrant search process defines how many extra vectors should be pre-selected using quantized index and then re-scored using original vectors to improve search quality.,"['### Oversampling and Rescoring\n\n\n\nA distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.\n\nThis enables the best combination of speed, accuracy, and RAM usage.\n\n\n\nQdrant stores the original vectors, so it is possible to rescore the top-k results with\n\nthe original vectors after doing the neighbours search in quantized space. That obviously', '### Oversampling for quantization\n\n\n\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.', 'Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user', '1. Asynchronous I/O interface: Reduce overhead by managing I/O operations asynchronously, thus minimizing context switches.\n\n2. Oversampling for Quantization: Improve the accuracy and performance of your queries while using Scalar or Product Quantization.\n\n3. Grouping API lookup: Storage optimization method that lets you look for points in another collection using group ids.\n\n4. Qdrant Web UI: A convenient dashboard to help you manage data stored in Qdrant.', '```http\n\nPOST /collections/{collection_name}/points/search\n\n{\n\n    ""params"": {\n\n        ""quantization"": {\n\n            ""ignore"": false,\n\n            ""rescore"": true,\n\n            ""oversampling"": 2.0\n\n        }\n\n    },\n\n    ""vector"": [0.2, 0.1, 0.9, 0.7],\n\n    ""limit"": 10\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.search(\n\n    collection_name=""{collection_name}"",']","### Oversampling and Rescoring



A distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.

This enables the best combination of speed, accuracy, and RAM usage.



Qdrant stores the original vectors, so it is possible to rescore the top-k results with

the original vectors after doing the neighbours search in quantized space. That obviously
### Oversampling for quantization



We are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.
Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user
1. Asynchronous I/O interface: Reduce overhead by managing I/O operations asynchronously, thus minimizing context switches.

2. Oversampling for Quantization: Improve the accuracy and performance of your queries while using Scalar or Product Quantization.

3. Grouping API lookup: Storage optimization method that lets you look for points in another collection using group ids.

4. Qdrant Web UI: A convenient dashboard to help you manage data stored in Qdrant.
```http

POST /collections/{collection_name}/points/search

{

    ""params"": {

        ""quantization"": {

            ""ignore"": false,

            ""rescore"": true,

            ""oversampling"": 2.0

        }

    },

    ""vector"": [0.2, 0.1, 0.9, 0.7],

    ""limit"": 10

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.search(

    collection_name=""{collection_name}"",",0.2073170731707317,0.6071428571428571,0.3090909090909091,0.08641975308641975,0.25925925925925924,0.12962962962962962,0.13414634146341464,0.39285714285714285,0.19999999999999998,0.18292682926829268,0.5357142857142857,0.2727272727272727,0.8531549572944641,0.6305100321769714,0.7483362555503845,0.6843888759613037,0.9984677076863591,0.8672286123037338,93,3.576923076923077,0.42819255584697846
True,,2878.1569999999997,af0ef26c,How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?,"Qdrant uses a different approach that does not require pre- or post-filtering, effectively addressing the accuracy problem while maintaining search efficiency.","Qdrant addresses the search accuracy problem in comparison to other search engines by utilizing the HNSW (Hierarchical Navigable Small World) algorithm in a unique way. Instead of relying on pre-filtering or post-filtering, Qdrant's custom modification of the HNSW algorithm allows for querying results with state-of-the-art speed while also applying filters without compromising on accuracy.

By not requiring a binary mask of the whole dataset for pre-filtering, Qdrant avoids scalability issues associated with other search engines. This approach helps maintain connectivity within the HNSW graph even when many vectors are filtered out, thus improving search accuracy.

Moreover, Qdrant provides the flexibility to configure HNSW parameters on a collection and named vector level, allowing for fine-tuning of search performance. This customization, combined with cloud-native support for distributed deployment and replications, makes Qdrant suitable for high-throughput applications with real-time latency requirements.

Overall, Qdrant's approach using the modified HNSW algorithm offers a balance between speed, accuracy, and scalability, distinguishing it from other search engines that rely on pre-filtering methods.","- Others use **pre-filtering**, which requires a binary mask of the whole dataset to be passed into the ANN algorithm. It is also not scalable, as the mask size grows linearly with the dataset size.



On top of it, there is also a problem with search accuracy.

It appears if too many vectors are filtered out, so the HNSW graph becomes disconnected.



Qdrant uses a different approach, not requiring pre- or post-filtering while addressing the accuracy problem.
HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements
Read more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.
## Minor improvements



Beyond introducing new features, Qdrant 1.7.0 enhances performance and addresses various minor issues. Here's a rundown of the key improvements:



1. Improvement of HNSW Index Building on High CPU Systems ([PR#2869](https://github.com/qdrant/qdrant/pull/2869)).



2. Improving [Search Tail Latencies](https://github.com/qdrant/qdrant/pull/2931): improvement for high CPU systems with many parallel searches, directly impacting the user experience by reducing latency.","['- Others use **pre-filtering**, which requires a binary mask of the whole dataset to be passed into the ANN algorithm. It is also not scalable, as the mask size grows linearly with the dataset size.\n\n\n\nOn top of it, there is also a problem with search accuracy.\n\nIt appears if too many vectors are filtered out, so the HNSW graph becomes disconnected.\n\n\n\nQdrant uses a different approach, not requiring pre- or post-filtering while addressing the accuracy problem.', 'HNSW is chosen for several reasons.\n\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\n\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\n\n\n\n*Available as of v1.1.1*\n\n\n\nThe HNSW parameters can also be configured on a collection and named vector\n\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\n\nperformance.', 'Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements', 'Read more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.', ""## Minor improvements\n\n\n\nBeyond introducing new features, Qdrant 1.7.0 enhances performance and addresses various minor issues. Here's a rundown of the key improvements:\n\n\n\n1. Improvement of HNSW Index Building on High CPU Systems ([PR#2869](https://github.com/qdrant/qdrant/pull/2869)).\n\n\n\n2. Improving [Search Tail Latencies](https://github.com/qdrant/qdrant/pull/2931): improvement for high CPU systems with many parallel searches, directly impacting the user experience by reducing latency.""]","Question: How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?\n\nContext: - Others use **pre-filtering**, which requires a binary mask of the whole dataset to be passed into the ANN algorithm. It is also not scalable, as the mask size grows linearly with the dataset size.



On top of it, there is also a problem with search accuracy.

It appears if too many vectors are filtered out, so the HNSW graph becomes disconnected.



Qdrant uses a different approach, not requiring pre- or post-filtering while addressing the accuracy problem.
HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements
Read more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.
## Minor improvements



Beyond introducing new features, Qdrant 1.7.0 enhances performance and addresses various minor issues. Here's a rundown of the key improvements:



1. Improvement of HNSW Index Building on High CPU Systems ([PR#2869](https://github.com/qdrant/qdrant/pull/2869)).



2. Improving [Search Tail Latencies](https://github.com/qdrant/qdrant/pull/2931): improvement for high CPU systems with many parallel searches, directly impacting the user experience by reducing latency.\n\nAnswer:",How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?,"Qdrant uses a different approach that does not require pre- or post-filtering, effectively addressing the accuracy problem while maintaining search efficiency.","['- Others use **pre-filtering**, which requires a binary mask of the whole dataset to be passed into the ANN algorithm. It is also not scalable, as the mask size grows linearly with the dataset size.\n\n\n\nOn top of it, there is also a problem with search accuracy.\n\nIt appears if too many vectors are filtered out, so the HNSW graph becomes disconnected.\n\n\n\nQdrant uses a different approach, not requiring pre- or post-filtering while addressing the accuracy problem.', 'HNSW is chosen for several reasons.\n\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\n\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\n\n\n\n*Available as of v1.1.1*\n\n\n\nThe HNSW parameters can also be configured on a collection and named vector\n\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\n\nperformance.', 'Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements', 'Read more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.', ""## Minor improvements\n\n\n\nBeyond introducing new features, Qdrant 1.7.0 enhances performance and addresses various minor issues. Here's a rundown of the key improvements:\n\n\n\n1. Improvement of HNSW Index Building on High CPU Systems ([PR#2869](https://github.com/qdrant/qdrant/pull/2869)).\n\n\n\n2. Improving [Search Tail Latencies](https://github.com/qdrant/qdrant/pull/2931): improvement for high CPU systems with many parallel searches, directly impacting the user experience by reducing latency.""]","- Others use **pre-filtering**, which requires a binary mask of the whole dataset to be passed into the ANN algorithm. It is also not scalable, as the mask size grows linearly with the dataset size.



On top of it, there is also a problem with search accuracy.

It appears if too many vectors are filtered out, so the HNSW graph becomes disconnected.



Qdrant uses a different approach, not requiring pre- or post-filtering while addressing the accuracy problem.
HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements
Read more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.
## Minor improvements



Beyond introducing new features, Qdrant 1.7.0 enhances performance and addresses various minor issues. Here's a rundown of the key improvements:



1. Improvement of HNSW Index Building on High CPU Systems ([PR#2869](https://github.com/qdrant/qdrant/pull/2869)).



2. Improving [Search Tail Latencies](https://github.com/qdrant/qdrant/pull/2931): improvement for high CPU systems with many parallel searches, directly impacting the user experience by reducing latency.",0.0963855421686747,0.8,0.17204301075268819,0.024242424242424242,0.21052631578947367,0.04347826086956522,0.04819277108433735,0.4,0.08602150537634409,0.08433734939759036,0.7,0.15053763440860216,0.6812425255775452,0.4994025230407715,0.6564987301826477,0.56727534532547,0.9985128239745141,0.9981409424217418,167,7.9523809523809526,0.5401797852900786
True,,3209.749,b740c0cd,What is the difference between regular and neural search?,"Regular full-text search involves searching for keywords within a document, while neural search considers the real meaning of the query and documents, allowing for more accurate results.","Regular search and neural search differ in their approach to information retrieval. 

Regular search, like Google's full-text search, primarily involves searching for keywords within a document. This approach can lead to limitations as the algorithm may not fully understand the actual meaning of the query and the documents being searched. Consequently, many relevant documents might not be found due to variations in wording.

On the other hand, neural search utilizes neural networks to enhance search capabilities. Neural networks can analyze and understand the context, semantics, and relationships within the text, enabling more sophisticated and accurate search results. This allows for a more nuanced understanding of the query and the documents, leading to improved search relevance and performance.

In summary, the main difference between regular and neural search lies in the depth of understanding and analysis of the search queries and documents. Neural search leverages advanced technology to provide more precise and contextually relevant search results compared to traditional keyword-based searches.","In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



**What is neural search?**
These days, search technology is the heart of a variety of applications.

From web-pages search to product recommendations.

For many years, this technology didn't get much change until neural networks came into play.



In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?
**What is neural search?**



A regular full-text search, such as Google’s, consists of searching for keywords inside a document. For this reason, the algorithm can not take into account the real meaning of the query and documents. Many documents that might be of interest to the user are not found because they use different wording.
* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



## What is neural search?



A regular full-text search, such as Google's, consists of searching for keywords inside a document.

For this reason, the algorithm can not take into account the real meaning of the query and documents.

Many documents that might be of interest to the user are not found because they use different wording.
The demo contains a switch that selects between neural and full-text searches.

You can turn neural search on and off to compare the result with regular full-text search.

Try to use startup description to find similar ones. 



## Conclusion



In this tutorial, I have tried to give minimal information about neural search, but enough to start using it.

Many potential applications are not mentioned here, this is a space to go further into the subject.","['In this tutorial we are going to find answers to these questions:\n\n\n\n* What is the difference between regular and neural search?\n\n* What neural networks could be used for search?\n\n* In what tasks is neural network search useful?\n\n* How to build and deploy own neural search service step-by-step?\n\n\n\n**What is neural search?**', ""These days, search technology is the heart of a variety of applications.\n\nFrom web-pages search to product recommendations.\n\nFor many years, this technology didn't get much change until neural networks came into play.\n\n\n\nIn this tutorial we are going to find answers to these questions:\n\n\n\n* What is the difference between regular and neural search?\n\n* What neural networks could be used for search?\n\n* In what tasks is neural network search useful?"", '**What is neural search?**\n\n\n\nA regular full-text search, such as Google’s, consists of searching for keywords inside a document. For this reason, the algorithm can not take into account the real meaning of the query and documents. Many documents that might be of interest to the user are not found because they use different wording.', ""* In what tasks is neural network search useful?\n\n* How to build and deploy own neural search service step-by-step?\n\n\n\n## What is neural search?\n\n\n\nA regular full-text search, such as Google's, consists of searching for keywords inside a document.\n\nFor this reason, the algorithm can not take into account the real meaning of the query and documents.\n\nMany documents that might be of interest to the user are not found because they use different wording."", 'The demo contains a switch that selects between neural and full-text searches.\n\nYou can turn neural search on and off to compare the result with regular full-text search.\n\nTry to use startup description to find similar ones. \n\n\n\n## Conclusion\n\n\n\nIn this tutorial, I have tried to give minimal information about neural search, but enough to start using it.\n\nMany potential applications are not mentioned here, this is a space to go further into the subject.']","Question: What is the difference between regular and neural search?\n\nContext: In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



**What is neural search?**
These days, search technology is the heart of a variety of applications.

From web-pages search to product recommendations.

For many years, this technology didn't get much change until neural networks came into play.



In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?
**What is neural search?**



A regular full-text search, such as Google’s, consists of searching for keywords inside a document. For this reason, the algorithm can not take into account the real meaning of the query and documents. Many documents that might be of interest to the user are not found because they use different wording.
* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



## What is neural search?



A regular full-text search, such as Google's, consists of searching for keywords inside a document.

For this reason, the algorithm can not take into account the real meaning of the query and documents.

Many documents that might be of interest to the user are not found because they use different wording.
The demo contains a switch that selects between neural and full-text searches.

You can turn neural search on and off to compare the result with regular full-text search.

Try to use startup description to find similar ones. 



## Conclusion



In this tutorial, I have tried to give minimal information about neural search, but enough to start using it.

Many potential applications are not mentioned here, this is a space to go further into the subject.\n\nAnswer:",What is the difference between regular and neural search?,"Regular full-text search involves searching for keywords within a document, while neural search considers the real meaning of the query and documents, allowing for more accurate results.","['In this tutorial we are going to find answers to these questions:\n\n\n\n* What is the difference between regular and neural search?\n\n* What neural networks could be used for search?\n\n* In what tasks is neural network search useful?\n\n* How to build and deploy own neural search service step-by-step?\n\n\n\n**What is neural search?**', ""These days, search technology is the heart of a variety of applications.\n\nFrom web-pages search to product recommendations.\n\nFor many years, this technology didn't get much change until neural networks came into play.\n\n\n\nIn this tutorial we are going to find answers to these questions:\n\n\n\n* What is the difference between regular and neural search?\n\n* What neural networks could be used for search?\n\n* In what tasks is neural network search useful?"", '**What is neural search?**\n\n\n\nA regular full-text search, such as Google’s, consists of searching for keywords inside a document. For this reason, the algorithm can not take into account the real meaning of the query and documents. Many documents that might be of interest to the user are not found because they use different wording.', ""* In what tasks is neural network search useful?\n\n* How to build and deploy own neural search service step-by-step?\n\n\n\n## What is neural search?\n\n\n\nA regular full-text search, such as Google's, consists of searching for keywords inside a document.\n\nFor this reason, the algorithm can not take into account the real meaning of the query and documents.\n\nMany documents that might be of interest to the user are not found because they use different wording."", 'The demo contains a switch that selects between neural and full-text searches.\n\nYou can turn neural search on and off to compare the result with regular full-text search.\n\nTry to use startup description to find similar ones. \n\n\n\n## Conclusion\n\n\n\nIn this tutorial, I have tried to give minimal information about neural search, but enough to start using it.\n\nMany potential applications are not mentioned here, this is a space to go further into the subject.']","In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



**What is neural search?**
These days, search technology is the heart of a variety of applications.

From web-pages search to product recommendations.

For many years, this technology didn't get much change until neural networks came into play.



In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?
**What is neural search?**



A regular full-text search, such as Google’s, consists of searching for keywords inside a document. For this reason, the algorithm can not take into account the real meaning of the query and documents. Many documents that might be of interest to the user are not found because they use different wording.
* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



## What is neural search?



A regular full-text search, such as Google's, consists of searching for keywords inside a document.

For this reason, the algorithm can not take into account the real meaning of the query and documents.

Many documents that might be of interest to the user are not found because they use different wording.
The demo contains a switch that selects between neural and full-text searches.

You can turn neural search on and off to compare the result with regular full-text search.

Try to use startup description to find similar ones. 



## Conclusion



In this tutorial, I have tried to give minimal information about neural search, but enough to start using it.

Many potential applications are not mentioned here, this is a space to go further into the subject.",0.1476510067114094,0.88,0.25287356321839083,0.0945945945945946,0.5833333333333334,0.16279069767441862,0.12751677852348994,0.76,0.21839080459770116,0.1342281879194631,0.8,0.22988505747126436,0.8760069608688354,0.5511342287063599,0.7011467218399048,0.6171553730964661,0.8647978560410492,0.980741485953331,161,5.962962962962963,0.2202923699638814
True,,4662.188,a8e79d86,How can I use Qdrant as a vector store in Langchain Go?,"You can use Qdrant as a vector store in Langchain Go by installing the `langchain-go` project dependency and customizing the values for your configuration, such as the Qdrant REST URL and collection name.","To use Qdrant as a vector store in Langchain Go, you can follow these steps:

1. Install the `langchain-go` project dependency:

```bash
go get -u github.com/tmc/langchaingo
```

2. Customize the configuration values in the code sample below for your setup:

```go
import (
    ""fmt""
    ""log""
    ""net/url""

    ""github.com/tmc/langchaingo/embeddings""
    ""github.com/tmc/langchaingo/llms/openai""
    ""github.com/tmc/langchaingo/vectorstores""
    ""github.com/tmc/langchaingo/vectorstores/qdrant""
)

llm, err := openai.New()
if err != nil {
    log.Fatal(err)
}

e, err := embeddings.NewEmbedder(llm)
if err != nil {
    log.Fatal(err)
}

url, err := url.Parse(""YOUR_QDRANT_REST_URL"")
if err != nil {
    log.Fatal(err)
}

store, err := qdrant.New(
    qdrant.WithURL(*url),
    qdrant.WithCollectionName(""YOUR_COLLECTION_NAME""),
    qdrant.WithEmbedder(e),
)

if err != nil {
    log.Fatal(err)
}
```

Replace `YOUR_QDRANT_REST_URL` with the URL of your Qdrant instance (e.g., `http://localhost:6333`) and `YOUR_COLLECTION_NAME` with the name of your collection. Make sure you have set up Qdrant according to your requirements before using it as a vector store in Langchain Go.","---

title: Langchain Go

weight: 120

---



# Langchain Go



[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.



You can use Qdrant as a vector store in Langchain Go.



## Setup



Install the `langchain-go` project dependency



```bash

go get -u github.com/tmc/langchaingo

```



## Usage



Before you use the following code sample, customize the following values for your configuration:
""github.com/tmc/langchaingo/vectorstores""

        ""github.com/tmc/langchaingo/vectorstores/qdrant""

)



 llm, err := openai.New()

 if err != nil {

  log.Fatal(err)

 }



 e, err := embeddings.NewEmbedder(llm)

 if err != nil {

  log.Fatal(err)

 }



 url, err := url.Parse(""YOUR_QDRANT_REST_URL"")

 if err != nil {

  log.Fatal(err)

 }



 store, err := qdrant.New(

  qdrant.WithURL(*url),

  qdrant.WithCollectionName(""YOUR_COLLECTION_NAME""),

  qdrant.WithEmbedder(e),

 )

 if err != nil {
---

title: Langchain4J

weight: 110

---



# LangChain for Java



LangChain for Java, also known as [Langchain4J](https://github.com/langchain4j/langchain4j), is a community port of [Langchain](https://www.langchain.com/) for building context-aware AI applications in Java



You can use Qdrant as a vector store in Langchain4J through the [`langchain4j-qdrant`](https://central.sonatype.com/artifact/dev.langchain4j/langchain4j-qdrant) module.



## Setup
```bash

pip install langchain

```



Qdrant acts as a vector index that may store the embeddings with the documents used to generate them. There are various ways 

how to use it, but calling `Qdrant.from_texts` is probably the most straightforward way how to get started:



```python

from langchain.vectorstores import Qdrant

from langchain.embeddings import HuggingFaceEmbeddings



embeddings = HuggingFaceEmbeddings(

    model_name=""sentence-transformers/all-mpnet-base-v2""

)
- `YOUR_QDRANT_REST_URL`: If you've set up Qdrant using the [Quick Start](/documentation/quick-start/) guide,

  set this value to `http://localhost:6333`.

- `YOUR_COLLECTION_NAME`: Use our [Collections](/documentation/concepts/collections) guide to create or

  list collections.



```go

import (

        ""fmt""

        ""log""



        ""github.com/tmc/langchaingo/embeddings""

        ""github.com/tmc/langchaingo/llms/openai""

        ""github.com/tmc/langchaingo/vectorstores""","['---\n\ntitle: Langchain Go\n\nweight: 120\n\n---\n\n\n\n# Langchain Go\n\n\n\n[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.\n\n\n\nYou can use Qdrant as a vector store in Langchain Go.\n\n\n\n## Setup\n\n\n\nInstall the `langchain-go` project dependency\n\n\n\n```bash\n\ngo get -u github.com/tmc/langchaingo\n\n```\n\n\n\n## Usage\n\n\n\nBefore you use the following code sample, customize the following values for your configuration:', '""github.com/tmc/langchaingo/vectorstores""\n\n        ""github.com/tmc/langchaingo/vectorstores/qdrant""\n\n)\n\n\n\n llm, err := openai.New()\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n\n\n e, err := embeddings.NewEmbedder(llm)\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n\n\n url, err := url.Parse(""YOUR_QDRANT_REST_URL"")\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n\n\n store, err := qdrant.New(\n\n  qdrant.WithURL(*url),\n\n  qdrant.WithCollectionName(""YOUR_COLLECTION_NAME""),\n\n  qdrant.WithEmbedder(e),\n\n )\n\n if err != nil {', '---\n\ntitle: Langchain4J\n\nweight: 110\n\n---\n\n\n\n# LangChain for Java\n\n\n\nLangChain for Java, also known as [Langchain4J](https://github.com/langchain4j/langchain4j), is a community port of [Langchain](https://www.langchain.com/) for building context-aware AI applications in Java\n\n\n\nYou can use Qdrant as a vector store in Langchain4J through the [`langchain4j-qdrant`](https://central.sonatype.com/artifact/dev.langchain4j/langchain4j-qdrant) module.\n\n\n\n## Setup', '```bash\n\npip install langchain\n\n```\n\n\n\nQdrant acts as a vector index that may store the embeddings with the documents used to generate them. There are various ways \n\nhow to use it, but calling `Qdrant.from_texts` is probably the most straightforward way how to get started:\n\n\n\n```python\n\nfrom langchain.vectorstores import Qdrant\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\n\n\nembeddings = HuggingFaceEmbeddings(\n\n    model_name=""sentence-transformers/all-mpnet-base-v2""\n\n)', '- `YOUR_QDRANT_REST_URL`: If you\'ve set up Qdrant using the [Quick Start](/documentation/quick-start/) guide,\n\n  set this value to `http://localhost:6333`.\n\n- `YOUR_COLLECTION_NAME`: Use our [Collections](/documentation/concepts/collections) guide to create or\n\n  list collections.\n\n\n\n```go\n\nimport (\n\n        ""fmt""\n\n        ""log""\n\n\n\n        ""github.com/tmc/langchaingo/embeddings""\n\n        ""github.com/tmc/langchaingo/llms/openai""\n\n        ""github.com/tmc/langchaingo/vectorstores""']","Question: How can I use Qdrant as a vector store in Langchain Go?\n\nContext: ---

title: Langchain Go

weight: 120

---



# Langchain Go



[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.



You can use Qdrant as a vector store in Langchain Go.



## Setup



Install the `langchain-go` project dependency



```bash

go get -u github.com/tmc/langchaingo

```



## Usage



Before you use the following code sample, customize the following values for your configuration:
""github.com/tmc/langchaingo/vectorstores""

""github.com/tmc/langchaingo/vectorstores/qdrant""

)



llm, err := openai.New()

if err != nil {

log.Fatal(err)

}



e, err := embeddings.NewEmbedder(llm)

if err != nil {

log.Fatal(err)

}



url, err := url.Parse(""YOUR_QDRANT_REST_URL"")

if err != nil {

log.Fatal(err)

}



store, err := qdrant.New(

qdrant.WithURL(*url),

qdrant.WithCollectionName(""YOUR_COLLECTION_NAME""),

qdrant.WithEmbedder(e),

)

if err != nil {
---

title: Langchain4J

weight: 110

---



# LangChain for Java



LangChain for Java, also known as [Langchain4J](https://github.com/langchain4j/langchain4j), is a community port of [Langchain](https://www.langchain.com/) for building context-aware AI applications in Java



You can use Qdrant as a vector store in Langchain4J through the [`langchain4j-qdrant`](https://central.sonatype.com/artifact/dev.langchain4j/langchain4j-qdrant) module.



## Setup
```bash

pip install langchain

```



Qdrant acts as a vector index that may store the embeddings with the documents used to generate them. There are various ways 

how to use it, but calling `Qdrant.from_texts` is probably the most straightforward way how to get started:



```python

from langchain.vectorstores import Qdrant

from langchain.embeddings import HuggingFaceEmbeddings



embeddings = HuggingFaceEmbeddings(

model_name=""sentence-transformers/all-mpnet-base-v2""

)
- `YOUR_QDRANT_REST_URL`: If you've set up Qdrant using the [Quick Start](/documentation/quick-start/) guide,

set this value to `http://localhost:6333`.

- `YOUR_COLLECTION_NAME`: Use our [Collections](/documentation/concepts/collections) guide to create or

list collections.



```go

import (

""fmt""

""log""



""github.com/tmc/langchaingo/embeddings""

""github.com/tmc/langchaingo/llms/openai""

""github.com/tmc/langchaingo/vectorstores""\n\nAnswer:",How can I use Qdrant as a vector store in Langchain Go?,"You can use Qdrant as a vector store in Langchain Go by installing the `langchain-go` project dependency and customizing the values for your configuration, such as the Qdrant REST URL and collection name.","['---\n\ntitle: Langchain Go\n\nweight: 120\n\n---\n\n\n\n# Langchain Go\n\n\n\n[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.\n\n\n\nYou can use Qdrant as a vector store in Langchain Go.\n\n\n\n## Setup\n\n\n\nInstall the `langchain-go` project dependency\n\n\n\n```bash\n\ngo get -u github.com/tmc/langchaingo\n\n```\n\n\n\n## Usage\n\n\n\nBefore you use the following code sample, customize the following values for your configuration:', '""github.com/tmc/langchaingo/vectorstores""\n\n        ""github.com/tmc/langchaingo/vectorstores/qdrant""\n\n)\n\n\n\n llm, err := openai.New()\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n\n\n e, err := embeddings.NewEmbedder(llm)\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n\n\n url, err := url.Parse(""YOUR_QDRANT_REST_URL"")\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n\n\n store, err := qdrant.New(\n\n  qdrant.WithURL(*url),\n\n  qdrant.WithCollectionName(""YOUR_COLLECTION_NAME""),\n\n  qdrant.WithEmbedder(e),\n\n )\n\n if err != nil {', '---\n\ntitle: Langchain4J\n\nweight: 110\n\n---\n\n\n\n# LangChain for Java\n\n\n\nLangChain for Java, also known as [Langchain4J](https://github.com/langchain4j/langchain4j), is a community port of [Langchain](https://www.langchain.com/) for building context-aware AI applications in Java\n\n\n\nYou can use Qdrant as a vector store in Langchain4J through the [`langchain4j-qdrant`](https://central.sonatype.com/artifact/dev.langchain4j/langchain4j-qdrant) module.\n\n\n\n## Setup', '```bash\n\npip install langchain\n\n```\n\n\n\nQdrant acts as a vector index that may store the embeddings with the documents used to generate them. There are various ways \n\nhow to use it, but calling `Qdrant.from_texts` is probably the most straightforward way how to get started:\n\n\n\n```python\n\nfrom langchain.vectorstores import Qdrant\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\n\n\nembeddings = HuggingFaceEmbeddings(\n\n    model_name=""sentence-transformers/all-mpnet-base-v2""\n\n)', '- `YOUR_QDRANT_REST_URL`: If you\'ve set up Qdrant using the [Quick Start](/documentation/quick-start/) guide,\n\n  set this value to `http://localhost:6333`.\n\n- `YOUR_COLLECTION_NAME`: Use our [Collections](/documentation/concepts/collections) guide to create or\n\n  list collections.\n\n\n\n```go\n\nimport (\n\n        ""fmt""\n\n        ""log""\n\n\n\n        ""github.com/tmc/langchaingo/embeddings""\n\n        ""github.com/tmc/langchaingo/llms/openai""\n\n        ""github.com/tmc/langchaingo/vectorstores""']","---

title: Langchain Go

weight: 120

---



# Langchain Go



[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.



You can use Qdrant as a vector store in Langchain Go.



## Setup



Install the `langchain-go` project dependency



```bash

go get -u github.com/tmc/langchaingo

```



## Usage



Before you use the following code sample, customize the following values for your configuration:
""github.com/tmc/langchaingo/vectorstores""

        ""github.com/tmc/langchaingo/vectorstores/qdrant""

)



 llm, err := openai.New()

 if err != nil {

  log.Fatal(err)

 }



 e, err := embeddings.NewEmbedder(llm)

 if err != nil {

  log.Fatal(err)

 }



 url, err := url.Parse(""YOUR_QDRANT_REST_URL"")

 if err != nil {

  log.Fatal(err)

 }



 store, err := qdrant.New(

  qdrant.WithURL(*url),

  qdrant.WithCollectionName(""YOUR_COLLECTION_NAME""),

  qdrant.WithEmbedder(e),

 )

 if err != nil {
---

title: Langchain4J

weight: 110

---



# LangChain for Java



LangChain for Java, also known as [Langchain4J](https://github.com/langchain4j/langchain4j), is a community port of [Langchain](https://www.langchain.com/) for building context-aware AI applications in Java



You can use Qdrant as a vector store in Langchain4J through the [`langchain4j-qdrant`](https://central.sonatype.com/artifact/dev.langchain4j/langchain4j-qdrant) module.



## Setup
```bash

pip install langchain

```



Qdrant acts as a vector index that may store the embeddings with the documents used to generate them. There are various ways 

how to use it, but calling `Qdrant.from_texts` is probably the most straightforward way how to get started:



```python

from langchain.vectorstores import Qdrant

from langchain.embeddings import HuggingFaceEmbeddings



embeddings = HuggingFaceEmbeddings(

    model_name=""sentence-transformers/all-mpnet-base-v2""

)
- `YOUR_QDRANT_REST_URL`: If you've set up Qdrant using the [Quick Start](/documentation/quick-start/) guide,

  set this value to `http://localhost:6333`.

- `YOUR_COLLECTION_NAME`: Use our [Collections](/documentation/concepts/collections) guide to create or

  list collections.



```go

import (

        ""fmt""

        ""log""



        ""github.com/tmc/langchaingo/embeddings""

        ""github.com/tmc/langchaingo/llms/openai""

        ""github.com/tmc/langchaingo/vectorstores""",0.16071428571428573,0.9,0.27272727272727276,0.09580838323353294,0.5517241379310345,0.163265306122449,0.13690476190476192,0.7666666666666667,0.23232323232323232,0.14285714285714285,0.8,0.24242424242424243,0.9286729693412781,0.5177065134048462,0.6844338178634644,0.5895082950592041,0.5439115315675735,0.6491590440273285,129,3.909090909090909,0.49366091967667947
True,,2819.529,1f77fd1a,How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?,Dust leveraged the control of the MMAP payload threshold and Scalar Quantization in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively.,"Dust leveraged compression features in Qdrant, such as the control of the MMAP payload threshold and Scalar Quantization, to effectively manage the balance between storing vectors on disk and keeping quantized vectors in RAM. By utilizing these features, Dust was able to reduce the footprint of vectors in memory, leading to a significant cost reduction as they didn't have to run multiple nodes in parallel. The mechanism of Scalar Quantization with rescoring disabled allowed Dust to push the same instances further with the help of quantization, even when facing pressure on MMAP due to fully utilized RAM. This optimization strategy enabled Dust to reduce their costs by 2x while maintaining good performance.","compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP

payload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage

the balance between storing vectors on disk and keeping quantized vectors in RAM,

more effectively. “This allowed us to scale smoothly from there,” Polu says.
## Results



Dust has seen success in using Qdrant as their vector database of choice, as Polu

acknowledges: “Qdrant’s ability to handle large-scale models and the flexibility

it offers in terms of data management has been crucial for us. The observability

features, such as historical graphs of RAM, Disk, and CPU, provided by Qdrant are

also particularly useful, allowing us to plan our scaling strategy effectively.”
| 2Gb memory + SQ + no rescore | 1200 | 0.974     |



In those experiments, throughput was mainly defined by the number of disk reads, and quantization efficiently reduces it by allowing more vectors in RAM.

Read more about on-disk storage in Qdrant and how we measure its performance in our article: [Minimal RAM you need to serve a million vectors

](https://qdrant.tech/articles/memory-consumption/).



The mechanism of Scalar Quantization with rescoring disabled pushes the limits of low-end
![“We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as

we don’t have to run lots of nodes in parallel. While being memory-bound, we were

able to push the same instances further with the help of quantization. While you

get pressure on MMAP in this case you maintain very good performance even if the

RAM is fully used. With this we were able to reduce our cost by 2x.” - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)
Sorry, I'm double clicking, but I'll come back. The embedded are really not perfect. Are really not perfect. So that's interesting. When Qdrant release kind of optimization for storage of vectors, they come with obviously warnings that you may have a loss.

Of precision because of the compression, et cetera, et cetera.","['compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP\n\npayload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage\n\nthe balance between storing vectors on disk and keeping quantized vectors in RAM,\n\nmore effectively. “This allowed us to scale smoothly from there,” Polu says.', '## Results\n\n\n\nDust has seen success in using Qdrant as their vector database of choice, as Polu\n\nacknowledges: “Qdrant’s ability to handle large-scale models and the flexibility\n\nit offers in terms of data management has been crucial for us. The observability\n\nfeatures, such as historical graphs of RAM, Disk, and CPU, provided by Qdrant are\n\nalso particularly useful, allowing us to plan our scaling strategy effectively.”', '| 2Gb memory + SQ + no rescore | 1200 | 0.974     |\n\n\n\nIn those experiments, throughput was mainly defined by the number of disk reads, and quantization efficiently reduces it by allowing more vectors in RAM.\n\nRead more about on-disk storage in Qdrant and how we measure its performance in our article: [Minimal RAM you need to serve a million vectors\n\n](https://qdrant.tech/articles/memory-consumption/).\n\n\n\nThe mechanism of Scalar Quantization with rescoring disabled pushes the limits of low-end', '![“We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as\n\nwe don’t have to run lots of nodes in parallel. While being memory-bound, we were\n\nable to push the same instances further with the help of quantization. While you\n\nget pressure on MMAP in this case you maintain very good performance even if the\n\nRAM is fully used. With this we were able to reduce our cost by 2x.” - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)', ""Sorry, I'm double clicking, but I'll come back. The embedded are really not perfect. Are really not perfect. So that's interesting. When Qdrant release kind of optimization for storage of vectors, they come with obviously warnings that you may have a loss.\n\nOf precision because of the compression, et cetera, et cetera.""]","Question: How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?\n\nContext: compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP

payload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage

the balance between storing vectors on disk and keeping quantized vectors in RAM,

more effectively. “This allowed us to scale smoothly from there,” Polu says.
## Results



Dust has seen success in using Qdrant as their vector database of choice, as Polu

acknowledges: “Qdrant’s ability to handle large-scale models and the flexibility

it offers in terms of data management has been crucial for us. The observability

features, such as historical graphs of RAM, Disk, and CPU, provided by Qdrant are

also particularly useful, allowing us to plan our scaling strategy effectively.”
| 2Gb memory + SQ + no rescore | 1200 | 0.974     |



In those experiments, throughput was mainly defined by the number of disk reads, and quantization efficiently reduces it by allowing more vectors in RAM.

Read more about on-disk storage in Qdrant and how we measure its performance in our article: [Minimal RAM you need to serve a million vectors

](https://qdrant.tech/articles/memory-consumption/).



The mechanism of Scalar Quantization with rescoring disabled pushes the limits of low-end
![“We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as

we don’t have to run lots of nodes in parallel. While being memory-bound, we were

able to push the same instances further with the help of quantization. While you

get pressure on MMAP in this case you maintain very good performance even if the

RAM is fully used. With this we were able to reduce our cost by 2x.” - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)
Sorry, I'm double clicking, but I'll come back. The embedded are really not perfect. Are really not perfect. So that's interesting. When Qdrant release kind of optimization for storage of vectors, they come with obviously warnings that you may have a loss.

Of precision because of the compression, et cetera, et cetera.\n\nAnswer:",How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?,Dust leveraged the control of the MMAP payload threshold and Scalar Quantization in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively.,"['compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP\n\npayload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage\n\nthe balance between storing vectors on disk and keeping quantized vectors in RAM,\n\nmore effectively. “This allowed us to scale smoothly from there,” Polu says.', '## Results\n\n\n\nDust has seen success in using Qdrant as their vector database of choice, as Polu\n\nacknowledges: “Qdrant’s ability to handle large-scale models and the flexibility\n\nit offers in terms of data management has been crucial for us. The observability\n\nfeatures, such as historical graphs of RAM, Disk, and CPU, provided by Qdrant are\n\nalso particularly useful, allowing us to plan our scaling strategy effectively.”', '| 2Gb memory + SQ + no rescore | 1200 | 0.974     |\n\n\n\nIn those experiments, throughput was mainly defined by the number of disk reads, and quantization efficiently reduces it by allowing more vectors in RAM.\n\nRead more about on-disk storage in Qdrant and how we measure its performance in our article: [Minimal RAM you need to serve a million vectors\n\n](https://qdrant.tech/articles/memory-consumption/).\n\n\n\nThe mechanism of Scalar Quantization with rescoring disabled pushes the limits of low-end', '![“We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as\n\nwe don’t have to run lots of nodes in parallel. While being memory-bound, we were\n\nable to push the same instances further with the help of quantization. While you\n\nget pressure on MMAP in this case you maintain very good performance even if the\n\nRAM is fully used. With this we were able to reduce our cost by 2x.” - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)', ""Sorry, I'm double clicking, but I'll come back. The embedded are really not perfect. Are really not perfect. So that's interesting. When Qdrant release kind of optimization for storage of vectors, they come with obviously warnings that you may have a loss.\n\nOf precision because of the compression, et cetera, et cetera.""]","compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP

payload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage

the balance between storing vectors on disk and keeping quantized vectors in RAM,

more effectively. “This allowed us to scale smoothly from there,” Polu says.
## Results



Dust has seen success in using Qdrant as their vector database of choice, as Polu

acknowledges: “Qdrant’s ability to handle large-scale models and the flexibility

it offers in terms of data management has been crucial for us. The observability

features, such as historical graphs of RAM, Disk, and CPU, provided by Qdrant are

also particularly useful, allowing us to plan our scaling strategy effectively.”
| 2Gb memory + SQ + no rescore | 1200 | 0.974     |



In those experiments, throughput was mainly defined by the number of disk reads, and quantization efficiently reduces it by allowing more vectors in RAM.

Read more about on-disk storage in Qdrant and how we measure its performance in our article: [Minimal RAM you need to serve a million vectors

](https://qdrant.tech/articles/memory-consumption/).



The mechanism of Scalar Quantization with rescoring disabled pushes the limits of low-end
![“We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as

we don’t have to run lots of nodes in parallel. While being memory-bound, we were

able to push the same instances further with the help of quantization. While you

get pressure on MMAP in this case you maintain very good performance even if the

RAM is fully used. With this we were able to reduce our cost by 2x.” - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)
Sorry, I'm double clicking, but I'll come back. The embedded are really not perfect. Are really not perfect. So that's interesting. When Qdrant release kind of optimization for storage of vectors, they come with obviously warnings that you may have a loss.

Of precision because of the compression, et cetera, et cetera.",0.2571428571428571,1.0,0.40909090909090906,0.20192307692307693,0.8076923076923077,0.3230769230769231,0.22857142857142856,0.8888888888888888,0.3636363636363636,0.23809523809523808,0.9259259259259259,0.3787878787878788,0.9354457855224609,0.5957837700843811,0.7851638197898865,0.6774882078170776,0.966741653450299,0.9394594095647335,113,3.7666666666666666,0.427943268477585
True,,4811.4169999999995,3a6a0b8e,Why do we still need keyword search?,"Keyword search is still useful in cases of out-of-domain search, where words are just words regardless of their meaning.","Keyword search is still essential for several reasons:

1. **Familiarity:** Keyword-based search has been the traditional approach for search engines for a long time. Users are accustomed to it and find it intuitive to enter keywords to find information.

2. **Specificity:** In some cases, users may have very specific keywords in mind when searching for information. Keyword search allows them to input these specific terms and retrieve relevant results quickly.

3. **User Control:** Keyword search gives users direct control over the search process. They can choose the keywords they believe are most relevant to their query, giving them a sense of agency in finding the information they need.

4. **Speed:** Keyword search is often faster than other search methods, especially for simple queries. Users can quickly type in a few keywords and get immediate results.

5. **Complement to other methods:** While semantic search and other advanced search techniques offer benefits, keyword search can still complement them. By using a mix of keyword search and other methods, users can access a wider range of results and find information more efficiently.

Overall, keyword search remains a fundamental tool in information retrieval, offering familiarity, specificity, user control, speed, and the ability to complement other search methods.","2. Vector search with keyword-based search. This one is covered in this article.

3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.



## Why do we still need keyword search?



A keyword-based search was the obvious choice for search engines in the past. It struggled with some

common issues, but since we didn't have any alternatives, we had to overcome them with additional
. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.
needed to find what we’re looking for. Semantic search enables us to find things without knowing the exact 

terminology. For example, if an engineer wanted to understand “*What library is used for payment processing?*” 

a semantic code search engine would be able to retrieve results containing “*Stripe*” or “*PayPal*”. A traditional 

lexical search engine would not.



One peculiarity of this problem is that the **usefulness of the solution increases with the size of the code
# The fast search: A Fallback strategy



The easiest way to incorporate vector search into the existing stack is to treat it as some sort of

fallback strategy. So whenever your keyword search struggle with finding proper results, you can

run a semantic search to extend the results. That is especially important in cases like search-as-you-type

in which a new query is fired every single time your user types the next character in. For such cases
the speed of the search is crucial. Therefore, we can't use vector search on every query. At the same 

time, the simple prefix search might have a bad recall.



In this case, a good strategy is to use vector search only when the keyword/prefix search returns none 

or just a small number of results. A good candidate for this is [MeiliSearch](https://www.meilisearch.com/). 

It uses custom ranking rules to provide results as fast as the user can type.","[""2. Vector search with keyword-based search. This one is covered in this article.\n\n3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.\n\n\n\n## Why do we still need keyword search?\n\n\n\nA keyword-based search was the obvious choice for search engines in the past. It struggled with some\n\ncommon issues, but since we didn't have any alternatives, we had to overcome them with additional"", '. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.', 'needed to find what we’re looking for. Semantic search enables us to find things without knowing the exact \n\nterminology. For example, if an engineer wanted to understand “*What library is used for payment processing?*” \n\na semantic code search engine would be able to retrieve results containing “*Stripe*” or “*PayPal*”. A traditional \n\nlexical search engine would not.\n\n\n\nOne peculiarity of this problem is that the **usefulness of the solution increases with the size of the code', '# The fast search: A Fallback strategy\n\n\n\nThe easiest way to incorporate vector search into the existing stack is to treat it as some sort of\n\nfallback strategy. So whenever your keyword search struggle with finding proper results, you can\n\nrun a semantic search to extend the results. That is especially important in cases like search-as-you-type\n\nin which a new query is fired every single time your user types the next character in. For such cases', ""the speed of the search is crucial. Therefore, we can't use vector search on every query. At the same \n\ntime, the simple prefix search might have a bad recall.\n\n\n\nIn this case, a good strategy is to use vector search only when the keyword/prefix search returns none \n\nor just a small number of results. A good candidate for this is [MeiliSearch](https://www.meilisearch.com/). \n\nIt uses custom ranking rules to provide results as fast as the user can type.""]","Question: Why do we still need keyword search?\n\nContext: 2. Vector search with keyword-based search. This one is covered in this article.

3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.



## Why do we still need keyword search?



A keyword-based search was the obvious choice for search engines in the past. It struggled with some

common issues, but since we didn't have any alternatives, we had to overcome them with additional
. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.
needed to find what we’re looking for. Semantic search enables us to find things without knowing the exact 

terminology. For example, if an engineer wanted to understand “*What library is used for payment processing?*” 

a semantic code search engine would be able to retrieve results containing “*Stripe*” or “*PayPal*”. A traditional 

lexical search engine would not.



One peculiarity of this problem is that the **usefulness of the solution increases with the size of the code
# The fast search: A Fallback strategy



The easiest way to incorporate vector search into the existing stack is to treat it as some sort of

fallback strategy. So whenever your keyword search struggle with finding proper results, you can

run a semantic search to extend the results. That is especially important in cases like search-as-you-type

in which a new query is fired every single time your user types the next character in. For such cases
the speed of the search is crucial. Therefore, we can't use vector search on every query. At the same 

time, the simple prefix search might have a bad recall.



In this case, a good strategy is to use vector search only when the keyword/prefix search returns none 

or just a small number of results. A good candidate for this is [MeiliSearch](https://www.meilisearch.com/). 

It uses custom ranking rules to provide results as fast as the user can type.\n\nAnswer:",Why do we still need keyword search?,"Keyword search is still useful in cases of out-of-domain search, where words are just words regardless of their meaning.","[""2. Vector search with keyword-based search. This one is covered in this article.\n\n3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.\n\n\n\n## Why do we still need keyword search?\n\n\n\nA keyword-based search was the obvious choice for search engines in the past. It struggled with some\n\ncommon issues, but since we didn't have any alternatives, we had to overcome them with additional"", '. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.', 'needed to find what we’re looking for. Semantic search enables us to find things without knowing the exact \n\nterminology. For example, if an engineer wanted to understand “*What library is used for payment processing?*” \n\na semantic code search engine would be able to retrieve results containing “*Stripe*” or “*PayPal*”. A traditional \n\nlexical search engine would not.\n\n\n\nOne peculiarity of this problem is that the **usefulness of the solution increases with the size of the code', '# The fast search: A Fallback strategy\n\n\n\nThe easiest way to incorporate vector search into the existing stack is to treat it as some sort of\n\nfallback strategy. So whenever your keyword search struggle with finding proper results, you can\n\nrun a semantic search to extend the results. That is especially important in cases like search-as-you-type\n\nin which a new query is fired every single time your user types the next character in. For such cases', ""the speed of the search is crucial. Therefore, we can't use vector search on every query. At the same \n\ntime, the simple prefix search might have a bad recall.\n\n\n\nIn this case, a good strategy is to use vector search only when the keyword/prefix search returns none \n\nor just a small number of results. A good candidate for this is [MeiliSearch](https://www.meilisearch.com/). \n\nIt uses custom ranking rules to provide results as fast as the user can type.""]","2. Vector search with keyword-based search. This one is covered in this article.

3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.



## Why do we still need keyword search?



A keyword-based search was the obvious choice for search engines in the past. It struggled with some

common issues, but since we didn't have any alternatives, we had to overcome them with additional
. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.
needed to find what we’re looking for. Semantic search enables us to find things without knowing the exact 

terminology. For example, if an engineer wanted to understand “*What library is used for payment processing?*” 

a semantic code search engine would be able to retrieve results containing “*Stripe*” or “*PayPal*”. A traditional 

lexical search engine would not.



One peculiarity of this problem is that the **usefulness of the solution increases with the size of the code
# The fast search: A Fallback strategy



The easiest way to incorporate vector search into the existing stack is to treat it as some sort of

fallback strategy. So whenever your keyword search struggle with finding proper results, you can

run a semantic search to extend the results. That is especially important in cases like search-as-you-type

in which a new query is fired every single time your user types the next character in. For such cases
the speed of the search is crucial. Therefore, we can't use vector search on every query. At the same 

time, the simple prefix search might have a bad recall.



In this case, a good strategy is to use vector search only when the keyword/prefix search returns none 

or just a small number of results. A good candidate for this is [MeiliSearch](https://www.meilisearch.com/). 

It uses custom ranking rules to provide results as fast as the user can type.",0.06735751295336788,0.6190476190476191,0.12149532710280374,0.015625,0.15,0.02830188679245283,0.05181347150259067,0.47619047619047616,0.09345794392523364,0.05181347150259067,0.47619047619047616,0.09345794392523364,0.7205055952072144,0.46491801738739014,0.5900105237960815,0.5200476050376892,0.6267752728308551,0.0035759806632995605,203,10.68421052631579,0.3394901807847515
True,,4482.216,470e9b69,What principles did Qdrant follow while designing benchmarks for vector search engines?,"Qdrant followed the principles of doing comparative benchmarks focusing on relative numbers rather than absolute numbers, and using affordable hardware for easy result reproduction.","Qdrant followed several key principles while designing benchmarks for vector search engines. These principles include:

1. **Focus on Relative Numbers**: The benchmarks are designed to emphasize relative performance numbers rather than absolute values. This approach allows for a more meaningful comparison between different vector search engines.

2. **Use of Affordable Hardware**: The benchmarks are conducted using affordable hardware to make it easy for others to reproduce the results. This ensures that the performance measurements are not skewed by using high-end or specialized hardware.

3. **Consistent Machine Setup**: All benchmarks are run on the same exact machines to eliminate any potential bias that could arise from differences in hardware configurations. This standardization helps in providing fair and accurate performance comparisons.

4. **Open-Sourced Benchmarks**: All benchmarks conducted by Qdrant are open-sourced, allowing for transparency and enabling others to contribute and improve the benchmarking process. This open approach fosters collaboration and ensures the benchmarks are continuously refined and updated.

By adhering to these principles, Qdrant aims to provide reliable, comparable, and transparent performance benchmarks for vector search engines, facilitating informed decision-making for users and developers in the field.","Here are the principles we followed while designing these benchmarks:



- We do comparative benchmarks, which means we focus on **relative numbers** rather than absolute numbers.

- We use affordable hardware, so that you can reproduce the results easily.

- We run benchmarks on the same exact machines to avoid any possible hardware bias.

- All the benchmarks are [open-sourced](https://github.com/qdrant/vector-db-benchmark), so you can contribute and improve them.



<details>
tags: # Change this, related by tags posts will be shown on the blog page

  - qdrant

  - benchmarks

  - performance

---



It's time for an update to Qdrant's benchmarks! 



We've compared how Qdrant performs against the other vector search engines to give you a thorough performance analysis. Let's get into what's new and what remains the same in our approach. 



### What's Changed?



#### All engines have improved
All of those engines might be easily used in combination with the vector search offered by Qdrant. But the 

exact way how to combine the results of both algorithms to achieve the best search precision might be still 

unclear. So we need to understand how to do it effectively. We will be using reference datasets to benchmark 

the search quality.



## Why not linear combination?



It's often proposed to use full-text and vector search scores to form a linear combination formula to rerank
title: High-Performance Vector Search at Scale

    description: The leading open-source vector database designed to handle high-dimensional vectors for performance and massive-scale AI applications. Qdrant is purpose-built in Rust for unmatched speed and reliability even when processing billions of vectors.

    buttonLeft:

        text: Start Free

        link: https://qdrant.to/cloud

    buttonRight:

        text: See Benchmarks

        link: /benchmarks/
# ]



```



Each instance of the SearchRequest class may provide its own search parameters, including vector query but also some additional filters. The response will be a list of individual results for each request. In case any of the requests is malformed, there will be an exception thrown, so either all of them pass or none of them.



And that’s it! You no longer have to handle the multiple requests on your own. Qdrant will do it under the hood.



## Benchmark","['Here are the principles we followed while designing these benchmarks:\n\n\n\n- We do comparative benchmarks, which means we focus on **relative numbers** rather than absolute numbers.\n\n- We use affordable hardware, so that you can reproduce the results easily.\n\n- We run benchmarks on the same exact machines to avoid any possible hardware bias.\n\n- All the benchmarks are [open-sourced](https://github.com/qdrant/vector-db-benchmark), so you can contribute and improve them.\n\n\n\n<details>', ""tags: # Change this, related by tags posts will be shown on the blog page\n\n  - qdrant\n\n  - benchmarks\n\n  - performance\n\n---\n\n\n\nIt's time for an update to Qdrant's benchmarks! \n\n\n\nWe've compared how Qdrant performs against the other vector search engines to give you a thorough performance analysis. Let's get into what's new and what remains the same in our approach. \n\n\n\n### What's Changed?\n\n\n\n#### All engines have improved"", ""All of those engines might be easily used in combination with the vector search offered by Qdrant. But the \n\nexact way how to combine the results of both algorithms to achieve the best search precision might be still \n\nunclear. So we need to understand how to do it effectively. We will be using reference datasets to benchmark \n\nthe search quality.\n\n\n\n## Why not linear combination?\n\n\n\nIt's often proposed to use full-text and vector search scores to form a linear combination formula to rerank"", 'title: High-Performance Vector Search at Scale\n\n    description: The leading open-source vector database designed to handle high-dimensional vectors for performance and massive-scale AI applications. Qdrant is purpose-built in Rust for unmatched speed and reliability even when processing billions of vectors.\n\n    buttonLeft:\n\n        text: Start Free\n\n        link: https://qdrant.to/cloud\n\n    buttonRight:\n\n        text: See Benchmarks\n\n        link: /benchmarks/', '# ]\n\n\n\n```\n\n\n\nEach instance of the SearchRequest class may provide its own search parameters, including vector query but also some additional filters. The response will be a list of individual results for each request. In case any of the requests is malformed, there will be an exception thrown, so either all of them pass or none of them.\n\n\n\nAnd that’s it! You no longer have to handle the multiple requests on your own. Qdrant will do it under the hood.\n\n\n\n## Benchmark']","Question: What principles did Qdrant follow while designing benchmarks for vector search engines?\n\nContext: Here are the principles we followed while designing these benchmarks:



- We do comparative benchmarks, which means we focus on **relative numbers** rather than absolute numbers.

- We use affordable hardware, so that you can reproduce the results easily.

- We run benchmarks on the same exact machines to avoid any possible hardware bias.

- All the benchmarks are [open-sourced](https://github.com/qdrant/vector-db-benchmark), so you can contribute and improve them.



<details>
tags: # Change this, related by tags posts will be shown on the blog page

- qdrant

- benchmarks

- performance

---



It's time for an update to Qdrant's benchmarks! 



We've compared how Qdrant performs against the other vector search engines to give you a thorough performance analysis. Let's get into what's new and what remains the same in our approach. 



### What's Changed?



#### All engines have improved
All of those engines might be easily used in combination with the vector search offered by Qdrant. But the 

exact way how to combine the results of both algorithms to achieve the best search precision might be still 

unclear. So we need to understand how to do it effectively. We will be using reference datasets to benchmark 

the search quality.



## Why not linear combination?



It's often proposed to use full-text and vector search scores to form a linear combination formula to rerank
title: High-Performance Vector Search at Scale

description: The leading open-source vector database designed to handle high-dimensional vectors for performance and massive-scale AI applications. Qdrant is purpose-built in Rust for unmatched speed and reliability even when processing billions of vectors.

buttonLeft:

text: Start Free

link: https://qdrant.to/cloud

buttonRight:

text: See Benchmarks

link: /benchmarks/
# ]



```



Each instance of the SearchRequest class may provide its own search parameters, including vector query but also some additional filters. The response will be a list of individual results for each request. In case any of the requests is malformed, there will be an exception thrown, so either all of them pass or none of them.



And that’s it! You no longer have to handle the multiple requests on your own. Qdrant will do it under the hood.



## Benchmark\n\nAnswer:",What principles did Qdrant follow while designing benchmarks for vector search engines?,"Qdrant followed the principles of doing comparative benchmarks focusing on relative numbers rather than absolute numbers, and using affordable hardware for easy result reproduction.","['Here are the principles we followed while designing these benchmarks:\n\n\n\n- We do comparative benchmarks, which means we focus on **relative numbers** rather than absolute numbers.\n\n- We use affordable hardware, so that you can reproduce the results easily.\n\n- We run benchmarks on the same exact machines to avoid any possible hardware bias.\n\n- All the benchmarks are [open-sourced](https://github.com/qdrant/vector-db-benchmark), so you can contribute and improve them.\n\n\n\n<details>', ""tags: # Change this, related by tags posts will be shown on the blog page\n\n  - qdrant\n\n  - benchmarks\n\n  - performance\n\n---\n\n\n\nIt's time for an update to Qdrant's benchmarks! \n\n\n\nWe've compared how Qdrant performs against the other vector search engines to give you a thorough performance analysis. Let's get into what's new and what remains the same in our approach. \n\n\n\n### What's Changed?\n\n\n\n#### All engines have improved"", ""All of those engines might be easily used in combination with the vector search offered by Qdrant. But the \n\nexact way how to combine the results of both algorithms to achieve the best search precision might be still \n\nunclear. So we need to understand how to do it effectively. We will be using reference datasets to benchmark \n\nthe search quality.\n\n\n\n## Why not linear combination?\n\n\n\nIt's often proposed to use full-text and vector search scores to form a linear combination formula to rerank"", 'title: High-Performance Vector Search at Scale\n\n    description: The leading open-source vector database designed to handle high-dimensional vectors for performance and massive-scale AI applications. Qdrant is purpose-built in Rust for unmatched speed and reliability even when processing billions of vectors.\n\n    buttonLeft:\n\n        text: Start Free\n\n        link: https://qdrant.to/cloud\n\n    buttonRight:\n\n        text: See Benchmarks\n\n        link: /benchmarks/', '# ]\n\n\n\n```\n\n\n\nEach instance of the SearchRequest class may provide its own search parameters, including vector query but also some additional filters. The response will be a list of individual results for each request. In case any of the requests is malformed, there will be an exception thrown, so either all of them pass or none of them.\n\n\n\nAnd that’s it! You no longer have to handle the multiple requests on your own. Qdrant will do it under the hood.\n\n\n\n## Benchmark']","Here are the principles we followed while designing these benchmarks:



- We do comparative benchmarks, which means we focus on **relative numbers** rather than absolute numbers.

- We use affordable hardware, so that you can reproduce the results easily.

- We run benchmarks on the same exact machines to avoid any possible hardware bias.

- All the benchmarks are [open-sourced](https://github.com/qdrant/vector-db-benchmark), so you can contribute and improve them.



<details>
tags: # Change this, related by tags posts will be shown on the blog page

  - qdrant

  - benchmarks

  - performance

---



It's time for an update to Qdrant's benchmarks! 



We've compared how Qdrant performs against the other vector search engines to give you a thorough performance analysis. Let's get into what's new and what remains the same in our approach. 



### What's Changed?



#### All engines have improved
All of those engines might be easily used in combination with the vector search offered by Qdrant. But the 

exact way how to combine the results of both algorithms to achieve the best search precision might be still 

unclear. So we need to understand how to do it effectively. We will be using reference datasets to benchmark 

the search quality.



## Why not linear combination?



It's often proposed to use full-text and vector search scores to form a linear combination formula to rerank
title: High-Performance Vector Search at Scale

    description: The leading open-source vector database designed to handle high-dimensional vectors for performance and massive-scale AI applications. Qdrant is purpose-built in Rust for unmatched speed and reliability even when processing billions of vectors.

    buttonLeft:

        text: Start Free

        link: https://qdrant.to/cloud

    buttonRight:

        text: See Benchmarks

        link: /benchmarks/
# ]



```



Each instance of the SearchRequest class may provide its own search parameters, including vector query but also some additional filters. The response will be a list of individual results for each request. In case any of the requests is malformed, there will be an exception thrown, so either all of them pass or none of them.



And that’s it! You no longer have to handle the multiple requests on your own. Qdrant will do it under the hood.



## Benchmark",0.1111111111111111,0.8695652173913043,0.19704433497536947,0.0446927374301676,0.36363636363636365,0.07960199004975126,0.08333333333333333,0.6521739130434783,0.14778325123152708,0.1,0.782608695652174,0.17733990147783252,0.7597437500953674,0.4958113431930542,0.6540935039520264,0.564058780670166,0.8111927572463173,0.9990010352339596,185,7.708333333333333,0.36399849746912655
True,,2854.058,e9ad99dc,What models does Qdrant support for embedding generation?,"Qdrant supports a reasonable range of models, including a few multilingual ones.","Qdrant supports a range of models for embedding generation, including Cohere, Gemini, Jina Embeddings, OpenAI, Aleph Alpha, Mpnet model, and several multilingual models. The selection of the best model depends on the balance between speed and accuracy of the embeddings. Currently, only open source models are supported by Qdrant directly. However, support for other models like Embed V three could be considered based on user requirements. Qdrant simplifies the process by handling the creation and storage of embeddings in its vector database. It is recommended to experiment with different supported models to find the most suitable one for your specific use case.","Selecting the right embedding model for your use case is crucial to your application performance. Qdrant makes it easier by offering seamless integration with the best selection of embedding APIs, including [Cohere](https://qdrant.tech/documentation/embeddings/cohere/), [Gemini](https://qdrant.tech/documentation/embeddings/gemini/), [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/), [OpenAI](https://qdrant.tech/documentation/embeddings/openai/), [Aleph Alpha](https://qdrant
. So we did a lot of experiments. We used, I think, Mpnet model and a lot of multilingual models as well. But after doing those experiments, we realized that this is the best model that offers the best balance between speed and accuracy cool of the Embeddings. So we have deployed it in a serverless inference endpoint in SageMaker. And once we generate the Embeddings in a glue job, we then store them into the vector database Qdrant.
So one thing which I mean, one is definitely try the different models which we support. We support a reasonable range of models, including a few multilingual ones. Second is while we take care of this when you do use with Qdrants. So, for instance, let's say this is how you would have to manually specify, let's say, passage or query. When you do this, let's say add and query. What we do, we add the passage and query keys while creating the Embeddings for you. So this is taken care of
As of now, we only support models which are open source so that we can serve those models directly. Embed V three is cloud only at the moment, so that is why it is not supported yet. But that said, we are not opposed to it. In case there's a requirement for that, we are happy to support that so that people can use it seamlessly with Qdrant and fast embed does the heavy lifting of passing it to Qdrant, structuring the schema and all of those for you. So that's perfectly fair
. So this is taken care of. So whatever is your best practices for the Embedding model, make sure you use it when you're using it with Qdrant or just in isolation as well.","['Selecting the right embedding model for your use case is crucial to your application performance. Qdrant makes it easier by offering seamless integration with the best selection of embedding APIs, including [Cohere](https://qdrant.tech/documentation/embeddings/cohere/), [Gemini](https://qdrant.tech/documentation/embeddings/gemini/), [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/), [OpenAI](https://qdrant.tech/documentation/embeddings/openai/), [Aleph Alpha](https://qdrant', '. So we did a lot of experiments. We used, I think, Mpnet model and a lot of multilingual models as well. But after doing those experiments, we realized that this is the best model that offers the best balance between speed and accuracy cool of the Embeddings. So we have deployed it in a serverless inference endpoint in SageMaker. And once we generate the Embeddings in a glue job, we then store them into the vector database Qdrant.', ""So one thing which I mean, one is definitely try the different models which we support. We support a reasonable range of models, including a few multilingual ones. Second is while we take care of this when you do use with Qdrants. So, for instance, let's say this is how you would have to manually specify, let's say, passage or query. When you do this, let's say add and query. What we do, we add the passage and query keys while creating the Embeddings for you. So this is taken care of"", ""As of now, we only support models which are open source so that we can serve those models directly. Embed V three is cloud only at the moment, so that is why it is not supported yet. But that said, we are not opposed to it. In case there's a requirement for that, we are happy to support that so that people can use it seamlessly with Qdrant and fast embed does the heavy lifting of passing it to Qdrant, structuring the schema and all of those for you. So that's perfectly fair"", "". So this is taken care of. So whatever is your best practices for the Embedding model, make sure you use it when you're using it with Qdrant or just in isolation as well.""]","Question: What models does Qdrant support for embedding generation?\n\nContext: Selecting the right embedding model for your use case is crucial to your application performance. Qdrant makes it easier by offering seamless integration with the best selection of embedding APIs, including [Cohere](https://qdrant.tech/documentation/embeddings/cohere/), [Gemini](https://qdrant.tech/documentation/embeddings/gemini/), [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/), [OpenAI](https://qdrant.tech/documentation/embeddings/openai/), [Aleph Alpha](https://qdrant
. So we did a lot of experiments. We used, I think, Mpnet model and a lot of multilingual models as well. But after doing those experiments, we realized that this is the best model that offers the best balance between speed and accuracy cool of the Embeddings. So we have deployed it in a serverless inference endpoint in SageMaker. And once we generate the Embeddings in a glue job, we then store them into the vector database Qdrant.
So one thing which I mean, one is definitely try the different models which we support. We support a reasonable range of models, including a few multilingual ones. Second is while we take care of this when you do use with Qdrants. So, for instance, let's say this is how you would have to manually specify, let's say, passage or query. When you do this, let's say add and query. What we do, we add the passage and query keys while creating the Embeddings for you. So this is taken care of
As of now, we only support models which are open source so that we can serve those models directly. Embed V three is cloud only at the moment, so that is why it is not supported yet. But that said, we are not opposed to it. In case there's a requirement for that, we are happy to support that so that people can use it seamlessly with Qdrant and fast embed does the heavy lifting of passing it to Qdrant, structuring the schema and all of those for you. So that's perfectly fair
. So this is taken care of. So whatever is your best practices for the Embedding model, make sure you use it when you're using it with Qdrant or just in isolation as well.\n\nAnswer:",What models does Qdrant support for embedding generation?,"Qdrant supports a reasonable range of models, including a few multilingual ones.","['Selecting the right embedding model for your use case is crucial to your application performance. Qdrant makes it easier by offering seamless integration with the best selection of embedding APIs, including [Cohere](https://qdrant.tech/documentation/embeddings/cohere/), [Gemini](https://qdrant.tech/documentation/embeddings/gemini/), [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/), [OpenAI](https://qdrant.tech/documentation/embeddings/openai/), [Aleph Alpha](https://qdrant', '. So we did a lot of experiments. We used, I think, Mpnet model and a lot of multilingual models as well. But after doing those experiments, we realized that this is the best model that offers the best balance between speed and accuracy cool of the Embeddings. So we have deployed it in a serverless inference endpoint in SageMaker. And once we generate the Embeddings in a glue job, we then store them into the vector database Qdrant.', ""So one thing which I mean, one is definitely try the different models which we support. We support a reasonable range of models, including a few multilingual ones. Second is while we take care of this when you do use with Qdrants. So, for instance, let's say this is how you would have to manually specify, let's say, passage or query. When you do this, let's say add and query. What we do, we add the passage and query keys while creating the Embeddings for you. So this is taken care of"", ""As of now, we only support models which are open source so that we can serve those models directly. Embed V three is cloud only at the moment, so that is why it is not supported yet. But that said, we are not opposed to it. In case there's a requirement for that, we are happy to support that so that people can use it seamlessly with Qdrant and fast embed does the heavy lifting of passing it to Qdrant, structuring the schema and all of those for you. So that's perfectly fair"", "". So this is taken care of. So whatever is your best practices for the Embedding model, make sure you use it when you're using it with Qdrant or just in isolation as well.""]","Selecting the right embedding model for your use case is crucial to your application performance. Qdrant makes it easier by offering seamless integration with the best selection of embedding APIs, including [Cohere](https://qdrant.tech/documentation/embeddings/cohere/), [Gemini](https://qdrant.tech/documentation/embeddings/gemini/), [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/), [OpenAI](https://qdrant.tech/documentation/embeddings/openai/), [Aleph Alpha](https://qdrant
. So we did a lot of experiments. We used, I think, Mpnet model and a lot of multilingual models as well. But after doing those experiments, we realized that this is the best model that offers the best balance between speed and accuracy cool of the Embeddings. So we have deployed it in a serverless inference endpoint in SageMaker. And once we generate the Embeddings in a glue job, we then store them into the vector database Qdrant.
So one thing which I mean, one is definitely try the different models which we support. We support a reasonable range of models, including a few multilingual ones. Second is while we take care of this when you do use with Qdrants. So, for instance, let's say this is how you would have to manually specify, let's say, passage or query. When you do this, let's say add and query. What we do, we add the passage and query keys while creating the Embeddings for you. So this is taken care of
As of now, we only support models which are open source so that we can serve those models directly. Embed V three is cloud only at the moment, so that is why it is not supported yet. But that said, we are not opposed to it. In case there's a requirement for that, we are happy to support that so that people can use it seamlessly with Qdrant and fast embed does the heavy lifting of passing it to Qdrant, structuring the schema and all of those for you. So that's perfectly fair
. So this is taken care of. So whatever is your best practices for the Embedding model, make sure you use it when you're using it with Qdrant or just in isolation as well.",0.0851063829787234,0.8,0.15384615384615383,0.03225806451612903,0.3333333333333333,0.0588235294117647,0.0851063829787234,0.8,0.15384615384615383,0.0851063829787234,0.8,0.15384615384615383,0.6355135440826416,0.5205824375152588,0.6975219249725342,0.5962012410163879,0.9950821441501224,0.9906861605122685,102,8.5,0.43985414146569646
True,,5157.793,6e16a784,How can you parallelize the upload of a large dataset using shards in Qdrant?,"By creating multiple shards in Qdrant, you can parallelize the upload of a large dataset. It is recommended to have 2 to 4 shards per machine for efficient processing. When creating a collection in Qdrant, you can specify the number of shards to use for that collection. ","To parallelize the upload of a large dataset using shards in Qdrant, you can follow these steps:

1. Configure the number of shards: When creating a collection in Qdrant, specify the number of shards you want to use for that collection. For example, in the HTTP request example provided, the ""shard_number"" is set to 2. You can adjust this number based on the size of your dataset and the resources available.

2. Use a fast client library: Qdrant recommends using their Rust client library for faster performance. This library is optimized for working with Qdrant and can help speed up the upload process.

3. Disable indexing during upload: If you are uploading a large dataset for the first time, consider disabling indexing during the upload process. This can prevent unnecessary indexing of vectors that will be overwritten by subsequent batches, improving efficiency.

4. Utilize parallel processing: If you are not using the Rust client library, consider parallelizing your upload process using the parallel parameter in the Qdrant client library. This parameter controls the number of CPU cores used to encode data, enabling faster processing.

5. Leverage distributed deployment and sharding: Qdrant supports distributed deployment out of the box, allowing you to horizontally scale both the size of your collections and the throughput of your cluster. By combining sharding with distributed deployment, you can handle large datasets efficiently without compromising performance or reliability.

By following these steps and leveraging the capabilities of Qdrant, you can parallelize the upload of a large dataset using shards effectively.","## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""shard_number"": 2

}

```



```python
We recommend using our [Rust client library](https://github.com/qdrant/rust-client) for this purpose, as it is the fastest client library available for Qdrant.



If you are not using Rust, you might want to consider parallelizing your upload process.



## Disable indexing during upload



In case you are doing an initial upload of a large dataset, you might want to disable indexing during upload.

It will enable to avoid unnecessary indexing of vectors, which will be overwritten by the next batch.
},

    ""shard_number"": 2

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    shard_number=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });
collection_name=""startups"",

    documents=documents,

    metadata=metadata,

    parallel=0,  # Use all available CPU cores to encode data

)

```



The `add` method will encode all documents and upload them to Qdrant.

This is one of two fastembed-specific methods, that combines encoding and uploading into a single step.



The `parallel` parameter controls the number of CPU cores used to encode data.
setup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale 

both the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle 

large amounts of data without sacrificing performance or reliability.



## Administration API



Another new feature is the administration API, which allows you to disable write operations to the service. This is","['## Parallel upload into multiple shards\n\n\n\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\n\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python', 'We recommend using our [Rust client library](https://github.com/qdrant/rust-client) for this purpose, as it is the fastest client library available for Qdrant.\n\n\n\nIf you are not using Rust, you might want to consider parallelizing your upload process.\n\n\n\n## Disable indexing during upload\n\n\n\nIn case you are doing an initial upload of a large dataset, you might want to disable indexing during upload.\n\nIt will enable to avoid unnecessary indexing of vectors, which will be overwritten by the next batch.', '},\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    shard_number=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });', 'collection_name=""startups"",\n\n    documents=documents,\n\n    metadata=metadata,\n\n    parallel=0,  # Use all available CPU cores to encode data\n\n)\n\n```\n\n\n\nThe `add` method will encode all documents and upload them to Qdrant.\n\nThis is one of two fastembed-specific methods, that combines encoding and uploading into a single step.\n\n\n\nThe `parallel` parameter controls the number of CPU cores used to encode data.', 'setup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale \n\nboth the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle \n\nlarge amounts of data without sacrificing performance or reliability.\n\n\n\n## Administration API\n\n\n\nAnother new feature is the administration API, which allows you to disable write operations to the service. This is']","Question: How can you parallelize the upload of a large dataset using shards in Qdrant?\n\nContext: ## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""shard_number"": 2

}

```



```python
We recommend using our [Rust client library](https://github.com/qdrant/rust-client) for this purpose, as it is the fastest client library available for Qdrant.



If you are not using Rust, you might want to consider parallelizing your upload process.



## Disable indexing during upload



In case you are doing an initial upload of a large dataset, you might want to disable indexing during upload.

It will enable to avoid unnecessary indexing of vectors, which will be overwritten by the next batch.
},

""shard_number"": 2

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

shard_number=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });
collection_name=""startups"",

documents=documents,

metadata=metadata,

parallel=0,  # Use all available CPU cores to encode data

)

```



The `add` method will encode all documents and upload them to Qdrant.

This is one of two fastembed-specific methods, that combines encoding and uploading into a single step.



The `parallel` parameter controls the number of CPU cores used to encode data.
setup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale 

both the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle 

large amounts of data without sacrificing performance or reliability.



## Administration API



Another new feature is the administration API, which allows you to disable write operations to the service. This is\n\nAnswer:",How can you parallelize the upload of a large dataset using shards in Qdrant?,"By creating multiple shards in Qdrant, you can parallelize the upload of a large dataset. It is recommended to have 2 to 4 shards per machine for efficient processing. When creating a collection in Qdrant, you can specify the number of shards to use for that collection. ","['## Parallel upload into multiple shards\n\n\n\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\n\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python', 'We recommend using our [Rust client library](https://github.com/qdrant/rust-client) for this purpose, as it is the fastest client library available for Qdrant.\n\n\n\nIf you are not using Rust, you might want to consider parallelizing your upload process.\n\n\n\n## Disable indexing during upload\n\n\n\nIn case you are doing an initial upload of a large dataset, you might want to disable indexing during upload.\n\nIt will enable to avoid unnecessary indexing of vectors, which will be overwritten by the next batch.', '},\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    shard_number=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });', 'collection_name=""startups"",\n\n    documents=documents,\n\n    metadata=metadata,\n\n    parallel=0,  # Use all available CPU cores to encode data\n\n)\n\n```\n\n\n\nThe `add` method will encode all documents and upload them to Qdrant.\n\nThis is one of two fastembed-specific methods, that combines encoding and uploading into a single step.\n\n\n\nThe `parallel` parameter controls the number of CPU cores used to encode data.', 'setup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale \n\nboth the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle \n\nlarge amounts of data without sacrificing performance or reliability.\n\n\n\n## Administration API\n\n\n\nAnother new feature is the administration API, which allows you to disable write operations to the service. This is']","## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""shard_number"": 2

}

```



```python
We recommend using our [Rust client library](https://github.com/qdrant/rust-client) for this purpose, as it is the fastest client library available for Qdrant.



If you are not using Rust, you might want to consider parallelizing your upload process.



## Disable indexing during upload



In case you are doing an initial upload of a large dataset, you might want to disable indexing during upload.

It will enable to avoid unnecessary indexing of vectors, which will be overwritten by the next batch.
},

    ""shard_number"": 2

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    shard_number=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });
collection_name=""startups"",

    documents=documents,

    metadata=metadata,

    parallel=0,  # Use all available CPU cores to encode data

)

```



The `add` method will encode all documents and upload them to Qdrant.

This is one of two fastembed-specific methods, that combines encoding and uploading into a single step.



The `parallel` parameter controls the number of CPU cores used to encode data.
setup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale 

both the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle 

large amounts of data without sacrificing performance or reliability.



## Administration API



Another new feature is the administration API, which allows you to disable write operations to the service. This is",0.16017316017316016,0.8604651162790697,0.2700729927007299,0.09565217391304348,0.5238095238095238,0.16176470588235295,0.09090909090909091,0.4883720930232558,0.1532846715328467,0.16017316017316016,0.8604651162790697,0.2700729927007299,0.9085333347320557,0.5523746013641357,0.6852438449859619,0.6116769313812256,0.8956638018452477,0.9813668344480296,254,5.404255319148936,0.557882615086841
True,,1599.456,a31e3b71,What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?,"Maximizing the distance between all points in the response when utilizing vector similarity for diversity search is crucial as it allows for the algorithm to sequentially output dissimilar results. By doing so, a diverse selection of the collection can be made possible without the need for any labeling efforts. This approach ensures that the algorithm can effectively identify and retrieve a wide range of diverse items or data points.","Maximizing the distance between all points in the response when utilizing vector similarity for diversity search is significant because it allows for the selection of dissimilar results. By ensuring that the points chosen are not close to each other in terms of similarity, the algorithm can output a diverse set of results. This diversity in the selection process can lead to a more comprehensive and varied representation of the data, allowing for a richer analysis and exploration of the dataset.","{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=""Example of similarity-based sampling"" >}}





The power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.

By maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.
{{< figure src=/articles_data/vector-similarity-beyond-search/diversity.png caption=""Diversity Search"" >}}





Some forms of diversity sampling are already used in the industry and are known as [Maximum Margin Relevance](https://python.langchain.com/docs/integrations/vectorstores/qdrant#maximum-marginal-relevance-search-mmr) (MMR). Techniques like this were developed to enhance similarity on a universal search API.

However, there is still room for new ideas, particularly regarding diversity retrieval.
Let's imagine how to get 3 points with similarity search and then with diversity search.



Similarity:

1. Calculate distance matrix

2. Choose your anchor

3. Get a vector corresponding to the distances from the selected anchor from the distance matrix

4. Sort fetched vector

5. Get top-3 embeddings

 

Diversity:

1. Calculate distance matrix

2. Initialize starting point (randomly or according to the certain conditions)
You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. 

To mitigate this issue, you can perform a diversity search.



Diversity search is a method for finding the most distinctive examples in the data.

As similarity search, it also operates on embeddings and measures the distances between them.

The difference lies in deciding which point should be extracted next.
Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.





## Diversity Search



Even with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.



The naive approach is to do random sampling. 

However, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.","['{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=""Example of similarity-based sampling"" >}}\n\n\n\n\n\nThe power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.\n\nBy maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.', '{{< figure src=/articles_data/vector-similarity-beyond-search/diversity.png caption=""Diversity Search"" >}}\n\n\n\n\n\nSome forms of diversity sampling are already used in the industry and are known as [Maximum Margin Relevance](https://python.langchain.com/docs/integrations/vectorstores/qdrant#maximum-marginal-relevance-search-mmr) (MMR). Techniques like this were developed to enhance similarity on a universal search API.\n\nHowever, there is still room for new ideas, particularly regarding diversity retrieval.', ""Let's imagine how to get 3 points with similarity search and then with diversity search.\n\n\n\nSimilarity:\n\n1. Calculate distance matrix\n\n2. Choose your anchor\n\n3. Get a vector corresponding to the distances from the selected anchor from the distance matrix\n\n4. Sort fetched vector\n\n5. Get top-3 embeddings\n\n \n\nDiversity:\n\n1. Calculate distance matrix\n\n2. Initialize starting point (randomly or according to the certain conditions)"", 'You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. \n\nTo mitigate this issue, you can perform a diversity search.\n\n\n\nDiversity search is a method for finding the most distinctive examples in the data.\n\nAs similarity search, it also operates on embeddings and measures the distances between them.\n\nThe difference lies in deciding which point should be extracted next.', 'Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.\n\n\n\n\n\n## Diversity Search\n\n\n\nEven with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.\n\n\n\nThe naive approach is to do random sampling. \n\nHowever, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.']","Question: What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?\n\nContext: {{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=""Example of similarity-based sampling"" >}}





The power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.

By maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.
{{< figure src=/articles_data/vector-similarity-beyond-search/diversity.png caption=""Diversity Search"" >}}





Some forms of diversity sampling are already used in the industry and are known as [Maximum Margin Relevance](https://python.langchain.com/docs/integrations/vectorstores/qdrant#maximum-marginal-relevance-search-mmr) (MMR). Techniques like this were developed to enhance similarity on a universal search API.

However, there is still room for new ideas, particularly regarding diversity retrieval.
Let's imagine how to get 3 points with similarity search and then with diversity search.



Similarity:

1. Calculate distance matrix

2. Choose your anchor

3. Get a vector corresponding to the distances from the selected anchor from the distance matrix

4. Sort fetched vector

5. Get top-3 embeddings



Diversity:

1. Calculate distance matrix

2. Initialize starting point (randomly or according to the certain conditions)
You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. 

To mitigate this issue, you can perform a diversity search.



Diversity search is a method for finding the most distinctive examples in the data.

As similarity search, it also operates on embeddings and measures the distances between them.

The difference lies in deciding which point should be extracted next.
Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.





## Diversity Search



Even with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.



The naive approach is to do random sampling. 

However, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.\n\nAnswer:",What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?,"Maximizing the distance between all points in the response when utilizing vector similarity for diversity search is crucial as it allows for the algorithm to sequentially output dissimilar results. By doing so, a diverse selection of the collection can be made possible without the need for any labeling efforts. This approach ensures that the algorithm can effectively identify and retrieve a wide range of diverse items or data points.","['{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=""Example of similarity-based sampling"" >}}\n\n\n\n\n\nThe power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.\n\nBy maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.', '{{< figure src=/articles_data/vector-similarity-beyond-search/diversity.png caption=""Diversity Search"" >}}\n\n\n\n\n\nSome forms of diversity sampling are already used in the industry and are known as [Maximum Margin Relevance](https://python.langchain.com/docs/integrations/vectorstores/qdrant#maximum-marginal-relevance-search-mmr) (MMR). Techniques like this were developed to enhance similarity on a universal search API.\n\nHowever, there is still room for new ideas, particularly regarding diversity retrieval.', ""Let's imagine how to get 3 points with similarity search and then with diversity search.\n\n\n\nSimilarity:\n\n1. Calculate distance matrix\n\n2. Choose your anchor\n\n3. Get a vector corresponding to the distances from the selected anchor from the distance matrix\n\n4. Sort fetched vector\n\n5. Get top-3 embeddings\n\n \n\nDiversity:\n\n1. Calculate distance matrix\n\n2. Initialize starting point (randomly or according to the certain conditions)"", 'You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. \n\nTo mitigate this issue, you can perform a diversity search.\n\n\n\nDiversity search is a method for finding the most distinctive examples in the data.\n\nAs similarity search, it also operates on embeddings and measures the distances between them.\n\nThe difference lies in deciding which point should be extracted next.', 'Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.\n\n\n\n\n\n## Diversity Search\n\n\n\nEven with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.\n\n\n\nThe naive approach is to do random sampling. \n\nHowever, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.']","{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=""Example of similarity-based sampling"" >}}





The power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.

By maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.
{{< figure src=/articles_data/vector-similarity-beyond-search/diversity.png caption=""Diversity Search"" >}}





Some forms of diversity sampling are already used in the industry and are known as [Maximum Margin Relevance](https://python.langchain.com/docs/integrations/vectorstores/qdrant#maximum-marginal-relevance-search-mmr) (MMR). Techniques like this were developed to enhance similarity on a universal search API.

However, there is still room for new ideas, particularly regarding diversity retrieval.
Let's imagine how to get 3 points with similarity search and then with diversity search.



Similarity:

1. Calculate distance matrix

2. Choose your anchor

3. Get a vector corresponding to the distances from the selected anchor from the distance matrix

4. Sort fetched vector

5. Get top-3 embeddings

 

Diversity:

1. Calculate distance matrix

2. Initialize starting point (randomly or according to the certain conditions)
You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. 

To mitigate this issue, you can perform a diversity search.



Diversity search is a method for finding the most distinctive examples in the data.

As similarity search, it also operates on embeddings and measures the distances between them.

The difference lies in deciding which point should be extracted next.
Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.





## Diversity Search



Even with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.



The naive approach is to do random sampling. 

However, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.",0.5507246376811594,0.6229508196721312,0.5846153846153845,0.3088235294117647,0.35,0.328125,0.4057971014492754,0.45901639344262296,0.4307692307692308,0.4927536231884058,0.5573770491803278,0.5230769230769231,0.9294986724853516,0.71504807472229,0.7184139490127563,0.7167270183563232,0.9936654521152377,0.9959922964529445,80,1.1594202898550725,0.5433922534868147
True,,2348.358,810873ee,How can you ensure that collection shards are replicated in Qdrant after adding a new node to the cluster?,"To ensure that collection shards are replicated in Qdrant after adding a new node to the cluster, you can use the Replicate Shard Operation. This operation allows you to create another copy of the shard on the newly connected node. It's important to note that Qdrant does not automatically balance shards as it is considered an expensive operation.","To ensure that collection shards are replicated in Qdrant after adding a new node to the cluster, you can follow these steps:

1. Use the Replicate Shard Operation to create another copy of the shard on the newly connected node. This operation ensures that the data on the shard is replicated on the new node.

2. Qdrant will extract the shard data from the snapshot and properly register the shards in the cluster.

3. If there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.

By following these steps, you can ensure that the collection shards are replicated in Qdrant after adding a new node to the cluster, thereby maintaining data consistency and availability.","Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.

Use the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.
Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.

If there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.



### Recover from a URL or local file



*Available as of v0.11.3*
Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.



### Moving shards



*Available as of v0.9.0*
Practically, it means that if the cluster is in a transition state - either electing a new leader after a failure or starting up, the collection update operations will be denied.



You may use the cluster [REST API](https://qdrant.github.io/qdrant/redoc/index.html?v=master#tag/cluster) to check the state of the consensus.



## Sharding



A Collection in Qdrant is made of one or more shards.

A shard is an independent store of points which is able to perform all operations provided by collections.
```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

	shardNumber: 6,

	replicationFactor: 2

);

```



This code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.","['Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.\n\nUse the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.', 'Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.\n\nIf there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.\n\n\n\n### Recover from a URL or local file\n\n\n\n*Available as of v0.11.3*', 'Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.\n\n\n\n### Moving shards\n\n\n\n*Available as of v0.9.0*', 'Practically, it means that if the cluster is in a transition state - either electing a new leader after a failure or starting up, the collection update operations will be denied.\n\n\n\nYou may use the cluster [REST API](https://qdrant.github.io/qdrant/redoc/index.html?v=master#tag/cluster) to check the state of the consensus.\n\n\n\n## Sharding\n\n\n\nA Collection in Qdrant is made of one or more shards.\n\nA shard is an independent store of points which is able to perform all operations provided by collections.', '```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\n\tshardNumber: 6,\n\n\treplicationFactor: 2\n\n);\n\n```\n\n\n\nThis code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.']","Question: How can you ensure that collection shards are replicated in Qdrant after adding a new node to the cluster?\n\nContext: Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.

Use the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.
Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.

If there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.



### Recover from a URL or local file



*Available as of v0.11.3*
Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.



### Moving shards



*Available as of v0.9.0*
Practically, it means that if the cluster is in a transition state - either electing a new leader after a failure or starting up, the collection update operations will be denied.



You may use the cluster [REST API](https://qdrant.github.io/qdrant/redoc/index.html?v=master#tag/cluster) to check the state of the consensus.



## Sharding



A Collection in Qdrant is made of one or more shards.

A shard is an independent store of points which is able to perform all operations provided by collections.
```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

collectionName: ""{collection_name}"",

vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

shardNumber: 6,

replicationFactor: 2

);

```



This code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.\n\nAnswer:",How can you ensure that collection shards are replicated in Qdrant after adding a new node to the cluster?,"To ensure that collection shards are replicated in Qdrant after adding a new node to the cluster, you can use the Replicate Shard Operation. This operation allows you to create another copy of the shard on the newly connected node. It's important to note that Qdrant does not automatically balance shards as it is considered an expensive operation.","['Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.\n\nUse the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.', 'Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.\n\nIf there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.\n\n\n\n### Recover from a URL or local file\n\n\n\n*Available as of v0.11.3*', 'Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.\n\n\n\n### Moving shards\n\n\n\n*Available as of v0.9.0*', 'Practically, it means that if the cluster is in a transition state - either electing a new leader after a failure or starting up, the collection update operations will be denied.\n\n\n\nYou may use the cluster [REST API](https://qdrant.github.io/qdrant/redoc/index.html?v=master#tag/cluster) to check the state of the consensus.\n\n\n\n## Sharding\n\n\n\nA Collection in Qdrant is made of one or more shards.\n\nA shard is an independent store of points which is able to perform all operations provided by collections.', '```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\n\tshardNumber: 6,\n\n\treplicationFactor: 2\n\n);\n\n```\n\n\n\nThis code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.']","Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.

Use the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.
Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.

If there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.



### Recover from a URL or local file



*Available as of v0.11.3*
Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.



### Moving shards



*Available as of v0.9.0*
Practically, it means that if the cluster is in a transition state - either electing a new leader after a failure or starting up, the collection update operations will be denied.



You may use the cluster [REST API](https://qdrant.github.io/qdrant/redoc/index.html?v=master#tag/cluster) to check the state of the consensus.



## Sharding



A Collection in Qdrant is made of one or more shards.

A shard is an independent store of points which is able to perform all operations provided by collections.
```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

	shardNumber: 6,

	replicationFactor: 2

);

```



This code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.",0.34513274336283184,0.7358490566037735,0.4698795180722891,0.25892857142857145,0.5576923076923077,0.3536585365853659,0.3008849557522124,0.6415094339622641,0.4096385542168675,0.34513274336283184,0.7358490566037735,0.4698795180722891,0.9662380218505859,0.6403146386146545,0.7103132009506226,0.6735000014305115,0.8753000211436301,0.9968878569391867,131,2.2203389830508473,0.5873642345831819
True,,5239.125999999999,1fddee75,Why would someone use a vector database?,"A vector database is used for various reasons, primarily for its efficiency in handling complex data structures and enabling advanced search capabilities. In the context of search and discovery, a vector database allows for state-of-the-art vector-search functionalities, making it ideal for applications requiring similarity search, recommendation systems, and content-based search. By leveraging vectors to represent data points, a vector database can efficiently compute similarities between vectors, enabling quick and accurate retrieval of relevant information","Someone would use a vector database for several reasons, including:

1. Efficient storage and indexing of high-dimensional data: Vector databases are designed to handle high-dimensional vector data efficiently, making them suitable for storing and indexing complex data types like images, text, and audio.

2. Ability to handle large-scale datasets: Vector databases are capable of managing large-scale datasets with billions of data points, allowing for the storage and retrieval of vast amounts of information.

3. Support for real-time analytics and queries: Vector databases offer the capability to perform real-time analytics and queries on the data, enabling faster and more accurate retrieval of information.

Overall, the specialized design of vector databases allows for advanced data analysis and similarity-search operations that go beyond the capabilities of traditional databases, particularly when dealing with high-dimensional vector data.","## Why use a Vector Database?



The data flood is real. 



In 2024, we're drowning in unstructured data like images, text, and audio, that don’t fit into neatly organized tables. Still, we need a way to easily tap into the value within this chaos of almost 330 million terabytes of data being created each day.



Traditional databases, even with extensions that provide some vector handling capabilities, struggle with the complexities and demands of high-dimensional vector data.
aliases: [ /blog/what-is-a-vector-database/ ]

---



> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.





## Why use a Vector Database?



The data flood is real.
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query
Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.





### Starting Your First Vector Database Project","[""## Why use a Vector Database?\n\n\n\nThe data flood is real. \n\n\n\nIn 2024, we're drowning in unstructured data like images, text, and audio, that don’t fit into neatly organized tables. Still, we need a way to easily tap into the value within this chaos of almost 330 million terabytes of data being created each day.\n\n\n\nTraditional databases, even with extensions that provide some vector handling capabilities, struggle with the complexities and demands of high-dimensional vector data."", 'aliases: [ /blog/what-is-a-vector-database/ ]\n\n---\n\n\n\n> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.\n\n\n\n\n\n## Why use a Vector Database?\n\n\n\nThe data flood is real.', 'advantage of their efficient indexing and searching techniques, vector databases enable faster \n\nand more accurate retrieval of unstructured data already represented as vectors, which can \n\nhelp put in front of users the most relevant results to their queries.\n\n\n\nIn addition, other benefits of using vector databases include:\n\n1. Efficient storage and indexing of high-dimensional data.\n\n3. Ability to handle large-scale datasets with billions of data points.\n\n4. Support for real-time analytics and queries.', ""And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query"", 'Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.\n\n\n\n\n\n### Starting Your First Vector Database Project']","Question: Why would someone use a vector database?\n\nContext: ## Why use a Vector Database?



The data flood is real. 



In 2024, we're drowning in unstructured data like images, text, and audio, that don’t fit into neatly organized tables. Still, we need a way to easily tap into the value within this chaos of almost 330 million terabytes of data being created each day.



Traditional databases, even with extensions that provide some vector handling capabilities, struggle with the complexities and demands of high-dimensional vector data.
aliases: [ /blog/what-is-a-vector-database/ ]

---



> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.





## Why use a Vector Database?



The data flood is real.
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query
Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.





### Starting Your First Vector Database Project\n\nAnswer:",Why would someone use a vector database?,"A vector database is used for various reasons, primarily for its efficiency in handling complex data structures and enabling advanced search capabilities. In the context of search and discovery, a vector database allows for state-of-the-art vector-search functionalities, making it ideal for applications requiring similarity search, recommendation systems, and content-based search. By leveraging vectors to represent data points, a vector database can efficiently compute similarities between vectors, enabling quick and accurate retrieval of relevant information","[""## Why use a Vector Database?\n\n\n\nThe data flood is real. \n\n\n\nIn 2024, we're drowning in unstructured data like images, text, and audio, that don’t fit into neatly organized tables. Still, we need a way to easily tap into the value within this chaos of almost 330 million terabytes of data being created each day.\n\n\n\nTraditional databases, even with extensions that provide some vector handling capabilities, struggle with the complexities and demands of high-dimensional vector data."", 'aliases: [ /blog/what-is-a-vector-database/ ]\n\n---\n\n\n\n> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.\n\n\n\n\n\n## Why use a Vector Database?\n\n\n\nThe data flood is real.', 'advantage of their efficient indexing and searching techniques, vector databases enable faster \n\nand more accurate retrieval of unstructured data already represented as vectors, which can \n\nhelp put in front of users the most relevant results to their queries.\n\n\n\nIn addition, other benefits of using vector databases include:\n\n1. Efficient storage and indexing of high-dimensional data.\n\n3. Ability to handle large-scale datasets with billions of data points.\n\n4. Support for real-time analytics and queries.', ""And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query"", 'Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.\n\n\n\n\n\n### Starting Your First Vector Database Project']","## Why use a Vector Database?



The data flood is real. 



In 2024, we're drowning in unstructured data like images, text, and audio, that don’t fit into neatly organized tables. Still, we need a way to easily tap into the value within this chaos of almost 330 million terabytes of data being created each day.



Traditional databases, even with extensions that provide some vector handling capabilities, struggle with the complexities and demands of high-dimensional vector data.
aliases: [ /blog/what-is-a-vector-database/ ]

---



> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.





## Why use a Vector Database?



The data flood is real.
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query
Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.





### Starting Your First Vector Database Project",0.29850746268656714,0.5405405405405406,0.38461538461538464,0.07518796992481203,0.136986301369863,0.0970873786407767,0.1865671641791045,0.33783783783783783,0.2403846153846154,0.26865671641791045,0.4864864864864865,0.3461538461538462,0.6745564341545105,0.6405507326126099,0.6938791275024414,0.6661493182182312,0.835559180326527,0.9881010848718385,132,1.7837837837837838,0.4612378145591081
True,,3113.159,7f58b51d,What benefits does Qdrant Cloud on Microsoft Azure offer for rapid application development?,"Qdrant Cloud on Microsoft Azure offers the benefit of rapid application development by allowing users to deploy their own cluster through the Qdrant Cloud Console within seconds. This means that users can set up their environment on Azure quickly, reducing deployment time and enabling them to scale their resources as needed. This rapid deployment capability enables users to hit the ground running with their development projects, facilitating faster development cycles and improved scalability.","Qdrant Cloud on Microsoft Azure offers several benefits for rapid application development, including:

1. **Deployment Speed**: You can deploy your own cluster through the Qdrant Cloud Console within seconds, allowing you to start working on your applications quickly.

2. **Scalability**: With the ability to scale your resources as needed, you can adjust the capacity of your cluster to accommodate varying workloads and growing datasets.

3. **Billion Vector Scale**: Qdrant Cloud on Azure can seamlessly handle large-scale datasets with billions of vectors. This capability is crucial for applications that deal with extensive data sets and require efficient processing.

4. **Features Integration**: By leveraging Qdrant features like horizontal scaling and binary quantization within Microsoft Azure's scalable infrastructure, you can optimize the performance of your applications and enhance their capabilities.

Overall, Qdrant Cloud on Microsoft Azure provides a robust and efficient environment for rapid application development, enabling you to focus on building and iterating on your projects without worrying about infrastructure limitations.","What this means for you:



- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.

- **Billion vector scale**: Seamlessly grow and handle large-scale datasets with billions of vectors. Leverage Qdrant features like horizontal scaling and binary quantization with Microsoft Azure's scalable infrastructure.
---

draft: false

title: Introducing Qdrant Cloud on Microsoft Azure

slug: qdrant-cloud-on-microsoft-azure

short_description: Qdrant Cloud is now available on Microsoft Azure

description: ""Learn the benefits of Qdrant Cloud on Azure.""

preview_image: /blog/from_cms/qdrant-azure-2-1.png

date: 2024-01-17T08:40:42Z

author: Manuel Meyer

featured: false

tags:

  - Data Science

  - Vector Database

  - Machine Learning

  - Information Retrieval

  - Cloud

  - Azure

---
- Information Retrieval

  - Cloud

  - Azure

---

Great news! We've expanded Qdrant's managed vector database offering — [Qdrant Cloud](https://cloud.qdrant.io/) — to be available on Microsoft Azure. 

You can now effortlessly set up your environment on Azure, which reduces deployment time, so you can hit the ground running.



[Get started](https://cloud.qdrant.io/)



What this means for you:
- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:
---

title: AWS Marketplace

weight: 60

---



# Qdrant Cloud on AWS Marketplace



## Overview



Our [AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-rtphb42tydtzg) listing streamlines access to Qdrant for users who rely on Amazon Web Services for hosting and application development. Please note that, while Qdrant's clusters run on AWS, you will still use the Qdrant Cloud infrastructure. 



## Billing","[""What this means for you:\n\n\n\n- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.\n\n- **Billion vector scale**: Seamlessly grow and handle large-scale datasets with billions of vectors. Leverage Qdrant features like horizontal scaling and binary quantization with Microsoft Azure's scalable infrastructure."", '---\n\ndraft: false\n\ntitle: Introducing Qdrant Cloud on Microsoft Azure\n\nslug: qdrant-cloud-on-microsoft-azure\n\nshort_description: Qdrant Cloud is now available on Microsoft Azure\n\ndescription: ""Learn the benefits of Qdrant Cloud on Azure.""\n\npreview_image: /blog/from_cms/qdrant-azure-2-1.png\n\ndate: 2024-01-17T08:40:42Z\n\nauthor: Manuel Meyer\n\nfeatured: false\n\ntags:\n\n  - Data Science\n\n  - Vector Database\n\n  - Machine Learning\n\n  - Information Retrieval\n\n  - Cloud\n\n  - Azure\n\n---', ""- Information Retrieval\n\n  - Cloud\n\n  - Azure\n\n---\n\nGreat news! We've expanded Qdrant's managed vector database offering — [Qdrant Cloud](https://cloud.qdrant.io/) — to be available on Microsoft Azure. \n\nYou can now effortlessly set up your environment on Azure, which reduces deployment time, so you can hit the ground running.\n\n\n\n[Get started](https://cloud.qdrant.io/)\n\n\n\nWhat this means for you:"", '- Upgradeable to later versions of Qdrant as they are released.\n\n- Equipped with monitoring and logging to observe the health of each cluster. \n\n- Accessible through the Qdrant Cloud Console.\n\n- Vertically scalable.\n\n- Offered on AWS and GCP, with Azure currently in development. \n\n\n\n### Getting started with Qdrant Cloud\n\n\n\nTo use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:', ""---\n\ntitle: AWS Marketplace\n\nweight: 60\n\n---\n\n\n\n# Qdrant Cloud on AWS Marketplace\n\n\n\n## Overview\n\n\n\nOur [AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-rtphb42tydtzg) listing streamlines access to Qdrant for users who rely on Amazon Web Services for hosting and application development. Please note that, while Qdrant's clusters run on AWS, you will still use the Qdrant Cloud infrastructure. \n\n\n\n## Billing""]","Question: What benefits does Qdrant Cloud on Microsoft Azure offer for rapid application development?\n\nContext: What this means for you:



- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.

- **Billion vector scale**: Seamlessly grow and handle large-scale datasets with billions of vectors. Leverage Qdrant features like horizontal scaling and binary quantization with Microsoft Azure's scalable infrastructure.
---

draft: false

title: Introducing Qdrant Cloud on Microsoft Azure

slug: qdrant-cloud-on-microsoft-azure

short_description: Qdrant Cloud is now available on Microsoft Azure

description: ""Learn the benefits of Qdrant Cloud on Azure.""

preview_image: /blog/from_cms/qdrant-azure-2-1.png

date: 2024-01-17T08:40:42Z

author: Manuel Meyer

featured: false

tags:

- Data Science

- Vector Database

- Machine Learning

- Information Retrieval

- Cloud

- Azure

---
- Information Retrieval

- Cloud

- Azure

---

Great news! We've expanded Qdrant's managed vector database offering — [Qdrant Cloud](https://cloud.qdrant.io/) — to be available on Microsoft Azure. 

You can now effortlessly set up your environment on Azure, which reduces deployment time, so you can hit the ground running.



[Get started](https://cloud.qdrant.io/)



What this means for you:
- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:
---

title: AWS Marketplace

weight: 60

---



# Qdrant Cloud on AWS Marketplace



## Overview



Our [AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-rtphb42tydtzg) listing streamlines access to Qdrant for users who rely on Amazon Web Services for hosting and application development. Please note that, while Qdrant's clusters run on AWS, you will still use the Qdrant Cloud infrastructure. 



## Billing\n\nAnswer:",What benefits does Qdrant Cloud on Microsoft Azure offer for rapid application development?,"Qdrant Cloud on Microsoft Azure offers the benefit of rapid application development by allowing users to deploy their own cluster through the Qdrant Cloud Console within seconds. This means that users can set up their environment on Azure quickly, reducing deployment time and enabling them to scale their resources as needed. This rapid deployment capability enables users to hit the ground running with their development projects, facilitating faster development cycles and improved scalability.","[""What this means for you:\n\n\n\n- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.\n\n- **Billion vector scale**: Seamlessly grow and handle large-scale datasets with billions of vectors. Leverage Qdrant features like horizontal scaling and binary quantization with Microsoft Azure's scalable infrastructure."", '---\n\ndraft: false\n\ntitle: Introducing Qdrant Cloud on Microsoft Azure\n\nslug: qdrant-cloud-on-microsoft-azure\n\nshort_description: Qdrant Cloud is now available on Microsoft Azure\n\ndescription: ""Learn the benefits of Qdrant Cloud on Azure.""\n\npreview_image: /blog/from_cms/qdrant-azure-2-1.png\n\ndate: 2024-01-17T08:40:42Z\n\nauthor: Manuel Meyer\n\nfeatured: false\n\ntags:\n\n  - Data Science\n\n  - Vector Database\n\n  - Machine Learning\n\n  - Information Retrieval\n\n  - Cloud\n\n  - Azure\n\n---', ""- Information Retrieval\n\n  - Cloud\n\n  - Azure\n\n---\n\nGreat news! We've expanded Qdrant's managed vector database offering — [Qdrant Cloud](https://cloud.qdrant.io/) — to be available on Microsoft Azure. \n\nYou can now effortlessly set up your environment on Azure, which reduces deployment time, so you can hit the ground running.\n\n\n\n[Get started](https://cloud.qdrant.io/)\n\n\n\nWhat this means for you:"", '- Upgradeable to later versions of Qdrant as they are released.\n\n- Equipped with monitoring and logging to observe the health of each cluster. \n\n- Accessible through the Qdrant Cloud Console.\n\n- Vertically scalable.\n\n- Offered on AWS and GCP, with Azure currently in development. \n\n\n\n### Getting started with Qdrant Cloud\n\n\n\nTo use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:', ""---\n\ntitle: AWS Marketplace\n\nweight: 60\n\n---\n\n\n\n# Qdrant Cloud on AWS Marketplace\n\n\n\n## Overview\n\n\n\nOur [AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-rtphb42tydtzg) listing streamlines access to Qdrant for users who rely on Amazon Web Services for hosting and application development. Please note that, while Qdrant's clusters run on AWS, you will still use the Qdrant Cloud infrastructure. \n\n\n\n## Billing""]","What this means for you:



- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.

- **Billion vector scale**: Seamlessly grow and handle large-scale datasets with billions of vectors. Leverage Qdrant features like horizontal scaling and binary quantization with Microsoft Azure's scalable infrastructure.
---

draft: false

title: Introducing Qdrant Cloud on Microsoft Azure

slug: qdrant-cloud-on-microsoft-azure

short_description: Qdrant Cloud is now available on Microsoft Azure

description: ""Learn the benefits of Qdrant Cloud on Azure.""

preview_image: /blog/from_cms/qdrant-azure-2-1.png

date: 2024-01-17T08:40:42Z

author: Manuel Meyer

featured: false

tags:

  - Data Science

  - Vector Database

  - Machine Learning

  - Information Retrieval

  - Cloud

  - Azure

---
- Information Retrieval

  - Cloud

  - Azure

---

Great news! We've expanded Qdrant's managed vector database offering — [Qdrant Cloud](https://cloud.qdrant.io/) — to be available on Microsoft Azure. 

You can now effortlessly set up your environment on Azure, which reduces deployment time, so you can hit the ground running.



[Get started](https://cloud.qdrant.io/)



What this means for you:
- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:
---

title: AWS Marketplace

weight: 60

---



# Qdrant Cloud on AWS Marketplace



## Overview



Our [AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-rtphb42tydtzg) listing streamlines access to Qdrant for users who rely on Amazon Web Services for hosting and application development. Please note that, while Qdrant's clusters run on AWS, you will still use the Qdrant Cloud infrastructure. 



## Billing",0.3076923076923077,0.6857142857142857,0.4247787610619469,0.11612903225806452,0.2608695652173913,0.16071428571428573,0.20512820512820512,0.45714285714285713,0.2831858407079646,0.28846153846153844,0.6428571428571429,0.39823008849557523,0.9388482570648193,0.6451241970062256,0.7410062551498413,0.689749002456665,0.6400869466364384,0.9962183764825264,160,2.191780821917808,0.5206668994172993
True,,1524.9669999999999,3a8531ed,What is the purpose of the `groupBy` parameter?,"The `groupBy` parameter is used to specify how the results should be grouped based on a specific field, like ""document_id"". By setting the `groupBy` parameter to ""document_id"", the results are organized into groups where each group contains all the hits associated with a particular document_id.","The purpose of the `groupBy` parameter is to specify the field by which the results should be grouped when using the **groups** API for searching. This parameter allows you to organize the search results into distinct groups based on a particular field or criteria. By specifying the `groupBy` field, you can group the search results based on common attributes or values, making it easier to analyze and present the data in a structured format.","<aside role=""status"">This feature relies heavily on the `group_by` key provided. To improve performance, make sure to create a dedicated index for it.</aside>



**Limitations**:



* Only [keyword](../payload/#keyword) and [integer](../payload/#integer) payload values are supported for the `group_by` parameter. Payload values with other types will be ignored.

* At the moment, pagination is not enabled when using **groups**, so the `offset` parameter is not allowed.



### Lookup in groups
}

```



```python

client.search_groups(

    collection_name=""{collection_name}"",

    # Same as in the regular search() API

    query_vector=g,

    # Grouping parameters

    group_by=""document_id"",  # Path of the field to group by

    limit=4,  # Max amount of groups

    group_size=2,  # Max amount of points per group

)

```



```typescript

client.searchPointGroups(""{collection_name}"", {

  vector: [1.1],

  group_by: ""document_id"",

  limit: 4,

  group_size: 2,

});

```



```rust
},

        ""vector"": [0.6]

    }

]

```



With the ***groups*** API, you will be able to get the best *N* points for each document, assuming that the payload of the points contains the document ID. Of course there will be times where the best *N* points cannot be fulfilled due to lack of points or a big distance with respect to the query. In every case, the `group_size` is a best-effort parameter, akin to the `limit` parameter.



### Search groups
""with_payload"": [""title"", ""text""],



        // Options for specifying what to bring from the vector(s) 

        // of the looked up point, true by default

        ""with_vectors: false

    }

}

```



```python

client.search_groups(

    collection_name=""chunks"",

    # Same as in the regular search() API

    query_vector=[1.1],

    # Grouping parameters

    group_by=""document_id"",  # Path of the field to group by

    limit=2,  # Max amount of groups
One way of optimizing storage when using groups is to store the information shared by the points with the same group id in a single point in another collection. Then, when using the [**groups** API](#grouping-api), add the `with_lookup` parameter to bring the information from those points into each group.



![Group id matches point id](/docs/lookup_id_linking.png)



This has the extra benefit of having a single point to update when the information shared by the points in a group changes.","['<aside role=""status"">This feature relies heavily on the `group_by` key provided. To improve performance, make sure to create a dedicated index for it.</aside>\n\n\n\n**Limitations**:\n\n\n\n* Only [keyword](../payload/#keyword) and [integer](../payload/#integer) payload values are supported for the `group_by` parameter. Payload values with other types will be ignored.\n\n* At the moment, pagination is not enabled when using **groups**, so the `offset` parameter is not allowed.\n\n\n\n### Lookup in groups', '}\n\n```\n\n\n\n```python\n\nclient.search_groups(\n\n    collection_name=""{collection_name}"",\n\n    # Same as in the regular search() API\n\n    query_vector=g,\n\n    # Grouping parameters\n\n    group_by=""document_id"",  # Path of the field to group by\n\n    limit=4,  # Max amount of groups\n\n    group_size=2,  # Max amount of points per group\n\n)\n\n```\n\n\n\n```typescript\n\nclient.searchPointGroups(""{collection_name}"", {\n\n  vector: [1.1],\n\n  group_by: ""document_id"",\n\n  limit: 4,\n\n  group_size: 2,\n\n});\n\n```\n\n\n\n```rust', '},\n\n        ""vector"": [0.6]\n\n    }\n\n]\n\n```\n\n\n\nWith the ***groups*** API, you will be able to get the best *N* points for each document, assuming that the payload of the points contains the document ID. Of course there will be times where the best *N* points cannot be fulfilled due to lack of points or a big distance with respect to the query. In every case, the `group_size` is a best-effort parameter, akin to the `limit` parameter.\n\n\n\n### Search groups', '""with_payload"": [""title"", ""text""],\n\n\n\n        // Options for specifying what to bring from the vector(s) \n\n        // of the looked up point, true by default\n\n        ""with_vectors: false\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.search_groups(\n\n    collection_name=""chunks"",\n\n    # Same as in the regular search() API\n\n    query_vector=[1.1],\n\n    # Grouping parameters\n\n    group_by=""document_id"",  # Path of the field to group by\n\n    limit=2,  # Max amount of groups', 'One way of optimizing storage when using groups is to store the information shared by the points with the same group id in a single point in another collection. Then, when using the [**groups** API](#grouping-api), add the `with_lookup` parameter to bring the information from those points into each group.\n\n\n\n![Group id matches point id](/docs/lookup_id_linking.png)\n\n\n\nThis has the extra benefit of having a single point to update when the information shared by the points in a group changes.']","Question: What is the purpose of the `groupBy` parameter?\n\nContext: <aside role=""status"">This feature relies heavily on the `group_by` key provided. To improve performance, make sure to create a dedicated index for it.</aside>



**Limitations**:



* Only [keyword](../payload/#keyword) and [integer](../payload/#integer) payload values are supported for the `group_by` parameter. Payload values with other types will be ignored.

* At the moment, pagination is not enabled when using **groups**, so the `offset` parameter is not allowed.



### Lookup in groups
}

```



```python

client.search_groups(

collection_name=""{collection_name}"",

# Same as in the regular search() API

query_vector=g,

# Grouping parameters

group_by=""document_id"",  # Path of the field to group by

limit=4,  # Max amount of groups

group_size=2,  # Max amount of points per group

)

```



```typescript

client.searchPointGroups(""{collection_name}"", {

vector: [1.1],

group_by: ""document_id"",

limit: 4,

group_size: 2,

});

```



```rust
},

""vector"": [0.6]

}

]

```



With the ***groups*** API, you will be able to get the best *N* points for each document, assuming that the payload of the points contains the document ID. Of course there will be times where the best *N* points cannot be fulfilled due to lack of points or a big distance with respect to the query. In every case, the `group_size` is a best-effort parameter, akin to the `limit` parameter.



### Search groups
""with_payload"": [""title"", ""text""],



// Options for specifying what to bring from the vector(s) 

// of the looked up point, true by default

""with_vectors: false

}

}

```



```python

client.search_groups(

collection_name=""chunks"",

# Same as in the regular search() API

query_vector=[1.1],

# Grouping parameters

group_by=""document_id"",  # Path of the field to group by

limit=2,  # Max amount of groups
One way of optimizing storage when using groups is to store the information shared by the points with the same group id in a single point in another collection. Then, when using the [**groups** API](#grouping-api), add the `with_lookup` parameter to bring the information from those points into each group.



![Group id matches point id](/docs/lookup_id_linking.png)



This has the extra benefit of having a single point to update when the information shared by the points in a group changes.\n\nAnswer:",What is the purpose of the `groupBy` parameter?,"The `groupBy` parameter is used to specify how the results should be grouped based on a specific field, like ""document_id"". By setting the `groupBy` parameter to ""document_id"", the results are organized into groups where each group contains all the hits associated with a particular document_id.","['<aside role=""status"">This feature relies heavily on the `group_by` key provided. To improve performance, make sure to create a dedicated index for it.</aside>\n\n\n\n**Limitations**:\n\n\n\n* Only [keyword](../payload/#keyword) and [integer](../payload/#integer) payload values are supported for the `group_by` parameter. Payload values with other types will be ignored.\n\n* At the moment, pagination is not enabled when using **groups**, so the `offset` parameter is not allowed.\n\n\n\n### Lookup in groups', '}\n\n```\n\n\n\n```python\n\nclient.search_groups(\n\n    collection_name=""{collection_name}"",\n\n    # Same as in the regular search() API\n\n    query_vector=g,\n\n    # Grouping parameters\n\n    group_by=""document_id"",  # Path of the field to group by\n\n    limit=4,  # Max amount of groups\n\n    group_size=2,  # Max amount of points per group\n\n)\n\n```\n\n\n\n```typescript\n\nclient.searchPointGroups(""{collection_name}"", {\n\n  vector: [1.1],\n\n  group_by: ""document_id"",\n\n  limit: 4,\n\n  group_size: 2,\n\n});\n\n```\n\n\n\n```rust', '},\n\n        ""vector"": [0.6]\n\n    }\n\n]\n\n```\n\n\n\nWith the ***groups*** API, you will be able to get the best *N* points for each document, assuming that the payload of the points contains the document ID. Of course there will be times where the best *N* points cannot be fulfilled due to lack of points or a big distance with respect to the query. In every case, the `group_size` is a best-effort parameter, akin to the `limit` parameter.\n\n\n\n### Search groups', '""with_payload"": [""title"", ""text""],\n\n\n\n        // Options for specifying what to bring from the vector(s) \n\n        // of the looked up point, true by default\n\n        ""with_vectors: false\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.search_groups(\n\n    collection_name=""chunks"",\n\n    # Same as in the regular search() API\n\n    query_vector=[1.1],\n\n    # Grouping parameters\n\n    group_by=""document_id"",  # Path of the field to group by\n\n    limit=2,  # Max amount of groups', 'One way of optimizing storage when using groups is to store the information shared by the points with the same group id in a single point in another collection. Then, when using the [**groups** API](#grouping-api), add the `with_lookup` parameter to bring the information from those points into each group.\n\n\n\n![Group id matches point id](/docs/lookup_id_linking.png)\n\n\n\nThis has the extra benefit of having a single point to update when the information shared by the points in a group changes.']","<aside role=""status"">This feature relies heavily on the `group_by` key provided. To improve performance, make sure to create a dedicated index for it.</aside>



**Limitations**:



* Only [keyword](../payload/#keyword) and [integer](../payload/#integer) payload values are supported for the `group_by` parameter. Payload values with other types will be ignored.

* At the moment, pagination is not enabled when using **groups**, so the `offset` parameter is not allowed.



### Lookup in groups
}

```



```python

client.search_groups(

    collection_name=""{collection_name}"",

    # Same as in the regular search() API

    query_vector=g,

    # Grouping parameters

    group_by=""document_id"",  # Path of the field to group by

    limit=4,  # Max amount of groups

    group_size=2,  # Max amount of points per group

)

```



```typescript

client.searchPointGroups(""{collection_name}"", {

  vector: [1.1],

  group_by: ""document_id"",

  limit: 4,

  group_size: 2,

});

```



```rust
},

        ""vector"": [0.6]

    }

]

```



With the ***groups*** API, you will be able to get the best *N* points for each document, assuming that the payload of the points contains the document ID. Of course there will be times where the best *N* points cannot be fulfilled due to lack of points or a big distance with respect to the query. In every case, the `group_size` is a best-effort parameter, akin to the `limit` parameter.



### Search groups
""with_payload"": [""title"", ""text""],



        // Options for specifying what to bring from the vector(s) 

        // of the looked up point, true by default

        ""with_vectors: false

    }

}

```



```python

client.search_groups(

    collection_name=""chunks"",

    # Same as in the regular search() API

    query_vector=[1.1],

    # Grouping parameters

    group_by=""document_id"",  # Path of the field to group by

    limit=2,  # Max amount of groups
One way of optimizing storage when using groups is to store the information shared by the points with the same group id in a single point in another collection. Then, when using the [**groups** API](#grouping-api), add the `with_lookup` parameter to bring the information from those points into each group.



![Group id matches point id](/docs/lookup_id_linking.png)



This has the extra benefit of having a single point to update when the information shared by the points in a group changes.",0.36507936507936506,0.5609756097560976,0.4423076923076923,0.12903225806451613,0.2,0.1568627450980392,0.23809523809523808,0.36585365853658536,0.2884615384615385,0.3333333333333333,0.5121951219512195,0.40384615384615385,0.7896147966384888,0.656978964805603,0.6968545317649841,0.6763295531272888,0.8243692417939504,0.7629112005233765,74,1.6444444444444444,0.293785792166613
True,,4734.058,0b5f4717,How can one change or correct Qdrant's behavior and default collection settings using configuration files?,"To change or correct Qdrant's behavior and default collection settings, one can utilize configuration files. The default configuration file for Qdrant is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml). If you wish to modify the default configuration, you can add a new configuration file and specify the path using `--config-path path/to/custom_config.yaml","To change or correct Qdrant's behavior and default collection settings using configuration files, follow these steps:

1. Locate the default configuration file at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).

2. Add a new configuration file and specify the Qdrant settings by setting values under the `qdrant` property in the `settings.yaml` file.

3. Customize the default configuration of the collection used under the hood by providing settings when you create an instance of the `QdrantDocumentStore`.

4. To start a Qdrant instance with a custom configuration, use the following Docker command:

```bash
docker run -p 6333:6333 \
-v $(pwd)/path/to/data:/qdrant/storage \
qdrant/qdrant
```

5. To change the Qdrant configuration, overwrite the production configuration by accessing `config.yaml` and setting `storage.temp_path` to another directory location.

6. Make sure to review the available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) to tailor the settings according to your requirements.

By following these steps, you can effectively change or correct Qdrant's behavior and default collection settings using configuration files.","---

title: Configuration

weight: 160

aliases:

  - ../configuration

---



# Configuration



To change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.



The default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).



To change the default configuration, add a new configuration file and specify
## Configuration



Qdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.



Example: 

```yaml

qdrant:

    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""

    api_key: ""<your-api-key>""

```



The available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:

| Field        | Description |
```bash

pip install qdrant-haystack

```



`QdrantDocumentStore` supports [all the configuration properties](/documentation/collections/#create-collection) available in 

the Qdrant Python client. If you want to customize the default configuration of the collection used under the hood, you can

provide that settings when you create an instance of the `QdrantDocumentStore`. For example, if you'd like to enable the

Scalar Quantization, you'd make that in the following way:



```python
-v $(pwd)/path/to/data:/qdrant/storage \

    qdrant/qdrant

```



With this command, you start a Qdrant instance with the default configuration.

It stores all data in the `./path/to/data` directory.



By default, Qdrant uses port 6333, so at [localhost:6333](http://localhost:6333) you should see the welcome message.



To change the Qdrant configuration, you can overwrite the production configuration:



```bash

docker run -p 6333:6333 \

    -v $(pwd)/path/to/data:/qdrant/storage \
To change it, access `config.yaml` and set `storage.temp_path` to another directory location.



## Important changes



The latest release focuses not only on the new features but also introduces some changes making 

Qdrant even more reliable.



### Optimizing group requests","[""---\n\ntitle: Configuration\n\nweight: 160\n\naliases:\n\n  - ../configuration\n\n---\n\n\n\n# Configuration\n\n\n\nTo change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.\n\n\n\nThe default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\n\n\n\nTo change the default configuration, add a new configuration file and specify"", '## Configuration\n\n\n\nQdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.\n\n\n\nExample: \n\n```yaml\n\nqdrant:\n\n    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""\n\n    api_key: ""<your-api-key>""\n\n```\n\n\n\nThe available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:\n\n| Field        | Description |', ""```bash\n\npip install qdrant-haystack\n\n```\n\n\n\n`QdrantDocumentStore` supports [all the configuration properties](/documentation/collections/#create-collection) available in \n\nthe Qdrant Python client. If you want to customize the default configuration of the collection used under the hood, you can\n\nprovide that settings when you create an instance of the `QdrantDocumentStore`. For example, if you'd like to enable the\n\nScalar Quantization, you'd make that in the following way:\n\n\n\n```python"", '-v $(pwd)/path/to/data:/qdrant/storage \\\n\n    qdrant/qdrant\n\n```\n\n\n\nWith this command, you start a Qdrant instance with the default configuration.\n\nIt stores all data in the `./path/to/data` directory.\n\n\n\nBy default, Qdrant uses port 6333, so at [localhost:6333](http://localhost:6333) you should see the welcome message.\n\n\n\nTo change the Qdrant configuration, you can overwrite the production configuration:\n\n\n\n```bash\n\ndocker run -p 6333:6333 \\\n\n    -v $(pwd)/path/to/data:/qdrant/storage \\', 'To change it, access `config.yaml` and set `storage.temp_path` to another directory location.\n\n\n\n## Important changes\n\n\n\nThe latest release focuses not only on the new features but also introduces some changes making \n\nQdrant even more reliable.\n\n\n\n### Optimizing group requests']","Question: How can one change or correct Qdrant's behavior and default collection settings using configuration files?\n\nContext: ---

title: Configuration

weight: 160

aliases:

- ../configuration

---



# Configuration



To change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.



The default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).



To change the default configuration, add a new configuration file and specify
## Configuration



Qdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.



Example: 

```yaml

qdrant:

url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""

api_key: ""<your-api-key>""

```



The available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:

| Field        | Description |
```bash

pip install qdrant-haystack

```



`QdrantDocumentStore` supports [all the configuration properties](/documentation/collections/#create-collection) available in 

the Qdrant Python client. If you want to customize the default configuration of the collection used under the hood, you can

provide that settings when you create an instance of the `QdrantDocumentStore`. For example, if you'd like to enable the

Scalar Quantization, you'd make that in the following way:



```python
-v $(pwd)/path/to/data:/qdrant/storage \

qdrant/qdrant

```



With this command, you start a Qdrant instance with the default configuration.

It stores all data in the `./path/to/data` directory.



By default, Qdrant uses port 6333, so at [localhost:6333](http://localhost:6333) you should see the welcome message.



To change the Qdrant configuration, you can overwrite the production configuration:



```bash

docker run -p 6333:6333 \

-v $(pwd)/path/to/data:/qdrant/storage \
To change it, access `config.yaml` and set `storage.temp_path` to another directory location.



## Important changes



The latest release focuses not only on the new features but also introduces some changes making 

Qdrant even more reliable.



### Optimizing group requests\n\nAnswer:",How can one change or correct Qdrant's behavior and default collection settings using configuration files?,"To change or correct Qdrant's behavior and default collection settings, one can utilize configuration files. The default configuration file for Qdrant is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml). If you wish to modify the default configuration, you can add a new configuration file and specify the path using `--config-path path/to/custom_config.yaml","[""---\n\ntitle: Configuration\n\nweight: 160\n\naliases:\n\n  - ../configuration\n\n---\n\n\n\n# Configuration\n\n\n\nTo change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.\n\n\n\nThe default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\n\n\n\nTo change the default configuration, add a new configuration file and specify"", '## Configuration\n\n\n\nQdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.\n\n\n\nExample: \n\n```yaml\n\nqdrant:\n\n    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""\n\n    api_key: ""<your-api-key>""\n\n```\n\n\n\nThe available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:\n\n| Field        | Description |', ""```bash\n\npip install qdrant-haystack\n\n```\n\n\n\n`QdrantDocumentStore` supports [all the configuration properties](/documentation/collections/#create-collection) available in \n\nthe Qdrant Python client. If you want to customize the default configuration of the collection used under the hood, you can\n\nprovide that settings when you create an instance of the `QdrantDocumentStore`. For example, if you'd like to enable the\n\nScalar Quantization, you'd make that in the following way:\n\n\n\n```python"", '-v $(pwd)/path/to/data:/qdrant/storage \\\n\n    qdrant/qdrant\n\n```\n\n\n\nWith this command, you start a Qdrant instance with the default configuration.\n\nIt stores all data in the `./path/to/data` directory.\n\n\n\nBy default, Qdrant uses port 6333, so at [localhost:6333](http://localhost:6333) you should see the welcome message.\n\n\n\nTo change the Qdrant configuration, you can overwrite the production configuration:\n\n\n\n```bash\n\ndocker run -p 6333:6333 \\\n\n    -v $(pwd)/path/to/data:/qdrant/storage \\', 'To change it, access `config.yaml` and set `storage.temp_path` to another directory location.\n\n\n\n## Important changes\n\n\n\nThe latest release focuses not only on the new features but also introduces some changes making \n\nQdrant even more reliable.\n\n\n\n### Optimizing group requests']","---

title: Configuration

weight: 160

aliases:

  - ../configuration

---



# Configuration



To change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.



The default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).



To change the default configuration, add a new configuration file and specify
## Configuration



Qdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.



Example: 

```yaml

qdrant:

    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""

    api_key: ""<your-api-key>""

```



The available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:

| Field        | Description |
```bash

pip install qdrant-haystack

```



`QdrantDocumentStore` supports [all the configuration properties](/documentation/collections/#create-collection) available in 

the Qdrant Python client. If you want to customize the default configuration of the collection used under the hood, you can

provide that settings when you create an instance of the `QdrantDocumentStore`. For example, if you'd like to enable the

Scalar Quantization, you'd make that in the following way:



```python
-v $(pwd)/path/to/data:/qdrant/storage \

    qdrant/qdrant

```



With this command, you start a Qdrant instance with the default configuration.

It stores all data in the `./path/to/data` directory.



By default, Qdrant uses port 6333, so at [localhost:6333](http://localhost:6333) you should see the welcome message.



To change the Qdrant configuration, you can overwrite the production configuration:



```bash

docker run -p 6333:6333 \

    -v $(pwd)/path/to/data:/qdrant/storage \
To change it, access `config.yaml` and set `storage.temp_path` to another directory location.



## Important changes



The latest release focuses not only on the new features but also introduces some changes making 

Qdrant even more reliable.



### Optimizing group requests",0.3072289156626506,0.8360655737704918,0.44933920704845814,0.21212121212121213,0.5833333333333334,0.3111111111111111,0.2469879518072289,0.6721311475409836,0.3612334801762115,0.2891566265060241,0.7868852459016393,0.4229074889867841,0.9694802165031433,0.6195670962333679,0.7242096066474915,0.6678139567375183,0.3789312491814295,0.5997948807974656,159,3.18,0.4203683132720641
True,,1831.105,16141961,What are the two ways of creating batches supported by the Qdrant API and how do they differ internally?,"The Qdrant API supports two ways of creating batches - record-oriented and column-oriented. Internally, these options do not differ and are made only for the convenience of interaction. This means that both record-oriented and column-oriented batch creation methods achieve the same result in terms of processing and storing data within the Qdrant system.","The two ways of creating batches supported by the Qdrant API are record-oriented and column-oriented. These options do not differ internally and are provided for the convenience of interaction. 

In the context of uploading points in batches, you can use the batched approach in Python as shown in the code snippet provided. This code snippet demonstrates how to batch points for insertion into Qdrant. 

In summary, batch search in Qdrant allows for packing multiple queries into a single API call, which can significantly improve performance by reducing network overhead and simplifying application code.","}

	}

);



```



are both possible.



## Upload points



To optimize performance, Qdrant supports batch loading of points. I.e., you can load several points into the service in one API call.

Batching allows you to minimize the overhead of creating a network connection.



The Qdrant API supports two ways of creating batches - record-oriented and column-oriented.

Internally, these options do not differ and are made only for the convenience of interaction.



Create points with batch:



```http
batch by batch, into Qdrant:



```python

batch_size = 100



for batch in batched(dataset, batch_size):

    ids = [point.pop(""id"") for point in batch]

    vectors = [point.pop(""vector"") for point in batch]



    client.upsert(

        collection_name=""arxiv-titles-instructorxl-embeddings"",

        points=models.Batch(

            ids=ids,

            vectors=vectors,

            payloads=batch,

        ),

    )

```
location=""http://localhost:6333"",

            collection_name=""test"",

        ),

        write_config=QdrantWriteConfig(batch_size=80),

    )



if __name__ == ""__main__"":

    writer = get_writer()

    runner = LocalRunner(

        processor_config=ProcessorConfig(

            verbose=True,

            output_dir=""local-output-to-qdrant"",

            num_processes=2,

        ),

        connector_config=SimpleLocalConfig(
## Summary



Batch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!
Now, thanks to the new batch search, you don’t need to worry about that. Qdrant will handle multiple search requests in just one API call and will perform those requests in the most optimal way.



## An example of using the batch search



We’ve used the official Python client to show how the batch search might be integrated with your application. Since there have been some changes in the interfaces of Qdrant 0.10.0, we’ll go step by step.



## Creating the collection","['}\n\n\t}\n\n);\n\n\n\n```\n\n\n\nare both possible.\n\n\n\n## Upload points\n\n\n\nTo optimize performance, Qdrant supports batch loading of points. I.e., you can load several points into the service in one API call.\n\nBatching allows you to minimize the overhead of creating a network connection.\n\n\n\nThe Qdrant API supports two ways of creating batches - record-oriented and column-oriented.\n\nInternally, these options do not differ and are made only for the convenience of interaction.\n\n\n\nCreate points with batch:\n\n\n\n```http', 'batch by batch, into Qdrant:\n\n\n\n```python\n\nbatch_size = 100\n\n\n\nfor batch in batched(dataset, batch_size):\n\n    ids = [point.pop(""id"") for point in batch]\n\n    vectors = [point.pop(""vector"") for point in batch]\n\n\n\n    client.upsert(\n\n        collection_name=""arxiv-titles-instructorxl-embeddings"",\n\n        points=models.Batch(\n\n            ids=ids,\n\n            vectors=vectors,\n\n            payloads=batch,\n\n        ),\n\n    )\n\n```', 'location=""http://localhost:6333"",\n\n            collection_name=""test"",\n\n        ),\n\n        write_config=QdrantWriteConfig(batch_size=80),\n\n    )\n\n\n\nif __name__ == ""__main__"":\n\n    writer = get_writer()\n\n    runner = LocalRunner(\n\n        processor_config=ProcessorConfig(\n\n            verbose=True,\n\n            output_dir=""local-output-to-qdrant"",\n\n            num_processes=2,\n\n        ),\n\n        connector_config=SimpleLocalConfig(', '## Summary\n\n\n\nBatch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!', 'Now, thanks to the new batch search, you don’t need to worry about that. Qdrant will handle multiple search requests in just one API call and will perform those requests in the most optimal way.\n\n\n\n## An example of using the batch search\n\n\n\nWe’ve used the official Python client to show how the batch search might be integrated with your application. Since there have been some changes in the interfaces of Qdrant 0.10.0, we’ll go step by step.\n\n\n\n## Creating the collection']","Question: What are the two ways of creating batches supported by the Qdrant API and how do they differ internally?\n\nContext: }

}

);



```



are both possible.



## Upload points



To optimize performance, Qdrant supports batch loading of points. I.e., you can load several points into the service in one API call.

Batching allows you to minimize the overhead of creating a network connection.



The Qdrant API supports two ways of creating batches - record-oriented and column-oriented.

Internally, these options do not differ and are made only for the convenience of interaction.



Create points with batch:



```http
batch by batch, into Qdrant:



```python

batch_size = 100



for batch in batched(dataset, batch_size):

ids = [point.pop(""id"") for point in batch]

vectors = [point.pop(""vector"") for point in batch]



client.upsert(

collection_name=""arxiv-titles-instructorxl-embeddings"",

points=models.Batch(

ids=ids,

vectors=vectors,

payloads=batch,

),

)

```
location=""http://localhost:6333"",

collection_name=""test"",

),

write_config=QdrantWriteConfig(batch_size=80),

)



if __name__ == ""__main__"":

writer = get_writer()

runner = LocalRunner(

processor_config=ProcessorConfig(

verbose=True,

output_dir=""local-output-to-qdrant"",

num_processes=2,

),

connector_config=SimpleLocalConfig(
## Summary



Batch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!
Now, thanks to the new batch search, you don’t need to worry about that. Qdrant will handle multiple search requests in just one API call and will perform those requests in the most optimal way.



## An example of using the batch search



We’ve used the official Python client to show how the batch search might be integrated with your application. Since there have been some changes in the interfaces of Qdrant 0.10.0, we’ll go step by step.



## Creating the collection\n\nAnswer:",What are the two ways of creating batches supported by the Qdrant API and how do they differ internally?,"The Qdrant API supports two ways of creating batches - record-oriented and column-oriented. Internally, these options do not differ and are made only for the convenience of interaction. This means that both record-oriented and column-oriented batch creation methods achieve the same result in terms of processing and storing data within the Qdrant system.","['}\n\n\t}\n\n);\n\n\n\n```\n\n\n\nare both possible.\n\n\n\n## Upload points\n\n\n\nTo optimize performance, Qdrant supports batch loading of points. I.e., you can load several points into the service in one API call.\n\nBatching allows you to minimize the overhead of creating a network connection.\n\n\n\nThe Qdrant API supports two ways of creating batches - record-oriented and column-oriented.\n\nInternally, these options do not differ and are made only for the convenience of interaction.\n\n\n\nCreate points with batch:\n\n\n\n```http', 'batch by batch, into Qdrant:\n\n\n\n```python\n\nbatch_size = 100\n\n\n\nfor batch in batched(dataset, batch_size):\n\n    ids = [point.pop(""id"") for point in batch]\n\n    vectors = [point.pop(""vector"") for point in batch]\n\n\n\n    client.upsert(\n\n        collection_name=""arxiv-titles-instructorxl-embeddings"",\n\n        points=models.Batch(\n\n            ids=ids,\n\n            vectors=vectors,\n\n            payloads=batch,\n\n        ),\n\n    )\n\n```', 'location=""http://localhost:6333"",\n\n            collection_name=""test"",\n\n        ),\n\n        write_config=QdrantWriteConfig(batch_size=80),\n\n    )\n\n\n\nif __name__ == ""__main__"":\n\n    writer = get_writer()\n\n    runner = LocalRunner(\n\n        processor_config=ProcessorConfig(\n\n            verbose=True,\n\n            output_dir=""local-output-to-qdrant"",\n\n            num_processes=2,\n\n        ),\n\n        connector_config=SimpleLocalConfig(', '## Summary\n\n\n\nBatch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!', 'Now, thanks to the new batch search, you don’t need to worry about that. Qdrant will handle multiple search requests in just one API call and will perform those requests in the most optimal way.\n\n\n\n## An example of using the batch search\n\n\n\nWe’ve used the official Python client to show how the batch search might be integrated with your application. Since there have been some changes in the interfaces of Qdrant 0.10.0, we’ll go step by step.\n\n\n\n## Creating the collection']","}

	}

);



```



are both possible.



## Upload points



To optimize performance, Qdrant supports batch loading of points. I.e., you can load several points into the service in one API call.

Batching allows you to minimize the overhead of creating a network connection.



The Qdrant API supports two ways of creating batches - record-oriented and column-oriented.

Internally, these options do not differ and are made only for the convenience of interaction.



Create points with batch:



```http
batch by batch, into Qdrant:



```python

batch_size = 100



for batch in batched(dataset, batch_size):

    ids = [point.pop(""id"") for point in batch]

    vectors = [point.pop(""vector"") for point in batch]



    client.upsert(

        collection_name=""arxiv-titles-instructorxl-embeddings"",

        points=models.Batch(

            ids=ids,

            vectors=vectors,

            payloads=batch,

        ),

    )

```
location=""http://localhost:6333"",

            collection_name=""test"",

        ),

        write_config=QdrantWriteConfig(batch_size=80),

    )



if __name__ == ""__main__"":

    writer = get_writer()

    runner = LocalRunner(

        processor_config=ProcessorConfig(

            verbose=True,

            output_dir=""local-output-to-qdrant"",

            num_processes=2,

        ),

        connector_config=SimpleLocalConfig(
## Summary



Batch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!
Now, thanks to the new batch search, you don’t need to worry about that. Qdrant will handle multiple search requests in just one API call and will perform those requests in the most optimal way.



## An example of using the batch search



We’ve used the official Python client to show how the batch search might be integrated with your application. Since there have been some changes in the interfaces of Qdrant 0.10.0, we’ll go step by step.



## Creating the collection",0.3522727272727273,0.5961538461538461,0.4428571428571429,0.19540229885057472,0.3333333333333333,0.24637681159420285,0.2840909090909091,0.4807692307692308,0.35714285714285715,0.32954545454545453,0.5576923076923077,0.41428571428571426,0.7493405342102051,0.6425495743751526,0.6979228258132935,0.6690924763679504,0.9790843422990292,0.961579135308663,93,1.7884615384615385,0.5484701082514619
True,,5121.9349999999995,406731a6,How can you create an index for a specific field in a payload using the Qdrant library?,"To create an index for a specific field in a payload using the Qdrant library, you can utilize the `CreatePayloadIndexAsync` method provided by the client. This method takes in the parameters `collectionName` and `fieldName`. ","To create an index for a specific field in a payload using the Qdrant library, you can use the following code snippets in different programming languages:

1. JavaScript:

```javascript
client.createPayloadIndex(""{collection_name}"", {
    field_name: ""name_of_the_field_to_index"",
    field_schema: {
        type: ""text"",
        tokenizer: ""word"",
        min_token_len: 2,
        max_token_len: 15,
        lowercase: true,
    },
});
```

2. Rust:

```rust
client.create_field_index(
    ""{collection_name}"",
    ""name_of_the_field_to_index"",
    FieldType::Keyword,
    None,
    None,
).await?;
```

3. Java:

```java
client.createPayloadIndexAsync(
    ""{collection_name}"",
    ""name_of_the_field_to_index"",
    PayloadSchemaType.Keyword,
    null,
    true,
    null,
    null
);
```

4. C#:

```csharp
await client.CreatePayloadIndexAsync(
    collectionName: ""{collection_name}"",
    fieldName: ""name_of_the_field_to_index"",
    schemaType: PayloadSchemaType.Text
);
```

These code snippets demonstrate how to create a payload index for a specific field in the Qdrant library in various programming languages.","const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createPayloadIndex(""{collection_name}"", {

  field_name: ""name_of_the_field_to_index"",

  field_schema: {

    type: ""text"",

    tokenizer: ""word"",

    min_token_len: 2,

    max_token_len: 15,

    lowercase: true,

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,

        TokenizerType,
client

    .create_field_index(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        FieldType::Keyword,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createPayloadIndexAsync(
});

```



```rust

use qdrant_client::qdrant::FieldType;



client

    .create_field_index(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        FieldType::Keyword,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



client.createPayloadIndexAsync(

    ""{collection_name}"",

    ""name_of_the_field_to_index"",

    PayloadSchemaType.Keyword,

    null,

    true,

    null,

    null);

```
keys: [""color"", ""price""],

	filter: MatchKeyword(""color"", ""red"")

);

```



## Payload indexing



To search more efficiently with filters, Qdrant allows you to create indexes for payload fields by specifying the name and type of field it is intended to be.



The indexed fields also affect the vector index. See [Indexing](../indexing) for details.



In practice, we recommend creating an index on those fields that could potentially constrain the results the most.
.setMaxTokenLen(10)

                    .setLowercase(true)

                    .build())

            .build(),

        null,

        null,

        null)

    .get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(

	collectionName: ""{collection_name}"",

	fieldName: ""name_of_the_field_to_index"",

	schemaType: PayloadSchemaType.Text,","['const client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createPayloadIndex(""{collection_name}"", {\n\n  field_name: ""name_of_the_field_to_index"",\n\n  field_schema: {\n\n    type: ""text"",\n\n    tokenizer: ""word"",\n\n    min_token_len: 2,\n\n    max_token_len: 15,\n\n    lowercase: true,\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\n\n        TokenizerType,', 'client\n\n    .create_field_index(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(', '});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::qdrant::FieldType;\n\n\n\nclient\n\n    .create_field_index(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nclient.createPayloadIndexAsync(\n\n    ""{collection_name}"",\n\n    ""name_of_the_field_to_index"",\n\n    PayloadSchemaType.Keyword,\n\n    null,\n\n    true,\n\n    null,\n\n    null);\n\n```', 'keys: [""color"", ""price""],\n\n\tfilter: MatchKeyword(""color"", ""red"")\n\n);\n\n```\n\n\n\n## Payload indexing\n\n\n\nTo search more efficiently with filters, Qdrant allows you to create indexes for payload fields by specifying the name and type of field it is intended to be.\n\n\n\nThe indexed fields also affect the vector index. See [Indexing](../indexing) for details.\n\n\n\nIn practice, we recommend creating an index on those fields that could potentially constrain the results the most.', '.setMaxTokenLen(10)\n\n                    .setLowercase(true)\n\n                    .build())\n\n            .build(),\n\n        null,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tfieldName: ""name_of_the_field_to_index"",\n\n\tschemaType: PayloadSchemaType.Text,']","Question: How can you create an index for a specific field in a payload using the Qdrant library?\n\nContext: const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createPayloadIndex(""{collection_name}"", {

field_name: ""name_of_the_field_to_index"",

field_schema: {

type: ""text"",

tokenizer: ""word"",

min_token_len: 2,

max_token_len: 15,

lowercase: true,

},

});

```



```rust

use qdrant_client::{

client::QdrantClient,

qdrant::{

payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,

TokenizerType,
client

.create_field_index(

""{collection_name}"",

""name_of_the_field_to_index"",

FieldType::Keyword,

None,

None,

)

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createPayloadIndexAsync(
});

```



```rust

use qdrant_client::qdrant::FieldType;



client

.create_field_index(

""{collection_name}"",

""name_of_the_field_to_index"",

FieldType::Keyword,

None,

None,

)

.await?;

```



```java

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



client.createPayloadIndexAsync(

""{collection_name}"",

""name_of_the_field_to_index"",

PayloadSchemaType.Keyword,

null,

true,

null,

null);

```
keys: [""color"", ""price""],

filter: MatchKeyword(""color"", ""red"")

);

```



## Payload indexing



To search more efficiently with filters, Qdrant allows you to create indexes for payload fields by specifying the name and type of field it is intended to be.



The indexed fields also affect the vector index. See [Indexing](../indexing) for details.



In practice, we recommend creating an index on those fields that could potentially constrain the results the most.
.setMaxTokenLen(10)

.setLowercase(true)

.build())

.build(),

null,

null,

null)

.get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(

collectionName: ""{collection_name}"",

fieldName: ""name_of_the_field_to_index"",

schemaType: PayloadSchemaType.Text,\n\nAnswer:",How can you create an index for a specific field in a payload using the Qdrant library?,"To create an index for a specific field in a payload using the Qdrant library, you can utilize the `CreatePayloadIndexAsync` method provided by the client. This method takes in the parameters `collectionName` and `fieldName`. ","['const client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createPayloadIndex(""{collection_name}"", {\n\n  field_name: ""name_of_the_field_to_index"",\n\n  field_schema: {\n\n    type: ""text"",\n\n    tokenizer: ""word"",\n\n    min_token_len: 2,\n\n    max_token_len: 15,\n\n    lowercase: true,\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\n\n        TokenizerType,', 'client\n\n    .create_field_index(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(', '});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::qdrant::FieldType;\n\n\n\nclient\n\n    .create_field_index(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nclient.createPayloadIndexAsync(\n\n    ""{collection_name}"",\n\n    ""name_of_the_field_to_index"",\n\n    PayloadSchemaType.Keyword,\n\n    null,\n\n    true,\n\n    null,\n\n    null);\n\n```', 'keys: [""color"", ""price""],\n\n\tfilter: MatchKeyword(""color"", ""red"")\n\n);\n\n```\n\n\n\n## Payload indexing\n\n\n\nTo search more efficiently with filters, Qdrant allows you to create indexes for payload fields by specifying the name and type of field it is intended to be.\n\n\n\nThe indexed fields also affect the vector index. See [Indexing](../indexing) for details.\n\n\n\nIn practice, we recommend creating an index on those fields that could potentially constrain the results the most.', '.setMaxTokenLen(10)\n\n                    .setLowercase(true)\n\n                    .build())\n\n            .build(),\n\n        null,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tfieldName: ""name_of_the_field_to_index"",\n\n\tschemaType: PayloadSchemaType.Text,']","const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createPayloadIndex(""{collection_name}"", {

  field_name: ""name_of_the_field_to_index"",

  field_schema: {

    type: ""text"",

    tokenizer: ""word"",

    min_token_len: 2,

    max_token_len: 15,

    lowercase: true,

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,

        TokenizerType,
client

    .create_field_index(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        FieldType::Keyword,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createPayloadIndexAsync(
});

```



```rust

use qdrant_client::qdrant::FieldType;



client

    .create_field_index(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        FieldType::Keyword,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



client.createPayloadIndexAsync(

    ""{collection_name}"",

    ""name_of_the_field_to_index"",

    PayloadSchemaType.Keyword,

    null,

    true,

    null,

    null);

```
keys: [""color"", ""price""],

	filter: MatchKeyword(""color"", ""red"")

);

```



## Payload indexing



To search more efficiently with filters, Qdrant allows you to create indexes for payload fields by specifying the name and type of field it is intended to be.



The indexed fields also affect the vector index. See [Indexing](../indexing) for details.



In practice, we recommend creating an index on those fields that could potentially constrain the results the most.
.setMaxTokenLen(10)

                    .setLowercase(true)

                    .build())

            .build(),

        null,

        null,

        null)

    .get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(

	collectionName: ""{collection_name}"",

	fieldName: ""name_of_the_field_to_index"",

	schemaType: PayloadSchemaType.Text,",0.13953488372093023,0.6666666666666666,0.23076923076923078,0.09375,0.46153846153846156,0.15584415584415584,0.13178294573643412,0.6296296296296297,0.21794871794871795,0.13178294573643412,0.6296296296296297,0.21794871794871795,0.9308973550796509,0.47976428270339966,0.6178125143051147,0.5401068329811096,0.4241549074649811,0.7631462290883064,99,2.911764705882353,0.3768089325003732
True,,1485.171,8e8a161b,What is the purpose of the Quantization?,"Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces. In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.","Quantization is necessary in scenarios involving high-dimensional vectors to achieve efficient storage and faster search operations. The primary purpose of quantization is to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces. By transforming original vectors into new representations, quantization compresses data while maintaining relative distances between vectors. This process allows for significant reductions in storage requirements and search times.","Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.

In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.



There are tradeoffs associated with quantization.

On the one hand, quantization allows for significant reductions in storage requirements and faster search times.
---

title: Quantization

weight: 120

aliases:

  - ../quantization

---



# Quantization



Quantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.

By transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.

Different quantization methods have different mechanics and tradeoffs. We will cover them in this section.
Demetrios:

I see, okay, so basically this is the quantization. And now let's dive into the binary quantization and how it works.



Andrey Vasnetsov:
Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear
. So without further ado, let's jump into this, Andrey, we're talking about binary quantization, but let's maybe start a step back. Why do we need any quantization at all? Why not just use original vectors?","['Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\n\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\n\n\n\nThere are tradeoffs associated with quantization.\n\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.', '---\n\ntitle: Quantization\n\nweight: 120\n\naliases:\n\n  - ../quantization\n\n---\n\n\n\n# Quantization\n\n\n\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\n\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\n\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section.', ""Demetrios:\n\nI see, okay, so basically this is the quantization. And now let's dive into the binary quantization and how it works.\n\n\n\nAndrey Vasnetsov:"", ""Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear"", "". So without further ado, let's jump into this, Andrey, we're talking about binary quantization, but let's maybe start a step back. Why do we need any quantization at all? Why not just use original vectors?""]","Question: What is the purpose of the Quantization?\n\nContext: Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.

In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.



There are tradeoffs associated with quantization.

On the one hand, quantization allows for significant reductions in storage requirements and faster search times.
---

title: Quantization

weight: 120

aliases:

- ../quantization

---



# Quantization



Quantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.

By transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.

Different quantization methods have different mechanics and tradeoffs. We will cover them in this section.
Demetrios:

I see, okay, so basically this is the quantization. And now let's dive into the binary quantization and how it works.



Andrey Vasnetsov:
Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear
. So without further ado, let's jump into this, Andrey, we're talking about binary quantization, but let's maybe start a step back. Why do we need any quantization at all? Why not just use original vectors?\n\nAnswer:",What is the purpose of the Quantization?,"Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces. In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.","['Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\n\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\n\n\n\nThere are tradeoffs associated with quantization.\n\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.', '---\n\ntitle: Quantization\n\nweight: 120\n\naliases:\n\n  - ../quantization\n\n---\n\n\n\n# Quantization\n\n\n\nQuantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.\n\nBy transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.\n\nDifferent quantization methods have different mechanics and tradeoffs. We will cover them in this section.', ""Demetrios:\n\nI see, okay, so basically this is the quantization. And now let's dive into the binary quantization and how it works.\n\n\n\nAndrey Vasnetsov:"", ""Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear"", "". So without further ado, let's jump into this, Andrey, we're talking about binary quantization, but let's maybe start a step back. Why do we need any quantization at all? Why not just use original vectors?""]","Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.

In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.



There are tradeoffs associated with quantization.

On the one hand, quantization allows for significant reductions in storage requirements and faster search times.
---

title: Quantization

weight: 120

aliases:

  - ../quantization

---



# Quantization



Quantization is an optional feature in Qdrant that enables efficient storage and search of high-dimensional vectors.

By transforming original vectors into a new representations, quantization compresses data while preserving close to original relative distances between vectors.

Different quantization methods have different mechanics and tradeoffs. We will cover them in this section.
Demetrios:

I see, okay, so basically this is the quantization. And now let's dive into the binary quantization and how it works.



Andrey Vasnetsov:
Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear
. So without further ado, let's jump into this, Andrey, we're talking about binary quantization, but let's maybe start a step back. Why do we need any quantization at all? Why not just use original vectors?",0.42857142857142855,0.6585365853658537,0.5192307692307693,0.22580645161290322,0.35,0.2745098039215686,0.3333333333333333,0.5121951219512195,0.40384615384615385,0.3968253968253968,0.6097560975609756,0.4807692307692307,0.7520666718482971,0.6788134574890137,0.6797687411308289,0.679290771484375,0.985207098252431,0.9921272344654426,64,1.391304347826087,0.6998312970649706
True,,5304.3240000000005,863ae984,How can the retrieval quality of an approximation be measured in the context of semantic search?,"The retrieval quality of an approximation in semantic search can be measured using various quality metrics. Some of these metrics include Precision@k, Mean Reciprocal Rank (MRR), and DCG and NDCG.","In the context of semantic search, the retrieval quality of an approximation can be measured through various metrics that evaluate the effectiveness of the search results. Some common quality metrics include:

1. Precision@k: This metric measures the proportion of relevant documents retrieved in the top-k search results. A higher precision@k value indicates better retrieval quality as more relevant documents are included in the top results.

2. Mean Reciprocal Rank (MRR): MRR calculates the average of the reciprocal ranks of the first relevant document retrieved across multiple queries. A higher MRR score signifies better retrieval quality as relevant documents are ranked higher on average.

3. Recall: Recall measures the proportion of relevant documents that were retrieved out of the total number of relevant documents. Higher recall values indicate better retrieval quality as more relevant documents are successfully retrieved.

4. F1 Score: The F1 score is a metric that combines precision and recall into a single value, providing a balanced assessment of retrieval quality. It considers both false positives and false negatives in the search results.

By using these metrics and comparing the retrieved documents with ground truth data, the retrieval quality of an approximation in semantic search can be effectively evaluated to ensure the accuracy and relevance of the search results.","to do it. 



## Wrapping up



Assessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.

your search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, 

even in an automated way, as part of your CI/CD pipeline.
perform pure kNN search. Instead, they use **Approximate Nearest Neighbors** (ANN) algorithms, which are much faster than the exact search, 

but can return suboptimal results. We can also **measure the retrieval quality of that approximation** which also contributes to the overall

search quality.



### Quality metrics
them with the ground truth. In that setup, **finding the most similar documents is implemented as full kNN search, without any approximation**.

As a result, we can measure the quality of the embeddings themselves, without the influence of the ANN algorithm.



## Retrieval quality



Embeddings quality is indeed the most important factor in the semantic search quality. However, vector search engines, such as Qdrant, do not
---

title: Measure retrieval quality

weight: 21

---



# Measure retrieval quality



| Time: 30 min | Level: Intermediate |  |    |

|--------------|---------------------|--|----|



Semantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might

be far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another
search quality.



### Quality metrics



There are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), 

are based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),","['to do it. \n\n\n\n## Wrapping up\n\n\n\nAssessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.\n\nyour search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, \n\neven in an automated way, as part of your CI/CD pipeline.', 'perform pure kNN search. Instead, they use **Approximate Nearest Neighbors** (ANN) algorithms, which are much faster than the exact search, \n\nbut can return suboptimal results. We can also **measure the retrieval quality of that approximation** which also contributes to the overall\n\nsearch quality.\n\n\n\n### Quality metrics', 'them with the ground truth. In that setup, **finding the most similar documents is implemented as full kNN search, without any approximation**.\n\nAs a result, we can measure the quality of the embeddings themselves, without the influence of the ANN algorithm.\n\n\n\n## Retrieval quality\n\n\n\nEmbeddings quality is indeed the most important factor in the semantic search quality. However, vector search engines, such as Qdrant, do not', '---\n\ntitle: Measure retrieval quality\n\nweight: 21\n\n---\n\n\n\n# Measure retrieval quality\n\n\n\n| Time: 30 min | Level: Intermediate |  |    |\n\n|--------------|---------------------|--|----|\n\n\n\nSemantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might\n\nbe far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another', 'search quality.\n\n\n\n### Quality metrics\n\n\n\nThere are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), \n\nare based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),']","Question: How can the retrieval quality of an approximation be measured in the context of semantic search?\n\nContext: to do it. 



## Wrapping up



Assessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.

your search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, 

even in an automated way, as part of your CI/CD pipeline.
perform pure kNN search. Instead, they use **Approximate Nearest Neighbors** (ANN) algorithms, which are much faster than the exact search, 

but can return suboptimal results. We can also **measure the retrieval quality of that approximation** which also contributes to the overall

search quality.



### Quality metrics
them with the ground truth. In that setup, **finding the most similar documents is implemented as full kNN search, without any approximation**.

As a result, we can measure the quality of the embeddings themselves, without the influence of the ANN algorithm.



## Retrieval quality



Embeddings quality is indeed the most important factor in the semantic search quality. However, vector search engines, such as Qdrant, do not
---

title: Measure retrieval quality

weight: 21

---



# Measure retrieval quality



| Time: 30 min | Level: Intermediate |  |    |

|--------------|---------------------|--|----|



Semantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might

be far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another
search quality.



### Quality metrics



There are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), 

are based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),\n\nAnswer:",How can the retrieval quality of an approximation be measured in the context of semantic search?,"The retrieval quality of an approximation in semantic search can be measured using various quality metrics. Some of these metrics include Precision@k, Mean Reciprocal Rank (MRR), and DCG and NDCG.","['to do it. \n\n\n\n## Wrapping up\n\n\n\nAssessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.\n\nyour search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, \n\neven in an automated way, as part of your CI/CD pipeline.', 'perform pure kNN search. Instead, they use **Approximate Nearest Neighbors** (ANN) algorithms, which are much faster than the exact search, \n\nbut can return suboptimal results. We can also **measure the retrieval quality of that approximation** which also contributes to the overall\n\nsearch quality.\n\n\n\n### Quality metrics', 'them with the ground truth. In that setup, **finding the most similar documents is implemented as full kNN search, without any approximation**.\n\nAs a result, we can measure the quality of the embeddings themselves, without the influence of the ANN algorithm.\n\n\n\n## Retrieval quality\n\n\n\nEmbeddings quality is indeed the most important factor in the semantic search quality. However, vector search engines, such as Qdrant, do not', '---\n\ntitle: Measure retrieval quality\n\nweight: 21\n\n---\n\n\n\n# Measure retrieval quality\n\n\n\n| Time: 30 min | Level: Intermediate |  |    |\n\n|--------------|---------------------|--|----|\n\n\n\nSemantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might\n\nbe far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another', 'search quality.\n\n\n\n### Quality metrics\n\n\n\nThere are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), \n\nare based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),']","to do it. 



## Wrapping up



Assessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.

your search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, 

even in an automated way, as part of your CI/CD pipeline.
perform pure kNN search. Instead, they use **Approximate Nearest Neighbors** (ANN) algorithms, which are much faster than the exact search, 

but can return suboptimal results. We can also **measure the retrieval quality of that approximation** which also contributes to the overall

search quality.



### Quality metrics
them with the ground truth. In that setup, **finding the most similar documents is implemented as full kNN search, without any approximation**.

As a result, we can measure the quality of the embeddings themselves, without the influence of the ANN algorithm.



## Retrieval quality



Embeddings quality is indeed the most important factor in the semantic search quality. However, vector search engines, such as Qdrant, do not
---

title: Measure retrieval quality

weight: 21

---



# Measure retrieval quality



| Time: 30 min | Level: Intermediate |  |    |

|--------------|---------------------|--|----|



Semantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might

be far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another
search quality.



### Quality metrics



There are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), 

are based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),",0.14361702127659576,0.9310344827586207,0.2488479262672811,0.0855614973262032,0.5714285714285714,0.14883720930232555,0.10638297872340426,0.6896551724137931,0.18433179723502302,0.13829787234042554,0.896551724137931,0.23963133640552994,0.8676345348358154,0.5297763347625732,0.6638549566268921,0.5892852544784546,0.6834420428145677,0.8517300420789979,212,6.838709677419355,0.5588481235671164
True,,2361.206,b7165f4e,Why does Qdrant deliberately exclude libraries or algorithm implementations in their benchmark comparisons?,"Qdrant excludes libraries or algorithm implementations in their benchmark comparisons because their primary focus is on vector databases. By limiting their comparisons to open-source solutions and avoiding external cloud components, Qdrant ensures hardware parity and minimizes biases. This allows them to provide accurate and unbiased benchmarks specifically tailored to the performance of vector databases, enabling users to make informed decisions based on the data provided.","Qdrant deliberately excludes libraries or algorithm implementations in their benchmark comparisons because their primary focus is on vector databases. By excluding external libraries or algorithms, they can accurately measure the performance of Qdrant itself without any external factors affecting the results. This allows them to identify bottlenecks and areas for improvement within Qdrant specifically. Additionally, by collecting information on how Qdrant is used and utilizing internal heuristics to optimize performance, they can tailor the database to better meet the needs of their users and improve overall performance for everyone.","We deliberately don't include libraries or algorithm implementations in our comparisons because our focus is squarely on vector databases. 



Why?
. We had elasticsearch PG vector, that is an extension of PostgreSQL and Qdrants. And on this slide you can see Pycon for example, and Pycon was excluded because of the lack of geospatial filtering. And so we benchmark them regarding the qps.
| **Supported Technologies**          | Python, JavaScript/TypeScript | Python, JavaScript/TypeScript, Rust, Go      | Qdrant supports a broader range of programming languages |

| **Performance** (e.g., query speed) | TnC Prohibit Benchmarking     | [Benchmark result](/benchmarks/)             | Compare performance metrics                              |
We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.



In order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.



Additionally, Qdrant uses a bunch of internal heuristics to optimize the performance.

To better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.

With this information, we can make Qdrant faster for everyone.
## Why Qdrant?","[""We deliberately don't include libraries or algorithm implementations in our comparisons because our focus is squarely on vector databases. \n\n\n\nWhy?"", '. We had elasticsearch PG vector, that is an extension of PostgreSQL and Qdrants. And on this slide you can see Pycon for example, and Pycon was excluded because of the lack of geospatial filtering. And so we benchmark them regarding the qps.', '| **Supported Technologies**          | Python, JavaScript/TypeScript | Python, JavaScript/TypeScript, Rust, Go      | Qdrant supports a broader range of programming languages |\n\n| **Performance** (e.g., query speed) | TnC Prohibit Benchmarking     | [Benchmark result](/benchmarks/)             | Compare performance metrics                              |', 'We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.\n\n\n\nIn order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.\n\n\n\nAdditionally, Qdrant uses a bunch of internal heuristics to optimize the performance.\n\nTo better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.\n\nWith this information, we can make Qdrant faster for everyone.', '## Why Qdrant?']","Question: Why does Qdrant deliberately exclude libraries or algorithm implementations in their benchmark comparisons?\n\nContext: We deliberately don't include libraries or algorithm implementations in our comparisons because our focus is squarely on vector databases. 



Why?
. We had elasticsearch PG vector, that is an extension of PostgreSQL and Qdrants. And on this slide you can see Pycon for example, and Pycon was excluded because of the lack of geospatial filtering. And so we benchmark them regarding the qps.
| **Supported Technologies**          | Python, JavaScript/TypeScript | Python, JavaScript/TypeScript, Rust, Go      | Qdrant supports a broader range of programming languages |

| **Performance** (e.g., query speed) | TnC Prohibit Benchmarking     | [Benchmark result](/benchmarks/)             | Compare performance metrics                              |
We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.



In order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.



Additionally, Qdrant uses a bunch of internal heuristics to optimize the performance.

To better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.

With this information, we can make Qdrant faster for everyone.
## Why Qdrant?\n\nAnswer:",Why does Qdrant deliberately exclude libraries or algorithm implementations in their benchmark comparisons?,"Qdrant excludes libraries or algorithm implementations in their benchmark comparisons because their primary focus is on vector databases. By limiting their comparisons to open-source solutions and avoiding external cloud components, Qdrant ensures hardware parity and minimizes biases. This allows them to provide accurate and unbiased benchmarks specifically tailored to the performance of vector databases, enabling users to make informed decisions based on the data provided.","[""We deliberately don't include libraries or algorithm implementations in our comparisons because our focus is squarely on vector databases. \n\n\n\nWhy?"", '. We had elasticsearch PG vector, that is an extension of PostgreSQL and Qdrants. And on this slide you can see Pycon for example, and Pycon was excluded because of the lack of geospatial filtering. And so we benchmark them regarding the qps.', '| **Supported Technologies**          | Python, JavaScript/TypeScript | Python, JavaScript/TypeScript, Rust, Go      | Qdrant supports a broader range of programming languages |\n\n| **Performance** (e.g., query speed) | TnC Prohibit Benchmarking     | [Benchmark result](/benchmarks/)             | Compare performance metrics                              |', 'We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.\n\n\n\nIn order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.\n\n\n\nAdditionally, Qdrant uses a bunch of internal heuristics to optimize the performance.\n\nTo better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.\n\nWith this information, we can make Qdrant faster for everyone.', '## Why Qdrant?']","We deliberately don't include libraries or algorithm implementations in our comparisons because our focus is squarely on vector databases. 



Why?
. We had elasticsearch PG vector, that is an extension of PostgreSQL and Qdrants. And on this slide you can see Pycon for example, and Pycon was excluded because of the lack of geospatial filtering. And so we benchmark them regarding the qps.
| **Supported Technologies**          | Python, JavaScript/TypeScript | Python, JavaScript/TypeScript, Rust, Go      | Qdrant supports a broader range of programming languages |

| **Performance** (e.g., query speed) | TnC Prohibit Benchmarking     | [Benchmark result](/benchmarks/)             | Compare performance metrics                              |
We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.



In order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.



Additionally, Qdrant uses a bunch of internal heuristics to optimize the performance.

To better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.

With this information, we can make Qdrant faster for everyone.
## Why Qdrant?",0.47058823529411764,0.625,0.5369127516778524,0.25,0.3333333333333333,0.28571428571428575,0.36470588235294116,0.484375,0.4161073825503355,0.43529411764705883,0.578125,0.4966442953020134,0.9269392490386963,0.6707248091697693,0.7094231247901917,0.6895314455032349,0.8925437397556379,0.6845313064598788,89,1.3692307692307693,0.6570949327065653
True,,2815.855,bf535edb,What is the primary purpose of a Vector Database and why would someone choose to use it over traditional databases?,"A Vector Database is a specialized database system that is specifically designed for efficiently indexing, querying, and retrieving high-dimensional vector data. The primary purpose of a Vector Database is to enable advanced data analysis and similarity-search operations that go beyond the capabilities of traditional, structured query approaches used in conventional databases.","A Vector Database is primarily used for efficiently indexing, querying, and retrieving high-dimensional vector data. The main advantage of using a Vector Database over traditional databases is its ability to handle the complexities of high-dimensional vector data, such as images, text, and audio, which do not fit neatly into structured tables. Vector databases enable advanced data analysis and similarity-search operations, making them ideal for applications requiring fast and accurate retrieval of unstructured data represented as vectors. Additionally, they offer benefits such as efficient storage and indexing of high-dimensional data, support for large-scale datasets with billions of data points, and capabilities for real-time analytics and queries. Overall, the specialized nature of Vector Databases makes them essential for applications that require matching data with queries in non-obvious ways, such as in image similarity, text analysis, user preferences, and pattern recognition.","## Why use a Vector Database?



The data flood is real. 



In 2024, we're drowning in unstructured data like images, text, and audio, that don’t fit into neatly organized tables. Still, we need a way to easily tap into the value within this chaos of almost 330 million terabytes of data being created each day.



Traditional databases, even with extensions that provide some vector handling capabilities, struggle with the complexities and demands of high-dimensional vector data.
aliases: [ /blog/what-is-a-vector-database/ ]

---



> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.





## Why use a Vector Database?



The data flood is real.
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
### Vector Database Use Cases



If we had to summarize the use cases for vector databases into a single word, it would be ""match"". They are great at finding non-obvious ways to correspond or “match” data with a given query. Whether it's through similarity in images, text, user preferences, or patterns in data.



Here’s some examples on how to take advantage of using vector databases:
Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.





### Starting Your First Vector Database Project","[""## Why use a Vector Database?\n\n\n\nThe data flood is real. \n\n\n\nIn 2024, we're drowning in unstructured data like images, text, and audio, that don’t fit into neatly organized tables. Still, we need a way to easily tap into the value within this chaos of almost 330 million terabytes of data being created each day.\n\n\n\nTraditional databases, even with extensions that provide some vector handling capabilities, struggle with the complexities and demands of high-dimensional vector data."", 'aliases: [ /blog/what-is-a-vector-database/ ]\n\n---\n\n\n\n> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.\n\n\n\n\n\n## Why use a Vector Database?\n\n\n\nThe data flood is real.', 'advantage of their efficient indexing and searching techniques, vector databases enable faster \n\nand more accurate retrieval of unstructured data already represented as vectors, which can \n\nhelp put in front of users the most relevant results to their queries.\n\n\n\nIn addition, other benefits of using vector databases include:\n\n1. Efficient storage and indexing of high-dimensional data.\n\n3. Ability to handle large-scale datasets with billions of data points.\n\n4. Support for real-time analytics and queries.', '### Vector Database Use Cases\n\n\n\nIf we had to summarize the use cases for vector databases into a single word, it would be ""match"". They are great at finding non-obvious ways to correspond or “match” data with a given query. Whether it\'s through similarity in images, text, user preferences, or patterns in data.\n\n\n\nHere’s some examples on how to take advantage of using vector databases:', 'Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.\n\n\n\n\n\n### Starting Your First Vector Database Project']","Question: What is the primary purpose of a Vector Database and why would someone choose to use it over traditional databases?\n\nContext: ## Why use a Vector Database?



The data flood is real. 



In 2024, we're drowning in unstructured data like images, text, and audio, that don’t fit into neatly organized tables. Still, we need a way to easily tap into the value within this chaos of almost 330 million terabytes of data being created each day.



Traditional databases, even with extensions that provide some vector handling capabilities, struggle with the complexities and demands of high-dimensional vector data.
aliases: [ /blog/what-is-a-vector-database/ ]

---



> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.





## Why use a Vector Database?



The data flood is real.
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
### Vector Database Use Cases



If we had to summarize the use cases for vector databases into a single word, it would be ""match"". They are great at finding non-obvious ways to correspond or “match” data with a given query. Whether it's through similarity in images, text, user preferences, or patterns in data.



Here’s some examples on how to take advantage of using vector databases:
Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.





### Starting Your First Vector Database Project\n\nAnswer:",What is the primary purpose of a Vector Database and why would someone choose to use it over traditional databases?,"A Vector Database is a specialized database system that is specifically designed for efficiently indexing, querying, and retrieving high-dimensional vector data. The primary purpose of a Vector Database is to enable advanced data analysis and similarity-search operations that go beyond the capabilities of traditional, structured query approaches used in conventional databases.","[""## Why use a Vector Database?\n\n\n\nThe data flood is real. \n\n\n\nIn 2024, we're drowning in unstructured data like images, text, and audio, that don’t fit into neatly organized tables. Still, we need a way to easily tap into the value within this chaos of almost 330 million terabytes of data being created each day.\n\n\n\nTraditional databases, even with extensions that provide some vector handling capabilities, struggle with the complexities and demands of high-dimensional vector data."", 'aliases: [ /blog/what-is-a-vector-database/ ]\n\n---\n\n\n\n> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.\n\n\n\n\n\n## Why use a Vector Database?\n\n\n\nThe data flood is real.', 'advantage of their efficient indexing and searching techniques, vector databases enable faster \n\nand more accurate retrieval of unstructured data already represented as vectors, which can \n\nhelp put in front of users the most relevant results to their queries.\n\n\n\nIn addition, other benefits of using vector databases include:\n\n1. Efficient storage and indexing of high-dimensional data.\n\n3. Ability to handle large-scale datasets with billions of data points.\n\n4. Support for real-time analytics and queries.', '### Vector Database Use Cases\n\n\n\nIf we had to summarize the use cases for vector databases into a single word, it would be ""match"". They are great at finding non-obvious ways to correspond or “match” data with a given query. Whether it\'s through similarity in images, text, user preferences, or patterns in data.\n\n\n\nHere’s some examples on how to take advantage of using vector databases:', 'Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.\n\n\n\n\n\n### Starting Your First Vector Database Project']","## Why use a Vector Database?



The data flood is real. 



In 2024, we're drowning in unstructured data like images, text, and audio, that don’t fit into neatly organized tables. Still, we need a way to easily tap into the value within this chaos of almost 330 million terabytes of data being created each day.



Traditional databases, even with extensions that provide some vector handling capabilities, struggle with the complexities and demands of high-dimensional vector data.
aliases: [ /blog/what-is-a-vector-database/ ]

---



> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.





## Why use a Vector Database?



The data flood is real.
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
### Vector Database Use Cases



If we had to summarize the use cases for vector databases into a single word, it would be ""match"". They are great at finding non-obvious ways to correspond or “match” data with a given query. Whether it's through similarity in images, text, user preferences, or patterns in data.



Here’s some examples on how to take advantage of using vector databases:
Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.





### Starting Your First Vector Database Project",0.2642857142857143,0.7708333333333334,0.39361702127659576,0.1510791366906475,0.44680851063829785,0.22580645161290322,0.21428571428571427,0.625,0.3191489361702127,0.22857142857142856,0.6666666666666666,0.3404255319148936,0.8322180509567261,0.6569588780403137,0.7401301264762878,0.6960688233375549,0.9969230143702589,0.9905652608722448,138,2.7058823529411766,0.4754040422335546
True,,2786.8940000000002,390c87c7,"How does oversampling impact the performance of machine learning models, especially in the context of imbalanced datasets?","Oversampling is a technique used in machine learning to address imbalances in datasets, where one class significantly outnumbers others. This imbalance can lead to skewed model performance, as the model may favor the majority class at the expense of minority classes. By generating additional samples from the minority classes, oversampling helps to equalize the representation of different classes in the training dataset.","Influence of Search Limit:

The search limit parameter plays a crucial role in the context of oversampling and its impact on machine learning models. By controlling the search limit, you can determine the extent to which oversampling is applied in the training process. A higher search limit allows for more oversampling, which can lead to a better balance in the dataset and potentially improve model performance, especially in the case of imbalanced datasets.

However, it is important to note that the influence of search limit on model performance may vary depending on the specific dataset and the characteristics of the problem being addressed. It is recommended to experiment with different search limit values to find the optimal balance between oversampling and model performance for a given dataset.

Overall, the search limit parameter provides a way to fine-tune the application of oversampling in machine learning models, allowing for better adaptation to the specific requirements of the problem at hand.","acc = average_accuracy.pivot(

        index=""limit"", columns=[""oversampling"", ""rescore""], values=""accuracy""

    )

    print(acc)

```



#### Impact of Oversampling



You can use oversampling in machine learning to counteract imbalances in datasets.

It works well when one class significantly outnumbers others. This imbalance

can skew the performance of models, which favors the majority class at the

expense of others. By creating additional samples from the minority classes,
The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one.
Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects.



![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png)



### Leveraging Binary Quantization: Best Practices
oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
In contrast, for lower dimension models (such as text-embedding-3-small with 512 dimensions), the incremental accuracy gains from increased oversampling levels are less significant, even with rescoring enabled. This suggests a diminishing return on accuracy improvement with higher oversampling in lower dimension spaces.



3. **Influence of Search Limit**:","['acc = average_accuracy.pivot(\n\n        index=""limit"", columns=[""oversampling"", ""rescore""], values=""accuracy""\n\n    )\n\n    print(acc)\n\n```\n\n\n\n#### Impact of Oversampling\n\n\n\nYou can use oversampling in machine learning to counteract imbalances in datasets.\n\nIt works well when one class significantly outnumbers others. This imbalance\n\ncan skew the performance of models, which favors the majority class at the\n\nexpense of others. By creating additional samples from the minority classes,', ""The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one."", 'Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects.\n\n\n\n![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png)\n\n\n\n### Leveraging Binary Quantization: Best Practices', 'oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.', 'In contrast, for lower dimension models (such as text-embedding-3-small with 512 dimensions), the incremental accuracy gains from increased oversampling levels are less significant, even with rescoring enabled. This suggests a diminishing return on accuracy improvement with higher oversampling in lower dimension spaces.\n\n\n\n3. **Influence of Search Limit**:']","Question: How does oversampling impact the performance of machine learning models, especially in the context of imbalanced datasets?\n\nContext: acc = average_accuracy.pivot(

index=""limit"", columns=[""oversampling"", ""rescore""], values=""accuracy""

)

print(acc)

```



#### Impact of Oversampling



You can use oversampling in machine learning to counteract imbalances in datasets.

It works well when one class significantly outnumbers others. This imbalance

can skew the performance of models, which favors the majority class at the

expense of others. By creating additional samples from the minority classes,
The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one.
Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects.



![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png)



### Leveraging Binary Quantization: Best Practices
oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
In contrast, for lower dimension models (such as text-embedding-3-small with 512 dimensions), the incremental accuracy gains from increased oversampling levels are less significant, even with rescoring enabled. This suggests a diminishing return on accuracy improvement with higher oversampling in lower dimension spaces.



3. **Influence of Search Limit**:\n\nAnswer:","How does oversampling impact the performance of machine learning models, especially in the context of imbalanced datasets?","Oversampling is a technique used in machine learning to address imbalances in datasets, where one class significantly outnumbers others. This imbalance can lead to skewed model performance, as the model may favor the majority class at the expense of minority classes. By generating additional samples from the minority classes, oversampling helps to equalize the representation of different classes in the training dataset.","['acc = average_accuracy.pivot(\n\n        index=""limit"", columns=[""oversampling"", ""rescore""], values=""accuracy""\n\n    )\n\n    print(acc)\n\n```\n\n\n\n#### Impact of Oversampling\n\n\n\nYou can use oversampling in machine learning to counteract imbalances in datasets.\n\nIt works well when one class significantly outnumbers others. This imbalance\n\ncan skew the performance of models, which favors the majority class at the\n\nexpense of others. By creating additional samples from the minority classes,', ""The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one."", 'Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects.\n\n\n\n![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png)\n\n\n\n### Leveraging Binary Quantization: Best Practices', 'oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.', 'In contrast, for lower dimension models (such as text-embedding-3-small with 512 dimensions), the incremental accuracy gains from increased oversampling levels are less significant, even with rescoring enabled. This suggests a diminishing return on accuracy improvement with higher oversampling in lower dimension spaces.\n\n\n\n3. **Influence of Search Limit**:']","acc = average_accuracy.pivot(

        index=""limit"", columns=[""oversampling"", ""rescore""], values=""accuracy""

    )

    print(acc)

```



#### Impact of Oversampling



You can use oversampling in machine learning to counteract imbalances in datasets.

It works well when one class significantly outnumbers others. This imbalance

can skew the performance of models, which favors the majority class at the

expense of others. By creating additional samples from the minority classes,
The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one.
Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects.



![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png)



### Leveraging Binary Quantization: Best Practices
oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
In contrast, for lower dimension models (such as text-embedding-3-small with 512 dimensions), the incremental accuracy gains from increased oversampling levels are less significant, even with rescoring enabled. This suggests a diminishing return on accuracy improvement with higher oversampling in lower dimension spaces.



3. **Influence of Search Limit**:",0.18840579710144928,0.4727272727272727,0.2694300518134715,0.058394160583941604,0.14814814814814814,0.0837696335078534,0.12318840579710146,0.3090909090909091,0.1761658031088083,0.18840579710144928,0.4727272727272727,0.2694300518134715,0.636559247970581,0.5643577575683594,0.6028510928153992,0.5829696655273438,0.9176860665902495,0.9997340876482971,158,2.5483870967741935,0.6438293083642594
True,,2434.3469999999998,32eaaa11,How does binary quantization work in the context of vector comparisons?,"Binary quantization is a method used in indexing and data compression, particularly by Qdrant, that involves splitting a data point's vector in half at a certain point. This process essentially divides the vector into two parts, marking everything above the split point as ""1"" and everything below as ""0"". The result is a string of bits that represents the original vector in a compressed form. This quantized code is much smaller and easier to compare. Especially for OpenAI embeddings, this type of quantization has proven to achieve a massive performance improvement at a lower cost of accuracy.","Binary quantization in the context of vector comparisons involves converting the floating-point elements of a vector into binary representation, where each element is represented as either zero or one depending on its sign. By doing this, we can approximate vector comparisons based on metrics like dot product or cosine similarity using Hamming distance, which is simpler to compute and uses highly optimized CPU instructions.

The binary quantization process makes it efficient to compare vectors using their binary representations, enabling faster query processing times and reducing the overall size of the database. This approach can significantly speed up vector comparisons and similarity searches in applications where accuracy can be traded for speed and efficiency.","#### What is Binary Quantization?



Quantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.



[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).
Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever
| 0        | 0        | 0                |



</details>



As you can see, both functions are equal up to a constant factor, which makes similarity search equivalent.

Binary quantization makes it efficient to compare vectors using this representation.





## Product Quantization



*Available as of v1.2.0*



Product quantization is a method of compressing vectors to minimize their memory usage by dividing them into 

chunks and quantizing each segment individually.
Andrey Vasnetsov:
Andrey Vasnetsov:","['#### What is Binary Quantization?\n\n\n\nQuantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.\n\n\n\n[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).', ""Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever"", '| 0        | 0        | 0                |\n\n\n\n</details>\n\n\n\nAs you can see, both functions are equal up to a constant factor, which makes similarity search equivalent.\n\nBinary quantization makes it efficient to compare vectors using this representation.\n\n\n\n\n\n## Product Quantization\n\n\n\n*Available as of v1.2.0*\n\n\n\nProduct quantization is a method of compressing vectors to minimize their memory usage by dividing them into \n\nchunks and quantizing each segment individually.', 'Andrey Vasnetsov:', 'Andrey Vasnetsov:']","Question: How does binary quantization work in the context of vector comparisons?\n\nContext: #### What is Binary Quantization?



Quantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.



[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).
Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever
| 0        | 0        | 0                |



</details>



As you can see, both functions are equal up to a constant factor, which makes similarity search equivalent.

Binary quantization makes it efficient to compare vectors using this representation.





## Product Quantization



*Available as of v1.2.0*



Product quantization is a method of compressing vectors to minimize their memory usage by dividing them into 

chunks and quantizing each segment individually.
Andrey Vasnetsov:
Andrey Vasnetsov:\n\nAnswer:",How does binary quantization work in the context of vector comparisons?,"Binary quantization is a method used in indexing and data compression, particularly by Qdrant, that involves splitting a data point's vector in half at a certain point. This process essentially divides the vector into two parts, marking everything above the split point as ""1"" and everything below as ""0"". The result is a string of bits that represents the original vector in a compressed form. This quantized code is much smaller and easier to compare. Especially for OpenAI embeddings, this type of quantization has proven to achieve a massive performance improvement at a lower cost of accuracy.","['#### What is Binary Quantization?\n\n\n\nQuantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.\n\n\n\n[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).', ""Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever"", '| 0        | 0        | 0                |\n\n\n\n</details>\n\n\n\nAs you can see, both functions are equal up to a constant factor, which makes similarity search equivalent.\n\nBinary quantization makes it efficient to compare vectors using this representation.\n\n\n\n\n\n## Product Quantization\n\n\n\n*Available as of v1.2.0*\n\n\n\nProduct quantization is a method of compressing vectors to minimize their memory usage by dividing them into \n\nchunks and quantizing each segment individually.', 'Andrey Vasnetsov:', 'Andrey Vasnetsov:']","#### What is Binary Quantization?



Quantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.



[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).
Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever
| 0        | 0        | 0                |



</details>



As you can see, both functions are equal up to a constant factor, which makes similarity search equivalent.

Binary quantization makes it efficient to compare vectors using this representation.





## Product Quantization



*Available as of v1.2.0*



Product quantization is a method of compressing vectors to minimize their memory usage by dividing them into 

chunks and quantizing each segment individually.
Andrey Vasnetsov:
Andrey Vasnetsov:",0.28703703703703703,0.3563218390804598,0.31794871794871793,0.028037383177570093,0.03488372093023256,0.031088082901554407,0.14814814814814814,0.1839080459770115,0.1641025641025641,0.26851851851851855,0.3333333333333333,0.29743589743589743,0.6802817583084106,0.63935387134552,0.6395151615142822,0.6394345164299011,0.9977244958718074,0.6394367095082998,113,1.153061224489796,0.4148895612577667
True,,1949.731,70dcef97,What is the significance of the 'always_ram' parameter in the context of vector quantization in Qdrant?,"In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times.","In the context of vector quantization in Qdrant, the 'always_ram' parameter has the significance of controlling whether quantized vectors are stored in RAM or not. When 'always_ram' is set to true, it indicates that the quantized vectors should be kept in RAM to speed up the search process. This can be beneficial for faster search operations, particularly in setups where speed is crucial. Conversely, setting 'always_ram' to false would imply that the quantized vectors are not stored in RAM, which may be preferable in certain scenarios depending on the storage and performance requirements.","always_ram=True,

        ),

    ),

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createCollection(""{collection_name}"", {

  vectors: {

    size: 768,

    distance: ""Cosine"",

  },

  quantization_config: {

    product: {

      compression: ""x16"",

      always_ram: true,

    },

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{
However, in some setups you might want to keep quantized vectors in RAM to speed up the search process. Then set `always_ram` to `true`.



### Searching with Quantization



Once you have configured quantization for a collection, you don't need to do anything extra to search with quantization.

Qdrant will automatically use quantized vectors if they are available.



However, there are a few options that you can use to control the search process:



```http
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": false

        }

    }

}

```



```python

from qdrant_client import QdrantClient, models
```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": true

        }

    }

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(
""size"": 1536,

      ""distance"": ""Cosine""

    },

    ""quantization_config"": {

        ""binary"": {

            ""always_ram"": true

        }

    }

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),","['always_ram=True,\n\n        ),\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createCollection(""{collection_name}"", {\n\n  vectors: {\n\n    size: 768,\n\n    distance: ""Cosine"",\n\n  },\n\n  quantization_config: {\n\n    product: {\n\n      compression: ""x16"",\n\n      always_ram: true,\n\n    },\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{', ""However, in some setups you might want to keep quantized vectors in RAM to speed up the search process. Then set `always_ram` to `true`.\n\n\n\n### Searching with Quantization\n\n\n\nOnce you have configured quantization for a collection, you don't need to do anything extra to search with quantization.\n\nQdrant will automatically use quantized vectors if they are available.\n\n\n\nHowever, there are a few options that you can use to control the search process:\n\n\n\n```http"", 'This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models', '```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(', '""size"": 1536,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""quantization_config"": {\n\n        ""binary"": {\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),']","Question: What is the significance of the 'always_ram' parameter in the context of vector quantization in Qdrant?\n\nContext: always_ram=True,

),

),

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createCollection(""{collection_name}"", {

vectors: {

size: 768,

distance: ""Cosine"",

},

quantization_config: {

product: {

compression: ""x16"",

always_ram: true,

},

},

});

```



```rust

use qdrant_client::{

client::QdrantClient,

qdrant::{
However, in some setups you might want to keep quantized vectors in RAM to speed up the search process. Then set `always_ram` to `true`.



### Searching with Quantization



Once you have configured quantization for a collection, you don't need to do anything extra to search with quantization.

Qdrant will automatically use quantized vectors if they are available.



However, there are a few options that you can use to control the search process:



```http
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""optimizers_config"": {

""memmap_threshold"": 20000

},

""quantization_config"": {

""scalar"": {

""type"": ""int8"",

""always_ram"": false

}

}

}

```



```python

from qdrant_client import QdrantClient, models
```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""optimizers_config"": {

""memmap_threshold"": 20000

},

""quantization_config"": {

""scalar"": {

""type"": ""int8"",

""always_ram"": true

}

}

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(
""size"": 1536,

""distance"": ""Cosine""

},

""quantization_config"": {

""binary"": {

""always_ram"": true

}

}

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),\n\nAnswer:",What is the significance of the 'always_ram' parameter in the context of vector quantization in Qdrant?,"In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times.","['always_ram=True,\n\n        ),\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createCollection(""{collection_name}"", {\n\n  vectors: {\n\n    size: 768,\n\n    distance: ""Cosine"",\n\n  },\n\n  quantization_config: {\n\n    product: {\n\n      compression: ""x16"",\n\n      always_ram: true,\n\n    },\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{', ""However, in some setups you might want to keep quantized vectors in RAM to speed up the search process. Then set `always_ram` to `true`.\n\n\n\n### Searching with Quantization\n\n\n\nOnce you have configured quantization for a collection, you don't need to do anything extra to search with quantization.\n\nQdrant will automatically use quantized vectors if they are available.\n\n\n\nHowever, there are a few options that you can use to control the search process:\n\n\n\n```http"", 'This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models', '```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(', '""size"": 1536,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""quantization_config"": {\n\n        ""binary"": {\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),']","always_ram=True,

        ),

    ),

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createCollection(""{collection_name}"", {

  vectors: {

    size: 768,

    distance: ""Cosine"",

  },

  quantization_config: {

    product: {

      compression: ""x16"",

      always_ram: true,

    },

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{
However, in some setups you might want to keep quantized vectors in RAM to speed up the search process. Then set `always_ram` to `true`.



### Searching with Quantization



Once you have configured quantization for a collection, you don't need to do anything extra to search with quantization.

Qdrant will automatically use quantized vectors if they are available.



However, there are a few options that you can use to control the search process:



```http
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": false

        }

    }

}

```



```python

from qdrant_client import QdrantClient, models
```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": true

        }

    }

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(
""size"": 1536,

      ""distance"": ""Cosine""

    },

    ""quantization_config"": {

        ""binary"": {

            ""always_ram"": true

        }

    }

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),",0.4157303370786517,0.6981132075471698,0.5211267605633803,0.3068181818181818,0.5192307692307693,0.38571428571428573,0.33707865168539325,0.5660377358490566,0.42253521126760557,0.39325842696629215,0.660377358490566,0.4929577464788733,0.9499084949493408,0.6786069273948669,0.7440532445907593,0.709824800491333,0.9538536127656698,0.9323121247192224,93,1.6607142857142858,0.7358273499015794
True,,4601.647,676246b1,How can automatic backups be set up for clusters using the Cloud UI?,"Automatic backups for clusters can be set up using the Cloud UI by following the procedures listed on the page. These procedures allow you to configure snapshots on a daily, weekly, or monthly basis. You have the flexibility to keep as many snapshots as needed and can restore a cluster from the snapshot of your choice. It is important to note that during the restoration of a snapshot, the affected cluster will not be available.","To set up automatic backups for clusters using the Cloud UI, follow these steps:

1. Sign in to the [Qdrant Cloud Dashboard](https://cloud.qdrant.io).
2. Select Clusters from the dashboard.
3. Choose the cluster that you want to back up.
4. Navigate to the Backups tab.
5. Configure a backup schedule based on your preferences. You can set the frequency of snapshots (daily/weekly/monthly) and specify the number of days for retention.
6. Optionally, you can initiate an immediate backup by selecting Backup now.
7. For disaster recovery or other use-cases, you can establish automatic or self-service backups.

To restore a backup:
1. If you have backups available, they will be listed under Available Backups.
2. From the list of backups, you can choose to restore or delete a specific backup.

Additionally, Qdrant offers a snapshot API that allows you to create snapshots. You can utilize this feature by prepending `node-{num}-` to your cluster URL and calling the snapshot endpoint on individual hosts. The process involves creating a snapshot, deleting and recreating the collection, and then recovering the snapshot.

By following these steps, you can effectively set up automatic backups for your Qdrant clusters using the Cloud UI and manage backups for disaster recovery or data restoration purposes.","- [Create a cluster](/documentation/cloud/create-cluster/)

- Set up [Authentication](/documentation/cloud/authentication/)

- Configure one or more [Collections](/documentation/concepts/collections/)



## Automatic backups



You can set up automatic backups of your clusters with our Cloud UI. With the

procedures listed in this page, you can set up

snapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you

need. You can restore a cluster from the snapshot of your choice.
[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:



1. Sign in to the dashboard

1. Select Clusters.

1. Select the cluster that you want to back up.

   ![Select a cluster](/documentation/cloud/select-cluster.png)

1. Find and select the **Backups** tab.

1. Now you can set up a backup schedule.

   The **Days of Retention** is the number of days after a backup snapshot is

   deleted.

1. Alternatively, you can select **Backup now** to take an immediate snapshot.
recommend replication. It guarantees the proper cluster functionality as long as

at least one replica is running.



For other use-cases such as disaster recovery, you can set up automatic or

self-service backups.



## Prerequisites



You can back up your Qdrant clusters though the Qdrant Cloud

Dashboard at https://cloud.qdrant.io. This section assumes that you've already

set up your cluster, as described in the following sections:



- [Create a cluster](/documentation/cloud/create-cluster/)
![Configure a cluster backup](/documentation/cloud/backup-schedule.png)



### Restore a backup



If you have a backup, it appears in the list of **Available Backups**. You can

choose to restore or delete the backups of your choice.



![Restore or delete a cluster backup](/documentation/cloud/restore-delete.png)



<!-- I think we should move this to the Snapshot page, but I'll do it later -->



## Backups with a snapshot



Qdrant also offers a snapshot API which allows you to create a snapshot
Specifically, prepend `node-{num}-` to your cluster URL.

     Then call the [snapshot endpoint](../../concepts/snapshots/#create-snapshot) on the individual hosts. Start with node 0.

   - In the response, you'll see the name of the snapshot.

2. Delete and recreate the collection.

3. Recover the snapshot:","['- [Create a cluster](/documentation/cloud/create-cluster/)\n\n- Set up [Authentication](/documentation/cloud/authentication/)\n\n- Configure one or more [Collections](/documentation/concepts/collections/)\n\n\n\n## Automatic backups\n\n\n\nYou can set up automatic backups of your clusters with our Cloud UI. With the\n\nprocedures listed in this page, you can set up\n\nsnapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you\n\nneed. You can restore a cluster from the snapshot of your choice.', '[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:\n\n\n\n1. Sign in to the dashboard\n\n1. Select Clusters.\n\n1. Select the cluster that you want to back up.\n\n   ![Select a cluster](/documentation/cloud/select-cluster.png)\n\n1. Find and select the **Backups** tab.\n\n1. Now you can set up a backup schedule.\n\n   The **Days of Retention** is the number of days after a backup snapshot is\n\n   deleted.\n\n1. Alternatively, you can select **Backup now** to take an immediate snapshot.', ""recommend replication. It guarantees the proper cluster functionality as long as\n\nat least one replica is running.\n\n\n\nFor other use-cases such as disaster recovery, you can set up automatic or\n\nself-service backups.\n\n\n\n## Prerequisites\n\n\n\nYou can back up your Qdrant clusters though the Qdrant Cloud\n\nDashboard at https://cloud.qdrant.io. This section assumes that you've already\n\nset up your cluster, as described in the following sections:\n\n\n\n- [Create a cluster](/documentation/cloud/create-cluster/)"", ""![Configure a cluster backup](/documentation/cloud/backup-schedule.png)\n\n\n\n### Restore a backup\n\n\n\nIf you have a backup, it appears in the list of **Available Backups**. You can\n\nchoose to restore or delete the backups of your choice.\n\n\n\n![Restore or delete a cluster backup](/documentation/cloud/restore-delete.png)\n\n\n\n<!-- I think we should move this to the Snapshot page, but I'll do it later -->\n\n\n\n## Backups with a snapshot\n\n\n\nQdrant also offers a snapshot API which allows you to create a snapshot"", ""Specifically, prepend `node-{num}-` to your cluster URL.\n\n     Then call the [snapshot endpoint](../../concepts/snapshots/#create-snapshot) on the individual hosts. Start with node 0.\n\n   - In the response, you'll see the name of the snapshot.\n\n2. Delete and recreate the collection.\n\n3. Recover the snapshot:""]","Question: How can automatic backups be set up for clusters using the Cloud UI?\n\nContext: - [Create a cluster](/documentation/cloud/create-cluster/)

- Set up [Authentication](/documentation/cloud/authentication/)

- Configure one or more [Collections](/documentation/concepts/collections/)



## Automatic backups



You can set up automatic backups of your clusters with our Cloud UI. With the

procedures listed in this page, you can set up

snapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you

need. You can restore a cluster from the snapshot of your choice.
[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:



1. Sign in to the dashboard

1. Select Clusters.

1. Select the cluster that you want to back up.

![Select a cluster](/documentation/cloud/select-cluster.png)

1. Find and select the **Backups** tab.

1. Now you can set up a backup schedule.

The **Days of Retention** is the number of days after a backup snapshot is

deleted.

1. Alternatively, you can select **Backup now** to take an immediate snapshot.
recommend replication. It guarantees the proper cluster functionality as long as

at least one replica is running.



For other use-cases such as disaster recovery, you can set up automatic or

self-service backups.



## Prerequisites



You can back up your Qdrant clusters though the Qdrant Cloud

Dashboard at https://cloud.qdrant.io. This section assumes that you've already

set up your cluster, as described in the following sections:



- [Create a cluster](/documentation/cloud/create-cluster/)
![Configure a cluster backup](/documentation/cloud/backup-schedule.png)



### Restore a backup



If you have a backup, it appears in the list of **Available Backups**. You can

choose to restore or delete the backups of your choice.



![Restore or delete a cluster backup](/documentation/cloud/restore-delete.png)



<!-- I think we should move this to the Snapshot page, but I'll do it later -->



## Backups with a snapshot



Qdrant also offers a snapshot API which allows you to create a snapshot
Specifically, prepend `node-{num}-` to your cluster URL.

Then call the [snapshot endpoint](../../concepts/snapshots/#create-snapshot) on the individual hosts. Start with node 0.

- In the response, you'll see the name of the snapshot.

2. Delete and recreate the collection.

3. Recover the snapshot:\n\nAnswer:",How can automatic backups be set up for clusters using the Cloud UI?,"Automatic backups for clusters can be set up using the Cloud UI by following the procedures listed on the page. These procedures allow you to configure snapshots on a daily, weekly, or monthly basis. You have the flexibility to keep as many snapshots as needed and can restore a cluster from the snapshot of your choice. It is important to note that during the restoration of a snapshot, the affected cluster will not be available.","['- [Create a cluster](/documentation/cloud/create-cluster/)\n\n- Set up [Authentication](/documentation/cloud/authentication/)\n\n- Configure one or more [Collections](/documentation/concepts/collections/)\n\n\n\n## Automatic backups\n\n\n\nYou can set up automatic backups of your clusters with our Cloud UI. With the\n\nprocedures listed in this page, you can set up\n\nsnapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you\n\nneed. You can restore a cluster from the snapshot of your choice.', '[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:\n\n\n\n1. Sign in to the dashboard\n\n1. Select Clusters.\n\n1. Select the cluster that you want to back up.\n\n   ![Select a cluster](/documentation/cloud/select-cluster.png)\n\n1. Find and select the **Backups** tab.\n\n1. Now you can set up a backup schedule.\n\n   The **Days of Retention** is the number of days after a backup snapshot is\n\n   deleted.\n\n1. Alternatively, you can select **Backup now** to take an immediate snapshot.', ""recommend replication. It guarantees the proper cluster functionality as long as\n\nat least one replica is running.\n\n\n\nFor other use-cases such as disaster recovery, you can set up automatic or\n\nself-service backups.\n\n\n\n## Prerequisites\n\n\n\nYou can back up your Qdrant clusters though the Qdrant Cloud\n\nDashboard at https://cloud.qdrant.io. This section assumes that you've already\n\nset up your cluster, as described in the following sections:\n\n\n\n- [Create a cluster](/documentation/cloud/create-cluster/)"", ""![Configure a cluster backup](/documentation/cloud/backup-schedule.png)\n\n\n\n### Restore a backup\n\n\n\nIf you have a backup, it appears in the list of **Available Backups**. You can\n\nchoose to restore or delete the backups of your choice.\n\n\n\n![Restore or delete a cluster backup](/documentation/cloud/restore-delete.png)\n\n\n\n<!-- I think we should move this to the Snapshot page, but I'll do it later -->\n\n\n\n## Backups with a snapshot\n\n\n\nQdrant also offers a snapshot API which allows you to create a snapshot"", ""Specifically, prepend `node-{num}-` to your cluster URL.\n\n     Then call the [snapshot endpoint](../../concepts/snapshots/#create-snapshot) on the individual hosts. Start with node 0.\n\n   - In the response, you'll see the name of the snapshot.\n\n2. Delete and recreate the collection.\n\n3. Recover the snapshot:""]","- [Create a cluster](/documentation/cloud/create-cluster/)

- Set up [Authentication](/documentation/cloud/authentication/)

- Configure one or more [Collections](/documentation/concepts/collections/)



## Automatic backups



You can set up automatic backups of your clusters with our Cloud UI. With the

procedures listed in this page, you can set up

snapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you

need. You can restore a cluster from the snapshot of your choice.
[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:



1. Sign in to the dashboard

1. Select Clusters.

1. Select the cluster that you want to back up.

   ![Select a cluster](/documentation/cloud/select-cluster.png)

1. Find and select the **Backups** tab.

1. Now you can set up a backup schedule.

   The **Days of Retention** is the number of days after a backup snapshot is

   deleted.

1. Alternatively, you can select **Backup now** to take an immediate snapshot.
recommend replication. It guarantees the proper cluster functionality as long as

at least one replica is running.



For other use-cases such as disaster recovery, you can set up automatic or

self-service backups.



## Prerequisites



You can back up your Qdrant clusters though the Qdrant Cloud

Dashboard at https://cloud.qdrant.io. This section assumes that you've already

set up your cluster, as described in the following sections:



- [Create a cluster](/documentation/cloud/create-cluster/)
![Configure a cluster backup](/documentation/cloud/backup-schedule.png)



### Restore a backup



If you have a backup, it appears in the list of **Available Backups**. You can

choose to restore or delete the backups of your choice.



![Restore or delete a cluster backup](/documentation/cloud/restore-delete.png)



<!-- I think we should move this to the Snapshot page, but I'll do it later -->



## Backups with a snapshot



Qdrant also offers a snapshot API which allows you to create a snapshot
Specifically, prepend `node-{num}-` to your cluster URL.

     Then call the [snapshot endpoint](../../concepts/snapshots/#create-snapshot) on the individual hosts. Start with node 0.

   - In the response, you'll see the name of the snapshot.

2. Delete and recreate the collection.

3. Recover the snapshot:",0.2358974358974359,0.7076923076923077,0.3538461538461539,0.06701030927835051,0.203125,0.10077519379844962,0.1282051282051282,0.38461538461538464,0.19230769230769232,0.23076923076923078,0.6923076923076923,0.34615384615384615,0.8346301317214966,0.5618672370910645,0.6389057636260986,0.5979151725769043,0.6208046200626995,0.7433802886225749,208,2.7733333333333334,0.4621671068387109
True,,6480.893,342b7e1c,What are snapshots in the context of Qdrant Cloud and how are they used in a distributed setup?,"Snapshots in Qdrant Cloud are `tar` archive files that contain data and configuration of a specific collection on a specific node at a specific time. In a distributed setup with multiple nodes in a cluster, snapshots must be created for each node separately when dealing with a single collection. These snapshots can be used to archive data or easily replicate an existing deployment.","In the context of Qdrant Cloud, snapshots are a way to export and import collections efficiently. Snapshots contain all the necessary components to restore an entire collection. 

In a distributed setup, snapshots play a crucial role in migrating data from one cluster to another or in replicating an existing deployment easily. However, it's important to note that certain limitations apply, such as not being able to restore a Qdrant Cloud cluster from a whole storage snapshot directly. Instead, using backups is recommended for disaster recovery in Qdrant Cloud.

To create a full storage snapshot in Qdrant Cloud, you can use the following method:

```http
POST /snapshots
```

```python
from qdrant_client import QdrantClient

client = QdrantClient(""localhost"", port=6333)
client.create_full_snapshot()
```

This method is commonly used for data migration between clusters, and setting the priority to ""snapshot"" is recommended for this use case.

To recover a collection using a snapshot during startup, you can use the following method:

```bash
curl -X POST 'http://qdrant-node-1:6333/collections/{collection_name}/snapshots/upload?priority=snapshot' \
-H 'api-key: ********' \
-H 'Content-Type:multipart/form-data' \
-F 'snapshot=@/path/to/snapshot-2022-10-10.shapshot'
```

This process allows you to archive data or replicate an existing deployment easily. Remember that this method cannot be used in a multi-node deployment or in Qdrant Cloud. For disaster recovery in Qdrant Cloud, it's recommended to use backups instead.

Overall, snapshots in Qdrant Cloud are an efficient way to export, import, and recover collections, making data management and migration more streamlined and reliable.","That's why using snapshots is the best way to export and import Qdrant collections, as they contain all the bits and pieces required to restore the entire collection efficiently.



This tutorial will show you how to create a snapshot of a collection and restore it. Since working with snapshots in a distributed environment might be thought to be a bit more complex, we will use a 3-node Qdrant cluster. However, the same approach applies to a single-node setup.
<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>



### Create full storage snapshot



```http

POST /snapshots

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.create_full_snapshot()

```
-H 'api-key: ********' \

    -H 'Content-Type:multipart/form-data' \

    -F 'snapshot=@/path/to/snapshot-2022-10-10.shapshot'

```



This method is typically used to migrate data from one cluster to another, so we recommend setting the [priority](#snapshot-priority) to ""snapshot"" for that use-case.



### Recover during start-up



<aside role=""alert"">This method cannot be used in a multi-node deployment and cannot be used in Qdrant Cloud.</aside>
This feature can be used to archive data or easily replicate an existing deployment. For disaster recovery, Qdrant Cloud users may prefer to use [Backups](/documentation/cloud/backups/) instead, which are physical disk-level copies of your data.



For a step-by-step guide on how to use snapshots, see our [tutorial](/documentation/tutorials/create-snapshot/).



## Store snapshots
client.recoverSnapshot(""{collection_name}"", {

  location: ""http://qdrant-node-1:6333/collections/{collection_name}/snapshots/snapshot-2022-10-10.shapshot"",

  priority: ""snapshot""

});

```



```bash

curl -X POST 'http://qdrant-node-1:6333/collections/{collection_name}/snapshots/upload?priority=snapshot' \

    -H 'api-key: ********' \

    -H 'Content-Type:multipart/form-data' \

    -F 'snapshot=@/path/to/snapshot-2022-10-10.shapshot'

```



## Snapshots for the whole storage","[""That's why using snapshots is the best way to export and import Qdrant collections, as they contain all the bits and pieces required to restore the entire collection efficiently.\n\n\n\nThis tutorial will show you how to create a snapshot of a collection and restore it. Since working with snapshots in a distributed environment might be thought to be a bit more complex, we will use a 3-node Qdrant cluster. However, the same approach applies to a single-node setup."", '<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>\n\n\n\n### Create full storage snapshot\n\n\n\n```http\n\nPOST /snapshots\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_full_snapshot()\n\n```', '-H \'api-key: ********\' \\\n\n    -H \'Content-Type:multipart/form-data\' \\\n\n    -F \'snapshot=@/path/to/snapshot-2022-10-10.shapshot\'\n\n```\n\n\n\nThis method is typically used to migrate data from one cluster to another, so we recommend setting the [priority](#snapshot-priority) to ""snapshot"" for that use-case.\n\n\n\n### Recover during start-up\n\n\n\n<aside role=""alert"">This method cannot be used in a multi-node deployment and cannot be used in Qdrant Cloud.</aside>', 'This feature can be used to archive data or easily replicate an existing deployment. For disaster recovery, Qdrant Cloud users may prefer to use [Backups](/documentation/cloud/backups/) instead, which are physical disk-level copies of your data.\n\n\n\nFor a step-by-step guide on how to use snapshots, see our [tutorial](/documentation/tutorials/create-snapshot/).\n\n\n\n## Store snapshots', 'client.recoverSnapshot(""{collection_name}"", {\n\n  location: ""http://qdrant-node-1:6333/collections/{collection_name}/snapshots/snapshot-2022-10-10.shapshot"",\n\n  priority: ""snapshot""\n\n});\n\n```\n\n\n\n```bash\n\ncurl -X POST \'http://qdrant-node-1:6333/collections/{collection_name}/snapshots/upload?priority=snapshot\' \\\n\n    -H \'api-key: ********\' \\\n\n    -H \'Content-Type:multipart/form-data\' \\\n\n    -F \'snapshot=@/path/to/snapshot-2022-10-10.shapshot\'\n\n```\n\n\n\n## Snapshots for the whole storage']","Question: What are snapshots in the context of Qdrant Cloud and how are they used in a distributed setup?\n\nContext: That's why using snapshots is the best way to export and import Qdrant collections, as they contain all the bits and pieces required to restore the entire collection efficiently.



This tutorial will show you how to create a snapshot of a collection and restore it. Since working with snapshots in a distributed environment might be thought to be a bit more complex, we will use a 3-node Qdrant cluster. However, the same approach applies to a single-node setup.
<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>



### Create full storage snapshot



```http

POST /snapshots

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.create_full_snapshot()

```
-H 'api-key: ********' \

-H 'Content-Type:multipart/form-data' \

-F 'snapshot=@/path/to/snapshot-2022-10-10.shapshot'

```



This method is typically used to migrate data from one cluster to another, so we recommend setting the [priority](#snapshot-priority) to ""snapshot"" for that use-case.



### Recover during start-up



<aside role=""alert"">This method cannot be used in a multi-node deployment and cannot be used in Qdrant Cloud.</aside>
This feature can be used to archive data or easily replicate an existing deployment. For disaster recovery, Qdrant Cloud users may prefer to use [Backups](/documentation/cloud/backups/) instead, which are physical disk-level copies of your data.



For a step-by-step guide on how to use snapshots, see our [tutorial](/documentation/tutorials/create-snapshot/).



## Store snapshots
client.recoverSnapshot(""{collection_name}"", {

location: ""http://qdrant-node-1:6333/collections/{collection_name}/snapshots/snapshot-2022-10-10.shapshot"",

priority: ""snapshot""

});

```



```bash

curl -X POST 'http://qdrant-node-1:6333/collections/{collection_name}/snapshots/upload?priority=snapshot' \

-H 'api-key: ********' \

-H 'Content-Type:multipart/form-data' \

-F 'snapshot=@/path/to/snapshot-2022-10-10.shapshot'

```



## Snapshots for the whole storage\n\nAnswer:",What are snapshots in the context of Qdrant Cloud and how are they used in a distributed setup?,"Snapshots in Qdrant Cloud are `tar` archive files that contain data and configuration of a specific collection on a specific node at a specific time. In a distributed setup with multiple nodes in a cluster, snapshots must be created for each node separately when dealing with a single collection. These snapshots can be used to archive data or easily replicate an existing deployment.","[""That's why using snapshots is the best way to export and import Qdrant collections, as they contain all the bits and pieces required to restore the entire collection efficiently.\n\n\n\nThis tutorial will show you how to create a snapshot of a collection and restore it. Since working with snapshots in a distributed environment might be thought to be a bit more complex, we will use a 3-node Qdrant cluster. However, the same approach applies to a single-node setup."", '<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>\n\n\n\n### Create full storage snapshot\n\n\n\n```http\n\nPOST /snapshots\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_full_snapshot()\n\n```', '-H \'api-key: ********\' \\\n\n    -H \'Content-Type:multipart/form-data\' \\\n\n    -F \'snapshot=@/path/to/snapshot-2022-10-10.shapshot\'\n\n```\n\n\n\nThis method is typically used to migrate data from one cluster to another, so we recommend setting the [priority](#snapshot-priority) to ""snapshot"" for that use-case.\n\n\n\n### Recover during start-up\n\n\n\n<aside role=""alert"">This method cannot be used in a multi-node deployment and cannot be used in Qdrant Cloud.</aside>', 'This feature can be used to archive data or easily replicate an existing deployment. For disaster recovery, Qdrant Cloud users may prefer to use [Backups](/documentation/cloud/backups/) instead, which are physical disk-level copies of your data.\n\n\n\nFor a step-by-step guide on how to use snapshots, see our [tutorial](/documentation/tutorials/create-snapshot/).\n\n\n\n## Store snapshots', 'client.recoverSnapshot(""{collection_name}"", {\n\n  location: ""http://qdrant-node-1:6333/collections/{collection_name}/snapshots/snapshot-2022-10-10.shapshot"",\n\n  priority: ""snapshot""\n\n});\n\n```\n\n\n\n```bash\n\ncurl -X POST \'http://qdrant-node-1:6333/collections/{collection_name}/snapshots/upload?priority=snapshot\' \\\n\n    -H \'api-key: ********\' \\\n\n    -H \'Content-Type:multipart/form-data\' \\\n\n    -F \'snapshot=@/path/to/snapshot-2022-10-10.shapshot\'\n\n```\n\n\n\n## Snapshots for the whole storage']","That's why using snapshots is the best way to export and import Qdrant collections, as they contain all the bits and pieces required to restore the entire collection efficiently.



This tutorial will show you how to create a snapshot of a collection and restore it. Since working with snapshots in a distributed environment might be thought to be a bit more complex, we will use a 3-node Qdrant cluster. However, the same approach applies to a single-node setup.
<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>



### Create full storage snapshot



```http

POST /snapshots

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.create_full_snapshot()

```
-H 'api-key: ********' \

    -H 'Content-Type:multipart/form-data' \

    -F 'snapshot=@/path/to/snapshot-2022-10-10.shapshot'

```



This method is typically used to migrate data from one cluster to another, so we recommend setting the [priority](#snapshot-priority) to ""snapshot"" for that use-case.



### Recover during start-up



<aside role=""alert"">This method cannot be used in a multi-node deployment and cannot be used in Qdrant Cloud.</aside>
This feature can be used to archive data or easily replicate an existing deployment. For disaster recovery, Qdrant Cloud users may prefer to use [Backups](/documentation/cloud/backups/) instead, which are physical disk-level copies of your data.



For a step-by-step guide on how to use snapshots, see our [tutorial](/documentation/tutorials/create-snapshot/).



## Store snapshots
client.recoverSnapshot(""{collection_name}"", {

  location: ""http://qdrant-node-1:6333/collections/{collection_name}/snapshots/snapshot-2022-10-10.shapshot"",

  priority: ""snapshot""

});

```



```bash

curl -X POST 'http://qdrant-node-1:6333/collections/{collection_name}/snapshots/upload?priority=snapshot' \

    -H 'api-key: ********' \

    -H 'Content-Type:multipart/form-data' \

    -F 'snapshot=@/path/to/snapshot-2022-10-10.shapshot'

```



## Snapshots for the whole storage",0.14522821576763487,0.625,0.2356902356902357,0.05,0.21818181818181817,0.08135593220338984,0.1037344398340249,0.44642857142857145,0.16835016835016833,0.12448132780082988,0.5357142857142857,0.20202020202020202,0.8901506662368774,0.5050384402275085,0.6259813904762268,0.5590435862541199,0.9332871176302433,0.5015907138586044,240,3.8095238095238093,0.38455426257468445
True,,2846.323,0c22dd8b,What is the significance of the lowercase parameter in the context of text filters in Qdrant?,"The lowercase parameter in Qdrant is used to specify whether the index should be case-insensitive or not. When set to true, Qdrant will convert all the texts to lowercase before indexing them. This means that during searches, the case of the letters in the query will not affect the results.","In the context of text filters in Qdrant, the significance of the lowercase parameter is that it controls whether the index will be case-insensitive. When the lowercase parameter is set to true, Qdrant will convert all the texts to lowercase before indexing them. This means that during search operations, the case of the letters in the search query will not affect the results, as all text data has been normalized to lowercase.","## Using text filters in practice



![](/blog/from_cms/1_pbtd2tzqtjqqlbi61r8czg.webp ""There are also some additional parameters you can provide, such as  min_token_len — minimal length of the token max_token_len — maximal length of the token lowercase — if set to true, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase Using text filters in practice"")
* **min_token_len** — minimal length of the token

* **max_token_len** — maximal length of the token

* **lowercase** — if set to *true*, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase



## Using text filters in practice
### Full-text index



*Available as of v0.10.0*



Qdrant supports full-text search for string payload.

Full-text index allows you to filter points by the presence of a word or a phrase in the payload field.



Full-text index configuration is a bit more complex than other indexes, as you can specify the tokenization parameters.

Tokenization is the process of splitting a string into tokens, which are then indexed in the inverted index.



To create a full-text index, you can use the following:
```



```java

import static io.qdrant.client.ConditionFactory.matchText;



matchText(""description"", ""good cheap"");

```



```csharp

using static Qdrant.Client.Grpc.Conditions;



MatchText(""description"", ""good cheap"");

```



If the query has several words, then the condition will be satisfied only if all of them are present in the text.



### Range



```json

{

  ""key"": ""price"",

  ""range"": {

    ""gt"": null,

    ""gte"": 100.0,

    ""lt"": null,

    ""lte"": 450.0

  }

}

```



```python
filter: Some(Filter::should([

            Condition::matches(""city"", ""london"".to_string()),

            Condition::matches(""color"", ""red"".to_string()),

        ])),

        ..Default::default()

    })

    .await?;

```



```java

import static io.qdrant.client.ConditionFactory.matchKeyword;



import io.qdrant.client.grpc.Points.Filter;

import io.qdrant.client.grpc.Points.ScrollPoints;

import java.util.List;



client

    .scrollAsync(

        ScrollPoints.newBuilder()","['## Using text filters in practice\n\n\n\n![](/blog/from_cms/1_pbtd2tzqtjqqlbi61r8czg.webp ""There are also some additional parameters you can provide, such as  min_token_len — minimal length of the token max_token_len — maximal length of the token lowercase — if set to true, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase Using text filters in practice"")', '* **min_token_len**\xa0— minimal length of the token\n\n* **max_token_len**\xa0— maximal length of the token\n\n* **lowercase**\xa0— if set to\xa0*true*, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase\n\n\n\n## Using text filters in practice', '### Full-text index\n\n\n\n*Available as of v0.10.0*\n\n\n\nQdrant supports full-text search for string payload.\n\nFull-text index allows you to filter points by the presence of a word or a phrase in the payload field.\n\n\n\nFull-text index configuration is a bit more complex than other indexes, as you can specify the tokenization parameters.\n\nTokenization is the process of splitting a string into tokens, which are then indexed in the inverted index.\n\n\n\nTo create a full-text index, you can use the following:', '```\n\n\n\n```java\n\nimport static io.qdrant.client.ConditionFactory.matchText;\n\n\n\nmatchText(""description"", ""good cheap"");\n\n```\n\n\n\n```csharp\n\nusing static Qdrant.Client.Grpc.Conditions;\n\n\n\nMatchText(""description"", ""good cheap"");\n\n```\n\n\n\nIf the query has several words, then the condition will be satisfied only if all of them are present in the text.\n\n\n\n### Range\n\n\n\n```json\n\n{\n\n  ""key"": ""price"",\n\n  ""range"": {\n\n    ""gt"": null,\n\n    ""gte"": 100.0,\n\n    ""lt"": null,\n\n    ""lte"": 450.0\n\n  }\n\n}\n\n```\n\n\n\n```python', 'filter: Some(Filter::should([\n\n            Condition::matches(""city"", ""london"".to_string()),\n\n            Condition::matches(""color"", ""red"".to_string()),\n\n        ])),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport static io.qdrant.client.ConditionFactory.matchKeyword;\n\n\n\nimport io.qdrant.client.grpc.Points.Filter;\n\nimport io.qdrant.client.grpc.Points.ScrollPoints;\n\nimport java.util.List;\n\n\n\nclient\n\n    .scrollAsync(\n\n        ScrollPoints.newBuilder()']","Question: What is the significance of the lowercase parameter in the context of text filters in Qdrant?\n\nContext: ## Using text filters in practice



![](/blog/from_cms/1_pbtd2tzqtjqqlbi61r8czg.webp ""There are also some additional parameters you can provide, such as  min_token_len — minimal length of the token max_token_len — maximal length of the token lowercase — if set to true, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase Using text filters in practice"")
* **min_token_len** — minimal length of the token

* **max_token_len** — maximal length of the token

* **lowercase** — if set to *true*, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase



## Using text filters in practice
### Full-text index



*Available as of v0.10.0*



Qdrant supports full-text search for string payload.

Full-text index allows you to filter points by the presence of a word or a phrase in the payload field.



Full-text index configuration is a bit more complex than other indexes, as you can specify the tokenization parameters.

Tokenization is the process of splitting a string into tokens, which are then indexed in the inverted index.



To create a full-text index, you can use the following:
```



```java

import static io.qdrant.client.ConditionFactory.matchText;



matchText(""description"", ""good cheap"");

```



```csharp

using static Qdrant.Client.Grpc.Conditions;



MatchText(""description"", ""good cheap"");

```



If the query has several words, then the condition will be satisfied only if all of them are present in the text.



### Range



```json

{

""key"": ""price"",

""range"": {

""gt"": null,

""gte"": 100.0,

""lt"": null,

""lte"": 450.0

}

}

```



```python
filter: Some(Filter::should([

Condition::matches(""city"", ""london"".to_string()),

Condition::matches(""color"", ""red"".to_string()),

])),

..Default::default()

})

.await?;

```



```java

import static io.qdrant.client.ConditionFactory.matchKeyword;



import io.qdrant.client.grpc.Points.Filter;

import io.qdrant.client.grpc.Points.ScrollPoints;

import java.util.List;



client

.scrollAsync(

ScrollPoints.newBuilder()\n\nAnswer:",What is the significance of the lowercase parameter in the context of text filters in Qdrant?,"The lowercase parameter in Qdrant is used to specify whether the index should be case-insensitive or not. When set to true, Qdrant will convert all the texts to lowercase before indexing them. This means that during searches, the case of the letters in the query will not affect the results.","['## Using text filters in practice\n\n\n\n![](/blog/from_cms/1_pbtd2tzqtjqqlbi61r8czg.webp ""There are also some additional parameters you can provide, such as  min_token_len — minimal length of the token max_token_len — maximal length of the token lowercase — if set to true, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase Using text filters in practice"")', '* **min_token_len**\xa0— minimal length of the token\n\n* **max_token_len**\xa0— maximal length of the token\n\n* **lowercase**\xa0— if set to\xa0*true*, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase\n\n\n\n## Using text filters in practice', '### Full-text index\n\n\n\n*Available as of v0.10.0*\n\n\n\nQdrant supports full-text search for string payload.\n\nFull-text index allows you to filter points by the presence of a word or a phrase in the payload field.\n\n\n\nFull-text index configuration is a bit more complex than other indexes, as you can specify the tokenization parameters.\n\nTokenization is the process of splitting a string into tokens, which are then indexed in the inverted index.\n\n\n\nTo create a full-text index, you can use the following:', '```\n\n\n\n```java\n\nimport static io.qdrant.client.ConditionFactory.matchText;\n\n\n\nmatchText(""description"", ""good cheap"");\n\n```\n\n\n\n```csharp\n\nusing static Qdrant.Client.Grpc.Conditions;\n\n\n\nMatchText(""description"", ""good cheap"");\n\n```\n\n\n\nIf the query has several words, then the condition will be satisfied only if all of them are present in the text.\n\n\n\n### Range\n\n\n\n```json\n\n{\n\n  ""key"": ""price"",\n\n  ""range"": {\n\n    ""gt"": null,\n\n    ""gte"": 100.0,\n\n    ""lt"": null,\n\n    ""lte"": 450.0\n\n  }\n\n}\n\n```\n\n\n\n```python', 'filter: Some(Filter::should([\n\n            Condition::matches(""city"", ""london"".to_string()),\n\n            Condition::matches(""color"", ""red"".to_string()),\n\n        ])),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport static io.qdrant.client.ConditionFactory.matchKeyword;\n\n\n\nimport io.qdrant.client.grpc.Points.Filter;\n\nimport io.qdrant.client.grpc.Points.ScrollPoints;\n\nimport java.util.List;\n\n\n\nclient\n\n    .scrollAsync(\n\n        ScrollPoints.newBuilder()']","## Using text filters in practice



![](/blog/from_cms/1_pbtd2tzqtjqqlbi61r8czg.webp ""There are also some additional parameters you can provide, such as  min_token_len — minimal length of the token max_token_len — maximal length of the token lowercase — if set to true, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase Using text filters in practice"")
* **min_token_len** — minimal length of the token

* **max_token_len** — maximal length of the token

* **lowercase** — if set to *true*, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase



## Using text filters in practice
### Full-text index



*Available as of v0.10.0*



Qdrant supports full-text search for string payload.

Full-text index allows you to filter points by the presence of a word or a phrase in the payload field.



Full-text index configuration is a bit more complex than other indexes, as you can specify the tokenization parameters.

Tokenization is the process of splitting a string into tokens, which are then indexed in the inverted index.



To create a full-text index, you can use the following:
```



```java

import static io.qdrant.client.ConditionFactory.matchText;



matchText(""description"", ""good cheap"");

```



```csharp

using static Qdrant.Client.Grpc.Conditions;



MatchText(""description"", ""good cheap"");

```



If the query has several words, then the condition will be satisfied only if all of them are present in the text.



### Range



```json

{

  ""key"": ""price"",

  ""range"": {

    ""gt"": null,

    ""gte"": 100.0,

    ""lt"": null,

    ""lte"": 450.0

  }

}

```



```python
filter: Some(Filter::should([

            Condition::matches(""city"", ""london"".to_string()),

            Condition::matches(""color"", ""red"".to_string()),

        ])),

        ..Default::default()

    })

    .await?;

```



```java

import static io.qdrant.client.ConditionFactory.matchKeyword;



import io.qdrant.client.grpc.Points.Filter;

import io.qdrant.client.grpc.Points.ScrollPoints;

import java.util.List;



client

    .scrollAsync(

        ScrollPoints.newBuilder()",0.6190476190476191,0.8863636363636364,0.7289719626168225,0.46774193548387094,0.6744186046511628,0.5523809523809524,0.5714285714285714,0.8181818181818182,0.6728971962616823,0.6190476190476191,0.8863636363636364,0.7289719626168225,0.9383991360664368,0.7717717289924622,0.8353664875030518,0.8023108839988708,0.9518454175752898,0.8356609055772424,72,1.44,0.40818228931946904
True,,2063.838,3a2eda6b,How does adjusting the `write_consistency_factor` parameter impact write operations in a distributed deployment using Qdrant?,"The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully.","Adjusting the `write_consistency_factor` parameter in a distributed deployment using Qdrant impacts the write operations by defining the number of replicas that must acknowledge a write operation before responding to the client. 

When you increase the `write_consistency_factor`, it makes write operations more tolerant to network partitions in the cluster. However, it also requires a higher number of replicas to be active in order to perform write operations successfully. 

In summary, increasing the `write_consistency_factor` enhances the reliability and fault tolerance of write operations in a distributed deployment by ensuring that a specified number of replicas confirm the write operation before it is considered successful.","""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

    shard_number=6,

    replication_factor=2,

    write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

    ""write_consistency_factor"": 2,

}

```



```python
- Read `consistency` param, can be used with search and retrieve operations to ensure that the results obtained from all replicas are the same. If this option is used, Qdrant will perform the read operation on multiple replicas and resolve the result according to the selected strategy. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if the update operations are frequent and the number of replicas is low.
replication_factor: Some(2),

        write_consistency_factor: Some(2),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =","['""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n\n    shard_number=6,\n\n    replication_factor=2,\n\n    write_consistency_factor=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', '- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.', '### Write consistency factor\n\n\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\n\nIt can be configured at the collection\'s creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python', '- Read `consistency` param, can be used with search and retrieve operations to ensure that the results obtained from all replicas are the same. If this option is used, Qdrant will perform the read operation on multiple replicas and resolve the result according to the selected strategy. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if the update operations are frequent and the number of replicas is low.', 'replication_factor: Some(2),\n\n        write_consistency_factor: Some(2),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =']","Question: How does adjusting the `write_consistency_factor` parameter impact write operations in a distributed deployment using Qdrant?\n\nContext: ""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

shard_number=6,

replication_factor=2,

write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 300,

""distance"": ""Cosine""

},

""shard_number"": 6,

""replication_factor"": 2,

""write_consistency_factor"": 2,

}

```



```python
- Read `consistency` param, can be used with search and retrieve operations to ensure that the results obtained from all replicas are the same. If this option is used, Qdrant will perform the read operation on multiple replicas and resolve the result according to the selected strategy. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if the update operations are frequent and the number of replicas is low.
replication_factor: Some(2),

write_consistency_factor: Some(2),

..Default::default()

})

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =\n\nAnswer:",How does adjusting the `write_consistency_factor` parameter impact write operations in a distributed deployment using Qdrant?,"The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully.","['""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n\n    shard_number=6,\n\n    replication_factor=2,\n\n    write_consistency_factor=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', '- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.', '### Write consistency factor\n\n\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\n\nIt can be configured at the collection\'s creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python', '- Read `consistency` param, can be used with search and retrieve operations to ensure that the results obtained from all replicas are the same. If this option is used, Qdrant will perform the read operation on multiple replicas and resolve the result according to the selected strategy. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if the update operations are frequent and the number of replicas is low.', 'replication_factor: Some(2),\n\n        write_consistency_factor: Some(2),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =']","""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

    shard_number=6,

    replication_factor=2,

    write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

    ""write_consistency_factor"": 2,

}

```



```python
- Read `consistency` param, can be used with search and retrieve operations to ensure that the results obtained from all replicas are the same. If this option is used, Qdrant will perform the read operation on multiple replicas and resolve the result according to the selected strategy. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if the update operations are frequent and the number of replicas is low.
replication_factor: Some(2),

        write_consistency_factor: Some(2),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =",0.5212765957446809,0.875,0.6533333333333333,0.40860215053763443,0.6909090909090909,0.5135135135135135,0.5,0.8392857142857143,0.6266666666666666,0.5212765957446809,0.875,0.6533333333333333,0.9630287885665894,0.7275974750518799,0.8149698972702026,0.7688091993331909,0.9710228670592187,0.9480537519363376,102,1.6451612903225807,0.6547947070515374
True,,1732.098,e18a0d1a,What are some of the features and support services that come pre-configured with each instance in Qdrant Cloud?,"Each instance in Qdrant Cloud comes pre-configured with the following tools, features, and support services:1. Automatically created with the latest available version of Qdrant.
2. Upgradeable to later versions of Qdrant as they are released.
3. Equipped with monitoring and logging to observe the health of each cluster.
4. Accessible through the Qdrant Cloud Console.
5. Vertically scalable.
6. Offered on AWS and GCP, with Azure currently in development.
","Some of the features and support services that come pre-configured with each instance in Qdrant Cloud are:

- Automatically created with the latest available version of Qdrant.
- Upgradeable to later versions of Qdrant as they are released.
- Equipped with monitoring and logging to observe the health of each cluster.
- Accessible through the Qdrant Cloud Console.
- Vertically scalable.
- Offered on AWS and GCP, with Azure currently in development.","The transition is even easier if you use the official client libraries. For example, the [Python Client](https://github.com/qdrant/qdrant-client) has the support of the API key already built-in, so you only need to provide it once, when the QdrantClient instance is created.



### Cluster configuration



Each instance comes pre-configured with the following tools, features and support services:



- Automatically created with the latest available version of Qdrant.
## Configuration



Qdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.



Example: 

```yaml

qdrant:

    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""

    api_key: ""<your-api-key>""

```



The available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:

| Field        | Description |
- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:
Transitioning from on-premise to the cloud version of Qdrant does not require changing anything in the way you interact with the service. All you have to do is [create a Qdrant Cloud account](https://qdrant.to/cloud) and [provide a new API key]({{< ref ""/documentation/cloud/authentication"" >}}) to each request.
qdrant_client = QdrantClient(

    ""xyz-example.eu-central.aws.cloud.qdrant.io"",

    api_key=""<paste-your-api-key-here>"",

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({

  host: ""xyz-example.eu-central.aws.cloud.qdrant.io"",

  apiKey: ""<paste-your-api-key-here>"",

});

```



```rust

use qdrant_client::client::QdrantClient;



let client = QdrantClient::from_url(""xyz-example.eu-central.aws.cloud.qdrant.io:6334"")","['The transition is even easier if you use the official client libraries. For example, the [Python Client](https://github.com/qdrant/qdrant-client) has the support of the API key already built-in, so you only need to provide it once, when the QdrantClient instance is created.\n\n\n\n### Cluster configuration\n\n\n\nEach instance comes pre-configured with the following tools, features and support services:\n\n\n\n- Automatically created with the latest available version of Qdrant.', '## Configuration\n\n\n\nQdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.\n\n\n\nExample: \n\n```yaml\n\nqdrant:\n\n    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""\n\n    api_key: ""<your-api-key>""\n\n```\n\n\n\nThe available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:\n\n| Field        | Description |', '- Upgradeable to later versions of Qdrant as they are released.\n\n- Equipped with monitoring and logging to observe the health of each cluster. \n\n- Accessible through the Qdrant Cloud Console.\n\n- Vertically scalable.\n\n- Offered on AWS and GCP, with Azure currently in development. \n\n\n\n### Getting started with Qdrant Cloud\n\n\n\nTo use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:', 'Transitioning from on-premise to the cloud version of Qdrant does not require changing anything in the way you interact with the service. All you have to do is [create a Qdrant Cloud account](https://qdrant.to/cloud) and [provide a new API key]({{< ref ""/documentation/cloud/authentication"" >}}) to each request.', 'qdrant_client = QdrantClient(\n\n    ""xyz-example.eu-central.aws.cloud.qdrant.io"",\n\n    api_key=""<paste-your-api-key-here>"",\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({\n\n  host: ""xyz-example.eu-central.aws.cloud.qdrant.io"",\n\n  apiKey: ""<paste-your-api-key-here>"",\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::client::QdrantClient;\n\n\n\nlet client = QdrantClient::from_url(""xyz-example.eu-central.aws.cloud.qdrant.io:6334"")']","Question: What are some of the features and support services that come pre-configured with each instance in Qdrant Cloud?\n\nContext: The transition is even easier if you use the official client libraries. For example, the [Python Client](https://github.com/qdrant/qdrant-client) has the support of the API key already built-in, so you only need to provide it once, when the QdrantClient instance is created.



### Cluster configuration



Each instance comes pre-configured with the following tools, features and support services:



- Automatically created with the latest available version of Qdrant.
## Configuration



Qdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.



Example: 

```yaml

qdrant:

url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""

api_key: ""<your-api-key>""

```



The available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:

| Field        | Description |
- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:
Transitioning from on-premise to the cloud version of Qdrant does not require changing anything in the way you interact with the service. All you have to do is [create a Qdrant Cloud account](https://qdrant.to/cloud) and [provide a new API key]({{< ref ""/documentation/cloud/authentication"" >}}) to each request.
qdrant_client = QdrantClient(

""xyz-example.eu-central.aws.cloud.qdrant.io"",

api_key=""<paste-your-api-key-here>"",

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({

host: ""xyz-example.eu-central.aws.cloud.qdrant.io"",

apiKey: ""<paste-your-api-key-here>"",

});

```



```rust

use qdrant_client::client::QdrantClient;



let client = QdrantClient::from_url(""xyz-example.eu-central.aws.cloud.qdrant.io:6334"")\n\nAnswer:",What are some of the features and support services that come pre-configured with each instance in Qdrant Cloud?,"Each instance in Qdrant Cloud comes pre-configured with the following tools, features, and support services:1. Automatically created with the latest available version of Qdrant.
2. Upgradeable to later versions of Qdrant as they are released.
3. Equipped with monitoring and logging to observe the health of each cluster.
4. Accessible through the Qdrant Cloud Console.
5. Vertically scalable.
6. Offered on AWS and GCP, with Azure currently in development.
","['The transition is even easier if you use the official client libraries. For example, the [Python Client](https://github.com/qdrant/qdrant-client) has the support of the API key already built-in, so you only need to provide it once, when the QdrantClient instance is created.\n\n\n\n### Cluster configuration\n\n\n\nEach instance comes pre-configured with the following tools, features and support services:\n\n\n\n- Automatically created with the latest available version of Qdrant.', '## Configuration\n\n\n\nQdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.\n\n\n\nExample: \n\n```yaml\n\nqdrant:\n\n    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""\n\n    api_key: ""<your-api-key>""\n\n```\n\n\n\nThe available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:\n\n| Field        | Description |', '- Upgradeable to later versions of Qdrant as they are released.\n\n- Equipped with monitoring and logging to observe the health of each cluster. \n\n- Accessible through the Qdrant Cloud Console.\n\n- Vertically scalable.\n\n- Offered on AWS and GCP, with Azure currently in development. \n\n\n\n### Getting started with Qdrant Cloud\n\n\n\nTo use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:', 'Transitioning from on-premise to the cloud version of Qdrant does not require changing anything in the way you interact with the service. All you have to do is [create a Qdrant Cloud account](https://qdrant.to/cloud) and [provide a new API key]({{< ref ""/documentation/cloud/authentication"" >}}) to each request.', 'qdrant_client = QdrantClient(\n\n    ""xyz-example.eu-central.aws.cloud.qdrant.io"",\n\n    api_key=""<paste-your-api-key-here>"",\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({\n\n  host: ""xyz-example.eu-central.aws.cloud.qdrant.io"",\n\n  apiKey: ""<paste-your-api-key-here>"",\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::client::QdrantClient;\n\n\n\nlet client = QdrantClient::from_url(""xyz-example.eu-central.aws.cloud.qdrant.io:6334"")']","The transition is even easier if you use the official client libraries. For example, the [Python Client](https://github.com/qdrant/qdrant-client) has the support of the API key already built-in, so you only need to provide it once, when the QdrantClient instance is created.



### Cluster configuration



Each instance comes pre-configured with the following tools, features and support services:



- Automatically created with the latest available version of Qdrant.
## Configuration



Qdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.



Example: 

```yaml

qdrant:

    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""

    api_key: ""<your-api-key>""

```



The available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:

| Field        | Description |
- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:
Transitioning from on-premise to the cloud version of Qdrant does not require changing anything in the way you interact with the service. All you have to do is [create a Qdrant Cloud account](https://qdrant.to/cloud) and [provide a new API key]({{< ref ""/documentation/cloud/authentication"" >}}) to each request.
qdrant_client = QdrantClient(

    ""xyz-example.eu-central.aws.cloud.qdrant.io"",

    api_key=""<paste-your-api-key-here>"",

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({

  host: ""xyz-example.eu-central.aws.cloud.qdrant.io"",

  apiKey: ""<paste-your-api-key-here>"",

});

```



```rust

use qdrant_client::client::QdrantClient;



let client = QdrantClient::from_url(""xyz-example.eu-central.aws.cloud.qdrant.io:6334"")",0.9365079365079365,0.8805970149253731,0.9076923076923077,0.8064516129032258,0.7575757575757576,0.7812499999999999,0.8095238095238095,0.7611940298507462,0.7846153846153846,0.8412698412698413,0.7910447761194029,0.8153846153846153,0.9726189970970154,0.8961983919143677,0.8568278551101685,0.876071035861969,0.8242719334860643,0.518962995459636,66,0.9565217391304348,0.5590278717330848
True,,4312.549,fb70fa14,How is the `must_not` clause used in conjunction with other clauses in filtering data?,"The `must_not` clause in filtering data is used to exclude documents that meet specific criteria. The `must_not` clause can be used in combination with the `must` clause to create a more refined filter. Specifically, the `must_not` clause is used to specify conditions that must not be met for a document to be included in the results.","The `must_not` clause is used in conjunction with other clauses in filtering data to specify conditions that must not be satisfied. When using `must_not`, the clause becomes `true` if none of the conditions listed inside it are satisfied. It is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.

The `must_not` clause can be combined with other clauses like `must` using `AND` logic. For example, you can have a filter that includes both `must` and `must_not` conditions. The conditions can also be nested recursively for more complex filtering scenarios.

Here is an example of using the `must_not` clause in a filter query:

```http
POST /collections/{collection_name}/points/scroll
{
  ""filter"": {
    ""must_not"": [
      { ""key"": ""city"", ""match"": { ""value"": ""London"" } },
      { ""key"": ""color"", ""match"": { ""value"": ""red"" } }
    ]
  }
}
```

In this example, the filter query specifies that the points to be retrieved should not have the city value of ""London"" and the color value of ""red"".

The `must_not` clause is a powerful tool for fine-tuning data retrieval by excluding specific conditions that should not be present in the results.","{ ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }

]

```



When using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.

In this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.



### Clauses combination



It is also possible to use several clauses simultaneously:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""must"": [
```



In this case, the conditions are combined by `AND`.



Also, the conditions could be recursively nested. Example:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""must_not"": [

            {

                ""must"": [

                    { ""key"": ""city"", ""match"": { ""value"": ""London"" } },

                    { ""key"": ""color"", ""match"": { ""value"": ""red"" } }

                ]

            }

        ]

    }

}

```



```python

client.scroll(
{

    ""filter"": {

        ""must"": [

            { ""key"": ""city"", ""match"": { ""value"": ""London"" } }

        ],

        ""must_not"": [

            { ""key"": ""color"", ""match"": { ""value"": ""red"" } }

        ]

    }

}

```



```python

client.scroll(

    collection_name=""{collection_name}"",

    scroll_filter=models.Filter(

        must=[

            models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")),

        ],

        must_not=[
{ ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },

  { ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" }

]

```



When using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied.

In this sense, `should` is equivalent to the operator `OR`.



### Must Not



Example:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""must_not"": [

            { ""key"": ""city"", ""match"": { ""value"": ""London"" } },
],

        must_not=[

            models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")),

        ],

    ),

)

```



```typescript

client.scroll(""{collection_name}"", {

  filter: {

    must: [

      {

        key: ""city"",

        match: { value: ""London"" },

      },

    ],

    must_not: [

      {

        key: ""color"",

        match: { value: ""red"" },

      },

    ],

  },

});

```



```rust

use qdrant_client::qdrant::{Condition, Filter, ScrollPoints};","['{ ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }\n\n]\n\n```\n\n\n\nWhen using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.\n\nIn this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.\n\n\n\n### Clauses combination\n\n\n\nIt is also possible to use several clauses simultaneously:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""must"": [', '```\n\n\n\nIn this case, the conditions are combined by `AND`.\n\n\n\nAlso, the conditions could be recursively nested. Example:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""must_not"": [\n\n            {\n\n                ""must"": [\n\n                    { ""key"": ""city"", ""match"": { ""value"": ""London"" } },\n\n                    { ""key"": ""color"", ""match"": { ""value"": ""red"" } }\n\n                ]\n\n            }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.scroll(', '{\n\n    ""filter"": {\n\n        ""must"": [\n\n            { ""key"": ""city"", ""match"": { ""value"": ""London"" } }\n\n        ],\n\n        ""must_not"": [\n\n            { ""key"": ""color"", ""match"": { ""value"": ""red"" } }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.scroll(\n\n    collection_name=""{collection_name}"",\n\n    scroll_filter=models.Filter(\n\n        must=[\n\n            models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")),\n\n        ],\n\n        must_not=[', '{ ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },\n\n  { ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" }\n\n]\n\n```\n\n\n\nWhen using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied.\n\nIn this sense, `should` is equivalent to the operator `OR`.\n\n\n\n### Must Not\n\n\n\nExample:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""must_not"": [\n\n            { ""key"": ""city"", ""match"": { ""value"": ""London"" } },', '],\n\n        must_not=[\n\n            models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")),\n\n        ],\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nclient.scroll(""{collection_name}"", {\n\n  filter: {\n\n    must: [\n\n      {\n\n        key: ""city"",\n\n        match: { value: ""London"" },\n\n      },\n\n    ],\n\n    must_not: [\n\n      {\n\n        key: ""color"",\n\n        match: { value: ""red"" },\n\n      },\n\n    ],\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::qdrant::{Condition, Filter, ScrollPoints};']","Question: How is the `must_not` clause used in conjunction with other clauses in filtering data?\n\nContext: { ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }

]

```



When using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.

In this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.



### Clauses combination



It is also possible to use several clauses simultaneously:



```http

POST /collections/{collection_name}/points/scroll

{

""filter"": {

""must"": [
```



In this case, the conditions are combined by `AND`.



Also, the conditions could be recursively nested. Example:



```http

POST /collections/{collection_name}/points/scroll

{

""filter"": {

""must_not"": [

{

""must"": [

{ ""key"": ""city"", ""match"": { ""value"": ""London"" } },

{ ""key"": ""color"", ""match"": { ""value"": ""red"" } }

]

}

]

}

}

```



```python

client.scroll(
{

""filter"": {

""must"": [

{ ""key"": ""city"", ""match"": { ""value"": ""London"" } }

],

""must_not"": [

{ ""key"": ""color"", ""match"": { ""value"": ""red"" } }

]

}

}

```



```python

client.scroll(

collection_name=""{collection_name}"",

scroll_filter=models.Filter(

must=[

models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")),

],

must_not=[
{ ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },

{ ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" }

]

```



When using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied.

In this sense, `should` is equivalent to the operator `OR`.



### Must Not



Example:



```http

POST /collections/{collection_name}/points/scroll

{

""filter"": {

""must_not"": [

{ ""key"": ""city"", ""match"": { ""value"": ""London"" } },
],

must_not=[

models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")),

],

),

)

```



```typescript

client.scroll(""{collection_name}"", {

filter: {

must: [

{

key: ""city"",

match: { value: ""London"" },

},

],

must_not: [

{

key: ""color"",

match: { value: ""red"" },

},

],

},

});

```



```rust

use qdrant_client::qdrant::{Condition, Filter, ScrollPoints};\n\nAnswer:",How is the `must_not` clause used in conjunction with other clauses in filtering data?,"The `must_not` clause in filtering data is used to exclude documents that meet specific criteria. The `must_not` clause can be used in combination with the `must` clause to create a more refined filter. Specifically, the `must_not` clause is used to specify conditions that must not be met for a document to be included in the results.","['{ ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }\n\n]\n\n```\n\n\n\nWhen using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.\n\nIn this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.\n\n\n\n### Clauses combination\n\n\n\nIt is also possible to use several clauses simultaneously:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""must"": [', '```\n\n\n\nIn this case, the conditions are combined by `AND`.\n\n\n\nAlso, the conditions could be recursively nested. Example:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""must_not"": [\n\n            {\n\n                ""must"": [\n\n                    { ""key"": ""city"", ""match"": { ""value"": ""London"" } },\n\n                    { ""key"": ""color"", ""match"": { ""value"": ""red"" } }\n\n                ]\n\n            }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.scroll(', '{\n\n    ""filter"": {\n\n        ""must"": [\n\n            { ""key"": ""city"", ""match"": { ""value"": ""London"" } }\n\n        ],\n\n        ""must_not"": [\n\n            { ""key"": ""color"", ""match"": { ""value"": ""red"" } }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.scroll(\n\n    collection_name=""{collection_name}"",\n\n    scroll_filter=models.Filter(\n\n        must=[\n\n            models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")),\n\n        ],\n\n        must_not=[', '{ ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },\n\n  { ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" }\n\n]\n\n```\n\n\n\nWhen using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied.\n\nIn this sense, `should` is equivalent to the operator `OR`.\n\n\n\n### Must Not\n\n\n\nExample:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""must_not"": [\n\n            { ""key"": ""city"", ""match"": { ""value"": ""London"" } },', '],\n\n        must_not=[\n\n            models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")),\n\n        ],\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nclient.scroll(""{collection_name}"", {\n\n  filter: {\n\n    must: [\n\n      {\n\n        key: ""city"",\n\n        match: { value: ""London"" },\n\n      },\n\n    ],\n\n    must_not: [\n\n      {\n\n        key: ""color"",\n\n        match: { value: ""red"" },\n\n      },\n\n    ],\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::qdrant::{Condition, Filter, ScrollPoints};']","{ ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }

]

```



When using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.

In this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.



### Clauses combination



It is also possible to use several clauses simultaneously:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""must"": [
```



In this case, the conditions are combined by `AND`.



Also, the conditions could be recursively nested. Example:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""must_not"": [

            {

                ""must"": [

                    { ""key"": ""city"", ""match"": { ""value"": ""London"" } },

                    { ""key"": ""color"", ""match"": { ""value"": ""red"" } }

                ]

            }

        ]

    }

}

```



```python

client.scroll(
{

    ""filter"": {

        ""must"": [

            { ""key"": ""city"", ""match"": { ""value"": ""London"" } }

        ],

        ""must_not"": [

            { ""key"": ""color"", ""match"": { ""value"": ""red"" } }

        ]

    }

}

```



```python

client.scroll(

    collection_name=""{collection_name}"",

    scroll_filter=models.Filter(

        must=[

            models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")),

        ],

        must_not=[
{ ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },

  { ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" }

]

```



When using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied.

In this sense, `should` is equivalent to the operator `OR`.



### Must Not



Example:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""must_not"": [

            { ""key"": ""city"", ""match"": { ""value"": ""London"" } },
],

        must_not=[

            models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")),

        ],

    ),

)

```



```typescript

client.scroll(""{collection_name}"", {

  filter: {

    must: [

      {

        key: ""city"",

        match: { value: ""London"" },

      },

    ],

    must_not: [

      {

        key: ""color"",

        match: { value: ""red"" },

      },

    ],

  },

});

```



```rust

use qdrant_client::qdrant::{Condition, Filter, ScrollPoints};",0.2621951219512195,0.8269230769230769,0.39814814814814814,0.1411042944785276,0.45098039215686275,0.21495327102803735,0.17682926829268292,0.5576923076923077,0.2685185185185185,0.25,0.7884615384615384,0.37962962962962965,0.8458126783370972,0.5889629125595093,0.7204541563987732,0.6481063961982727,0.7337044924497604,0.8406334122021993,172,3.0714285714285716,0.6209320484841103
True,,2097.794,682d362a,What are some advantages of vector search over keyword-based search?,"Vector search has clear advantages over keyword-based search in various scenarios as outlined in the document chunk. Some of these advantages include:

1. Multi-lingual & multi-modal search: Vector search is effective in handling searches across multiple languages and modalities.
2. For short texts with typos and ambiguous content-dependent meanings: Vector search excels in situations where the search queries are short, contain typos, or have ambiguous meanings.
3. Specialized domains with tuned encoder models: Vector search is beneficial in specialized domains where encoder models can be fine-tuned for better search results.
4. Document-as-a-Query similarity search: Vector search allows for similarity searches where the entire document can be used as a query to find similar documents.

While vector search offers these advantages, it is important to note that keyword-based search still has its relevance in certain cases. For example, in out-of-domain search.","Some advantages of vector search over keyword-based search include:

1. Better handling of multi-lingual and multi-modal search queries.
2. Improved performance for short texts with typos and ambiguous content-dependent meanings.
3. Effectiveness in specialized domains with finely-tuned encoder models.
4. Enhanced document-as-a-query similarity search capabilities.

Additionally, vector search offers efficient storage and indexing of high-dimensional data, support for large-scale datasets with billions of data points, and enables real-time analytics and queries.","preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has

some clear advantages in the following scenarios:



- 🌍 Multi-lingual & multi-modal search

- 🤔 For short texts with typos and ambiguous content-dependent meanings

- 👨‍🔬 Specialized domains with tuned encoder models

- 📄 Document-as-a-Query similarity search



It doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method

might be useful:
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
2. Vector search with keyword-based search. This one is covered in this article.

3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.



## Why do we still need keyword search?



A keyword-based search was the obvious choice for search engines in the past. It struggled with some

common issues, but since we didn't have any alternatives, we had to overcome them with additional
- Fast search-as-you-type

- Search over less-than-natural texts (logs, transactions, code, etc.)



Each of those scenarios has a specific tool, which performs better for that specific use case. If you 

already expose search capabilities, then you probably have one of them in your tech stack. And we can 

easily combine those tools with vector search to get the best of both worlds. 



# The fast search: A Fallback strategy
</tr>

   </tbody>

</table>





# A wrap up



Each search scenario requires a specialized tool to achieve the best results possible. Still, combining multiple tools with 

minimal overhead is possible to improve the search precision even further. Introducing vector search into an existing search 

stack doesn't need to be a revolution but just one small step at a time. 



You'll never cover all the possible queries with a list of synonyms, so a full-text search may not find all the relevant","[""preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has\n\nsome clear advantages in the following scenarios:\n\n\n\n- 🌍 Multi-lingual & multi-modal search\n\n- 🤔 For short texts with typos and ambiguous content-dependent meanings\n\n- 👨\u200d🔬 Specialized domains with tuned encoder models\n\n- 📄 Document-as-a-Query similarity search\n\n\n\nIt doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method\n\nmight be useful:"", 'advantage of their efficient indexing and searching techniques, vector databases enable faster \n\nand more accurate retrieval of unstructured data already represented as vectors, which can \n\nhelp put in front of users the most relevant results to their queries.\n\n\n\nIn addition, other benefits of using vector databases include:\n\n1. Efficient storage and indexing of high-dimensional data.\n\n3. Ability to handle large-scale datasets with billions of data points.\n\n4. Support for real-time analytics and queries.', ""2. Vector search with keyword-based search. This one is covered in this article.\n\n3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.\n\n\n\n## Why do we still need keyword search?\n\n\n\nA keyword-based search was the obvious choice for search engines in the past. It struggled with some\n\ncommon issues, but since we didn't have any alternatives, we had to overcome them with additional"", '- Fast search-as-you-type\n\n- Search over less-than-natural texts (logs, transactions, code, etc.)\n\n\n\nEach of those scenarios has a specific tool, which performs better for that specific use case. If you \n\nalready expose search capabilities, then you probably have one of them in your tech stack. And we can \n\neasily combine those tools with vector search to get the best of both worlds. \n\n\n\n# The fast search: A Fallback strategy', ""</tr>\n\n   </tbody>\n\n</table>\n\n\n\n\n\n# A wrap up\n\n\n\nEach search scenario requires a specialized tool to achieve the best results possible. Still, combining multiple tools with \n\nminimal overhead is possible to improve the search precision even further. Introducing vector search into an existing search \n\nstack doesn't need to be a revolution but just one small step at a time. \n\n\n\nYou'll never cover all the possible queries with a list of synonyms, so a full-text search may not find all the relevant""]","Question: What are some advantages of vector search over keyword-based search?\n\nContext: preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has

some clear advantages in the following scenarios:



- 🌍 Multi-lingual & multi-modal search

- 🤔 For short texts with typos and ambiguous content-dependent meanings

- 👨‍🔬 Specialized domains with tuned encoder models

- 📄 Document-as-a-Query similarity search



It doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method

might be useful:
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
2. Vector search with keyword-based search. This one is covered in this article.

3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.



## Why do we still need keyword search?



A keyword-based search was the obvious choice for search engines in the past. It struggled with some

common issues, but since we didn't have any alternatives, we had to overcome them with additional
- Fast search-as-you-type

- Search over less-than-natural texts (logs, transactions, code, etc.)



Each of those scenarios has a specific tool, which performs better for that specific use case. If you 

already expose search capabilities, then you probably have one of them in your tech stack. And we can 

easily combine those tools with vector search to get the best of both worlds. 



# The fast search: A Fallback strategy
</tr>

</tbody>

</table>





# A wrap up



Each search scenario requires a specialized tool to achieve the best results possible. Still, combining multiple tools with 

minimal overhead is possible to improve the search precision even further. Introducing vector search into an existing search 

stack doesn't need to be a revolution but just one small step at a time. 



You'll never cover all the possible queries with a list of synonyms, so a full-text search may not find all the relevant\n\nAnswer:",What are some advantages of vector search over keyword-based search?,"Vector search has clear advantages over keyword-based search in various scenarios as outlined in the document chunk. Some of these advantages include:

1. Multi-lingual & multi-modal search: Vector search is effective in handling searches across multiple languages and modalities.
2. For short texts with typos and ambiguous content-dependent meanings: Vector search excels in situations where the search queries are short, contain typos, or have ambiguous meanings.
3. Specialized domains with tuned encoder models: Vector search is beneficial in specialized domains where encoder models can be fine-tuned for better search results.
4. Document-as-a-Query similarity search: Vector search allows for similarity searches where the entire document can be used as a query to find similar documents.

While vector search offers these advantages, it is important to note that keyword-based search still has its relevance in certain cases. For example, in out-of-domain search.","[""preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has\n\nsome clear advantages in the following scenarios:\n\n\n\n- 🌍 Multi-lingual & multi-modal search\n\n- 🤔 For short texts with typos and ambiguous content-dependent meanings\n\n- 👨\u200d🔬 Specialized domains with tuned encoder models\n\n- 📄 Document-as-a-Query similarity search\n\n\n\nIt doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method\n\nmight be useful:"", 'advantage of their efficient indexing and searching techniques, vector databases enable faster \n\nand more accurate retrieval of unstructured data already represented as vectors, which can \n\nhelp put in front of users the most relevant results to their queries.\n\n\n\nIn addition, other benefits of using vector databases include:\n\n1. Efficient storage and indexing of high-dimensional data.\n\n3. Ability to handle large-scale datasets with billions of data points.\n\n4. Support for real-time analytics and queries.', ""2. Vector search with keyword-based search. This one is covered in this article.\n\n3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.\n\n\n\n## Why do we still need keyword search?\n\n\n\nA keyword-based search was the obvious choice for search engines in the past. It struggled with some\n\ncommon issues, but since we didn't have any alternatives, we had to overcome them with additional"", '- Fast search-as-you-type\n\n- Search over less-than-natural texts (logs, transactions, code, etc.)\n\n\n\nEach of those scenarios has a specific tool, which performs better for that specific use case. If you \n\nalready expose search capabilities, then you probably have one of them in your tech stack. And we can \n\neasily combine those tools with vector search to get the best of both worlds. \n\n\n\n# The fast search: A Fallback strategy', ""</tr>\n\n   </tbody>\n\n</table>\n\n\n\n\n\n# A wrap up\n\n\n\nEach search scenario requires a specialized tool to achieve the best results possible. Still, combining multiple tools with \n\nminimal overhead is possible to improve the search precision even further. Introducing vector search into an existing search \n\nstack doesn't need to be a revolution but just one small step at a time. \n\n\n\nYou'll never cover all the possible queries with a list of synonyms, so a full-text search may not find all the relevant""]","preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has

some clear advantages in the following scenarios:



- 🌍 Multi-lingual & multi-modal search

- 🤔 For short texts with typos and ambiguous content-dependent meanings

- 👨‍🔬 Specialized domains with tuned encoder models

- 📄 Document-as-a-Query similarity search



It doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method

might be useful:
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
2. Vector search with keyword-based search. This one is covered in this article.

3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.



## Why do we still need keyword search?



A keyword-based search was the obvious choice for search engines in the past. It struggled with some

common issues, but since we didn't have any alternatives, we had to overcome them with additional
- Fast search-as-you-type

- Search over less-than-natural texts (logs, transactions, code, etc.)



Each of those scenarios has a specific tool, which performs better for that specific use case. If you 

already expose search capabilities, then you probably have one of them in your tech stack. And we can 

easily combine those tools with vector search to get the best of both worlds. 



# The fast search: A Fallback strategy
</tr>

   </tbody>

</table>





# A wrap up



Each search scenario requires a specialized tool to achieve the best results possible. Still, combining multiple tools with 

minimal overhead is possible to improve the search precision even further. Introducing vector search into an existing search 

stack doesn't need to be a revolution but just one small step at a time. 



You'll never cover all the possible queries with a list of synonyms, so a full-text search may not find all the relevant",0.654320987654321,0.36551724137931035,0.4690265486725664,0.4,0.2222222222222222,0.2857142857142857,0.5185185185185185,0.2896551724137931,0.37168141592920356,0.6172839506172839,0.3448275862068966,0.4424778761061947,0.9463846683502197,0.6969885230064392,0.6469725370407104,0.6710498929023743,0.954860280174762,0.7297848078596871,71,0.5107913669064749,0.39640211092771455
True,,4412.376,33ed1c6d,What is the purpose of the DocArray library and how does it relate to the Qdrant engine?,"The DocArray library is designed to define a structure for unstructured data, making it easier to process a collection of documents that may include various data types such as audio, video, and text. This library simplifies the handling of these diverse types of data within a single framework. On the other hand, the Qdrant engine focuses on enabling the scaling of vector search and storage capabilities. By integrating the DocArray library with the Qdrant engine, users can benefit from efficient processing of unstructured data and enhanced scalability for vector search operations. This integration streamlines the handling of different data types while also optimizing the performance of vector search and storage functions.","The purpose of the DocArray library is to provide a structure for handling unstructured data, such as text, image, audio, video, and 3D mesh, in a nested format. It simplifies the processing of a collection of documents and enables deep-learning engineers to efficiently work with and manipulate this data using a Pythonic API.

The integration of the Qdrant engine with DocArray allows for scalable vector search and storage capabilities. Qdrant serves as a high-performance document store within DocArray, enabling users to leverage its performance and efficiency for handling and searching through large sets of data.

By using the DocArray library with Qdrant support, users can benefit from the combination of Pythonic API convenience and Rust performance in their workflows. This integration enhances the capabilities of DocArray by providing a powerful storage backend through the Qdrant engine.","DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,

including audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.



Read more about the integration by this [link](/documentation/install/#docarray)
---

title: DocArray

weight: 300

aliases: [ ../integrations/docarray/ ]

---



# DocArray

You can use Qdrant natively in DocArray, where Qdrant serves as a high-performance document store to enable scalable vector search.



DocArray is a library from Jina AI for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc.

It allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API.
To install DocArray with Qdrant support, please do



```bash

pip install ""docarray[qdrant]""

```



More information can be found in [DocArray's documentations](https://docarray.jina.ai/advanced/document-store/qdrant/).
date: 2022-03-15T15:00:00+03:00

author: Alyona Kavyerina

featured: false

author_link: https://medium.com/@alyona.kavyerina

tags:

  - jina integration

  - docarray

categories:

  - News

---

We are happy to announce that [Jina.AI](https://jina.ai/) integrates Qdrant engine as a storage backend to their [DocArray](https://docarray.jina.ai/) solution.



Now you can experience the convenience of Pythonic API and Rust performance in a single workflow.
---

draft: false

preview_image: /blog/from_cms/docarray.png

sitemapExclude: true

title: ""Qdrant and Jina integration: storage backend support for DocArray""

slug: qdrant-and-jina-integration

short_description: ""One more way to use Qdrant: Jina's DocArray is now

  supporting Qdrant as a storage backend.""

description: We are happy to announce that Jina.AI integrates Qdrant engine as a

  storage backend to their DocArray solution.

date: 2022-03-15T15:00:00+03:00

author: Alyona Kavyerina","['DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,\n\nincluding audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.\n\n\n\nRead more about the integration by this [link](/documentation/install/#docarray)', '---\n\ntitle: DocArray\n\nweight: 300\n\naliases: [ ../integrations/docarray/ ]\n\n---\n\n\n\n# DocArray\n\nYou can use Qdrant natively in DocArray, where Qdrant serves as a high-performance document store to enable scalable vector search.\n\n\n\nDocArray is a library from Jina AI for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc.\n\nIt allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API.', 'To install DocArray with Qdrant support, please do\n\n\n\n```bash\n\npip install ""docarray[qdrant]""\n\n```\n\n\n\nMore information can be found in [DocArray\'s documentations](https://docarray.jina.ai/advanced/document-store/qdrant/).', 'date: 2022-03-15T15:00:00+03:00\n\nauthor: Alyona Kavyerina\n\nfeatured: false\n\nauthor_link: https://medium.com/@alyona.kavyerina\n\ntags:\n\n  - jina integration\n\n  - docarray\n\ncategories:\n\n  - News\n\n---\n\nWe are happy to announce that [Jina.AI](https://jina.ai/) integrates Qdrant engine as a storage backend to their [DocArray](https://docarray.jina.ai/) solution.\n\n\n\nNow you can experience the convenience of Pythonic API and Rust performance in a single workflow.', '---\n\ndraft: false\n\npreview_image: /blog/from_cms/docarray.png\n\nsitemapExclude: true\n\ntitle: ""Qdrant and Jina integration: storage backend support for DocArray""\n\nslug: qdrant-and-jina-integration\n\nshort_description: ""One more way to use Qdrant: Jina\'s DocArray is now\n\n  supporting Qdrant as a storage backend.""\n\ndescription: We are happy to announce that Jina.AI integrates Qdrant engine as a\n\n  storage backend to their DocArray solution.\n\ndate: 2022-03-15T15:00:00+03:00\n\nauthor: Alyona Kavyerina']","Question: What is the purpose of the DocArray library and how does it relate to the Qdrant engine?\n\nContext: DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,

including audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.



Read more about the integration by this [link](/documentation/install/#docarray)
---

title: DocArray

weight: 300

aliases: [ ../integrations/docarray/ ]

---



# DocArray

You can use Qdrant natively in DocArray, where Qdrant serves as a high-performance document store to enable scalable vector search.



DocArray is a library from Jina AI for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc.

It allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API.
To install DocArray with Qdrant support, please do



```bash

pip install ""docarray[qdrant]""

```



More information can be found in [DocArray's documentations](https://docarray.jina.ai/advanced/document-store/qdrant/).
date: 2022-03-15T15:00:00+03:00

author: Alyona Kavyerina

featured: false

author_link: https://medium.com/@alyona.kavyerina

tags:

- jina integration

- docarray

categories:

- News

---

We are happy to announce that [Jina.AI](https://jina.ai/) integrates Qdrant engine as a storage backend to their [DocArray](https://docarray.jina.ai/) solution.



Now you can experience the convenience of Pythonic API and Rust performance in a single workflow.
---

draft: false

preview_image: /blog/from_cms/docarray.png

sitemapExclude: true

title: ""Qdrant and Jina integration: storage backend support for DocArray""

slug: qdrant-and-jina-integration

short_description: ""One more way to use Qdrant: Jina's DocArray is now

supporting Qdrant as a storage backend.""

description: We are happy to announce that Jina.AI integrates Qdrant engine as a

storage backend to their DocArray solution.

date: 2022-03-15T15:00:00+03:00

author: Alyona Kavyerina\n\nAnswer:",What is the purpose of the DocArray library and how does it relate to the Qdrant engine?,"The DocArray library is designed to define a structure for unstructured data, making it easier to process a collection of documents that may include various data types such as audio, video, and text. This library simplifies the handling of these diverse types of data within a single framework. On the other hand, the Qdrant engine focuses on enabling the scaling of vector search and storage capabilities. By integrating the DocArray library with the Qdrant engine, users can benefit from efficient processing of unstructured data and enhanced scalability for vector search operations. This integration streamlines the handling of different data types while also optimizing the performance of vector search and storage functions.","['DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,\n\nincluding audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.\n\n\n\nRead more about the integration by this [link](/documentation/install/#docarray)', '---\n\ntitle: DocArray\n\nweight: 300\n\naliases: [ ../integrations/docarray/ ]\n\n---\n\n\n\n# DocArray\n\nYou can use Qdrant natively in DocArray, where Qdrant serves as a high-performance document store to enable scalable vector search.\n\n\n\nDocArray is a library from Jina AI for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc.\n\nIt allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API.', 'To install DocArray with Qdrant support, please do\n\n\n\n```bash\n\npip install ""docarray[qdrant]""\n\n```\n\n\n\nMore information can be found in [DocArray\'s documentations](https://docarray.jina.ai/advanced/document-store/qdrant/).', 'date: 2022-03-15T15:00:00+03:00\n\nauthor: Alyona Kavyerina\n\nfeatured: false\n\nauthor_link: https://medium.com/@alyona.kavyerina\n\ntags:\n\n  - jina integration\n\n  - docarray\n\ncategories:\n\n  - News\n\n---\n\nWe are happy to announce that [Jina.AI](https://jina.ai/) integrates Qdrant engine as a storage backend to their [DocArray](https://docarray.jina.ai/) solution.\n\n\n\nNow you can experience the convenience of Pythonic API and Rust performance in a single workflow.', '---\n\ndraft: false\n\npreview_image: /blog/from_cms/docarray.png\n\nsitemapExclude: true\n\ntitle: ""Qdrant and Jina integration: storage backend support for DocArray""\n\nslug: qdrant-and-jina-integration\n\nshort_description: ""One more way to use Qdrant: Jina\'s DocArray is now\n\n  supporting Qdrant as a storage backend.""\n\ndescription: We are happy to announce that Jina.AI integrates Qdrant engine as a\n\n  storage backend to their DocArray solution.\n\ndate: 2022-03-15T15:00:00+03:00\n\nauthor: Alyona Kavyerina']","DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,

including audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.



Read more about the integration by this [link](/documentation/install/#docarray)
---

title: DocArray

weight: 300

aliases: [ ../integrations/docarray/ ]

---



# DocArray

You can use Qdrant natively in DocArray, where Qdrant serves as a high-performance document store to enable scalable vector search.



DocArray is a library from Jina AI for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc.

It allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API.
To install DocArray with Qdrant support, please do



```bash

pip install ""docarray[qdrant]""

```



More information can be found in [DocArray's documentations](https://docarray.jina.ai/advanced/document-store/qdrant/).
date: 2022-03-15T15:00:00+03:00

author: Alyona Kavyerina

featured: false

author_link: https://medium.com/@alyona.kavyerina

tags:

  - jina integration

  - docarray

categories:

  - News

---

We are happy to announce that [Jina.AI](https://jina.ai/) integrates Qdrant engine as a storage backend to their [DocArray](https://docarray.jina.ai/) solution.



Now you can experience the convenience of Pythonic API and Rust performance in a single workflow.
---

draft: false

preview_image: /blog/from_cms/docarray.png

sitemapExclude: true

title: ""Qdrant and Jina integration: storage backend support for DocArray""

slug: qdrant-and-jina-integration

short_description: ""One more way to use Qdrant: Jina's DocArray is now

  supporting Qdrant as a storage backend.""

description: We are happy to announce that Jina.AI integrates Qdrant engine as a

  storage backend to their DocArray solution.

date: 2022-03-15T15:00:00+03:00

author: Alyona Kavyerina",0.5121951219512195,0.6363636363636364,0.5675675675675675,0.19672131147540983,0.24489795918367346,0.21818181818181817,0.3170731707317073,0.3939393939393939,0.35135135135135137,0.4715447154471545,0.5858585858585859,0.5225225225225226,0.8248754739761353,0.6962175965309143,0.7169302701950073,0.7064220905303955,0.9993051693842668,0.9929982417641441,136,1.2252252252252251,0.67738642015352
True,,3616.1620000000003,1d8e9403,What are the search-time parameters that can be used to tune the search accuracy and speed?,"The search-time parameters that can be used to tune the search accuracy and speed are ""hnsw_ef"" and ""exact"". The ""hnsw_ef"" parameter is set to 128, which controls the search accuracy by specifying the number of neighbors to inspect during the search process. The ""exact"" parameter is set to false, indicating that an approximate search method is used for faster retrieval of results. These parameters are essential for optimizing the search process in the context of the QdrantClient for efficient retrieval of relevant data points.","Some of the search-time parameters that can be used to tune the search accuracy and speed include:

1. HNSW Parameters: HNSW (Hierarchical Navigable Small World) is a search algorithm that is known for its accuracy and speed. You can configure HNSW parameters on a collection and named vector level to fine-tune search performance. These parameters include:
   - `hnsw_ef`: This parameter helps balance speed versus accuracy. Setting it to a higher value can lead to more accurate results but might affect speed.
   - `exact`: When set to `true`, it performs exact search, which is slower but more accurate.

2. Latency vs Throughput: When measuring the speed of search, you can consider:
   - Latency of the request: The time taken from the moment the request is submitted to the moment a response is received.
   - Throughput: The number of requests per second the system can handle.

By adjusting these parameters, you can control the trade-off between speed and accuracy in search operations without needing to rebuild the index or make changes to the underlying data structures.","collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },

	optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 },

	quantizationConfig: new QuantizationConfig

	{

		Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }

	}

);

```



There are also some search-time parameters you can use to tune the search accuracy and speed:



```http

POST /collections/{collection_name}/points/search

{
HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
These two parameters are how you are going to balance speed versus accuracy. The larger the size of your oversample, the more items you need to read from disk and the more elements you have to search with the relatively slower full vector index. On the other hand, doing this will produce more accurate results.
- `exact` - if set to `true`, will perform exact search, which will be slower, but more accurate. You can use it to compare results of the search with different `hnsw_ef` values versus the ground truth.



## Latency vs Throughput



- There are two main approaches to measure the speed of search:

  - latency of the request - the time from the moment request is submitted to the moment a response is received

  - throughput - the number of requests per second the system can handle
. And that's how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself.","['collectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },\n\n\toptimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 },\n\n\tquantizationConfig: new QuantizationConfig\n\n\t{\n\n\t\tScalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }\n\n\t}\n\n);\n\n```\n\n\n\nThere are also some search-time parameters you can use to tune the search accuracy and speed:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{', 'HNSW is chosen for several reasons.\n\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\n\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\n\n\n\n*Available as of v1.1.1*\n\n\n\nThe HNSW parameters can also be configured on a collection and named vector\n\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\n\nperformance.', 'These two parameters are how you are going to balance speed versus accuracy. The larger the size of your oversample, the more items you need to read from disk and the more elements you have to search with the relatively slower full vector index. On the other hand, doing this will produce more accurate results.', '- `exact` - if set to `true`, will perform exact search, which will be slower, but more accurate. You can use it to compare results of the search with different `hnsw_ef` values versus the ground truth.\n\n\n\n## Latency vs Throughput\n\n\n\n- There are two main approaches to measure the speed of search:\n\n  - latency of the request - the time from the moment request is submitted to the moment a response is received\n\n  - throughput - the number of requests per second the system can handle', "". And that's how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself.""]","Question: What are the search-time parameters that can be used to tune the search accuracy and speed?\n\nContext: collectionName: ""{collection_name}"",

vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },

optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 },

quantizationConfig: new QuantizationConfig

{

Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }

}

);

```



There are also some search-time parameters you can use to tune the search accuracy and speed:



```http

POST /collections/{collection_name}/points/search

{
HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
These two parameters are how you are going to balance speed versus accuracy. The larger the size of your oversample, the more items you need to read from disk and the more elements you have to search with the relatively slower full vector index. On the other hand, doing this will produce more accurate results.
- `exact` - if set to `true`, will perform exact search, which will be slower, but more accurate. You can use it to compare results of the search with different `hnsw_ef` values versus the ground truth.



## Latency vs Throughput



- There are two main approaches to measure the speed of search:

- latency of the request - the time from the moment request is submitted to the moment a response is received

- throughput - the number of requests per second the system can handle
. And that's how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself.\n\nAnswer:",What are the search-time parameters that can be used to tune the search accuracy and speed?,"The search-time parameters that can be used to tune the search accuracy and speed are ""hnsw_ef"" and ""exact"". The ""hnsw_ef"" parameter is set to 128, which controls the search accuracy by specifying the number of neighbors to inspect during the search process. The ""exact"" parameter is set to false, indicating that an approximate search method is used for faster retrieval of results. These parameters are essential for optimizing the search process in the context of the QdrantClient for efficient retrieval of relevant data points.","['collectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },\n\n\toptimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 },\n\n\tquantizationConfig: new QuantizationConfig\n\n\t{\n\n\t\tScalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }\n\n\t}\n\n);\n\n```\n\n\n\nThere are also some search-time parameters you can use to tune the search accuracy and speed:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{', 'HNSW is chosen for several reasons.\n\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\n\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\n\n\n\n*Available as of v1.1.1*\n\n\n\nThe HNSW parameters can also be configured on a collection and named vector\n\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\n\nperformance.', 'These two parameters are how you are going to balance speed versus accuracy. The larger the size of your oversample, the more items you need to read from disk and the more elements you have to search with the relatively slower full vector index. On the other hand, doing this will produce more accurate results.', '- `exact` - if set to `true`, will perform exact search, which will be slower, but more accurate. You can use it to compare results of the search with different `hnsw_ef` values versus the ground truth.\n\n\n\n## Latency vs Throughput\n\n\n\n- There are two main approaches to measure the speed of search:\n\n  - latency of the request - the time from the moment request is submitted to the moment a response is received\n\n  - throughput - the number of requests per second the system can handle', "". And that's how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself.""]","collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },

	optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 },

	quantizationConfig: new QuantizationConfig

	{

		Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }

	}

);

```



There are also some search-time parameters you can use to tune the search accuracy and speed:



```http

POST /collections/{collection_name}/points/search

{
HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
These two parameters are how you are going to balance speed versus accuracy. The larger the size of your oversample, the more items you need to read from disk and the more elements you have to search with the relatively slower full vector index. On the other hand, doing this will produce more accurate results.
- `exact` - if set to `true`, will perform exact search, which will be slower, but more accurate. You can use it to compare results of the search with different `hnsw_ef` values versus the ground truth.



## Latency vs Throughput



- There are two main approaches to measure the speed of search:

  - latency of the request - the time from the moment request is submitted to the moment a response is received

  - throughput - the number of requests per second the system can handle
. And that's how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself.",0.31210191082802546,0.6447368421052632,0.42060085836909866,0.10256410256410256,0.21333333333333335,0.13852813852813853,0.19745222929936307,0.40789473684210525,0.2660944206008584,0.2802547770700637,0.5789473684210527,0.3776824034334764,0.7001360058784485,0.6024681925773621,0.6472390294075012,0.6240516304969788,0.9738304992897611,0.5434810542501509,170,2.0238095238095237,0.44403451430889174
True,,3604.387,33eb1bc5,What is the significance of using named vectors in a collection within the Qdrant system?,"Named vectors in a collection within the Qdrant system allow for the inclusion of multiple vectors in a single point, with each vector having its own dimensionality and metric requirements. This feature enables more flexibility in organizing and structuring data within a collection, as different vectors can represent distinct aspects or features of the data points. By utilizing named vectors, users can better tailor the representation of their data to suit specific analysis or search requirements, enhancing the overall efficiency and effectiveness of the system.","Using named vectors in a collection within the Qdrant system holds significant importance for data consistency and accurate search operations. By assigning names to vectors, users can clearly distinguish between different types of embeddings associated with each object. This ensures that during search operations, the engine knows which specific vector to utilize for comparison and retrieval.

Named vectors enable users to provide multiple vectors for each object, such as image and text embeddings, and specify the vector name along with the embedding during search operations. This streamlined approach simplifies the search process and enhances the efficiency of querying specific vectors within a collection.

In the context of creating a collection in Qdrant, defining the configuration of all vectors with their respective names and distance functions is crucial for organizing and managing the data effectively. By incorporating named vectors into the collection creation process, users can establish a structured framework for storing and retrieving various types of embeddings associated with objects.

Ultimately, the utilization of named vectors in a collection enhances the flexibility, accuracy, and performance of search operations within the Qdrant system, providing a robust solution for managing multi-dimensional data efficiently.","Even with such a system, Qdrant ensures data consistency.



[*Available as of v0.10.0*](#create-vector-name)



If the collection was created with multiple vectors, each vector data can be provided using the vector's name:



```http

PUT /collections/{collection_name}/points

{

    ""points"": [

        {

            ""id"": 1,

            ""vector"": {

                ""image"": [0.9, 0.1, 0.1, 0.2],

                ""text"": [0.4, 0.7, 0.1, 0.8, 0.1, 0.1, 0.9, 0.2]

            }

        },

        {
## Searching with multiple vectors



If you decided to describe each object with several neural embeddings, then at each search operation you need to provide the vector name along with the embedding, so the engine knows which one to use. The interface of the search operation is pretty straightforward and requires an instance of NamedVector.



```python

from qdrant_client.http.models import NamedVector



text_results = client.search(

   collection_name=""ms-coco-2017"",

   query_vector=NamedVector(
```



## Creating a collection



Adding new functionalities typically requires making some changes to the interfaces, so no surprise we had to do it to enable the multiple vectors support. Currently, if you want to create a collection, you need to define the configuration of all the vectors you want to store for each object. Each vector type has its own name and the distance function used to measure how far the points are.



```python

from qdrant_client import QdrantClient
collection_name=""MyCollection"",

    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),

)





qdrant_client.upsert(

    collection_name=""MyCollection"",

    points=Batch(

        ids=list(range(len(embeddings))),

        vectors=embeddings,

    ),

)



```
query_vector=NamedVector(

       name=""text"",

       vector=row[""text_vector""],

   ),

   limit=5,

   with_vectors=False,

   with_payload=True,

)

```



If we, on the other hand, decided to search using the image embedding, then we just provide the vector name we have chosen while creating the collection, so instead of “text”, we would provide “image”, as this is how we configured it at the very beginning.



## The results: image vs text search","['Even with such a system, Qdrant ensures data consistency.\n\n\n\n[*Available as of v0.10.0*](#create-vector-name)\n\n\n\nIf the collection was created with multiple vectors, each vector data can be provided using the vector\'s name:\n\n\n\n```http\n\nPUT /collections/{collection_name}/points\n\n{\n\n    ""points"": [\n\n        {\n\n            ""id"": 1,\n\n            ""vector"": {\n\n                ""image"": [0.9, 0.1, 0.1, 0.2],\n\n                ""text"": [0.4, 0.7, 0.1, 0.8, 0.1, 0.1, 0.9, 0.2]\n\n            }\n\n        },\n\n        {', '## Searching with multiple vectors\n\n\n\nIf you decided to describe each object with several neural embeddings, then at each search operation you need to provide the vector name along with the embedding, so the engine knows which one to use. The interface of the search operation is pretty straightforward and requires an instance of NamedVector.\n\n\n\n```python\n\nfrom qdrant_client.http.models import NamedVector\n\n\n\ntext_results = client.search(\n\n   collection_name=""ms-coco-2017"",\n\n   query_vector=NamedVector(', '```\n\n\n\n## Creating a collection\n\n\n\nAdding new functionalities typically requires making some changes to the interfaces, so no surprise we had to do it to enable the multiple vectors support. Currently, if you want to create a collection, you need to define the configuration of all the vectors you want to store for each object. Each vector type has its own name and the distance function used to measure how far the points are.\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient', 'collection_name=""MyCollection"",\n\n    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),\n\n)\n\n\n\n\n\nqdrant_client.upsert(\n\n    collection_name=""MyCollection"",\n\n    points=Batch(\n\n        ids=list(range(len(embeddings))),\n\n        vectors=embeddings,\n\n    ),\n\n)\n\n\n\n```', 'query_vector=NamedVector(\n\n       name=""text"",\n\n       vector=row[""text_vector""],\n\n   ),\n\n   limit=5,\n\n   with_vectors=False,\n\n   with_payload=True,\n\n)\n\n```\n\n\n\nIf we, on the other hand, decided to search using the image embedding, then we just provide the vector name we have chosen while creating the collection, so instead of “text”, we would provide “image”, as this is how we configured it at the very beginning.\n\n\n\n## The results: image vs text search']","Question: What is the significance of using named vectors in a collection within the Qdrant system?\n\nContext: Even with such a system, Qdrant ensures data consistency.



[*Available as of v0.10.0*](#create-vector-name)



If the collection was created with multiple vectors, each vector data can be provided using the vector's name:



```http

PUT /collections/{collection_name}/points

{

""points"": [

{

""id"": 1,

""vector"": {

""image"": [0.9, 0.1, 0.1, 0.2],

""text"": [0.4, 0.7, 0.1, 0.8, 0.1, 0.1, 0.9, 0.2]

}

},

{
## Searching with multiple vectors



If you decided to describe each object with several neural embeddings, then at each search operation you need to provide the vector name along with the embedding, so the engine knows which one to use. The interface of the search operation is pretty straightforward and requires an instance of NamedVector.



```python

from qdrant_client.http.models import NamedVector



text_results = client.search(

collection_name=""ms-coco-2017"",

query_vector=NamedVector(
```



## Creating a collection



Adding new functionalities typically requires making some changes to the interfaces, so no surprise we had to do it to enable the multiple vectors support. Currently, if you want to create a collection, you need to define the configuration of all the vectors you want to store for each object. Each vector type has its own name and the distance function used to measure how far the points are.



```python

from qdrant_client import QdrantClient
collection_name=""MyCollection"",

vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),

)





qdrant_client.upsert(

collection_name=""MyCollection"",

points=Batch(

ids=list(range(len(embeddings))),

vectors=embeddings,

),

)



```
query_vector=NamedVector(

name=""text"",

vector=row[""text_vector""],

),

limit=5,

with_vectors=False,

with_payload=True,

)

```



If we, on the other hand, decided to search using the image embedding, then we just provide the vector name we have chosen while creating the collection, so instead of “text”, we would provide “image”, as this is how we configured it at the very beginning.



## The results: image vs text search\n\nAnswer:",What is the significance of using named vectors in a collection within the Qdrant system?,"Named vectors in a collection within the Qdrant system allow for the inclusion of multiple vectors in a single point, with each vector having its own dimensionality and metric requirements. This feature enables more flexibility in organizing and structuring data within a collection, as different vectors can represent distinct aspects or features of the data points. By utilizing named vectors, users can better tailor the representation of their data to suit specific analysis or search requirements, enhancing the overall efficiency and effectiveness of the system.","['Even with such a system, Qdrant ensures data consistency.\n\n\n\n[*Available as of v0.10.0*](#create-vector-name)\n\n\n\nIf the collection was created with multiple vectors, each vector data can be provided using the vector\'s name:\n\n\n\n```http\n\nPUT /collections/{collection_name}/points\n\n{\n\n    ""points"": [\n\n        {\n\n            ""id"": 1,\n\n            ""vector"": {\n\n                ""image"": [0.9, 0.1, 0.1, 0.2],\n\n                ""text"": [0.4, 0.7, 0.1, 0.8, 0.1, 0.1, 0.9, 0.2]\n\n            }\n\n        },\n\n        {', '## Searching with multiple vectors\n\n\n\nIf you decided to describe each object with several neural embeddings, then at each search operation you need to provide the vector name along with the embedding, so the engine knows which one to use. The interface of the search operation is pretty straightforward and requires an instance of NamedVector.\n\n\n\n```python\n\nfrom qdrant_client.http.models import NamedVector\n\n\n\ntext_results = client.search(\n\n   collection_name=""ms-coco-2017"",\n\n   query_vector=NamedVector(', '```\n\n\n\n## Creating a collection\n\n\n\nAdding new functionalities typically requires making some changes to the interfaces, so no surprise we had to do it to enable the multiple vectors support. Currently, if you want to create a collection, you need to define the configuration of all the vectors you want to store for each object. Each vector type has its own name and the distance function used to measure how far the points are.\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient', 'collection_name=""MyCollection"",\n\n    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),\n\n)\n\n\n\n\n\nqdrant_client.upsert(\n\n    collection_name=""MyCollection"",\n\n    points=Batch(\n\n        ids=list(range(len(embeddings))),\n\n        vectors=embeddings,\n\n    ),\n\n)\n\n\n\n```', 'query_vector=NamedVector(\n\n       name=""text"",\n\n       vector=row[""text_vector""],\n\n   ),\n\n   limit=5,\n\n   with_vectors=False,\n\n   with_payload=True,\n\n)\n\n```\n\n\n\nIf we, on the other hand, decided to search using the image embedding, then we just provide the vector name we have chosen while creating the collection, so instead of “text”, we would provide “image”, as this is how we configured it at the very beginning.\n\n\n\n## The results: image vs text search']","Even with such a system, Qdrant ensures data consistency.



[*Available as of v0.10.0*](#create-vector-name)



If the collection was created with multiple vectors, each vector data can be provided using the vector's name:



```http

PUT /collections/{collection_name}/points

{

    ""points"": [

        {

            ""id"": 1,

            ""vector"": {

                ""image"": [0.9, 0.1, 0.1, 0.2],

                ""text"": [0.4, 0.7, 0.1, 0.8, 0.1, 0.1, 0.9, 0.2]

            }

        },

        {
## Searching with multiple vectors



If you decided to describe each object with several neural embeddings, then at each search operation you need to provide the vector name along with the embedding, so the engine knows which one to use. The interface of the search operation is pretty straightforward and requires an instance of NamedVector.



```python

from qdrant_client.http.models import NamedVector



text_results = client.search(

   collection_name=""ms-coco-2017"",

   query_vector=NamedVector(
```



## Creating a collection



Adding new functionalities typically requires making some changes to the interfaces, so no surprise we had to do it to enable the multiple vectors support. Currently, if you want to create a collection, you need to define the configuration of all the vectors you want to store for each object. Each vector type has its own name and the distance function used to measure how far the points are.



```python

from qdrant_client import QdrantClient
collection_name=""MyCollection"",

    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),

)





qdrant_client.upsert(

    collection_name=""MyCollection"",

    points=Batch(

        ids=list(range(len(embeddings))),

        vectors=embeddings,

    ),

)



```
query_vector=NamedVector(

       name=""text"",

       vector=row[""text_vector""],

   ),

   limit=5,

   with_vectors=False,

   with_payload=True,

)

```



If we, on the other hand, decided to search using the image embedding, then we just provide the vector name we have chosen while creating the collection, so instead of “text”, we would provide “image”, as this is how we configured it at the very beginning.



## The results: image vs text search",0.2947976878612717,0.6710526315789473,0.40963855421686746,0.08139534883720931,0.18666666666666668,0.11336032388663968,0.1676300578034682,0.3815789473684211,0.23293172690763048,0.2658959537572254,0.6052631578947368,0.36947791164658633,0.7774330973625183,0.6261451840400696,0.6679121851921082,0.6463546752929688,0.956427478173282,0.9393330638607343,191,2.2470588235294118,0.24652026133074434
True,,6283.813999999999,0bb922bc,What parameters can be configured in the configuration file to improve performance in HNSW indexing?,"In the context of HNSW indexing, the parameters that can be configured in the configuration file to enhance performance are `m`, `ef_construct`, and `ef`. 

- `m`: This parameter represents the number of edges per node in the index graph. A higher value for `m` leads to more accurate search results but also requires more space. By adjusting `m`, you can control the trade-off between search accuracy and space efficiency.

- `ef_construct`: When building the index, `ef_construct` specifies the number of neighbors to consider. Increasing the value of `ef_construct` improves the accuracy of the search but also increases the time required to build the index. 

- `ef`: This parameter is used when searching for targets. It determines the search range for finding nearest neighbors. By setting an appropriate value for `ef`, you can optimize the search process to balance efficiency and accuracy in finding the nearest neighbors within the specified search range.","To improve performance in HNSW indexing, the following parameters can be configured in the configuration file:

1. `m`: This parameter limits the maximum degree of nodes on each layer of the graph. Increasing `m` can lead to a more accurate search but may require more memory.

2. `ef_construct`: This parameter specifies the search range when building the index. Increasing `ef_construct` can improve search performance but may also increase indexing time.

3. `ef`: This parameter specifies the search range when searching for targets. Adjusting `ef` can impact the trade-off between search accuracy and speed.

4. `on_disk`: This parameter specifies whether the index should be stored on disk. Storing the index on disk can reduce memory usage but may impact search speed due to disk IOPS.

5. `on_disk_payload`: This parameter specifies whether payload data should be stored on disk. This can also affect memory usage and search performance.

6. `full_scan_threshold`: This parameter determines the threshold at which full scans are performed. Adjusting this can impact search performance for certain queries.

7. `max_indexing_threads`: This parameter controls the number of threads used during indexing. Increasing this value can speed up indexing but may consume more system resources.

By tweaking these parameters according to your specific requirements and system capabilities, you can optimize the performance of HNSW indexing for your application.","In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.



The corresponding parameters could be configured in the configuration file:



```yaml

storage:

  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually

  hnsw_index:

    # Number of edges per node in the index graph.
HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
hnswConfig: new HnswConfigDiff { OnDisk = true }

);

```



In this scenario you can increase the precision of the search by increasing the `ef` and `m` parameters of the HNSW index, even with limited RAM.



```json

...

""hnsw_config"": {

    ""m"": 64,

    ""ef_construct"": 512,

    ""on_disk"": true

}

...

```



The disk IOPS is a critical factor in this scenario, it will determine how fast you can perform search.
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
queries will be executed on the whole collection, we can also change its configuration, so the HNSW 

graph is not built globally. This is also done due to [performance reasons](/documentation/guides/multiple-partitions/#calibrate-performance).

**You should not be changing these parameters, if you know there will be some global search operations

done on the collection.**



```python

client.update_collection(

    collection_name=""my_collection"",","['In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.\n\n\n\nThe corresponding parameters could be configured in the configuration file:\n\n\n\n```yaml\n\nstorage:\n\n  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually\n\n  hnsw_index:\n\n    # Number of edges per node in the index graph.', 'HNSW is chosen for several reasons.\n\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\n\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\n\n\n\n*Available as of v1.1.1*\n\n\n\nThe HNSW parameters can also be configured on a collection and named vector\n\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\n\nperformance.', 'hnswConfig: new HnswConfigDiff { OnDisk = true }\n\n);\n\n```\n\n\n\nIn this scenario you can increase the precision of the search by increasing the `ef` and `m` parameters of the HNSW index, even with limited RAM.\n\n\n\n```json\n\n...\n\n""hnsw_config"": {\n\n    ""m"": 64,\n\n    ""ef_construct"": 512,\n\n    ""on_disk"": true\n\n}\n\n...\n\n```\n\n\n\nThe disk IOPS is a critical factor in this scenario, it will determine how fast you can perform search.', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', 'queries will be executed on the whole collection, we can also change its configuration, so the HNSW \n\ngraph is not built globally. This is also done due to [performance reasons](/documentation/guides/multiple-partitions/#calibrate-performance).\n\n**You should not be changing these parameters, if you know there will be some global search operations\n\ndone on the collection.**\n\n\n\n```python\n\nclient.update_collection(\n\n    collection_name=""my_collection"",']","Question: What parameters can be configured in the configuration file to improve performance in HNSW indexing?\n\nContext: In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.



The corresponding parameters could be configured in the configuration file:



```yaml

storage:

# Default parameters of HNSW Index. Could be overridden for each collection or named vector individually

hnsw_index:

# Number of edges per node in the index graph.
HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
hnswConfig: new HnswConfigDiff { OnDisk = true }

);

```



In this scenario you can increase the precision of the search by increasing the `ef` and `m` parameters of the HNSW index, even with limited RAM.



```json

...

""hnsw_config"": {

""m"": 64,

""ef_construct"": 512,

""on_disk"": true

}

...

```



The disk IOPS is a critical factor in this scenario, it will determine how fast you can perform search.
""on_disk_payload"": false

},

""hnsw_config"": {

""m"": 16,

""ef_construct"": 100,

""full_scan_threshold"": 10000,

""max_indexing_threads"": 0

},

""optimizer_config"": {

""deleted_threshold"": 0.2,

""vacuum_min_vector_number"": 1000,

""default_segment_number"": 0,

""max_segment_size"": null,
queries will be executed on the whole collection, we can also change its configuration, so the HNSW 

graph is not built globally. This is also done due to [performance reasons](/documentation/guides/multiple-partitions/#calibrate-performance).

**You should not be changing these parameters, if you know there will be some global search operations

done on the collection.**



```python

client.update_collection(

collection_name=""my_collection"",\n\nAnswer:",What parameters can be configured in the configuration file to improve performance in HNSW indexing?,"In the context of HNSW indexing, the parameters that can be configured in the configuration file to enhance performance are `m`, `ef_construct`, and `ef`. 

- `m`: This parameter represents the number of edges per node in the index graph. A higher value for `m` leads to more accurate search results but also requires more space. By adjusting `m`, you can control the trade-off between search accuracy and space efficiency.

- `ef_construct`: When building the index, `ef_construct` specifies the number of neighbors to consider. Increasing the value of `ef_construct` improves the accuracy of the search but also increases the time required to build the index. 

- `ef`: This parameter is used when searching for targets. It determines the search range for finding nearest neighbors. By setting an appropriate value for `ef`, you can optimize the search process to balance efficiency and accuracy in finding the nearest neighbors within the specified search range.","['In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.\n\n\n\nThe corresponding parameters could be configured in the configuration file:\n\n\n\n```yaml\n\nstorage:\n\n  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually\n\n  hnsw_index:\n\n    # Number of edges per node in the index graph.', 'HNSW is chosen for several reasons.\n\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\n\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\n\n\n\n*Available as of v1.1.1*\n\n\n\nThe HNSW parameters can also be configured on a collection and named vector\n\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\n\nperformance.', 'hnswConfig: new HnswConfigDiff { OnDisk = true }\n\n);\n\n```\n\n\n\nIn this scenario you can increase the precision of the search by increasing the `ef` and `m` parameters of the HNSW index, even with limited RAM.\n\n\n\n```json\n\n...\n\n""hnsw_config"": {\n\n    ""m"": 64,\n\n    ""ef_construct"": 512,\n\n    ""on_disk"": true\n\n}\n\n...\n\n```\n\n\n\nThe disk IOPS is a critical factor in this scenario, it will determine how fast you can perform search.', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', 'queries will be executed on the whole collection, we can also change its configuration, so the HNSW \n\ngraph is not built globally. This is also done due to [performance reasons](/documentation/guides/multiple-partitions/#calibrate-performance).\n\n**You should not be changing these parameters, if you know there will be some global search operations\n\ndone on the collection.**\n\n\n\n```python\n\nclient.update_collection(\n\n    collection_name=""my_collection"",']","In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.



The corresponding parameters could be configured in the configuration file:



```yaml

storage:

  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually

  hnsw_index:

    # Number of edges per node in the index graph.
HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
hnswConfig: new HnswConfigDiff { OnDisk = true }

);

```



In this scenario you can increase the precision of the search by increasing the `ef` and `m` parameters of the HNSW index, even with limited RAM.



```json

...

""hnsw_config"": {

    ""m"": 64,

    ""ef_construct"": 512,

    ""on_disk"": true

}

...

```



The disk IOPS is a critical factor in this scenario, it will determine how fast you can perform search.
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
queries will be executed on the whole collection, we can also change its configuration, so the HNSW 

graph is not built globally. This is also done due to [performance reasons](/documentation/guides/multiple-partitions/#calibrate-performance).

**You should not be changing these parameters, if you know there will be some global search operations

done on the collection.**



```python

client.update_collection(

    collection_name=""my_collection"",",0.41037735849056606,0.6492537313432836,0.5028901734104047,0.17061611374407584,0.2706766917293233,0.2093023255813954,0.25,0.39552238805970147,0.3063583815028902,0.4056603773584906,0.6417910447761194,0.4971098265895953,0.9066154956817627,0.6422455310821533,0.6934850811958313,0.6668825149536133,0.4979013381102546,0.5137543678283691,216,1.4594594594594594,0.3738907175587728
True,,4226.733,6a313c7d,How does product quantization differ from scalar quantization in terms of compression and computational efficiency?,"Product quantization involves dividing data into chunks and quantizing each segment individually, approximating each chunk with a centroid index that represents the original vector component. The positions of these centroids are determined through clustering algorithms like k-means, with Qdrant currently utilizing 256 centroids, allowing each centroid index to be represented by a single byte. Product quantization can achieve higher compression factors compared to scalar quantization. However, there are tradeoffs to consider. Product quantization distance calculations are not SIMD-friendly, resulting in slower computational speed compared to scalar quantization. Additionally, product quantization introduces a loss of accuracy, making it more suitable for high-dimensional vectors. To optimize quantization parameters for specific use cases, it is recommended to refer to the ""Quantization Tips"" section for more information.","Product quantization differs from scalar quantization in terms of compression and computational efficiency in the following ways:

1. **Compression Ratio:** Product quantization generally provides a better compression ratio compared to scalar quantization. This means that product quantization can achieve higher levels of data compression, which is beneficial for reducing memory usage.

2. **Accuracy:** Product quantization often results in a significant loss of accuracy compared to scalar quantization. The trade-off for higher compression is a reduction in the precision of the search results. Scalar quantization, on the other hand, may preserve more of the original data's accuracy but at the cost of lower compression.

3. **Computational Efficiency:** Scalar quantization is typically faster than product quantization in terms of computational efficiency. Scalar quantization processes data more quickly, making it suitable for scenarios where search speed is a critical factor. Product quantization, on the other hand, is slower due to the complexity of the quantization process.

4. **Memory Usage:** Product quantization is recommended when memory footprint is the top priority, as it offers better compression and thus reduces memory usage. Scalar quantization may not be as efficient in terms of memory compression compared to product quantization.

In summary, product quantization is preferred for scenarios where memory optimization is crucial, even though it may result in lower accuracy and slower computation compared to scalar quantization, which may be faster but less memory-efficient.","* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.



## Setting up Quantization in Qdrant



You can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.
<td>474 s</td>

      </tr>

   </tbody>

</table>



It turns out that in some cases, Product Quantization may not only reduce the memory usage, 

but also the search time.



## Good practices



Compared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.



Product Quantization tends to be favored in certain specific scenarios:
product=models.ProductQuantizationConfig(

                    compression=models.CompressionRatio.X32,

                    always_ram=True,

                ),

            ),

            on_disk=True,

        ),

    },

    hnsw_config=models.HnswConfigDiff(

        ef_construct=123,

    ),

    quantization_config=models.ScalarQuantization(

        scalar=models.ScalarQuantizationConfig(

            type=models.ScalarType.INT8,

            quantile=0.8,
. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
<td>147 s</td>

         <td>339 s</td>

         <td>217 s</td>

         <td>178 s</td>

      </tr>

   </tbody>

</table>



Product Quantization increases both indexing and searching time. The higher the compression ratio, 

the lower the search precision. The main benefit is undoubtedly the reduced usage of memory.



#### Arxiv-titles-384-angular-no-filters



<table>

   <thead>

      <tr>

         <th></th>

         <th>Original</th>

         <th>1D clusters</th>","['* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.\n\n\n\n## Setting up Quantization in Qdrant\n\n\n\nYou can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.', '<td>474 s</td>\n\n      </tr>\n\n   </tbody>\n\n</table>\n\n\n\nIt turns out that in some cases, Product Quantization may not only reduce the memory usage, \n\nbut also the search time.\n\n\n\n## Good practices\n\n\n\nCompared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\n\n\n\nProduct Quantization tends to be favored in certain specific scenarios:', 'product=models.ProductQuantizationConfig(\n\n                    compression=models.CompressionRatio.X32,\n\n                    always_ram=True,\n\n                ),\n\n            ),\n\n            on_disk=True,\n\n        ),\n\n    },\n\n    hnsw_config=models.HnswConfigDiff(\n\n        ef_construct=123,\n\n    ),\n\n    quantization_config=models.ScalarQuantization(\n\n        scalar=models.ScalarQuantizationConfig(\n\n            type=models.ScalarType.INT8,\n\n            quantile=0.8,', '. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.', '<td>147 s</td>\n\n         <td>339 s</td>\n\n         <td>217 s</td>\n\n         <td>178 s</td>\n\n      </tr>\n\n   </tbody>\n\n</table>\n\n\n\nProduct Quantization increases both indexing and searching time. The higher the compression ratio, \n\nthe lower the search precision. The main benefit is undoubtedly the reduced usage of memory.\n\n\n\n#### Arxiv-titles-384-angular-no-filters\n\n\n\n<table>\n\n   <thead>\n\n      <tr>\n\n         <th></th>\n\n         <th>Original</th>\n\n         <th>1D clusters</th>']","Question: How does product quantization differ from scalar quantization in terms of compression and computational efficiency?\n\nContext: * **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.



## Setting up Quantization in Qdrant



You can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.
<td>474 s</td>

</tr>

</tbody>

</table>



It turns out that in some cases, Product Quantization may not only reduce the memory usage, 

but also the search time.



## Good practices



Compared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.



Product Quantization tends to be favored in certain specific scenarios:
product=models.ProductQuantizationConfig(

compression=models.CompressionRatio.X32,

always_ram=True,

),

),

on_disk=True,

),

},

hnsw_config=models.HnswConfigDiff(

ef_construct=123,

),

quantization_config=models.ScalarQuantization(

scalar=models.ScalarQuantizationConfig(

type=models.ScalarType.INT8,

quantile=0.8,
. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
<td>147 s</td>

<td>339 s</td>

<td>217 s</td>

<td>178 s</td>

</tr>

</tbody>

</table>



Product Quantization increases both indexing and searching time. The higher the compression ratio, 

the lower the search precision. The main benefit is undoubtedly the reduced usage of memory.



#### Arxiv-titles-384-angular-no-filters



<table>

<thead>

<tr>

<th></th>

<th>Original</th>

<th>1D clusters</th>\n\nAnswer:",How does product quantization differ from scalar quantization in terms of compression and computational efficiency?,"Product quantization involves dividing data into chunks and quantizing each segment individually, approximating each chunk with a centroid index that represents the original vector component. The positions of these centroids are determined through clustering algorithms like k-means, with Qdrant currently utilizing 256 centroids, allowing each centroid index to be represented by a single byte. Product quantization can achieve higher compression factors compared to scalar quantization. However, there are tradeoffs to consider. Product quantization distance calculations are not SIMD-friendly, resulting in slower computational speed compared to scalar quantization. Additionally, product quantization introduces a loss of accuracy, making it more suitable for high-dimensional vectors. To optimize quantization parameters for specific use cases, it is recommended to refer to the ""Quantization Tips"" section for more information.","['* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.\n\n\n\n## Setting up Quantization in Qdrant\n\n\n\nYou can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.', '<td>474 s</td>\n\n      </tr>\n\n   </tbody>\n\n</table>\n\n\n\nIt turns out that in some cases, Product Quantization may not only reduce the memory usage, \n\nbut also the search time.\n\n\n\n## Good practices\n\n\n\nCompared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\n\n\n\nProduct Quantization tends to be favored in certain specific scenarios:', 'product=models.ProductQuantizationConfig(\n\n                    compression=models.CompressionRatio.X32,\n\n                    always_ram=True,\n\n                ),\n\n            ),\n\n            on_disk=True,\n\n        ),\n\n    },\n\n    hnsw_config=models.HnswConfigDiff(\n\n        ef_construct=123,\n\n    ),\n\n    quantization_config=models.ScalarQuantization(\n\n        scalar=models.ScalarQuantizationConfig(\n\n            type=models.ScalarType.INT8,\n\n            quantile=0.8,', '. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.', '<td>147 s</td>\n\n         <td>339 s</td>\n\n         <td>217 s</td>\n\n         <td>178 s</td>\n\n      </tr>\n\n   </tbody>\n\n</table>\n\n\n\nProduct Quantization increases both indexing and searching time. The higher the compression ratio, \n\nthe lower the search precision. The main benefit is undoubtedly the reduced usage of memory.\n\n\n\n#### Arxiv-titles-384-angular-no-filters\n\n\n\n<table>\n\n   <thead>\n\n      <tr>\n\n         <th></th>\n\n         <th>Original</th>\n\n         <th>1D clusters</th>']","* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.



## Setting up Quantization in Qdrant



You can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.
<td>474 s</td>

      </tr>

   </tbody>

</table>



It turns out that in some cases, Product Quantization may not only reduce the memory usage, 

but also the search time.



## Good practices



Compared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.



Product Quantization tends to be favored in certain specific scenarios:
product=models.ProductQuantizationConfig(

                    compression=models.CompressionRatio.X32,

                    always_ram=True,

                ),

            ),

            on_disk=True,

        ),

    },

    hnsw_config=models.HnswConfigDiff(

        ef_construct=123,

    ),

    quantization_config=models.ScalarQuantization(

        scalar=models.ScalarQuantizationConfig(

            type=models.ScalarType.INT8,

            quantile=0.8,
. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
<td>147 s</td>

         <td>339 s</td>

         <td>217 s</td>

         <td>178 s</td>

      </tr>

   </tbody>

</table>



Product Quantization increases both indexing and searching time. The higher the compression ratio, 

the lower the search precision. The main benefit is undoubtedly the reduced usage of memory.



#### Arxiv-titles-384-angular-no-filters



<table>

   <thead>

      <tr>

         <th></th>

         <th>Original</th>

         <th>1D clusters</th>",0.25462962962962965,0.4583333333333333,0.3273809523809524,0.09767441860465116,0.17647058823529413,0.12574850299401197,0.1527777777777778,0.275,0.19642857142857142,0.23148148148148148,0.4166666666666667,0.2976190476190476,0.7234185934066772,0.5791946649551392,0.5763000249862671,0.5777437090873718,0.8665756736338759,0.6067804199244295,229,1.8617886178861789,0.49594483635615355
True,,4602.581,dafc2d4b,What is the significance of Qdrant's approach to storing multiple vectors per object in data indexing?,"Qdrant's approach to storing multiple vectors per object in data indexing is significant as it opens up new possibilities in data representation and retrieval. By allowing multiple vectors to be associated with a single object, Qdrant enables more nuanced and detailed analysis of the data. This approach can be particularly beneficial in applications such as data science, neural networks, database management, and similarity search. It provides a more comprehensive understanding of the relationships between objects and enhances the accuracy and efficiency of search and retrieval processes. Overall, Qdrant's innovative vector storage strategy contributes to pushing the boundaries of data indexing and offers practical applications and benefits in various fields.","Storing multiple vectors per object in Qdrant is a significant advancement that brings new possibilities in data representation and retrieval. Prior to Qdrant 0.10, enabling semantic search with multiple vectors per object would require creating separate collections for each vector type, even if they shared some attributes in a payload. However, with the new release of Qdrant, it is now possible to store all these vectors together in the same collection and share a single copy of the payload.

This approach allows for a more efficient use of memory, potentially reducing memory requirements by up to 64 times. By storing multiple vectors per object and reusing the payloads, users can avoid the need to create multiple collections and query them separately. This not only streamlines the data indexing process but also enhances the quality of the search results.

The ability to store multiple vectors per object is particularly beneficial in scenarios where data points can be described using several vectors. This feature aligns well with the needs of applications that require different types of embeddings or multiple dimensions for calculating similarity. Overall, this novel vector storage strategy in Qdrant opens up new possibilities for data indexing, enabling more robust and efficient search capabilities.","---

draft: false

title: Storing multiple vectors per object in Qdrant

slug: storing-multiple-vectors-per-object-in-qdrant

short_description: Qdrant's approach to storing multiple vectors per object,

  unraveling new possibilities in data representation and retrieval.

description: Discover how Qdrant continues to push the boundaries of data

  indexing, providing insights into the practical applications and benefits of

  this novel vector storage strategy.
. Up till now, if you wanted to enable semantic search with multiple vectors per object, Qdrant would require you to create separate collections for each vector type, even though they could share some other attributes in a payload. However, since Qdrant 0.10 you are able to store all those vectors together in the same collection and share a single copy of the payload!
It is not surprising that a method used for creating neural encoding plays an important role in the search process and its quality. If your data points might be described using several vectors, then the latest release of Qdrant gives you an opportunity to store them together and reuse the payloads, instead of creating several collections and querying them separately.
detail. In a nutshell, you can **reduce the memory requirements by up to 64 times**!



### Optional named vectors



Qdrant has been supporting multiple named vectors per point for quite a long time. Those may have 

utterly different dimensionality and distance functions used to calculate similarity. Having multiple 

embeddings per item is an essential real-world scenario. For example, you might be encoding textual
author_link: https://medium.com/@lukawskikacper

date: 2022-09-19T13:30:00+02:00

draft: false

---



[Qdrant 0.10 is a new version](https://github.com/qdrant/qdrant/releases/tag/v0.10.0) that brings a lot of performance 

improvements, but also some new features which were heavily requested by our users. Here is an overview of what has changed.



## Storing multiple vectors per object","[""---\n\ndraft: false\n\ntitle: Storing multiple vectors per object in Qdrant\n\nslug: storing-multiple-vectors-per-object-in-qdrant\n\nshort_description: Qdrant's approach to storing multiple vectors per object,\n\n  unraveling new possibilities in data representation and retrieval.\n\ndescription: Discover how Qdrant continues to push the boundaries of data\n\n  indexing, providing insights into the practical applications and benefits of\n\n  this novel vector storage strategy."", '. Up till now, if you wanted to enable semantic search with multiple vectors per object, Qdrant would require you to create separate collections for each vector type, even though they could share some other attributes in a payload. However, since Qdrant 0.10 you are able to store all those vectors together in the same collection and share a single copy of the payload!', 'It is not surprising that a method used for creating neural encoding plays an important role in the search process and its quality. If your data points might be described using several vectors, then the latest release of Qdrant gives you an opportunity to store them together and reuse the payloads, instead of creating several collections and querying them separately.', 'detail. In a nutshell, you can **reduce the memory requirements by up to 64 times**!\n\n\n\n### Optional named vectors\n\n\n\nQdrant has been supporting multiple named vectors per point for quite a long time. Those may have \n\nutterly different dimensionality and distance functions used to calculate similarity. Having multiple \n\nembeddings per item is an essential real-world scenario. For example, you might be encoding textual', 'author_link: https://medium.com/@lukawskikacper\n\ndate: 2022-09-19T13:30:00+02:00\n\ndraft: false\n\n---\n\n\n\n[Qdrant 0.10 is a new version](https://github.com/qdrant/qdrant/releases/tag/v0.10.0) that brings a lot of performance \n\nimprovements, but also some new features which were heavily requested by our users. Here is an overview of what has changed.\n\n\n\n## Storing multiple vectors per object']","Question: What is the significance of Qdrant's approach to storing multiple vectors per object in data indexing?\n\nContext: ---

draft: false

title: Storing multiple vectors per object in Qdrant

slug: storing-multiple-vectors-per-object-in-qdrant

short_description: Qdrant's approach to storing multiple vectors per object,

unraveling new possibilities in data representation and retrieval.

description: Discover how Qdrant continues to push the boundaries of data

indexing, providing insights into the practical applications and benefits of

this novel vector storage strategy.
. Up till now, if you wanted to enable semantic search with multiple vectors per object, Qdrant would require you to create separate collections for each vector type, even though they could share some other attributes in a payload. However, since Qdrant 0.10 you are able to store all those vectors together in the same collection and share a single copy of the payload!
It is not surprising that a method used for creating neural encoding plays an important role in the search process and its quality. If your data points might be described using several vectors, then the latest release of Qdrant gives you an opportunity to store them together and reuse the payloads, instead of creating several collections and querying them separately.
detail. In a nutshell, you can **reduce the memory requirements by up to 64 times**!



### Optional named vectors



Qdrant has been supporting multiple named vectors per point for quite a long time. Those may have 

utterly different dimensionality and distance functions used to calculate similarity. Having multiple 

embeddings per item is an essential real-world scenario. For example, you might be encoding textual
author_link: https://medium.com/@lukawskikacper

date: 2022-09-19T13:30:00+02:00

draft: false

---



[Qdrant 0.10 is a new version](https://github.com/qdrant/qdrant/releases/tag/v0.10.0) that brings a lot of performance 

improvements, but also some new features which were heavily requested by our users. Here is an overview of what has changed.



## Storing multiple vectors per object\n\nAnswer:",What is the significance of Qdrant's approach to storing multiple vectors per object in data indexing?,"Qdrant's approach to storing multiple vectors per object in data indexing is significant as it opens up new possibilities in data representation and retrieval. By allowing multiple vectors to be associated with a single object, Qdrant enables more nuanced and detailed analysis of the data. This approach can be particularly beneficial in applications such as data science, neural networks, database management, and similarity search. It provides a more comprehensive understanding of the relationships between objects and enhances the accuracy and efficiency of search and retrieval processes. Overall, Qdrant's innovative vector storage strategy contributes to pushing the boundaries of data indexing and offers practical applications and benefits in various fields.","[""---\n\ndraft: false\n\ntitle: Storing multiple vectors per object in Qdrant\n\nslug: storing-multiple-vectors-per-object-in-qdrant\n\nshort_description: Qdrant's approach to storing multiple vectors per object,\n\n  unraveling new possibilities in data representation and retrieval.\n\ndescription: Discover how Qdrant continues to push the boundaries of data\n\n  indexing, providing insights into the practical applications and benefits of\n\n  this novel vector storage strategy."", '. Up till now, if you wanted to enable semantic search with multiple vectors per object, Qdrant would require you to create separate collections for each vector type, even though they could share some other attributes in a payload. However, since Qdrant 0.10 you are able to store all those vectors together in the same collection and share a single copy of the payload!', 'It is not surprising that a method used for creating neural encoding plays an important role in the search process and its quality. If your data points might be described using several vectors, then the latest release of Qdrant gives you an opportunity to store them together and reuse the payloads, instead of creating several collections and querying them separately.', 'detail. In a nutshell, you can **reduce the memory requirements by up to 64 times**!\n\n\n\n### Optional named vectors\n\n\n\nQdrant has been supporting multiple named vectors per point for quite a long time. Those may have \n\nutterly different dimensionality and distance functions used to calculate similarity. Having multiple \n\nembeddings per item is an essential real-world scenario. For example, you might be encoding textual', 'author_link: https://medium.com/@lukawskikacper\n\ndate: 2022-09-19T13:30:00+02:00\n\ndraft: false\n\n---\n\n\n\n[Qdrant 0.10 is a new version](https://github.com/qdrant/qdrant/releases/tag/v0.10.0) that brings a lot of performance \n\nimprovements, but also some new features which were heavily requested by our users. Here is an overview of what has changed.\n\n\n\n## Storing multiple vectors per object']","---

draft: false

title: Storing multiple vectors per object in Qdrant

slug: storing-multiple-vectors-per-object-in-qdrant

short_description: Qdrant's approach to storing multiple vectors per object,

  unraveling new possibilities in data representation and retrieval.

description: Discover how Qdrant continues to push the boundaries of data

  indexing, providing insights into the practical applications and benefits of

  this novel vector storage strategy.
. Up till now, if you wanted to enable semantic search with multiple vectors per object, Qdrant would require you to create separate collections for each vector type, even though they could share some other attributes in a payload. However, since Qdrant 0.10 you are able to store all those vectors together in the same collection and share a single copy of the payload!
It is not surprising that a method used for creating neural encoding plays an important role in the search process and its quality. If your data points might be described using several vectors, then the latest release of Qdrant gives you an opportunity to store them together and reuse the payloads, instead of creating several collections and querying them separately.
detail. In a nutshell, you can **reduce the memory requirements by up to 64 times**!



### Optional named vectors



Qdrant has been supporting multiple named vectors per point for quite a long time. Those may have 

utterly different dimensionality and distance functions used to calculate similarity. Having multiple 

embeddings per item is an essential real-world scenario. For example, you might be encoding textual
author_link: https://medium.com/@lukawskikacper

date: 2022-09-19T13:30:00+02:00

draft: false

---



[Qdrant 0.10 is a new version](https://github.com/qdrant/qdrant/releases/tag/v0.10.0) that brings a lot of performance 

improvements, but also some new features which were heavily requested by our users. Here is an overview of what has changed.



## Storing multiple vectors per object",0.3526315789473684,0.638095238095238,0.4542372881355932,0.14814814814814814,0.2692307692307692,0.1911262798634812,0.19473684210526315,0.3523809523809524,0.25084745762711863,0.3368421052631579,0.6095238095238096,0.4338983050847458,0.7917675971984863,0.6411048769950867,0.7068983912467957,0.6723960041999817,0.9959195209990462,0.9759175571496599,203,1.8288288288288288,0.5094712530146261
True,,3153.531,6121da2f,What configuration is required to enable in-memory quantization with on-disk original vectors in Qdrant?,"To configure in-memory quantization with on-disk original vectors in Qdrant, you need to create a collection with the following configuration:

```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 768,

        ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": true

        }

    }

}
```

Additionally, in the Python code snippet provided, you can use the QdrantClient library to create the collection with the specified configuration parameters. This includes defining vector size, distance metric (such as Cosine similarity), memmap threshold for optimization, and scalar quantization configuration with type set to INT8","To enable in-memory quantization with on-disk original vectors in Qdrant, you need to create a collection with the following configuration:

```http
PUT /collections/{collection_name}

{
  ""vectors"": {
    ""size"": 768,
    ""distance"": ""Cosine""
  },
  ""optimizers_config"": {
    ""memmap_threshold"": 20000
  },
  ""quantization_config"": {
    ""scalar"": {
      ""type"": ""int8"",
      ""always_ram"": true
    }
  }
}
```

In this configuration:
- `vectors` specifies the size and distance metric for the vectors.
- `optimizers_config` sets the `memmap_threshold` to 20000.
- `quantization_config` enables scalar quantization with type `int8` and `always_ram` set to true.

This configuration allows Qdrant to perform in-memory quantization with on-disk original vectors.","To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 768,

        ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": true

        }

    }

}

```



```python
- **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.





#### Memory and speed tuning



In this section, we will discuss how to tune the memory and speed of the search process with quantization.



There are 3 possible modes to place storage of vectors within the qdrant collection:
);

```



The following parameters can be updated:



* `optimizers_config` - see [optimizer](../optimizer/) for details.

* `hnsw_config` - see [indexing](../indexing/#vector-index) for details.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.

* `vectors` - vector-specific configuration, including individual `hnsw_config`, `quantization_config` and `on_disk` settings.
..Default::default()

        }),

        quantization_config: Some(QuantizationConfig {

            quantization: Some(Quantization::Scalar(ScalarQuantization {

                r#type: QuantizationType::Int8.into(),

                always_ram: Some(false),

                ..Default::default()

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;
..Default::default()

        }),

        quantization_config: Some(QuantizationConfig {

            quantization: Some(Quantization::Scalar(ScalarQuantization {

                r#type: QuantizationType::Int8.into(),

                always_ram: Some(true),

                ..Default::default()

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;","['To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 768,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python', '- **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.\n\n\n\n\n\n#### Memory and speed tuning\n\n\n\nIn this section, we will discuss how to tune the memory and speed of the search process with quantization.\n\n\n\nThere are 3 possible modes to place storage of vectors within the qdrant collection:', ');\n\n```\n\n\n\nThe following parameters can be updated:\n\n\n\n* `optimizers_config` - see [optimizer](../optimizer/) for details.\n\n* `hnsw_config` - see [indexing](../indexing/#vector-index) for details.\n\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\n\n* `vectors` - vector-specific configuration, including individual `hnsw_config`, `quantization_config` and `on_disk` settings.', '..Default::default()\n\n        }),\n\n        quantization_config: Some(QuantizationConfig {\n\n            quantization: Some(Quantization::Scalar(ScalarQuantization {\n\n                r#type: QuantizationType::Int8.into(),\n\n                always_ram: Some(false),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;', '..Default::default()\n\n        }),\n\n        quantization_config: Some(QuantizationConfig {\n\n            quantization: Some(Quantization::Scalar(ScalarQuantization {\n\n                r#type: QuantizationType::Int8.into(),\n\n                always_ram: Some(true),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;']","Question: What configuration is required to enable in-memory quantization with on-disk original vectors in Qdrant?\n\nContext: To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration:



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""optimizers_config"": {

""memmap_threshold"": 20000

},

""quantization_config"": {

""scalar"": {

""type"": ""int8"",

""always_ram"": true

}

}

}

```



```python
- **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.





#### Memory and speed tuning



In this section, we will discuss how to tune the memory and speed of the search process with quantization.



There are 3 possible modes to place storage of vectors within the qdrant collection:
);

```



The following parameters can be updated:



* `optimizers_config` - see [optimizer](../optimizer/) for details.

* `hnsw_config` - see [indexing](../indexing/#vector-index) for details.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.

* `vectors` - vector-specific configuration, including individual `hnsw_config`, `quantization_config` and `on_disk` settings.
..Default::default()

}),

quantization_config: Some(QuantizationConfig {

quantization: Some(Quantization::Scalar(ScalarQuantization {

r#type: QuantizationType::Int8.into(),

always_ram: Some(false),

..Default::default()

})),

}),

..Default::default()

})

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;
..Default::default()

}),

quantization_config: Some(QuantizationConfig {

quantization: Some(Quantization::Scalar(ScalarQuantization {

r#type: QuantizationType::Int8.into(),

always_ram: Some(true),

..Default::default()

})),

}),

..Default::default()

})

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;\n\nAnswer:",What configuration is required to enable in-memory quantization with on-disk original vectors in Qdrant?,"To configure in-memory quantization with on-disk original vectors in Qdrant, you need to create a collection with the following configuration:

```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 768,

        ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": true

        }

    }

}
```

Additionally, in the Python code snippet provided, you can use the QdrantClient library to create the collection with the specified configuration parameters. This includes defining vector size, distance metric (such as Cosine similarity), memmap threshold for optimization, and scalar quantization configuration with type set to INT8","['To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 768,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python', '- **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.\n\n\n\n\n\n#### Memory and speed tuning\n\n\n\nIn this section, we will discuss how to tune the memory and speed of the search process with quantization.\n\n\n\nThere are 3 possible modes to place storage of vectors within the qdrant collection:', ');\n\n```\n\n\n\nThe following parameters can be updated:\n\n\n\n* `optimizers_config` - see [optimizer](../optimizer/) for details.\n\n* `hnsw_config` - see [indexing](../indexing/#vector-index) for details.\n\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\n\n* `vectors` - vector-specific configuration, including individual `hnsw_config`, `quantization_config` and `on_disk` settings.', '..Default::default()\n\n        }),\n\n        quantization_config: Some(QuantizationConfig {\n\n            quantization: Some(Quantization::Scalar(ScalarQuantization {\n\n                r#type: QuantizationType::Int8.into(),\n\n                always_ram: Some(false),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;', '..Default::default()\n\n        }),\n\n        quantization_config: Some(QuantizationConfig {\n\n            quantization: Some(Quantization::Scalar(ScalarQuantization {\n\n                r#type: QuantizationType::Int8.into(),\n\n                always_ram: Some(true),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;']","To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 768,

        ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": true

        }

    }

}

```



```python
- **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.





#### Memory and speed tuning



In this section, we will discuss how to tune the memory and speed of the search process with quantization.



There are 3 possible modes to place storage of vectors within the qdrant collection:
);

```



The following parameters can be updated:



* `optimizers_config` - see [optimizer](../optimizer/) for details.

* `hnsw_config` - see [indexing](../indexing/#vector-index) for details.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.

* `vectors` - vector-specific configuration, including individual `hnsw_config`, `quantization_config` and `on_disk` settings.
..Default::default()

        }),

        quantization_config: Some(QuantizationConfig {

            quantization: Some(Quantization::Scalar(ScalarQuantization {

                r#type: QuantizationType::Int8.into(),

                always_ram: Some(false),

                ..Default::default()

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;
..Default::default()

        }),

        quantization_config: Some(QuantizationConfig {

            quantization: Some(Quantization::Scalar(ScalarQuantization {

                r#type: QuantizationType::Int8.into(),

                always_ram: Some(true),

                ..Default::default()

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;",0.7303370786516854,0.7647058823529411,0.7471264367816092,0.5113636363636364,0.5357142857142857,0.5232558139534884,0.6292134831460674,0.6588235294117647,0.6436781609195402,0.6404494382022472,0.6705882352941176,0.6551724137931035,0.9910657405853271,0.7299624681472778,0.7176225185394287,0.7237399220466614,0.5886543379165232,0.5214227586984634,81,0.9642857142857143,0.6271850307848658
True,,5256.668000000001,5144591d,How can dissimilarity search be used for outlier detection in a dataset?,"Dissimilarity search can be utilized for outlier detection in a dataset by first establishing a set of reference points that are considered ""normal"". These reference points serve as a benchmark for comparison. Subsequently, the dissimilarity search algorithm is applied to identify the data points that are the most dissimilar to the reference set. These identified points are then flagged as potential outliers or anomalies in the dataset. This approach allows for the detection of abnormal data points even in cases where labels are not available, enabling the identification of outliers based on their deviation from the established ""normal"" reference points. This method can be a valuable tool in data analysis and anomaly detection tasks.","Dissimilarity search can be used for outlier detection in a dataset by comparing the similarity of data points to a set of reference points that are considered ""normal"". Here's how dissimilarity search can be utilized for outlier detection:

1. **Select Normal Reference Points**: Begin by selecting a set of reference points in your dataset that are representative of normal behavior or characteristics. These reference points should be considered as the baseline for what is considered typical or non-anomalous.

2. **Calculate Dissimilarity**: Calculate the dissimilarity or distance between each data point in your dataset and the reference points. This can be done using various distance metrics such as Euclidean distance, cosine similarity, or others depending on the nature of your data.

3. **Identify Outliers**: Data points that exhibit a high dissimilarity or distance from the reference points can be flagged as potential outliers. These outliers may represent anomalies, deviations, or errors within the dataset.

4. **Thresholding**: Set a threshold for the dissimilarity score above which a data point is classified as an outlier. This threshold can be determined based on domain knowledge, statistical analysis, or experimentation.

5. **Visualization**: Visualize the dissimilarity scores or distances to gain insights into the distribution of outliers within the dataset. This can help in understanding the patterns and characteristics of outliers present.

6. **Iterative Refinement**: Refine the outlier detection process by fine-tuning parameters, adjusting the set of reference points, or exploring different distance metrics to improve the accuracy of outlier detection.

By leveraging dissimilarity search for outlier detection, you can effectively identify unusual or abnormal data points within a dataset, even in cases where labels are not available.","### Case: Outlier Detection



In some cases, we might not even have labels, but it is still possible to try to detect anomalies in our dataset.

Dissimilarity search can be used for this purpose as well.



{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/anomaly-detection.png caption=""Anomaly Detection"" >}}



The only thing we need is a bunch of reference points that we consider ""normal"".
On the contrary, if the proportion of out-of-place elements is high enough, outlier search methods are likely to be useless.



### Similarity search



The idea behind similarity search is to measure semantic similarity between related parts of the data.

E.g. between category title and item images.

The hypothesis is, that unsuitable items will be less similar.



We can't directly compare text and image data.

For this we need an intermediate representation - embeddings.
### Case: Mislabeling Detection



For example, we have a dataset of furniture in which we have classified our items into what kind of furniture they are: tables, chairs, lamps, etc.

To ensure our catalog is accurate, we can use a dissimilarity search to highlight items that are most likely mislabeled.



To do this, we only need to search for the most dissimilar items using the 

embedding of the category title itself as a query.
You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. 

To mitigate this issue, you can perform a diversity search.



Diversity search is a method for finding the most distinctive examples in the data.

As similarity search, it also operates on embeddings and measures the distances between them.

The difference lies in deciding which point should be extracted next.
If your data is huge and does not fit into memory, vector search engines like [Qdrant](https://qdrant.tech/) might be helpful.



Although the described methods can be used independently. But they are simple to combine and improve detection capabilities.

If the quality remains insufficient, you can fine-tune the models using a similarity learning approach (e.g. with [Quaterion](https://quaterion.qdrant.tech) both to provide a better representation of your data and pull apart dissimilar objects in space.","['### Case: Outlier Detection\n\n\n\nIn some cases, we might not even have labels, but it is still possible to try to detect anomalies in our dataset.\n\nDissimilarity search can be used for this purpose as well.\n\n\n\n{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/anomaly-detection.png caption=""Anomaly Detection"" >}}\n\n\n\nThe only thing we need is a bunch of reference points that we consider ""normal"".', ""On the contrary, if the proportion of out-of-place elements is high enough, outlier search methods are likely to be useless.\n\n\n\n### Similarity search\n\n\n\nThe idea behind similarity search is to measure semantic similarity between related parts of the data.\n\nE.g. between category title and item images.\n\nThe hypothesis is, that unsuitable items will be less similar.\n\n\n\nWe can't directly compare text and image data.\n\nFor this we need an intermediate representation - embeddings."", '### Case: Mislabeling Detection\n\n\n\nFor example, we have a dataset of furniture in which we have classified our items into what kind of furniture they are: tables, chairs, lamps, etc.\n\nTo ensure our catalog is accurate, we can use a dissimilarity search to highlight items that are most likely mislabeled.\n\n\n\nTo do this, we only need to search for the most dissimilar items using the \n\nembedding of the category title itself as a query.', 'You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. \n\nTo mitigate this issue, you can perform a diversity search.\n\n\n\nDiversity search is a method for finding the most distinctive examples in the data.\n\nAs similarity search, it also operates on embeddings and measures the distances between them.\n\nThe difference lies in deciding which point should be extracted next.', 'If your data is huge and does not fit into memory, vector search engines like [Qdrant](https://qdrant.tech/) might be helpful.\n\n\n\nAlthough the described methods can be used independently. But they are simple to combine and improve detection capabilities.\n\nIf the quality remains insufficient, you can fine-tune the models using a similarity learning approach (e.g. with [Quaterion](https://quaterion.qdrant.tech) both to provide a better representation of your data and pull apart dissimilar objects in space.']","Question: How can dissimilarity search be used for outlier detection in a dataset?\n\nContext: ### Case: Outlier Detection



In some cases, we might not even have labels, but it is still possible to try to detect anomalies in our dataset.

Dissimilarity search can be used for this purpose as well.



{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/anomaly-detection.png caption=""Anomaly Detection"" >}}



The only thing we need is a bunch of reference points that we consider ""normal"".
On the contrary, if the proportion of out-of-place elements is high enough, outlier search methods are likely to be useless.



### Similarity search



The idea behind similarity search is to measure semantic similarity between related parts of the data.

E.g. between category title and item images.

The hypothesis is, that unsuitable items will be less similar.



We can't directly compare text and image data.

For this we need an intermediate representation - embeddings.
### Case: Mislabeling Detection



For example, we have a dataset of furniture in which we have classified our items into what kind of furniture they are: tables, chairs, lamps, etc.

To ensure our catalog is accurate, we can use a dissimilarity search to highlight items that are most likely mislabeled.



To do this, we only need to search for the most dissimilar items using the 

embedding of the category title itself as a query.
You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. 

To mitigate this issue, you can perform a diversity search.



Diversity search is a method for finding the most distinctive examples in the data.

As similarity search, it also operates on embeddings and measures the distances between them.

The difference lies in deciding which point should be extracted next.
If your data is huge and does not fit into memory, vector search engines like [Qdrant](https://qdrant.tech/) might be helpful.



Although the described methods can be used independently. But they are simple to combine and improve detection capabilities.

If the quality remains insufficient, you can fine-tune the models using a similarity learning approach (e.g. with [Quaterion](https://quaterion.qdrant.tech) both to provide a better representation of your data and pull apart dissimilar objects in space.\n\nAnswer:",How can dissimilarity search be used for outlier detection in a dataset?,"Dissimilarity search can be utilized for outlier detection in a dataset by first establishing a set of reference points that are considered ""normal"". These reference points serve as a benchmark for comparison. Subsequently, the dissimilarity search algorithm is applied to identify the data points that are the most dissimilar to the reference set. These identified points are then flagged as potential outliers or anomalies in the dataset. This approach allows for the detection of abnormal data points even in cases where labels are not available, enabling the identification of outliers based on their deviation from the established ""normal"" reference points. This method can be a valuable tool in data analysis and anomaly detection tasks.","['### Case: Outlier Detection\n\n\n\nIn some cases, we might not even have labels, but it is still possible to try to detect anomalies in our dataset.\n\nDissimilarity search can be used for this purpose as well.\n\n\n\n{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/anomaly-detection.png caption=""Anomaly Detection"" >}}\n\n\n\nThe only thing we need is a bunch of reference points that we consider ""normal"".', ""On the contrary, if the proportion of out-of-place elements is high enough, outlier search methods are likely to be useless.\n\n\n\n### Similarity search\n\n\n\nThe idea behind similarity search is to measure semantic similarity between related parts of the data.\n\nE.g. between category title and item images.\n\nThe hypothesis is, that unsuitable items will be less similar.\n\n\n\nWe can't directly compare text and image data.\n\nFor this we need an intermediate representation - embeddings."", '### Case: Mislabeling Detection\n\n\n\nFor example, we have a dataset of furniture in which we have classified our items into what kind of furniture they are: tables, chairs, lamps, etc.\n\nTo ensure our catalog is accurate, we can use a dissimilarity search to highlight items that are most likely mislabeled.\n\n\n\nTo do this, we only need to search for the most dissimilar items using the \n\nembedding of the category title itself as a query.', 'You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. \n\nTo mitigate this issue, you can perform a diversity search.\n\n\n\nDiversity search is a method for finding the most distinctive examples in the data.\n\nAs similarity search, it also operates on embeddings and measures the distances between them.\n\nThe difference lies in deciding which point should be extracted next.', 'If your data is huge and does not fit into memory, vector search engines like [Qdrant](https://qdrant.tech/) might be helpful.\n\n\n\nAlthough the described methods can be used independently. But they are simple to combine and improve detection capabilities.\n\nIf the quality remains insufficient, you can fine-tune the models using a similarity learning approach (e.g. with [Quaterion](https://quaterion.qdrant.tech) both to provide a better representation of your data and pull apart dissimilar objects in space.']","### Case: Outlier Detection



In some cases, we might not even have labels, but it is still possible to try to detect anomalies in our dataset.

Dissimilarity search can be used for this purpose as well.



{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/anomaly-detection.png caption=""Anomaly Detection"" >}}



The only thing we need is a bunch of reference points that we consider ""normal"".
On the contrary, if the proportion of out-of-place elements is high enough, outlier search methods are likely to be useless.



### Similarity search



The idea behind similarity search is to measure semantic similarity between related parts of the data.

E.g. between category title and item images.

The hypothesis is, that unsuitable items will be less similar.



We can't directly compare text and image data.

For this we need an intermediate representation - embeddings.
### Case: Mislabeling Detection



For example, we have a dataset of furniture in which we have classified our items into what kind of furniture they are: tables, chairs, lamps, etc.

To ensure our catalog is accurate, we can use a dissimilarity search to highlight items that are most likely mislabeled.



To do this, we only need to search for the most dissimilar items using the 

embedding of the category title itself as a query.
You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. 

To mitigate this issue, you can perform a diversity search.



Diversity search is a method for finding the most distinctive examples in the data.

As similarity search, it also operates on embeddings and measures the distances between them.

The difference lies in deciding which point should be extracted next.
If your data is huge and does not fit into memory, vector search engines like [Qdrant](https://qdrant.tech/) might be helpful.



Although the described methods can be used independently. But they are simple to combine and improve detection capabilities.

If the quality remains insufficient, you can fine-tune the models using a similarity learning approach (e.g. with [Quaterion](https://quaterion.qdrant.tech) both to provide a better representation of your data and pull apart dissimilar objects in space.",0.31746031746031744,0.7843137254901961,0.45197740112994345,0.16733067729083664,0.4158415841584158,0.2386363636363636,0.20238095238095238,0.5,0.288135593220339,0.3134920634920635,0.7745098039215687,0.4463276836158192,0.8922013640403748,0.6349904537200928,0.720912754535675,0.6752292513847351,0.5790541736458086,0.9712902665875541,273,2.3947368421052633,0.36409511011136536
True,,3329.694,d90aa929,How does binary quantization work and what benefits does it offer in terms of query processing times and data compression?,"Binary quantization is a method used by Qdrant for fast indexing and data compression. In this process, each data point is represented as a vector. The quantization splits the vector in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This results in a string of bits that represents the original vector, making it much smaller and easier to compare. By supporting vector comparisons, binary quantization can significantly speed up query processing times, up to 40 times faster. This method is especially effective for OpenAI embeddings, where it has been shown to achieve a massive performance improvement at a lower cost of accuracy.","Binary quantization works by compressing vectors into a more compact binary representation, sacrificing some accuracy in exchange for efficiency gains. This technique simplifies distance calculations between vectors to bitwise operations, enabling faster query processing times and data compression. 

The benefits of binary quantization in terms of query processing times and data compression include:

1. Reduced storage footprint: Binary quantization significantly reduces the storage requirements for large-scale datasets, saving on memory and scaling up to 30x at the same cost.

2. Enhanced speed of data retrieval: Smaller data sizes resulting from binary quantization generally lead to faster searches, improving the efficiency of data retrieval operations.

3. Accelerated search process: By simplifying distance calculations between vectors into bitwise operations, binary quantization enables real-time querying even in extensive databases. This can lead to significant speed improvements in search processes.

Overall, binary quantization offers advantages in terms of storage efficiency, query processing speed, and data compression, making it a valuable technique for optimizing database operations.","#### What is Binary Quantization?



Quantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.



[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).
![](/blog/openai/Accuracy_Models.png)



The efficiency gains from Binary Quantization are as follows: 



- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. 

- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. 

- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.
By reducing storage needs, you can scale applications with lower costs. This addresses a critical challenge posed by the original embedding sizes. Binary Quantization also speeds the search process. It simplifies the complex distance calculations between vectors into more manageable bitwise operations, which supports potentially real-time searches across vast datasets.
|Cohere AI embed-english-v2.0|4096|[Wikipedia](https://huggingface.co/datasets/nreimers/wikipedia-22-12-large/tree/main) 1M|0.98|2x|



If you determine that binary quantization is appropriate for your datasets and queries then we suggest the following:

- Binary Quantization with always_ram=True 

- Vectors stored on disk

- Oversampling=2.0 (or more) 

- Rescore=True



## What's next?
`*` - for compatible models



* **Binary Quantization** is the fastest method and the most memory-efficient, but it requires a centered distribution of vector components. It is recommended to use with tested models only.

* **Scalar Quantization** is the most universal method, as it provides a good balance between accuracy, speed, and compression. It is recommended as default quantization if binary quantization is not applicable.","['#### What is Binary Quantization?\n\n\n\nQuantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.\n\n\n\n[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).', '![](/blog/openai/Accuracy_Models.png)\n\n\n\nThe efficiency gains from Binary Quantization are as follows: \n\n\n\n- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. \n\n- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. \n\n- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.', 'By reducing storage needs, you can scale applications with lower costs. This addresses a critical challenge posed by the original embedding sizes. Binary Quantization also speeds the search process. It simplifies the complex distance calculations between vectors into more manageable bitwise operations, which supports potentially real-time searches across vast datasets.', ""|Cohere AI embed-english-v2.0|4096|[Wikipedia](https://huggingface.co/datasets/nreimers/wikipedia-22-12-large/tree/main) 1M|0.98|2x|\n\n\n\nIf you determine that binary quantization is appropriate for your datasets and queries then we suggest the following:\n\n- Binary Quantization with always_ram=True \n\n- Vectors stored on disk\n\n- Oversampling=2.0 (or more) \n\n- Rescore=True\n\n\n\n## What's next?"", '`*` - for compatible models\n\n\n\n* **Binary Quantization** is the fastest method and the most memory-efficient, but it requires a centered distribution of vector components. It is recommended to use with tested models only.\n\n* **Scalar Quantization** is the most universal method, as it provides a good balance between accuracy, speed, and compression. It is recommended as default quantization if binary quantization is not applicable.']","Question: How does binary quantization work and what benefits does it offer in terms of query processing times and data compression?\n\nContext: #### What is Binary Quantization?



Quantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.



[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).
![](/blog/openai/Accuracy_Models.png)



The efficiency gains from Binary Quantization are as follows: 



- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. 

- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. 

- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.
By reducing storage needs, you can scale applications with lower costs. This addresses a critical challenge posed by the original embedding sizes. Binary Quantization also speeds the search process. It simplifies the complex distance calculations between vectors into more manageable bitwise operations, which supports potentially real-time searches across vast datasets.
|Cohere AI embed-english-v2.0|4096|[Wikipedia](https://huggingface.co/datasets/nreimers/wikipedia-22-12-large/tree/main) 1M|0.98|2x|



If you determine that binary quantization is appropriate for your datasets and queries then we suggest the following:

- Binary Quantization with always_ram=True 

- Vectors stored on disk

- Oversampling=2.0 (or more) 

- Rescore=True



## What's next?
`*` - for compatible models



* **Binary Quantization** is the fastest method and the most memory-efficient, but it requires a centered distribution of vector components. It is recommended to use with tested models only.

* **Scalar Quantization** is the most universal method, as it provides a good balance between accuracy, speed, and compression. It is recommended as default quantization if binary quantization is not applicable.\n\nAnswer:",How does binary quantization work and what benefits does it offer in terms of query processing times and data compression?,"Binary quantization is a method used by Qdrant for fast indexing and data compression. In this process, each data point is represented as a vector. The quantization splits the vector in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This results in a string of bits that represents the original vector, making it much smaller and easier to compare. By supporting vector comparisons, binary quantization can significantly speed up query processing times, up to 40 times faster. This method is especially effective for OpenAI embeddings, where it has been shown to achieve a massive performance improvement at a lower cost of accuracy.","['#### What is Binary Quantization?\n\n\n\nQuantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.\n\n\n\n[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).', '![](/blog/openai/Accuracy_Models.png)\n\n\n\nThe efficiency gains from Binary Quantization are as follows: \n\n\n\n- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. \n\n- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. \n\n- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.', 'By reducing storage needs, you can scale applications with lower costs. This addresses a critical challenge posed by the original embedding sizes. Binary Quantization also speeds the search process. It simplifies the complex distance calculations between vectors into more manageable bitwise operations, which supports potentially real-time searches across vast datasets.', ""|Cohere AI embed-english-v2.0|4096|[Wikipedia](https://huggingface.co/datasets/nreimers/wikipedia-22-12-large/tree/main) 1M|0.98|2x|\n\n\n\nIf you determine that binary quantization is appropriate for your datasets and queries then we suggest the following:\n\n- Binary Quantization with always_ram=True \n\n- Vectors stored on disk\n\n- Oversampling=2.0 (or more) \n\n- Rescore=True\n\n\n\n## What's next?"", '`*` - for compatible models\n\n\n\n* **Binary Quantization** is the fastest method and the most memory-efficient, but it requires a centered distribution of vector components. It is recommended to use with tested models only.\n\n* **Scalar Quantization** is the most universal method, as it provides a good balance between accuracy, speed, and compression. It is recommended as default quantization if binary quantization is not applicable.']","#### What is Binary Quantization?



Quantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.



[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).
![](/blog/openai/Accuracy_Models.png)



The efficiency gains from Binary Quantization are as follows: 



- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. 

- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. 

- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.
By reducing storage needs, you can scale applications with lower costs. This addresses a critical challenge posed by the original embedding sizes. Binary Quantization also speeds the search process. It simplifies the complex distance calculations between vectors into more manageable bitwise operations, which supports potentially real-time searches across vast datasets.
|Cohere AI embed-english-v2.0|4096|[Wikipedia](https://huggingface.co/datasets/nreimers/wikipedia-22-12-large/tree/main) 1M|0.98|2x|



If you determine that binary quantization is appropriate for your datasets and queries then we suggest the following:

- Binary Quantization with always_ram=True 

- Vectors stored on disk

- Oversampling=2.0 (or more) 

- Rescore=True



## What's next?
`*` - for compatible models



* **Binary Quantization** is the fastest method and the most memory-efficient, but it requires a centered distribution of vector components. It is recommended to use with tested models only.

* **Scalar Quantization** is the most universal method, as it provides a good balance between accuracy, speed, and compression. It is recommended as default quantization if binary quantization is not applicable.",0.29936305732484075,0.47,0.36575875486381326,0.05128205128205128,0.08080808080808081,0.06274509803921569,0.15286624203821655,0.24,0.1867704280155642,0.27388535031847133,0.43,0.3346303501945525,0.8164864778518677,0.5848450660705566,0.6046851277351379,0.5945996642112732,0.9544029529705378,0.7630085444543511,161,1.4770642201834863,0.6046468444174598
True,,1973.116,f612e18a,What is the primary function of vector embeddings in the context of machine learning and artificial intelligence?,"Vector embeddings play a crucial role in machine learning and artificial intelligence by converting complex data into a format that machines can easily understand and process. These embeddings represent data points in a multi-dimensional space, where similar data points are closer together and dissimilar data points are farther apart. By capturing the underlying relationships and similarities between data points, vector embeddings enable algorithms to perform tasks such as similarity search, recommendation systems, and natural language processing more effectively. In essence, vector embeddings help in transforming raw data into a structured and meaningful representation that can be utilized by machine learning models to make accurate predictions and decisions.","The primary function of vector embeddings in the context of machine learning and artificial intelligence is to capture the semantic meaning of complex, high-dimensional data such as text, images, or audio into numerical vectors. These embeddings enable algorithms to more efficiently process and analyze the data by representing it in a format that is easier to work with. By condensing the information into vectors, embeddings reduce storage requirements, improve computational efficiency, and help make sense of unstructured data. They are created through neural networks and are crucial for tasks like natural language processing, image recognition, recommendation systems, and more.","- machine-learning

  - artificial intelligence



---



> **Embeddings** are numerical machine learning representations of the semantic of the input data. They capture the meaning of complex, high-dimensional data, like text, images, or audio, into vectors. Enabling algorithms to process and analyze the data more efficiently.
![How embeddings are applied to perform recommendantions and other use cases](/articles_data/what-are-embeddings/Embeddings-Use-Case.jpg)





They make [high-dimensional](https://www.sciencedirect.com/topics/computer-science/high-dimensional-data) data more manageable. This reduces storage requirements, improves computational efficiency, and makes sense of a ton of **unstructured** data.





## Why Use Vector Embeddings?
## How do embeddings work?



Embeddings are created through neural networks. They capture complex relationships and semantics into [dense vectors](https://www1.se.cuhk.edu.hk/~seem5680/lecture/semantics-with-dense-vectors-2018.pdf) which are more suitable for machine learning and data processing applications. They can then project these vectors into a proper **high-dimensional** space, specifically, a [Vector Database](https://qdrant.tech/articles/what-is-a-vector-database/).
## Why Use Vector Embeddings?



The **nuances** of natural language or the hidden **meaning** in large datasets of images, sounds, or user interactions are hard to fit into a table. Traditional relational databases can't efficiently query most types of data being currently used and produced, making the **retrieval** of this information very limited.
> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.





### Creating Vector Embeddings



Embeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.","['- machine-learning\n\n  - artificial intelligence\n\n\n\n---\n\n\n\n> **Embeddings** are numerical machine learning representations of the semantic of the input data. They capture the meaning of complex, high-dimensional data, like text, images, or audio, into vectors. Enabling algorithms to process and analyze the data more efficiently.', '![How embeddings are applied to perform recommendantions and other use cases](/articles_data/what-are-embeddings/Embeddings-Use-Case.jpg)\n\n\n\n\n\nThey make [high-dimensional](https://www.sciencedirect.com/topics/computer-science/high-dimensional-data) data more manageable. This reduces storage requirements, improves computational efficiency, and makes sense of a ton of **unstructured** data.\n\n\n\n\n\n## Why Use Vector Embeddings?', '## How do embeddings work?\n\n\n\nEmbeddings are created through neural networks. They capture complex relationships and semantics into [dense vectors](https://www1.se.cuhk.edu.hk/~seem5680/lecture/semantics-with-dense-vectors-2018.pdf) which are more suitable for machine learning and data processing applications. They can then project these vectors into a proper **high-dimensional** space, specifically, a [Vector Database](https://qdrant.tech/articles/what-is-a-vector-database/).', ""## Why Use Vector Embeddings?\n\n\n\nThe **nuances** of natural language or the hidden **meaning** in large datasets of images, sounds, or user interactions are hard to fit into a table. Traditional relational databases can't efficiently query most types of data being currently used and produced, making the **retrieval** of this information very limited."", '> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.\n\n\n\n\n\n### Creating Vector Embeddings\n\n\n\nEmbeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.']","Question: What is the primary function of vector embeddings in the context of machine learning and artificial intelligence?\n\nContext: - machine-learning

- artificial intelligence



---



> **Embeddings** are numerical machine learning representations of the semantic of the input data. They capture the meaning of complex, high-dimensional data, like text, images, or audio, into vectors. Enabling algorithms to process and analyze the data more efficiently.
![How embeddings are applied to perform recommendantions and other use cases](/articles_data/what-are-embeddings/Embeddings-Use-Case.jpg)





They make [high-dimensional](https://www.sciencedirect.com/topics/computer-science/high-dimensional-data) data more manageable. This reduces storage requirements, improves computational efficiency, and makes sense of a ton of **unstructured** data.





## Why Use Vector Embeddings?
## How do embeddings work?



Embeddings are created through neural networks. They capture complex relationships and semantics into [dense vectors](https://www1.se.cuhk.edu.hk/~seem5680/lecture/semantics-with-dense-vectors-2018.pdf) which are more suitable for machine learning and data processing applications. They can then project these vectors into a proper **high-dimensional** space, specifically, a [Vector Database](https://qdrant.tech/articles/what-is-a-vector-database/).
## Why Use Vector Embeddings?



The **nuances** of natural language or the hidden **meaning** in large datasets of images, sounds, or user interactions are hard to fit into a table. Traditional relational databases can't efficiently query most types of data being currently used and produced, making the **retrieval** of this information very limited.
> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.





### Creating Vector Embeddings



Embeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.\n\nAnswer:",What is the primary function of vector embeddings in the context of machine learning and artificial intelligence?,"Vector embeddings play a crucial role in machine learning and artificial intelligence by converting complex data into a format that machines can easily understand and process. These embeddings represent data points in a multi-dimensional space, where similar data points are closer together and dissimilar data points are farther apart. By capturing the underlying relationships and similarities between data points, vector embeddings enable algorithms to perform tasks such as similarity search, recommendation systems, and natural language processing more effectively. In essence, vector embeddings help in transforming raw data into a structured and meaningful representation that can be utilized by machine learning models to make accurate predictions and decisions.","['- machine-learning\n\n  - artificial intelligence\n\n\n\n---\n\n\n\n> **Embeddings** are numerical machine learning representations of the semantic of the input data. They capture the meaning of complex, high-dimensional data, like text, images, or audio, into vectors. Enabling algorithms to process and analyze the data more efficiently.', '![How embeddings are applied to perform recommendantions and other use cases](/articles_data/what-are-embeddings/Embeddings-Use-Case.jpg)\n\n\n\n\n\nThey make [high-dimensional](https://www.sciencedirect.com/topics/computer-science/high-dimensional-data) data more manageable. This reduces storage requirements, improves computational efficiency, and makes sense of a ton of **unstructured** data.\n\n\n\n\n\n## Why Use Vector Embeddings?', '## How do embeddings work?\n\n\n\nEmbeddings are created through neural networks. They capture complex relationships and semantics into [dense vectors](https://www1.se.cuhk.edu.hk/~seem5680/lecture/semantics-with-dense-vectors-2018.pdf) which are more suitable for machine learning and data processing applications. They can then project these vectors into a proper **high-dimensional** space, specifically, a [Vector Database](https://qdrant.tech/articles/what-is-a-vector-database/).', ""## Why Use Vector Embeddings?\n\n\n\nThe **nuances** of natural language or the hidden **meaning** in large datasets of images, sounds, or user interactions are hard to fit into a table. Traditional relational databases can't efficiently query most types of data being currently used and produced, making the **retrieval** of this information very limited."", '> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.\n\n\n\n\n\n### Creating Vector Embeddings\n\n\n\nEmbeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.']","- machine-learning

  - artificial intelligence



---



> **Embeddings** are numerical machine learning representations of the semantic of the input data. They capture the meaning of complex, high-dimensional data, like text, images, or audio, into vectors. Enabling algorithms to process and analyze the data more efficiently.
![How embeddings are applied to perform recommendantions and other use cases](/articles_data/what-are-embeddings/Embeddings-Use-Case.jpg)





They make [high-dimensional](https://www.sciencedirect.com/topics/computer-science/high-dimensional-data) data more manageable. This reduces storage requirements, improves computational efficiency, and makes sense of a ton of **unstructured** data.





## Why Use Vector Embeddings?
## How do embeddings work?



Embeddings are created through neural networks. They capture complex relationships and semantics into [dense vectors](https://www1.se.cuhk.edu.hk/~seem5680/lecture/semantics-with-dense-vectors-2018.pdf) which are more suitable for machine learning and data processing applications. They can then project these vectors into a proper **high-dimensional** space, specifically, a [Vector Database](https://qdrant.tech/articles/what-is-a-vector-database/).
## Why Use Vector Embeddings?



The **nuances** of natural language or the hidden **meaning** in large datasets of images, sounds, or user interactions are hard to fit into a table. Traditional relational databases can't efficiently query most types of data being currently used and produced, making the **retrieval** of this information very limited.
> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.





### Creating Vector Embeddings



Embeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.",0.5319148936170213,0.4854368932038835,0.5076142131979696,0.17204301075268819,0.1568627450980392,0.1641025641025641,0.2553191489361702,0.23300970873786409,0.24365482233502536,0.4574468085106383,0.4174757281553398,0.43654822335025384,0.827072024345398,0.7203066349029541,0.7130202054977417,0.7166449427604675,0.9957261366289458,0.9954108099627774,99,0.9252336448598131,0.529585321267943
True,,2545.391,fbe586b4,What is the concept of the `best_score` strategy introduced in version 1.6.0 of the recommendation system?,"The `best_score` strategy, introduced in version 1.6.0 of the recommendation system, is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided.","The `best_score` strategy introduced in version 1.6.0 of the recommendation system is based on finding vectors that are closer to a positive example while avoiding those closer to a negative example. This strategy does not rely on averages like the `average_vector` strategy, making it more flexible. 

In the `best_score` strategy, the best score is chosen at every step of the HNSW graph traversal by calculating the distance between a traversed point and every positive and negative example separately. This strategy considers one positive and one negative example at a time, allowing for a more diverse set of results as more likes and dislikes are added. 

By taking into account each example separately, the `best_score` strategy enriches the variety of recommendations compared to the older strategy where all examples were considered similar to a single query vector.","This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
### The new hotness - Best score



The new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative 

samples and uses a slightly more sophisticated algorithm under the hood.



The best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point 

and every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a
##### One positive and one negative example



<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>



The more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there 

is just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making 

the variety richer.
embedding. Internally, that mechanism is based on the averages of positive and negative examples and was calculated with the 

following formula:



$$

\text{average vector} = \text{avg}(\text{positive vectors}) + \left( \text{avg}(\text{positive vectors}) - \text{avg}(\text{negative vectors}) \right)

$$



The `average_vector` converts the problem of recommendations into a single vector search.



### The new hotness - Best score
},

  ""strategy"": ""average_vector"",

  ""limit"": 3

}

```



There are two key changes in the request. First of all, we can adjust the strategy of search and set it to `average_vector` (the 

default) or `best_score`. Moreover, we can pass both IDs (`718`) and embeddings (`[0.2, 0.3, 0.4, 0.5]`) as both positive and 

negative examples. 



## HNSW ANN example and strategy","['This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.\n\n\n\n### Best score strategy\n\n\n\n*Available as of v1.6.0*\n\n\n\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.', '### The new hotness - Best score\n\n\n\nThe new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative \n\nsamples and uses a slightly more sophisticated algorithm under the hood.\n\n\n\nThe best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point \n\nand every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a', '##### One positive and one negative example\n\n\n\n<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>\n\n\n\nThe more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there \n\nis just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making \n\nthe variety richer.', 'embedding. Internally, that mechanism is based on the averages of positive and negative examples and was calculated with the \n\nfollowing formula:\n\n\n\n$$\n\n\\text{average vector} = \\text{avg}(\\text{positive vectors}) + \\left( \\text{avg}(\\text{positive vectors}) - \\text{avg}(\\text{negative vectors}) \\right)\n\n$$\n\n\n\nThe `average_vector` converts the problem of recommendations into a single vector search.\n\n\n\n### The new hotness - Best score', '},\n\n  ""strategy"": ""average_vector"",\n\n  ""limit"": 3\n\n}\n\n```\n\n\n\nThere are two key changes in the request. First of all, we can adjust the strategy of search and set it to `average_vector` (the \n\ndefault) or `best_score`. Moreover, we can pass both IDs (`718`) and embeddings (`[0.2, 0.3, 0.4, 0.5]`) as both positive and \n\nnegative examples. \n\n\n\n## HNSW ANN example and strategy']","Question: What is the concept of the `best_score` strategy introduced in version 1.6.0 of the recommendation system?\n\nContext: This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
### The new hotness - Best score



The new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative 

samples and uses a slightly more sophisticated algorithm under the hood.



The best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point 

and every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a
##### One positive and one negative example



<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>



The more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there 

is just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making 

the variety richer.
embedding. Internally, that mechanism is based on the averages of positive and negative examples and was calculated with the 

following formula:



$$

\text{average vector} = \text{avg}(\text{positive vectors}) + \left( \text{avg}(\text{positive vectors}) - \text{avg}(\text{negative vectors}) \right)

$$



The `average_vector` converts the problem of recommendations into a single vector search.



### The new hotness - Best score
},

""strategy"": ""average_vector"",

""limit"": 3

}

```



There are two key changes in the request. First of all, we can adjust the strategy of search and set it to `average_vector` (the 

default) or `best_score`. Moreover, we can pass both IDs (`718`) and embeddings (`[0.2, 0.3, 0.4, 0.5]`) as both positive and 

negative examples. 



## HNSW ANN example and strategy\n\nAnswer:",What is the concept of the `best_score` strategy introduced in version 1.6.0 of the recommendation system?,"The `best_score` strategy, introduced in version 1.6.0 of the recommendation system, is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided.","['This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.\n\n\n\n### Best score strategy\n\n\n\n*Available as of v1.6.0*\n\n\n\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.', '### The new hotness - Best score\n\n\n\nThe new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative \n\nsamples and uses a slightly more sophisticated algorithm under the hood.\n\n\n\nThe best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point \n\nand every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a', '##### One positive and one negative example\n\n\n\n<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>\n\n\n\nThe more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there \n\nis just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making \n\nthe variety richer.', 'embedding. Internally, that mechanism is based on the averages of positive and negative examples and was calculated with the \n\nfollowing formula:\n\n\n\n$$\n\n\\text{average vector} = \\text{avg}(\\text{positive vectors}) + \\left( \\text{avg}(\\text{positive vectors}) - \\text{avg}(\\text{negative vectors}) \\right)\n\n$$\n\n\n\nThe `average_vector` converts the problem of recommendations into a single vector search.\n\n\n\n### The new hotness - Best score', '},\n\n  ""strategy"": ""average_vector"",\n\n  ""limit"": 3\n\n}\n\n```\n\n\n\nThere are two key changes in the request. First of all, we can adjust the strategy of search and set it to `average_vector` (the \n\ndefault) or `best_score`. Moreover, we can pass both IDs (`718`) and embeddings (`[0.2, 0.3, 0.4, 0.5]`) as both positive and \n\nnegative examples. \n\n\n\n## HNSW ANN example and strategy']","This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
### The new hotness - Best score



The new strategy is called `best_score`. It does not rely on averages and is more flexible. It allows you to pass just negative 

samples and uses a slightly more sophisticated algorithm under the hood.



The best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point 

and every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a
##### One positive and one negative example



<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>



The more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there 

is just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making 

the variety richer.
embedding. Internally, that mechanism is based on the averages of positive and negative examples and was calculated with the 

following formula:



$$

\text{average vector} = \text{avg}(\text{positive vectors}) + \left( \text{avg}(\text{positive vectors}) - \text{avg}(\text{negative vectors}) \right)

$$



The `average_vector` converts the problem of recommendations into a single vector search.



### The new hotness - Best score
},

  ""strategy"": ""average_vector"",

  ""limit"": 3

}

```



There are two key changes in the request. First of all, we can adjust the strategy of search and set it to `average_vector` (the 

default) or `best_score`. Moreover, we can pass both IDs (`718`) and embeddings (`[0.2, 0.3, 0.4, 0.5]`) as both positive and 

negative examples. 



## HNSW ANN example and strategy",0.49206349206349204,0.5585585585585585,0.5232067510548524,0.232,0.2636363636363636,0.24680851063829787,0.36507936507936506,0.4144144144144144,0.38818565400843874,0.46825396825396826,0.5315315315315315,0.49789029535864976,0.8546206951141357,0.6430256366729736,0.6834276914596558,0.6626113653182983,0.9468302165158093,0.7609333708882332,136,1.0793650793650793,0.33180481288649977
True,,2094.199,82e3532b,How does Qdrant handle sparse vectors and what changes have been made to accommodate them?,"Qdrant, until recently, was not able to handle sparse vectors natively. The majority of dimensions in sparse vectors are usually zeros, making them theoretically high dimensional. However, Qdrant stores sparse vectors differently by only keeping track of the non-zero dimensions. Previously, some users attempted to convert sparse vectors to dense vectors as a workaround, but this was not considered the best solution. In response to user demand for a single tool that can handle both sparse and dense vectors, Qdrant has now introduced support for sparse vectors. This change allows users to work with both sparse and dense vectors seamlessly within the Qdrant tool, eliminating the need for alternative solutions or tools for keyword lookup.","Qdrant now handles sparse vectors as a first-class citizen, allowing for more efficient text search where each word is represented as a separate dimension. Users can now insert sparse vectors into Qdrant collections, enabling the building of datasets that can be quickly retrieved with the efficiency of sparse vectors. This feature is available starting from version 1.7.0 of Qdrant.

With the support for sparse vectors, users can now take advantage of the benefits they offer in text search applications. This enhancement aligns with the community's requests and demonstrates Qdrant's commitment to adapting to user needs and improving the search experience.","Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
`on_disk` to true to store all vectors on disk at all times. This will enable

the use of

[memmaps](../../concepts/storage/#configuring-memmap-storage),

which is suitable for ingesting a large amount of data.





### Collection with sparse vectors



*Available as of v1.7.0*



Qdrant supports sparse vectors as a first-class citizen.



Sparse vectors are useful for text search, where each word is represented as a separate dimension.
},

)

```





### 3. Inserting Sparse Vectors



Here, we see the process of inserting a sparse vector into the Qdrant collection. This step is key to building a dataset that can be quickly retrieved in the first stage of the retrieval process, utilizing the efficiency of sparse vectors. Since this is for demonstration purposes, we insert only one point with Sparse Vector and no dense vector.



```python

client.upsert(

    collection_name=COLLECTION_NAME,

    points=[
- hybrid search

  - bm25

  - tfidf

  - splade

---



Please welcome the long-awaited [Qdrant 1.7.0 release](https://github.com/qdrant/qdrant/releases/tag/v1.7.0). Except for a handful of minor fixes and improvements, this release brings some cool brand-new features that we are excited to share! 

The latest version of your favorite vector search engine finally supports **sparse vectors**. That's the feature many of you requested, so why should we ignore it?","['Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", '`on_disk` to true to store all vectors on disk at all times. This will enable\n\nthe use of\n\n[memmaps](../../concepts/storage/#configuring-memmap-storage),\n\nwhich is suitable for ingesting a large amount of data.\n\n\n\n\n\n### Collection with sparse vectors\n\n\n\n*Available as of v1.7.0*\n\n\n\nQdrant supports sparse vectors as a first-class citizen.\n\n\n\nSparse vectors are useful for text search, where each word is represented as a separate dimension.', '},\n\n)\n\n```\n\n\n\n\n\n### 3. Inserting Sparse Vectors\n\n\n\nHere, we see the process of inserting a sparse vector into the Qdrant collection. This step is key to building a dataset that can be quickly retrieved in the first stage of the retrieval process, utilizing the efficiency of sparse vectors. Since this is for demonstration purposes, we insert only one point with Sparse Vector and no dense vector.\n\n\n\n```python\n\nclient.upsert(\n\n    collection_name=COLLECTION_NAME,\n\n    points=[', ""- hybrid search\n\n  - bm25\n\n  - tfidf\n\n  - splade\n\n---\n\n\n\nPlease welcome the long-awaited [Qdrant 1.7.0 release](https://github.com/qdrant/qdrant/releases/tag/v1.7.0). Except for a handful of minor fixes and improvements, this release brings some cool brand-new features that we are excited to share! \n\nThe latest version of your favorite vector search engine finally supports **sparse vectors**. That's the feature many of you requested, so why should we ignore it?""]","Question: How does Qdrant handle sparse vectors and what changes have been made to accommodate them?\n\nContext: Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
`on_disk` to true to store all vectors on disk at all times. This will enable

the use of

[memmaps](../../concepts/storage/#configuring-memmap-storage),

which is suitable for ingesting a large amount of data.





### Collection with sparse vectors



*Available as of v1.7.0*



Qdrant supports sparse vectors as a first-class citizen.



Sparse vectors are useful for text search, where each word is represented as a separate dimension.
},

)

```





### 3. Inserting Sparse Vectors



Here, we see the process of inserting a sparse vector into the Qdrant collection. This step is key to building a dataset that can be quickly retrieved in the first stage of the retrieval process, utilizing the efficiency of sparse vectors. Since this is for demonstration purposes, we insert only one point with Sparse Vector and no dense vector.



```python

client.upsert(

collection_name=COLLECTION_NAME,

points=[
- hybrid search

- bm25

- tfidf

- splade

---



Please welcome the long-awaited [Qdrant 1.7.0 release](https://github.com/qdrant/qdrant/releases/tag/v1.7.0). Except for a handful of minor fixes and improvements, this release brings some cool brand-new features that we are excited to share! 

The latest version of your favorite vector search engine finally supports **sparse vectors**. That's the feature many of you requested, so why should we ignore it?\n\nAnswer:",How does Qdrant handle sparse vectors and what changes have been made to accommodate them?,"Qdrant, until recently, was not able to handle sparse vectors natively. The majority of dimensions in sparse vectors are usually zeros, making them theoretically high dimensional. However, Qdrant stores sparse vectors differently by only keeping track of the non-zero dimensions. Previously, some users attempted to convert sparse vectors to dense vectors as a workaround, but this was not considered the best solution. In response to user demand for a single tool that can handle both sparse and dense vectors, Qdrant has now introduced support for sparse vectors. This change allows users to work with both sparse and dense vectors seamlessly within the Qdrant tool, eliminating the need for alternative solutions or tools for keyword lookup.","['Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", '`on_disk` to true to store all vectors on disk at all times. This will enable\n\nthe use of\n\n[memmaps](../../concepts/storage/#configuring-memmap-storage),\n\nwhich is suitable for ingesting a large amount of data.\n\n\n\n\n\n### Collection with sparse vectors\n\n\n\n*Available as of v1.7.0*\n\n\n\nQdrant supports sparse vectors as a first-class citizen.\n\n\n\nSparse vectors are useful for text search, where each word is represented as a separate dimension.', '},\n\n)\n\n```\n\n\n\n\n\n### 3. Inserting Sparse Vectors\n\n\n\nHere, we see the process of inserting a sparse vector into the Qdrant collection. This step is key to building a dataset that can be quickly retrieved in the first stage of the retrieval process, utilizing the efficiency of sparse vectors. Since this is for demonstration purposes, we insert only one point with Sparse Vector and no dense vector.\n\n\n\n```python\n\nclient.upsert(\n\n    collection_name=COLLECTION_NAME,\n\n    points=[', ""- hybrid search\n\n  - bm25\n\n  - tfidf\n\n  - splade\n\n---\n\n\n\nPlease welcome the long-awaited [Qdrant 1.7.0 release](https://github.com/qdrant/qdrant/releases/tag/v1.7.0). Except for a handful of minor fixes and improvements, this release brings some cool brand-new features that we are excited to share! \n\nThe latest version of your favorite vector search engine finally supports **sparse vectors**. That's the feature many of you requested, so why should we ignore it?""]","Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
`on_disk` to true to store all vectors on disk at all times. This will enable

the use of

[memmaps](../../concepts/storage/#configuring-memmap-storage),

which is suitable for ingesting a large amount of data.





### Collection with sparse vectors



*Available as of v1.7.0*



Qdrant supports sparse vectors as a first-class citizen.



Sparse vectors are useful for text search, where each word is represented as a separate dimension.
},

)

```





### 3. Inserting Sparse Vectors



Here, we see the process of inserting a sparse vector into the Qdrant collection. This step is key to building a dataset that can be quickly retrieved in the first stage of the retrieval process, utilizing the efficiency of sparse vectors. Since this is for demonstration purposes, we insert only one point with Sparse Vector and no dense vector.



```python

client.upsert(

    collection_name=COLLECTION_NAME,

    points=[
- hybrid search

  - bm25

  - tfidf

  - splade

---



Please welcome the long-awaited [Qdrant 1.7.0 release](https://github.com/qdrant/qdrant/releases/tag/v1.7.0). Except for a handful of minor fixes and improvements, this release brings some cool brand-new features that we are excited to share! 

The latest version of your favorite vector search engine finally supports **sparse vectors**. That's the feature many of you requested, so why should we ignore it?",0.3711340206185567,0.3302752293577982,0.34951456310679613,0.11458333333333333,0.10185185185185185,0.10784313725490195,0.23711340206185566,0.21100917431192662,0.22330097087378642,0.3402061855670103,0.30275229357798167,0.32038834951456313,0.8092806935310364,0.5962629318237305,0.5982748866081238,0.597267210483551,0.8612156383693218,0.5693764125583887,102,0.8869565217391304,0.4232398958882655
True,,2103.691,980327f4,What is the purpose of the `prefix` parameter in the configuration options for the Qdrant instance?,"The `prefix` parameter in the configuration options for the Qdrant instance serves the purpose of adding a specific prefix to the REST URL path. For example, setting the `prefix` parameter to `service/v1` would result in the REST API endpoint being accessed at `http://localhost:6333/service/v1/{qdrant-endpoint}`. This allows for customization and organization of the API endpoints under a specific path, making it easier to manage and access different functionalities within the Qdrant instance.","The purpose of the `prefix` parameter in the configuration options for the Qdrant instance is to create prefixes for every single token in addition to the token boundaries defined by the ""word"" tokenizer. This means that when using the `prefix` parameter, each token will be indexed with all its prefixes. For example, the word ""Qdrant"" will be indexed as ""Q"", ""Qd"", ""Qdr"", ""Qdra"", ""Qdran"", and ""Qdrant"". This can be useful for enhancing search capabilities and improving the accuracy of search results in Qdrant.","* **word** — spaces, punctuation marks and special characters define the token boundaries

* **whitespace** — token boundaries defined by whitespace characters

* **prefix** — token boundaries are the same as for the “word” tokenizer, but in addition to that, there are prefixes created for every single token. As a result, “Qdrant” will be indexed as “Q”, “Qd”, “Qdr”, “Qdra”, “Qdran”, and “Qdrant”.



There are also some additional parameters you can provide, such as
(see [ordering](#order-and-priority)).



All environment variables are prefixed with `QDRANT__` and are separated with

`__`.



These variables:



```bash

QDRANT__LOG_LEVEL=INFO

QDRANT__SERVICE__HTTP_PORT=6333

QDRANT__SERVICE__ENABLE_TLS=1

QDRANT__TLS__CERT=./tls/cert.pem

QDRANT__TLS__CERT_TTL=3600

```



result in this configuration:



```yaml

log_level: INFO

service:

  http_port: 6333

  enable_tls: true

tls:

  cert: ./tls/cert.pem

  cert_ttl: 3600

```
## Configuration



Qdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.



Example: 

```yaml

qdrant:

    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""

    api_key: ""<your-api-key>""

```



The available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:

| Field        | Description |
| Field        | Description |

|--------------|-------------|

| location     | If `:memory:` - use in-memory Qdrant instance.<br>If `str` - use it as a `url` parameter.|

| url          | Either host or str of `Optional[scheme], host, Optional[port], Optional[prefix]`.<br> Eg. `http://localhost:6333` |

| port         | Port of the REST API interface. Default: `6333` |

| grpc_port    | Port of the gRPC interface. Default: `6334` |
---

title: Configuration

weight: 160

aliases:

  - ../configuration

---



# Configuration



To change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.



The default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).



To change the default configuration, add a new configuration file and specify","['* **word**\xa0— spaces, punctuation marks and special characters define the token boundaries\n\n* **whitespace**\xa0— token boundaries defined by whitespace characters\n\n* **prefix**\xa0— token boundaries are the same as for the “word” tokenizer, but in addition to that, there are prefixes created for every single token. As a result, “Qdrant” will be indexed as “Q”, “Qd”, “Qdr”, “Qdra”, “Qdran”, and “Qdrant”.\n\n\n\nThere are also some additional parameters you can provide, such as', '(see [ordering](#order-and-priority)).\n\n\n\nAll environment variables are prefixed with `QDRANT__` and are separated with\n\n`__`.\n\n\n\nThese variables:\n\n\n\n```bash\n\nQDRANT__LOG_LEVEL=INFO\n\nQDRANT__SERVICE__HTTP_PORT=6333\n\nQDRANT__SERVICE__ENABLE_TLS=1\n\nQDRANT__TLS__CERT=./tls/cert.pem\n\nQDRANT__TLS__CERT_TTL=3600\n\n```\n\n\n\nresult in this configuration:\n\n\n\n```yaml\n\nlog_level: INFO\n\nservice:\n\n  http_port: 6333\n\n  enable_tls: true\n\ntls:\n\n  cert: ./tls/cert.pem\n\n  cert_ttl: 3600\n\n```', '## Configuration\n\n\n\nQdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.\n\n\n\nExample: \n\n```yaml\n\nqdrant:\n\n    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""\n\n    api_key: ""<your-api-key>""\n\n```\n\n\n\nThe available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:\n\n| Field        | Description |', '| Field        | Description |\n\n|--------------|-------------|\n\n| location     | If `:memory:` - use in-memory Qdrant instance.<br>If `str` - use it as a `url` parameter.|\n\n| url          | Either host or str of `Optional[scheme], host, Optional[port], Optional[prefix]`.<br> Eg. `http://localhost:6333` |\n\n| port         | Port of the REST API interface. Default: `6333` |\n\n| grpc_port    | Port of the gRPC interface. Default: `6334` |', ""---\n\ntitle: Configuration\n\nweight: 160\n\naliases:\n\n  - ../configuration\n\n---\n\n\n\n# Configuration\n\n\n\nTo change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.\n\n\n\nThe default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\n\n\n\nTo change the default configuration, add a new configuration file and specify""]","Question: What is the purpose of the `prefix` parameter in the configuration options for the Qdrant instance?\n\nContext: * **word** — spaces, punctuation marks and special characters define the token boundaries

* **whitespace** — token boundaries defined by whitespace characters

* **prefix** — token boundaries are the same as for the “word” tokenizer, but in addition to that, there are prefixes created for every single token. As a result, “Qdrant” will be indexed as “Q”, “Qd”, “Qdr”, “Qdra”, “Qdran”, and “Qdrant”.



There are also some additional parameters you can provide, such as
(see [ordering](#order-and-priority)).



All environment variables are prefixed with `QDRANT__` and are separated with

`__`.



These variables:



```bash

QDRANT__LOG_LEVEL=INFO

QDRANT__SERVICE__HTTP_PORT=6333

QDRANT__SERVICE__ENABLE_TLS=1

QDRANT__TLS__CERT=./tls/cert.pem

QDRANT__TLS__CERT_TTL=3600

```



result in this configuration:



```yaml

log_level: INFO

service:

http_port: 6333

enable_tls: true

tls:

cert: ./tls/cert.pem

cert_ttl: 3600

```
## Configuration



Qdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.



Example: 

```yaml

qdrant:

url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""

api_key: ""<your-api-key>""

```



The available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:

| Field        | Description |
| Field        | Description |

|--------------|-------------|

| location     | If `:memory:` - use in-memory Qdrant instance.<br>If `str` - use it as a `url` parameter.|

| url          | Either host or str of `Optional[scheme], host, Optional[port], Optional[prefix]`.<br> Eg. `http://localhost:6333` |

| port         | Port of the REST API interface. Default: `6333` |

| grpc_port    | Port of the gRPC interface. Default: `6334` |
---

title: Configuration

weight: 160

aliases:

- ../configuration

---



# Configuration



To change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.



The default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).



To change the default configuration, add a new configuration file and specify\n\nAnswer:",What is the purpose of the `prefix` parameter in the configuration options for the Qdrant instance?,"The `prefix` parameter in the configuration options for the Qdrant instance serves the purpose of adding a specific prefix to the REST URL path. For example, setting the `prefix` parameter to `service/v1` would result in the REST API endpoint being accessed at `http://localhost:6333/service/v1/{qdrant-endpoint}`. This allows for customization and organization of the API endpoints under a specific path, making it easier to manage and access different functionalities within the Qdrant instance.","['* **word**\xa0— spaces, punctuation marks and special characters define the token boundaries\n\n* **whitespace**\xa0— token boundaries defined by whitespace characters\n\n* **prefix**\xa0— token boundaries are the same as for the “word” tokenizer, but in addition to that, there are prefixes created for every single token. As a result, “Qdrant” will be indexed as “Q”, “Qd”, “Qdr”, “Qdra”, “Qdran”, and “Qdrant”.\n\n\n\nThere are also some additional parameters you can provide, such as', '(see [ordering](#order-and-priority)).\n\n\n\nAll environment variables are prefixed with `QDRANT__` and are separated with\n\n`__`.\n\n\n\nThese variables:\n\n\n\n```bash\n\nQDRANT__LOG_LEVEL=INFO\n\nQDRANT__SERVICE__HTTP_PORT=6333\n\nQDRANT__SERVICE__ENABLE_TLS=1\n\nQDRANT__TLS__CERT=./tls/cert.pem\n\nQDRANT__TLS__CERT_TTL=3600\n\n```\n\n\n\nresult in this configuration:\n\n\n\n```yaml\n\nlog_level: INFO\n\nservice:\n\n  http_port: 6333\n\n  enable_tls: true\n\ntls:\n\n  cert: ./tls/cert.pem\n\n  cert_ttl: 3600\n\n```', '## Configuration\n\n\n\nQdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.\n\n\n\nExample: \n\n```yaml\n\nqdrant:\n\n    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""\n\n    api_key: ""<your-api-key>""\n\n```\n\n\n\nThe available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:\n\n| Field        | Description |', '| Field        | Description |\n\n|--------------|-------------|\n\n| location     | If `:memory:` - use in-memory Qdrant instance.<br>If `str` - use it as a `url` parameter.|\n\n| url          | Either host or str of `Optional[scheme], host, Optional[port], Optional[prefix]`.<br> Eg. `http://localhost:6333` |\n\n| port         | Port of the REST API interface. Default: `6333` |\n\n| grpc_port    | Port of the gRPC interface. Default: `6334` |', ""---\n\ntitle: Configuration\n\nweight: 160\n\naliases:\n\n  - ../configuration\n\n---\n\n\n\n# Configuration\n\n\n\nTo change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.\n\n\n\nThe default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\n\n\n\nTo change the default configuration, add a new configuration file and specify""]","* **word** — spaces, punctuation marks and special characters define the token boundaries

* **whitespace** — token boundaries defined by whitespace characters

* **prefix** — token boundaries are the same as for the “word” tokenizer, but in addition to that, there are prefixes created for every single token. As a result, “Qdrant” will be indexed as “Q”, “Qd”, “Qdr”, “Qdra”, “Qdran”, and “Qdrant”.



There are also some additional parameters you can provide, such as
(see [ordering](#order-and-priority)).



All environment variables are prefixed with `QDRANT__` and are separated with

`__`.



These variables:



```bash

QDRANT__LOG_LEVEL=INFO

QDRANT__SERVICE__HTTP_PORT=6333

QDRANT__SERVICE__ENABLE_TLS=1

QDRANT__TLS__CERT=./tls/cert.pem

QDRANT__TLS__CERT_TTL=3600

```



result in this configuration:



```yaml

log_level: INFO

service:

  http_port: 6333

  enable_tls: true

tls:

  cert: ./tls/cert.pem

  cert_ttl: 3600

```
## Configuration



Qdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.



Example: 

```yaml

qdrant:

    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""

    api_key: ""<your-api-key>""

```



The available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:

| Field        | Description |
| Field        | Description |

|--------------|-------------|

| location     | If `:memory:` - use in-memory Qdrant instance.<br>If `str` - use it as a `url` parameter.|

| url          | Either host or str of `Optional[scheme], host, Optional[port], Optional[prefix]`.<br> Eg. `http://localhost:6333` |

| port         | Port of the REST API interface. Default: `6333` |

| grpc_port    | Port of the gRPC interface. Default: `6334` |
---

title: Configuration

weight: 160

aliases:

  - ../configuration

---



# Configuration



To change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.



The default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).



To change the default configuration, add a new configuration file and specify",0.36486486486486486,0.4090909090909091,0.38571428571428573,0.1506849315068493,0.16923076923076924,0.15942028985507248,0.25675675675675674,0.2878787878787879,0.2714285714285714,0.33783783783783783,0.3787878787878788,0.3571428571428571,0.7573504447937012,0.6739053726196289,0.6745216846466064,0.6742134094238281,0.859253667993471,0.7723132173220316,83,1.1527777777777777,0.3834132794801059
True,,593.936,9709fb55,What is the default scoring metric used in Qdrant for search operations?,"The default scoring metric used in Qdrant for search operations is `Dot product`. It is not necessary to specify this metric explicitly as it is the default setting. However, Qdrant also supports other distance metrics such as Cosine for scoring. When performing a search operation in Qdrant, if no specific metric is mentioned, the system will automatically use the Dot product as the scoring metric.",The default scoring metric used in Qdrant for search operations is the Dot product.,"| Index| Sparse Query | Dense Query |

| --- | --- | --- | 

| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |

| Search Type | Always exact in Qdrant | HNSW is an approximate NN |

| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |



In general, the speed of the search is proportional to the number of non-zero values in the query vector.



```http
qdrant: &QdrantClient,

) -> Result<Vec<ScoredPoint>> {

    Ok(qdrant.search_points(&SearchPoints {

        collection_name,

        limit: 5, // use what fits your use case here

        with_payload: Some(true.into()),

        vector: embed(client, text, api_key)?,

        ..Default::default()

    }).await?.result)

}

```
Once all shards of the collection are recovered, the collection will become operational again.



## Consistency guarantees



By default, Qdrant focuses on availability and maximum throughput of search operations.

For the majority of use cases, this is a preferable trade-off.



During the normal state of operation, it is possible to search and modify data from any peers in the cluster.
search_points: searches,

        read_consistency: None,

        ..Default::default()

    })

    .await?;

```



```java

import java.util.List;



import static io.qdrant.client.ConditionFactory.matchKeyword;



import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Points.Filter;

import io.qdrant.client.grpc.Points.SearchPoints;



QdrantClient client =
let search_result = client

    .search_points(&SearchPoints {

        collection_name: ""test_collection"".to_string(),

        vector: vec![0.2, 0.1, 0.9, 0.7],

        filter: Some(Filter::all([Condition::matches(

            ""city"",

            ""London"".to_string(),

        )])),

        limit: 2,

        ..Default::default()

    })

    .await?;



dbg!(search_result);

```



```java

import static io.qdrant.client.ConditionFactory.matchKeyword;



List<ScoredPoint> searchResult =

    client","['| Index| Sparse Query | Dense Query |\n\n| --- | --- | --- | \n\n| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |\n\n| Search Type | Always exact in Qdrant | HNSW is an approximate NN |\n\n| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |\n\n\n\nIn general, the speed of the search is proportional to the number of non-zero values in the query vector.\n\n\n\n```http', 'qdrant: &QdrantClient,\n\n) -> Result<Vec<ScoredPoint>> {\n\n    Ok(qdrant.search_points(&SearchPoints {\n\n        collection_name,\n\n        limit: 5, // use what fits your use case here\n\n        with_payload: Some(true.into()),\n\n        vector: embed(client, text, api_key)?,\n\n        ..Default::default()\n\n    }).await?.result)\n\n}\n\n```', 'Once all shards of the collection are recovered, the collection will become operational again.\n\n\n\n## Consistency guarantees\n\n\n\nBy default, Qdrant focuses on availability and maximum throughput of search operations.\n\nFor the majority of use cases, this is a preferable trade-off.\n\n\n\nDuring the normal state of operation, it is possible to search and modify data from any peers in the cluster.', 'search_points: searches,\n\n        read_consistency: None,\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport java.util.List;\n\n\n\nimport static io.qdrant.client.ConditionFactory.matchKeyword;\n\n\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Points.Filter;\n\nimport io.qdrant.client.grpc.Points.SearchPoints;\n\n\n\nQdrantClient client =', 'let search_result = client\n\n    .search_points(&SearchPoints {\n\n        collection_name: ""test_collection"".to_string(),\n\n        vector: vec![0.2, 0.1, 0.9, 0.7],\n\n        filter: Some(Filter::all([Condition::matches(\n\n            ""city"",\n\n            ""London"".to_string(),\n\n        )])),\n\n        limit: 2,\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n\n\ndbg!(search_result);\n\n```\n\n\n\n```java\n\nimport static io.qdrant.client.ConditionFactory.matchKeyword;\n\n\n\nList<ScoredPoint> searchResult =\n\n    client']","Question: What is the default scoring metric used in Qdrant for search operations?\n\nContext: | Index| Sparse Query | Dense Query |

| --- | --- | --- | 

| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |

| Search Type | Always exact in Qdrant | HNSW is an approximate NN |

| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |



In general, the speed of the search is proportional to the number of non-zero values in the query vector.



```http
qdrant: &QdrantClient,

) -> Result<Vec<ScoredPoint>> {

Ok(qdrant.search_points(&SearchPoints {

collection_name,

limit: 5, // use what fits your use case here

with_payload: Some(true.into()),

vector: embed(client, text, api_key)?,

..Default::default()

}).await?.result)

}

```
Once all shards of the collection are recovered, the collection will become operational again.



## Consistency guarantees



By default, Qdrant focuses on availability and maximum throughput of search operations.

For the majority of use cases, this is a preferable trade-off.



During the normal state of operation, it is possible to search and modify data from any peers in the cluster.
search_points: searches,

read_consistency: None,

..Default::default()

})

.await?;

```



```java

import java.util.List;



import static io.qdrant.client.ConditionFactory.matchKeyword;



import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Points.Filter;

import io.qdrant.client.grpc.Points.SearchPoints;



QdrantClient client =
let search_result = client

.search_points(&SearchPoints {

collection_name: ""test_collection"".to_string(),

vector: vec![0.2, 0.1, 0.9, 0.7],

filter: Some(Filter::all([Condition::matches(

""city"",

""London"".to_string(),

)])),

limit: 2,

..Default::default()

})

.await?;



dbg!(search_result);

```



```java

import static io.qdrant.client.ConditionFactory.matchKeyword;



List<ScoredPoint> searchResult =

client\n\nAnswer:",What is the default scoring metric used in Qdrant for search operations?,"The default scoring metric used in Qdrant for search operations is `Dot product`. It is not necessary to specify this metric explicitly as it is the default setting. However, Qdrant also supports other distance metrics such as Cosine for scoring. When performing a search operation in Qdrant, if no specific metric is mentioned, the system will automatically use the Dot product as the scoring metric.","['| Index| Sparse Query | Dense Query |\n\n| --- | --- | --- | \n\n| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |\n\n| Search Type | Always exact in Qdrant | HNSW is an approximate NN |\n\n| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |\n\n\n\nIn general, the speed of the search is proportional to the number of non-zero values in the query vector.\n\n\n\n```http', 'qdrant: &QdrantClient,\n\n) -> Result<Vec<ScoredPoint>> {\n\n    Ok(qdrant.search_points(&SearchPoints {\n\n        collection_name,\n\n        limit: 5, // use what fits your use case here\n\n        with_payload: Some(true.into()),\n\n        vector: embed(client, text, api_key)?,\n\n        ..Default::default()\n\n    }).await?.result)\n\n}\n\n```', 'Once all shards of the collection are recovered, the collection will become operational again.\n\n\n\n## Consistency guarantees\n\n\n\nBy default, Qdrant focuses on availability and maximum throughput of search operations.\n\nFor the majority of use cases, this is a preferable trade-off.\n\n\n\nDuring the normal state of operation, it is possible to search and modify data from any peers in the cluster.', 'search_points: searches,\n\n        read_consistency: None,\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport java.util.List;\n\n\n\nimport static io.qdrant.client.ConditionFactory.matchKeyword;\n\n\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Points.Filter;\n\nimport io.qdrant.client.grpc.Points.SearchPoints;\n\n\n\nQdrantClient client =', 'let search_result = client\n\n    .search_points(&SearchPoints {\n\n        collection_name: ""test_collection"".to_string(),\n\n        vector: vec![0.2, 0.1, 0.9, 0.7],\n\n        filter: Some(Filter::all([Condition::matches(\n\n            ""city"",\n\n            ""London"".to_string(),\n\n        )])),\n\n        limit: 2,\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n\n\ndbg!(search_result);\n\n```\n\n\n\n```java\n\nimport static io.qdrant.client.ConditionFactory.matchKeyword;\n\n\n\nList<ScoredPoint> searchResult =\n\n    client']","| Index| Sparse Query | Dense Query |

| --- | --- | --- | 

| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |

| Search Type | Always exact in Qdrant | HNSW is an approximate NN |

| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |



In general, the speed of the search is proportional to the number of non-zero values in the query vector.



```http
qdrant: &QdrantClient,

) -> Result<Vec<ScoredPoint>> {

    Ok(qdrant.search_points(&SearchPoints {

        collection_name,

        limit: 5, // use what fits your use case here

        with_payload: Some(true.into()),

        vector: embed(client, text, api_key)?,

        ..Default::default()

    }).await?.result)

}

```
Once all shards of the collection are recovered, the collection will become operational again.



## Consistency guarantees



By default, Qdrant focuses on availability and maximum throughput of search operations.

For the majority of use cases, this is a preferable trade-off.



During the normal state of operation, it is possible to search and modify data from any peers in the cluster.
search_points: searches,

        read_consistency: None,

        ..Default::default()

    })

    .await?;

```



```java

import java.util.List;



import static io.qdrant.client.ConditionFactory.matchKeyword;



import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Points.Filter;

import io.qdrant.client.grpc.Points.SearchPoints;



QdrantClient client =
let search_result = client

    .search_points(&SearchPoints {

        collection_name: ""test_collection"".to_string(),

        vector: vec![0.2, 0.1, 0.9, 0.7],

        filter: Some(Filter::all([Condition::matches(

            ""city"",

            ""London"".to_string(),

        )])),

        limit: 2,

        ..Default::default()

    })

    .await?;



dbg!(search_result);

```



```java

import static io.qdrant.client.ConditionFactory.matchKeyword;



List<ScoredPoint> searchResult =

    client",1.0,0.2033898305084746,0.33802816901408456,1.0,0.1896551724137931,0.3188405797101449,1.0,0.2033898305084746,0.33802816901408456,1.0,0.2033898305084746,0.33802816901408456,0.9470928907394409,0.7628515958786011,0.5855770707130432,0.6625614166259766,0.9907938148826361,0.9675267378042918,14,0.2153846153846154,0.14701701023471894
True,,3322.001,fbf6ea4a,What options are available for using Qdrant if the official clients do not support the language you are using?,"If the official clients for Qdrant do not support the language you are using, you have the option to either use the REST API directly or generate a client for your language using OpenAPI definitions or protobuf definitions. By utilizing the OpenAPI definitions available at [this link](https://github.com/qdrant/qdrant/blob/master/docs/redoc/master/openapi.json) or the protobuf definitions at [this link](https://github.com/qdrant/qdrant/tree/master/lib/api/src/grpc/proto), you can interact with Qdrant's functionalities even if there is no official client available for your specific programming language. This flexibility allows developers to integrate Qdrant into a wide range of applications and environments.","If the official clients do not support the language you are using, you have the following options for using Qdrant:

1. Use the REST API directly: You can interact with Qdrant using the REST API directly, regardless of the programming language you are using.

2. Generate a client for your language using OpenAPI or protobuf definitions:
   - OpenAPI: You can generate a client for your language by utilizing the OpenAPI specification provided by Qdrant. The OpenAPI specification can be found at [OpenAPI](https://github.com/qdrant/qdrant/blob/master/docs/redoc/master/openapi.json).
   - Protobuf: Alternatively, you can generate a client for your language using the protobuf definitions provided by Qdrant. The protobuf definitions can be found at [protobuf](https://github.com/qdrant/qdrant/tree/master/lib/api/src/grpc/proto).

By leveraging these options, you can still integrate your preferred programming language with Qdrant even if the official clients do not directly support it.","---

title: Interfaces

weight: 14

---



# Interfaces



Qdrant supports these ""official"" clients. 



> **Note:** If you are using a language that is not listed here, you can use the REST API directly or generate a client for your language 

using [OpenAPI](https://github.com/qdrant/qdrant/blob/master/docs/redoc/master/openapi.json)

or [protobuf](https://github.com/qdrant/qdrant/tree/master/lib/api/src/grpc/proto) definitions. 



## Client Libraries

||Client Repository|Installation|Version|

|-|-|-|-|
**Clients**: Qdrant supports various programming languages for client interaction, such as Python, Go, Rust, and Typescript. This way developers can connect to and interact with Qdrant using the programming language they prefer.





### Vector Database Use Cases
. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.
If you prefer reusing an existing collection, you can create an instance of Qdrant on your own:



```python

import qdrant_client



embeddings = HuggingFaceEmbeddings(

    model_name=""sentence-transformers/all-mpnet-base-v2""

)



client = qdrant_client.QdrantClient(

    ""<qdrant-url>"",

    api_key=""<qdrant-api-key>"", # For Qdrant Cloud, None for local instance

)



doc_store = Qdrant(

    client=client, collection_name=""texts"", 

    embeddings=embeddings,

)

```



## Local mode
of code, thanks to the recent integration with Qdrant. We're not even going to work directly with `QdrantClient`, as everything is already done in the background

by LangChain. If you want to get into the source code right away, all the processing is available as a 

[Google Colab notebook](https://colab.research.google.com/drive/19RxxkZdnq_YqBH5kBV10Rt0Rax-kminD?usp=sharing).



## Implementing Question Answering with LangChain and Qdrant



### Configuration","['---\n\ntitle: Interfaces\n\nweight: 14\n\n---\n\n\n\n# Interfaces\n\n\n\nQdrant supports these ""official"" clients. \n\n\n\n> **Note:** If you are using a language that is not listed here, you can use the REST API directly or generate a client for your language \n\nusing [OpenAPI](https://github.com/qdrant/qdrant/blob/master/docs/redoc/master/openapi.json)\n\nor [protobuf](https://github.com/qdrant/qdrant/tree/master/lib/api/src/grpc/proto) definitions. \n\n\n\n## Client Libraries\n\n||Client Repository|Installation|Version|\n\n|-|-|-|-|', '**Clients**: Qdrant supports various programming languages for client interaction, such as Python, Go, Rust, and Typescript. This way developers can connect to and interact with Qdrant using the programming language they prefer.\n\n\n\n\n\n### Vector Database Use Cases', '. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.', 'If you prefer reusing an existing collection, you can create an instance of Qdrant on your own:\n\n\n\n```python\n\nimport qdrant_client\n\n\n\nembeddings = HuggingFaceEmbeddings(\n\n    model_name=""sentence-transformers/all-mpnet-base-v2""\n\n)\n\n\n\nclient = qdrant_client.QdrantClient(\n\n    ""<qdrant-url>"",\n\n    api_key=""<qdrant-api-key>"", # For Qdrant Cloud, None for local instance\n\n)\n\n\n\ndoc_store = Qdrant(\n\n    client=client, collection_name=""texts"", \n\n    embeddings=embeddings,\n\n)\n\n```\n\n\n\n## Local mode', ""of code, thanks to the recent integration with Qdrant. We're not even going to work directly with `QdrantClient`, as everything is already done in the background\n\nby LangChain. If you want to get into the source code right away, all the processing is available as a \n\n[Google Colab notebook](https://colab.research.google.com/drive/19RxxkZdnq_YqBH5kBV10Rt0Rax-kminD?usp=sharing).\n\n\n\n## Implementing Question Answering with LangChain and Qdrant\n\n\n\n### Configuration""]","Question: What options are available for using Qdrant if the official clients do not support the language you are using?\n\nContext: ---

title: Interfaces

weight: 14

---



# Interfaces



Qdrant supports these ""official"" clients. 



> **Note:** If you are using a language that is not listed here, you can use the REST API directly or generate a client for your language 

using [OpenAPI](https://github.com/qdrant/qdrant/blob/master/docs/redoc/master/openapi.json)

or [protobuf](https://github.com/qdrant/qdrant/tree/master/lib/api/src/grpc/proto) definitions. 



## Client Libraries

||Client Repository|Installation|Version|

|-|-|-|-|
**Clients**: Qdrant supports various programming languages for client interaction, such as Python, Go, Rust, and Typescript. This way developers can connect to and interact with Qdrant using the programming language they prefer.





### Vector Database Use Cases
. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.
If you prefer reusing an existing collection, you can create an instance of Qdrant on your own:



```python

import qdrant_client



embeddings = HuggingFaceEmbeddings(

model_name=""sentence-transformers/all-mpnet-base-v2""

)



client = qdrant_client.QdrantClient(

""<qdrant-url>"",

api_key=""<qdrant-api-key>"", # For Qdrant Cloud, None for local instance

)



doc_store = Qdrant(

client=client, collection_name=""texts"", 

embeddings=embeddings,

)

```



## Local mode
of code, thanks to the recent integration with Qdrant. We're not even going to work directly with `QdrantClient`, as everything is already done in the background

by LangChain. If you want to get into the source code right away, all the processing is available as a 

[Google Colab notebook](https://colab.research.google.com/drive/19RxxkZdnq_YqBH5kBV10Rt0Rax-kminD?usp=sharing).



## Implementing Question Answering with LangChain and Qdrant



### Configuration\n\nAnswer:",What options are available for using Qdrant if the official clients do not support the language you are using?,"If the official clients for Qdrant do not support the language you are using, you have the option to either use the REST API directly or generate a client for your language using OpenAPI definitions or protobuf definitions. By utilizing the OpenAPI definitions available at [this link](https://github.com/qdrant/qdrant/blob/master/docs/redoc/master/openapi.json) or the protobuf definitions at [this link](https://github.com/qdrant/qdrant/tree/master/lib/api/src/grpc/proto), you can interact with Qdrant's functionalities even if there is no official client available for your specific programming language. This flexibility allows developers to integrate Qdrant into a wide range of applications and environments.","['---\n\ntitle: Interfaces\n\nweight: 14\n\n---\n\n\n\n# Interfaces\n\n\n\nQdrant supports these ""official"" clients. \n\n\n\n> **Note:** If you are using a language that is not listed here, you can use the REST API directly or generate a client for your language \n\nusing [OpenAPI](https://github.com/qdrant/qdrant/blob/master/docs/redoc/master/openapi.json)\n\nor [protobuf](https://github.com/qdrant/qdrant/tree/master/lib/api/src/grpc/proto) definitions. \n\n\n\n## Client Libraries\n\n||Client Repository|Installation|Version|\n\n|-|-|-|-|', '**Clients**: Qdrant supports various programming languages for client interaction, such as Python, Go, Rust, and Typescript. This way developers can connect to and interact with Qdrant using the programming language they prefer.\n\n\n\n\n\n### Vector Database Use Cases', '. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.', 'If you prefer reusing an existing collection, you can create an instance of Qdrant on your own:\n\n\n\n```python\n\nimport qdrant_client\n\n\n\nembeddings = HuggingFaceEmbeddings(\n\n    model_name=""sentence-transformers/all-mpnet-base-v2""\n\n)\n\n\n\nclient = qdrant_client.QdrantClient(\n\n    ""<qdrant-url>"",\n\n    api_key=""<qdrant-api-key>"", # For Qdrant Cloud, None for local instance\n\n)\n\n\n\ndoc_store = Qdrant(\n\n    client=client, collection_name=""texts"", \n\n    embeddings=embeddings,\n\n)\n\n```\n\n\n\n## Local mode', ""of code, thanks to the recent integration with Qdrant. We're not even going to work directly with `QdrantClient`, as everything is already done in the background\n\nby LangChain. If you want to get into the source code right away, all the processing is available as a \n\n[Google Colab notebook](https://colab.research.google.com/drive/19RxxkZdnq_YqBH5kBV10Rt0Rax-kminD?usp=sharing).\n\n\n\n## Implementing Question Answering with LangChain and Qdrant\n\n\n\n### Configuration""]","---

title: Interfaces

weight: 14

---



# Interfaces



Qdrant supports these ""official"" clients. 



> **Note:** If you are using a language that is not listed here, you can use the REST API directly or generate a client for your language 

using [OpenAPI](https://github.com/qdrant/qdrant/blob/master/docs/redoc/master/openapi.json)

or [protobuf](https://github.com/qdrant/qdrant/tree/master/lib/api/src/grpc/proto) definitions. 



## Client Libraries

||Client Repository|Installation|Version|

|-|-|-|-|
**Clients**: Qdrant supports various programming languages for client interaction, such as Python, Go, Rust, and Typescript. This way developers can connect to and interact with Qdrant using the programming language they prefer.





### Vector Database Use Cases
. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.
If you prefer reusing an existing collection, you can create an instance of Qdrant on your own:



```python

import qdrant_client



embeddings = HuggingFaceEmbeddings(

    model_name=""sentence-transformers/all-mpnet-base-v2""

)



client = qdrant_client.QdrantClient(

    ""<qdrant-url>"",

    api_key=""<qdrant-api-key>"", # For Qdrant Cloud, None for local instance

)



doc_store = Qdrant(

    client=client, collection_name=""texts"", 

    embeddings=embeddings,

)

```



## Local mode
of code, thanks to the recent integration with Qdrant. We're not even going to work directly with `QdrantClient`, as everything is already done in the background

by LangChain. If you want to get into the source code right away, all the processing is available as a 

[Google Colab notebook](https://colab.research.google.com/drive/19RxxkZdnq_YqBH5kBV10Rt0Rax-kminD?usp=sharing).



## Implementing Question Answering with LangChain and Qdrant



### Configuration",0.5571428571428572,0.7428571428571429,0.636734693877551,0.39568345323741005,0.5288461538461539,0.45267489711934156,0.4714285714285714,0.6285714285714286,0.5387755102040815,0.5428571428571428,0.7238095238095238,0.6204081632653061,0.9647461771965027,0.750755786895752,0.7763577103614807,0.7633422017097473,0.7162292177478473,0.8550723499308029,134,1.4408602150537635,0.5819239580527971
True,,1471.79,244f3e63,What are the two main interfaces available for interacting with Qdrant and where can the documentation for each be found?,"The two main interfaces available for interacting with Qdrant are REST API and gRPC API. The documentation for REST API can be found in the [OpenAPI Specification](https://qdrant.github.io/qdrant/redoc/index.html), while the documentation for gRPC API can be found [here](https://github.com/qdrant/qdrant/blob/master/docs/grpc/docs.md). The gRPC methods in Qdrant follow the same principles as REST, with each REST endpoint having a corresponding gRPC method. The gRPC interface in Qdrant is available on a specified port as outlined in the configuration file, with the default port being 6334. If choosing to use gRPC, it is necessary to expose the port when starting Qdrant, such as when running the service inside a Docker container.","The two main interfaces available for interacting with Qdrant are the REST API and the gRPC API. 

- Documentation for the REST API can be found in the Qdrant documentation under the API section.
- Documentation for the gRPC API can be found in the Qdrant repository on GitHub.","|![java](/docs/misc/java.webp)|**[Java](https://github.com/qdrant/java-client)**|[Available on Maven Central](https://central.sonatype.com/artifact/io.qdrant/client)|[Latest Release](https://github.com/qdrant/java-client/releases)|





## API Reference



All interaction with Qdrant takes place via the REST API. We recommend using REST API if you are using Qdrant for the first time or if you are working on a prototype.



|API|Documentation|

|-|-|
In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. 



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Accessing the Web UI



First, run the Docker container:



```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```



The GUI is available at `http://localhost:6333/dashboard`
## Project Overview



Qdrant is a powerful vector search engine widely used for similarity search and clustering. However, it lacked a user-friendly web-based UI for data visualization and exploration. My project aimed to bridge this gap by developing a web-based user interface that allows users to easily interact with and explore their vector data.



## Milestones and Achievements
maintained, thus not mentioned here. However, it is still possible to interact with both engines through the HTTP REST or gRPC API. 

That makes it easy to integrate with any technology of your choice.



If you are a Python user, then both tools are well-integrated with the most popular libraries like [LangChain](../integrations/langchain/), [LlamaIndex](../integrations/llama-index/), [Haystack](../integrations/haystack/), and more.
.                                                                                    |","['|![java](/docs/misc/java.webp)|**[Java](https://github.com/qdrant/java-client)**|[Available on Maven Central](https://central.sonatype.com/artifact/io.qdrant/client)|[Latest Release](https://github.com/qdrant/java-client/releases)|\n\n\n\n\n\n## API Reference\n\n\n\nAll interaction with Qdrant takes place via the REST API. We recommend using REST API if you are using Qdrant for the first time or if you are working on a prototype.\n\n\n\n|API|Documentation|\n\n|-|-|', 'In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. \n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Accessing the Web UI\n\n\n\nFirst, run the Docker container:\n\n\n\n```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```\n\n\n\nThe GUI is available at `http://localhost:6333/dashboard`', '## Project Overview\n\n\n\nQdrant is a powerful vector search engine widely used for similarity search and clustering. However, it lacked a user-friendly web-based UI for data visualization and exploration. My project aimed to bridge this gap by developing a web-based user interface that allows users to easily interact with and explore their vector data.\n\n\n\n## Milestones and Achievements', 'maintained, thus not mentioned here. However, it is still possible to interact with both engines through the HTTP REST or gRPC API. \n\nThat makes it easy to integrate with any technology of your choice.\n\n\n\nIf you are a Python user, then both tools are well-integrated with the most popular libraries like [LangChain](../integrations/langchain/), [LlamaIndex](../integrations/llama-index/), [Haystack](../integrations/haystack/), and more.', '.                                                                                    |']","Question: What are the two main interfaces available for interacting with Qdrant and where can the documentation for each be found?\n\nContext: |![java](/docs/misc/java.webp)|**[Java](https://github.com/qdrant/java-client)**|[Available on Maven Central](https://central.sonatype.com/artifact/io.qdrant/client)|[Latest Release](https://github.com/qdrant/java-client/releases)|





## API Reference



All interaction with Qdrant takes place via the REST API. We recommend using REST API if you are using Qdrant for the first time or if you are working on a prototype.



|API|Documentation|

|-|-|
In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. 



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Accessing the Web UI



First, run the Docker container:



```bash

docker run -p 6333:6333 -p 6334:6334 \

-v $(pwd)/qdrant_storage:/qdrant/storage:z \

qdrant/qdrant

```



The GUI is available at `http://localhost:6333/dashboard`
## Project Overview



Qdrant is a powerful vector search engine widely used for similarity search and clustering. However, it lacked a user-friendly web-based UI for data visualization and exploration. My project aimed to bridge this gap by developing a web-based user interface that allows users to easily interact with and explore their vector data.



## Milestones and Achievements
maintained, thus not mentioned here. However, it is still possible to interact with both engines through the HTTP REST or gRPC API. 

That makes it easy to integrate with any technology of your choice.



If you are a Python user, then both tools are well-integrated with the most popular libraries like [LangChain](../integrations/langchain/), [LlamaIndex](../integrations/llama-index/), [Haystack](../integrations/haystack/), and more.
.                                                                                    |\n\nAnswer:",What are the two main interfaces available for interacting with Qdrant and where can the documentation for each be found?,"The two main interfaces available for interacting with Qdrant are REST API and gRPC API. The documentation for REST API can be found in the [OpenAPI Specification](https://qdrant.github.io/qdrant/redoc/index.html), while the documentation for gRPC API can be found [here](https://github.com/qdrant/qdrant/blob/master/docs/grpc/docs.md). The gRPC methods in Qdrant follow the same principles as REST, with each REST endpoint having a corresponding gRPC method. The gRPC interface in Qdrant is available on a specified port as outlined in the configuration file, with the default port being 6334. If choosing to use gRPC, it is necessary to expose the port when starting Qdrant, such as when running the service inside a Docker container.","['|![java](/docs/misc/java.webp)|**[Java](https://github.com/qdrant/java-client)**|[Available on Maven Central](https://central.sonatype.com/artifact/io.qdrant/client)|[Latest Release](https://github.com/qdrant/java-client/releases)|\n\n\n\n\n\n## API Reference\n\n\n\nAll interaction with Qdrant takes place via the REST API. We recommend using REST API if you are using Qdrant for the first time or if you are working on a prototype.\n\n\n\n|API|Documentation|\n\n|-|-|', 'In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. \n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Accessing the Web UI\n\n\n\nFirst, run the Docker container:\n\n\n\n```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```\n\n\n\nThe GUI is available at `http://localhost:6333/dashboard`', '## Project Overview\n\n\n\nQdrant is a powerful vector search engine widely used for similarity search and clustering. However, it lacked a user-friendly web-based UI for data visualization and exploration. My project aimed to bridge this gap by developing a web-based user interface that allows users to easily interact with and explore their vector data.\n\n\n\n## Milestones and Achievements', 'maintained, thus not mentioned here. However, it is still possible to interact with both engines through the HTTP REST or gRPC API. \n\nThat makes it easy to integrate with any technology of your choice.\n\n\n\nIf you are a Python user, then both tools are well-integrated with the most popular libraries like [LangChain](../integrations/langchain/), [LlamaIndex](../integrations/llama-index/), [Haystack](../integrations/haystack/), and more.', '.                                                                                    |']","|![java](/docs/misc/java.webp)|**[Java](https://github.com/qdrant/java-client)**|[Available on Maven Central](https://central.sonatype.com/artifact/io.qdrant/client)|[Latest Release](https://github.com/qdrant/java-client/releases)|





## API Reference



All interaction with Qdrant takes place via the REST API. We recommend using REST API if you are using Qdrant for the first time or if you are working on a prototype.



|API|Documentation|

|-|-|
In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. 



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Accessing the Web UI



First, run the Docker container:



```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```



The GUI is available at `http://localhost:6333/dashboard`
## Project Overview



Qdrant is a powerful vector search engine widely used for similarity search and clustering. However, it lacked a user-friendly web-based UI for data visualization and exploration. My project aimed to bridge this gap by developing a web-based user interface that allows users to easily interact with and explore their vector data.



## Milestones and Achievements
maintained, thus not mentioned here. However, it is still possible to interact with both engines through the HTTP REST or gRPC API. 

That makes it easy to integrate with any technology of your choice.



If you are a Python user, then both tools are well-integrated with the most popular libraries like [LangChain](../integrations/langchain/), [LlamaIndex](../integrations/llama-index/), [Haystack](../integrations/haystack/), and more.
.                                                                                    |",0.8717948717948718,0.3090909090909091,0.4563758389261745,0.7631578947368421,0.26605504587155965,0.3945578231292517,0.8461538461538461,0.3,0.4429530201342282,0.7692307692307693,0.2727272727272727,0.40268456375838924,0.9476631283760071,0.8105306625366211,0.6439216136932373,0.7176835536956787,0.9740497572347522,0.7572957647498697,47,0.43119266055045874,0.5103145444819586
True,,4115.4039999999995,1bb6d673,When should one use gRPC with Qdrant and what factors should be considered in making this choice?,"gRPC is recommended for use with Qdrant when optimizing the performance of an application is a priority and when the user is already familiar with Qdrant. The decision to use gRPC over the REST API is a trade-off between convenience and speed. gRPC is a binary protocol that offers higher performance compared to REST API, but it can be more challenging to debug. Therefore, if the goal is to maximize performance and the user is comfortable with the potential debugging complexities associated with gRPC, it is advisable to opt for gRPC when interacting with Qdrant.","When considering whether to use gRPC with Qdrant, several factors should be taken into account:

1. **Performance:** gRPC is known for its high performance due to its binary protocol and efficient serialization. If you need to optimize the performance of your application, using gRPC with Qdrant can be beneficial.

2. **Familiarity:** If you are already familiar with Qdrant and have experience working with gRPC, it might be easier for you to integrate gRPC into your application.

3. **Debugging:** gRPC can be more challenging to debug compared to REST API due to its binary nature. If easy debugging is a priority for your project, you may want to consider this factor.

4. **Security:** By default, Qdrant starts with no encryption or authentication when using gRPC. It is important to consider security implications and take necessary steps to secure your Qdrant instance if you choose to use gRPC.

5. **Compatibility:** Ensure that the language or framework you are using supports gRPC. Qdrant provides examples for using gRPC with languages like C#, Python, TypeScript, and Rust, so compatibility should not be a major issue.

Ultimately, the decision to use gRPC with Qdrant should be based on your specific project requirements and considerations of performance, familiarity, debugging needs, security, and compatibility.","qdrant/qdrant

```



**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.



## Qdrant Web UI



Qdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points.
As per the [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml), the gRPC interface is available on the specified port.



```yaml

service:

  grpc_port: 6334

```

<aside role=""status"">If you decide to use gRPC, you must expose the port when starting Qdrant.</aside>

 

Running the service inside of Docker will look like this:



```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```
![REST API vs gRPC upload time, sec](/blog/qdrant-v-0-6-0-engine-with-grpc-released/upload_time.png)



Read more about the gRPC interface and whether you should use it by this [link](/documentation/quick_start/#grpc).



The release v0.6.0 includes several bug fixes. More information is available in a [changelog](https://github.com/qdrant/qdrant/releases/tag/v0.6.0).



New version was provided in addition to the REST API that the company keeps supporting due to its easy debugging.
// The C# client uses Qdrant's GRPC interface

var client = new QdrantClient(""localhost"", 6334);

```



<aside role=""status"">By default, Qdrant starts with no encryption or authentication . This means anyone with network access to your machine can access your Qdrant container instance. Please read <a href=""https://qdrant.tech/documentation/security/"">Security</a> carefully for details on how to secure your instance.</aside>



## Create a collection
- GRPC API: [localhost:6334](http://localhost:6334)



## Initialize the client



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });

```



```rust

use qdrant_client::client::QdrantClient;



// The Rust client uses Qdrant's GRPC interface","[""qdrant/qdrant\n\n```\n\n\n\n**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.\n\n\n\n## Qdrant Web UI\n\n\n\nQdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points."", 'As per the [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml), the gRPC interface is available on the specified port.\n\n\n\n```yaml\n\nservice:\n\n  grpc_port: 6334\n\n```\n\n<aside role=""status"">If you decide to use gRPC, you must expose the port when starting Qdrant.</aside>\n\n \n\nRunning the service inside of Docker will look like this:\n\n\n\n```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```', '![REST API vs gRPC upload time, sec](/blog/qdrant-v-0-6-0-engine-with-grpc-released/upload_time.png)\n\n\n\nRead more about the gRPC interface and whether you should use it by this [link](/documentation/quick_start/#grpc).\n\n\n\nThe release v0.6.0 includes several bug fixes. More information is available in a [changelog](https://github.com/qdrant/qdrant/releases/tag/v0.6.0).\n\n\n\nNew version was provided in addition to the REST API that the company keeps supporting due to its easy debugging.', '// The C# client uses Qdrant\'s GRPC interface\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n```\n\n\n\n<aside role=""status"">By default, Qdrant starts with no encryption or authentication . This means anyone with network access to your machine can access your Qdrant container instance. Please read <a href=""https://qdrant.tech/documentation/security/"">Security</a> carefully for details on how to secure your instance.</aside>\n\n\n\n## Create a collection', '- GRPC API: [localhost:6334](http://localhost:6334)\n\n\n\n## Initialize the client\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n```\n\n\n\n```rust\n\nuse qdrant_client::client::QdrantClient;\n\n\n\n// The Rust client uses Qdrant\'s GRPC interface']","Question: When should one use gRPC with Qdrant and what factors should be considered in making this choice?\n\nContext: qdrant/qdrant

```



**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.



## Qdrant Web UI



Qdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points.
As per the [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml), the gRPC interface is available on the specified port.



```yaml

service:

grpc_port: 6334

```

<aside role=""status"">If you decide to use gRPC, you must expose the port when starting Qdrant.</aside>



Running the service inside of Docker will look like this:



```bash

docker run -p 6333:6333 -p 6334:6334 \

-v $(pwd)/qdrant_storage:/qdrant/storage:z \

qdrant/qdrant

```
![REST API vs gRPC upload time, sec](/blog/qdrant-v-0-6-0-engine-with-grpc-released/upload_time.png)



Read more about the gRPC interface and whether you should use it by this [link](/documentation/quick_start/#grpc).



The release v0.6.0 includes several bug fixes. More information is available in a [changelog](https://github.com/qdrant/qdrant/releases/tag/v0.6.0).



New version was provided in addition to the REST API that the company keeps supporting due to its easy debugging.
// The C# client uses Qdrant's GRPC interface

var client = new QdrantClient(""localhost"", 6334);

```



<aside role=""status"">By default, Qdrant starts with no encryption or authentication . This means anyone with network access to your machine can access your Qdrant container instance. Please read <a href=""https://qdrant.tech/documentation/security/"">Security</a> carefully for details on how to secure your instance.</aside>



## Create a collection
- GRPC API: [localhost:6334](http://localhost:6334)



## Initialize the client



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });

```



```rust

use qdrant_client::client::QdrantClient;



// The Rust client uses Qdrant's GRPC interface\n\nAnswer:",When should one use gRPC with Qdrant and what factors should be considered in making this choice?,"gRPC is recommended for use with Qdrant when optimizing the performance of an application is a priority and when the user is already familiar with Qdrant. The decision to use gRPC over the REST API is a trade-off between convenience and speed. gRPC is a binary protocol that offers higher performance compared to REST API, but it can be more challenging to debug. Therefore, if the goal is to maximize performance and the user is comfortable with the potential debugging complexities associated with gRPC, it is advisable to opt for gRPC when interacting with Qdrant.","[""qdrant/qdrant\n\n```\n\n\n\n**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.\n\n\n\n## Qdrant Web UI\n\n\n\nQdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points."", 'As per the [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml), the gRPC interface is available on the specified port.\n\n\n\n```yaml\n\nservice:\n\n  grpc_port: 6334\n\n```\n\n<aside role=""status"">If you decide to use gRPC, you must expose the port when starting Qdrant.</aside>\n\n \n\nRunning the service inside of Docker will look like this:\n\n\n\n```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```', '![REST API vs gRPC upload time, sec](/blog/qdrant-v-0-6-0-engine-with-grpc-released/upload_time.png)\n\n\n\nRead more about the gRPC interface and whether you should use it by this [link](/documentation/quick_start/#grpc).\n\n\n\nThe release v0.6.0 includes several bug fixes. More information is available in a [changelog](https://github.com/qdrant/qdrant/releases/tag/v0.6.0).\n\n\n\nNew version was provided in addition to the REST API that the company keeps supporting due to its easy debugging.', '// The C# client uses Qdrant\'s GRPC interface\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n```\n\n\n\n<aside role=""status"">By default, Qdrant starts with no encryption or authentication . This means anyone with network access to your machine can access your Qdrant container instance. Please read <a href=""https://qdrant.tech/documentation/security/"">Security</a> carefully for details on how to secure your instance.</aside>\n\n\n\n## Create a collection', '- GRPC API: [localhost:6334](http://localhost:6334)\n\n\n\n## Initialize the client\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n```\n\n\n\n```rust\n\nuse qdrant_client::client::QdrantClient;\n\n\n\n// The Rust client uses Qdrant\'s GRPC interface']","qdrant/qdrant

```



**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.



## Qdrant Web UI



Qdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points.
As per the [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml), the gRPC interface is available on the specified port.



```yaml

service:

  grpc_port: 6334

```

<aside role=""status"">If you decide to use gRPC, you must expose the port when starting Qdrant.</aside>

 

Running the service inside of Docker will look like this:



```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```
![REST API vs gRPC upload time, sec](/blog/qdrant-v-0-6-0-engine-with-grpc-released/upload_time.png)



Read more about the gRPC interface and whether you should use it by this [link](/documentation/quick_start/#grpc).



The release v0.6.0 includes several bug fixes. More information is available in a [changelog](https://github.com/qdrant/qdrant/releases/tag/v0.6.0).



New version was provided in addition to the REST API that the company keeps supporting due to its easy debugging.
// The C# client uses Qdrant's GRPC interface

var client = new QdrantClient(""localhost"", 6334);

```



<aside role=""status"">By default, Qdrant starts with no encryption or authentication . This means anyone with network access to your machine can access your Qdrant container instance. Please read <a href=""https://qdrant.tech/documentation/security/"">Security</a> carefully for details on how to secure your instance.</aside>



## Create a collection
- GRPC API: [localhost:6334](http://localhost:6334)



## Initialize the client



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });

```



```rust

use qdrant_client::client::QdrantClient;



// The Rust client uses Qdrant's GRPC interface",0.2722772277227723,0.6470588235294118,0.3832752613240418,0.12437810945273632,0.2976190476190476,0.1754385964912281,0.15841584158415842,0.3764705882352941,0.22299651567944248,0.24257425742574257,0.5764705882352941,0.3414634146341463,0.8934964537620544,0.6018152236938477,0.6807120442390442,0.6388369202613831,0.6314287706588706,0.9873776113963686,207,2.1789473684210527,0.6857492331297582
True,,1118.727,352823ca,Where can the GUI for Qdrant be accessed?,"The GUI for Qdrant can be accessed at `http://localhost:6333/dashboard`. This web address will lead you to the graphical user interface (GUI) for Qdrant, providing a visual interface for interacting with the Qdrant system. The GUI allows users to perform various operations and tasks related to Qdrant, such as managing indexes, querying data, and configuring settings. By accessing the GUI at the specified URL, users can easily navigate and interact with the Qdrant system in a user-friendly and intuitive manner.",The GUI for Qdrant can be accessed at `http://localhost:6333/dashboard` after running the Docker container as mentioned in the provided code snippet.,"In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. 



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Accessing the Web UI



First, run the Docker container:



```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```



The GUI is available at `http://localhost:6333/dashboard`
Try it out now! If you have Docker running, you can [quickstart Qdrant](https://qdrant.tech/documentation/quick-start/) and access the Dashboard locally from [http://localhost:6333/dashboard](http://localhost:6333/dashboard). You should see this simple access point to Qdrant:



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Temporary directory for Snapshots
||Client Repository|Installation|Version|

|-|-|-|-|

|[![python](/docs/misc/python.webp)](https://python-client.qdrant.tech/)|**[Python](https://github.com/qdrant/qdrant-client)** + **[(Client Docs)](https://python-client.qdrant.tech/)**|`pip install qdrant-client[fastembed]`|[Latest Release](https://github.com/qdrant/qdrant-client/releases)|
access is required only for persistence), or **Memmap** storage, (creates a virtual address 

space associated with the file on disk).

- Clients: the programming languages you can use to connect to Qdrant.



## Next Steps



Now that you know more about vector databases and Qdrant, you are ready to get started with one 

of our tutorials. If you've never used a vector database, go ahead and jump straight into 

the **Getting Started** section. Conversely, if you are a seasoned developer in these
.                                                                                                                                                                                                        |","['In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. \n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Accessing the Web UI\n\n\n\nFirst, run the Docker container:\n\n\n\n```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```\n\n\n\nThe GUI is available at `http://localhost:6333/dashboard`', 'Try it out now! If you have Docker running, you can [quickstart Qdrant](https://qdrant.tech/documentation/quick-start/) and access the Dashboard locally from [http://localhost:6333/dashboard](http://localhost:6333/dashboard). You should see this simple access point to Qdrant:\n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Temporary directory for Snapshots', '||Client Repository|Installation|Version|\n\n|-|-|-|-|\n\n|[![python](/docs/misc/python.webp)](https://python-client.qdrant.tech/)|**[Python](https://github.com/qdrant/qdrant-client)** + **[(Client Docs)](https://python-client.qdrant.tech/)**|`pip install qdrant-client[fastembed]`|[Latest Release](https://github.com/qdrant/qdrant-client/releases)|', ""access is required only for persistence), or **Memmap** storage, (creates a virtual address \n\nspace associated with the file on disk).\n\n- Clients: the programming languages you can use to connect to Qdrant.\n\n\n\n## Next Steps\n\n\n\nNow that you know more about vector databases and Qdrant, you are ready to get started with one \n\nof our tutorials. If you've never used a vector database, go ahead and jump straight into \n\nthe **Getting Started** section. Conversely, if you are a seasoned developer in these"", '.                                                                                                                                                                                                        |']","Question: Where can the GUI for Qdrant be accessed?\n\nContext: In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. 



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Accessing the Web UI



First, run the Docker container:



```bash

docker run -p 6333:6333 -p 6334:6334 \

-v $(pwd)/qdrant_storage:/qdrant/storage:z \

qdrant/qdrant

```



The GUI is available at `http://localhost:6333/dashboard`
Try it out now! If you have Docker running, you can [quickstart Qdrant](https://qdrant.tech/documentation/quick-start/) and access the Dashboard locally from [http://localhost:6333/dashboard](http://localhost:6333/dashboard). You should see this simple access point to Qdrant:



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Temporary directory for Snapshots
||Client Repository|Installation|Version|

|-|-|-|-|

|[![python](/docs/misc/python.webp)](https://python-client.qdrant.tech/)|**[Python](https://github.com/qdrant/qdrant-client)** + **[(Client Docs)](https://python-client.qdrant.tech/)**|`pip install qdrant-client[fastembed]`|[Latest Release](https://github.com/qdrant/qdrant-client/releases)|
access is required only for persistence), or **Memmap** storage, (creates a virtual address 

space associated with the file on disk).

- Clients: the programming languages you can use to connect to Qdrant.



## Next Steps



Now that you know more about vector databases and Qdrant, you are ready to get started with one 

of our tutorials. If you've never used a vector database, go ahead and jump straight into 

the **Getting Started** section. Conversely, if you are a seasoned developer in these
.                                                                                                                                                                                                        |\n\nAnswer:",Where can the GUI for Qdrant be accessed?,"The GUI for Qdrant can be accessed at `http://localhost:6333/dashboard`. This web address will lead you to the graphical user interface (GUI) for Qdrant, providing a visual interface for interacting with the Qdrant system. The GUI allows users to perform various operations and tasks related to Qdrant, such as managing indexes, querying data, and configuring settings. By accessing the GUI at the specified URL, users can easily navigate and interact with the Qdrant system in a user-friendly and intuitive manner.","['In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. \n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Accessing the Web UI\n\n\n\nFirst, run the Docker container:\n\n\n\n```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```\n\n\n\nThe GUI is available at `http://localhost:6333/dashboard`', 'Try it out now! If you have Docker running, you can [quickstart Qdrant](https://qdrant.tech/documentation/quick-start/) and access the Dashboard locally from [http://localhost:6333/dashboard](http://localhost:6333/dashboard). You should see this simple access point to Qdrant:\n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Temporary directory for Snapshots', '||Client Repository|Installation|Version|\n\n|-|-|-|-|\n\n|[![python](/docs/misc/python.webp)](https://python-client.qdrant.tech/)|**[Python](https://github.com/qdrant/qdrant-client)** + **[(Client Docs)](https://python-client.qdrant.tech/)**|`pip install qdrant-client[fastembed]`|[Latest Release](https://github.com/qdrant/qdrant-client/releases)|', ""access is required only for persistence), or **Memmap** storage, (creates a virtual address \n\nspace associated with the file on disk).\n\n- Clients: the programming languages you can use to connect to Qdrant.\n\n\n\n## Next Steps\n\n\n\nNow that you know more about vector databases and Qdrant, you are ready to get started with one \n\nof our tutorials. If you've never used a vector database, go ahead and jump straight into \n\nthe **Getting Started** section. Conversely, if you are a seasoned developer in these"", '.                                                                                                                                                                                                        |']","In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. 



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Accessing the Web UI



First, run the Docker container:



```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```



The GUI is available at `http://localhost:6333/dashboard`
Try it out now! If you have Docker running, you can [quickstart Qdrant](https://qdrant.tech/documentation/quick-start/) and access the Dashboard locally from [http://localhost:6333/dashboard](http://localhost:6333/dashboard). You should see this simple access point to Qdrant:



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Temporary directory for Snapshots
||Client Repository|Installation|Version|

|-|-|-|-|

|[![python](/docs/misc/python.webp)](https://python-client.qdrant.tech/)|**[Python](https://github.com/qdrant/qdrant-client)** + **[(Client Docs)](https://python-client.qdrant.tech/)**|`pip install qdrant-client[fastembed]`|[Latest Release](https://github.com/qdrant/qdrant-client/releases)|
access is required only for persistence), or **Memmap** storage, (creates a virtual address 

space associated with the file on disk).

- Clients: the programming languages you can use to connect to Qdrant.



## Next Steps



Now that you know more about vector databases and Qdrant, you are ready to get started with one 

of our tutorials. If you've never used a vector database, go ahead and jump straight into 

the **Getting Started** section. Conversely, if you are a seasoned developer in these
.                                                                                                                                                                                                        |",0.6666666666666666,0.1891891891891892,0.2947368421052632,0.5,0.136986301369863,0.21505376344086022,0.6190476190476191,0.17567567567567569,0.2736842105263158,0.6666666666666666,0.1891891891891892,0.2947368421052632,0.7347590327262878,0.7243656516075134,0.5894615054130554,0.6499875783920288,0.9894822938367724,0.9342172187753022,22,0.275,0.4001456937738891
True,,2846.955,92ac1ddc,What are the three ways to use Qdrant?,"According to the documentation, there are three ways to use Qdrant:

1. Run a Docker image: This method is recommended for users who do not have a Python development environment. By running a Docker image, users can quickly set up a local Qdrant server and storage.

2. Get the Python client: Users familiar with Python can install the Qdrant client by using `pip install qdrant-client`. The Python client also supports an in-memory database.

3. Spin up a Qdrant Cloud cluster: This method is recommended for running Qdrant in a production environment. Users can set up their first instance by following the Quickstart guide provided in the documentation.","The three ways to use Qdrant are:

1. Run a Docker image: This is suitable for those who don't have a Python development environment. By setting up a local Qdrant server and storage using a Docker image, you can start using Qdrant quickly.

2. Get the Python client: If you are familiar with Python, you can install the Qdrant Python client by using `pip install qdrant-client`. This client supports all necessary operations with vectors, allows storing additional payload along with vectors, and can be used for filtering search results. Additionally, it supports an in-memory database.

3. Use the API schema: Qdrant also defines an API schema, making it possible to use Qdrant from other programming languages besides Python.

By following these three ways, users can easily utilize Qdrant for vector search and other related operations.","## First-Time Users:



There are three ways to use Qdrant:



1. [**Run a Docker image**](quick-start/) if you don't have a Python development environment. Setup a local Qdrant server and storage in a few moments. 

2. [**Get the Python client**](https://github.com/qdrant/qdrant-client) if you're familiar with Python. Just `pip install qdrant-client`. The client also supports an in-memory database.
It not only supports all necessary operations with vectors but also allows to store additional payload along with vectors and use it to perform filtering of the search result.

Qdrant has a client for python and also defines the API schema if you need to use it from other languages.



The easiest way to use Qdrant is to run a pre-built image.

So make sure you have Docker installed on your system.



To start Qdrant, use the instructions on its [homepage](https://github.com/qdrant/qdrant).
In our tutorial we will use [Qdrant](https://qdrant.tech/) vector search engine. It not only supports all necessary operations with vectors but also allows to store additional payload along with vectors and use it to perform filtering of the search result. Qdrant has a client for python and also defines the API schema if you need to use it from other languages.



The easiest way to use Qdrant is to run a pre-built image. So make sure you have Docker installed on your system.
## Why Qdrant?
.                                                                                    |","[""## First-Time Users:\n\n\n\nThere are three ways to use Qdrant:\n\n\n\n1. [**Run a Docker image**](quick-start/) if you don't have a Python development environment. Setup a local Qdrant server and storage in a few moments. \n\n2. [**Get the Python client**](https://github.com/qdrant/qdrant-client) if you're familiar with Python. Just `pip install qdrant-client`. The client also supports an in-memory database."", 'It not only supports all necessary operations with vectors but also allows to store additional payload along with vectors and use it to perform filtering of the search result.\n\nQdrant has a client for python and also defines the API schema if you need to use it from other languages.\n\n\n\nThe easiest way to use Qdrant is to run a pre-built image.\n\nSo make sure you have Docker installed on your system.\n\n\n\nTo start Qdrant, use the instructions on its [homepage](https://github.com/qdrant/qdrant).', 'In our tutorial we will use\xa0[Qdrant](https://qdrant.tech/)\xa0vector search engine. It not only supports all necessary operations with vectors but also allows to store additional payload along with vectors and use it to perform filtering of the search result. Qdrant has a client for python and also defines the API schema if you need to use it from other languages.\n\n\n\nThe easiest way to use Qdrant is to run a pre-built image. So make sure you have Docker installed on your system.', '## Why Qdrant?', '.                                                                                    |']","Question: What are the three ways to use Qdrant?\n\nContext: ## First-Time Users:



There are three ways to use Qdrant:



1. [**Run a Docker image**](quick-start/) if you don't have a Python development environment. Setup a local Qdrant server and storage in a few moments. 

2. [**Get the Python client**](https://github.com/qdrant/qdrant-client) if you're familiar with Python. Just `pip install qdrant-client`. The client also supports an in-memory database.
It not only supports all necessary operations with vectors but also allows to store additional payload along with vectors and use it to perform filtering of the search result.

Qdrant has a client for python and also defines the API schema if you need to use it from other languages.



The easiest way to use Qdrant is to run a pre-built image.

So make sure you have Docker installed on your system.



To start Qdrant, use the instructions on its [homepage](https://github.com/qdrant/qdrant).
In our tutorial we will use [Qdrant](https://qdrant.tech/) vector search engine. It not only supports all necessary operations with vectors but also allows to store additional payload along with vectors and use it to perform filtering of the search result. Qdrant has a client for python and also defines the API schema if you need to use it from other languages.



The easiest way to use Qdrant is to run a pre-built image. So make sure you have Docker installed on your system.
## Why Qdrant?
.                                                                                    |\n\nAnswer:",What are the three ways to use Qdrant?,"According to the documentation, there are three ways to use Qdrant:

1. Run a Docker image: This method is recommended for users who do not have a Python development environment. By running a Docker image, users can quickly set up a local Qdrant server and storage.

2. Get the Python client: Users familiar with Python can install the Qdrant client by using `pip install qdrant-client`. The Python client also supports an in-memory database.

3. Spin up a Qdrant Cloud cluster: This method is recommended for running Qdrant in a production environment. Users can set up their first instance by following the Quickstart guide provided in the documentation.","[""## First-Time Users:\n\n\n\nThere are three ways to use Qdrant:\n\n\n\n1. [**Run a Docker image**](quick-start/) if you don't have a Python development environment. Setup a local Qdrant server and storage in a few moments. \n\n2. [**Get the Python client**](https://github.com/qdrant/qdrant-client) if you're familiar with Python. Just `pip install qdrant-client`. The client also supports an in-memory database."", 'It not only supports all necessary operations with vectors but also allows to store additional payload along with vectors and use it to perform filtering of the search result.\n\nQdrant has a client for python and also defines the API schema if you need to use it from other languages.\n\n\n\nThe easiest way to use Qdrant is to run a pre-built image.\n\nSo make sure you have Docker installed on your system.\n\n\n\nTo start Qdrant, use the instructions on its [homepage](https://github.com/qdrant/qdrant).', 'In our tutorial we will use\xa0[Qdrant](https://qdrant.tech/)\xa0vector search engine. It not only supports all necessary operations with vectors but also allows to store additional payload along with vectors and use it to perform filtering of the search result. Qdrant has a client for python and also defines the API schema if you need to use it from other languages.\n\n\n\nThe easiest way to use Qdrant is to run a pre-built image. So make sure you have Docker installed on your system.', '## Why Qdrant?', '.                                                                                    |']","## First-Time Users:



There are three ways to use Qdrant:



1. [**Run a Docker image**](quick-start/) if you don't have a Python development environment. Setup a local Qdrant server and storage in a few moments. 

2. [**Get the Python client**](https://github.com/qdrant/qdrant-client) if you're familiar with Python. Just `pip install qdrant-client`. The client also supports an in-memory database.
It not only supports all necessary operations with vectors but also allows to store additional payload along with vectors and use it to perform filtering of the search result.

Qdrant has a client for python and also defines the API schema if you need to use it from other languages.



The easiest way to use Qdrant is to run a pre-built image.

So make sure you have Docker installed on your system.



To start Qdrant, use the instructions on its [homepage](https://github.com/qdrant/qdrant).
In our tutorial we will use [Qdrant](https://qdrant.tech/) vector search engine. It not only supports all necessary operations with vectors but also allows to store additional payload along with vectors and use it to perform filtering of the search result. Qdrant has a client for python and also defines the API schema if you need to use it from other languages.



The easiest way to use Qdrant is to run a pre-built image. So make sure you have Docker installed on your system.
## Why Qdrant?
.                                                                                    |",0.5,0.6666666666666666,0.5714285714285715,0.30708661417322836,0.4105263157894737,0.35135135135135137,0.40625,0.5416666666666666,0.46428571428571425,0.46875,0.625,0.5357142857142857,0.8349003791809082,0.7200998067855835,0.7526753544807434,0.7360273003578186,0.995083834289939,0.8424746842550425,136,1.2710280373831775,0.7137590981029159
True,,4930.327,0ce2460d,What are the recommended ways for Qdrant users to provide feedback or contribute to the project?,"Qdrant encourages feedback and contributions from its users, specifically Data Scientists, ML Engineers, and MLOps professionals. The best way for Qdrant users to contribute is by providing feedback on their experience with Qdrant. This can include reporting problems, unexpected behaviors, or suggesting improvements in documentation. Users can share their feedback through various channels such as creating an issue on the Qdrant GitHub repository, starting a discussion on GitHub, or sending a message on Discord. Additionally, users who use Qdrant or Metric Learning in their projects are encouraged to share their stories, articles, and demos with the Qdrant community. For those familiar with Rust, there is a specific contribution guide available on the Qdrant GitHub repository. If users encounter problems with understanding the code or architecture, they can reach out to the Qdrant team at any time for assistance.","The recommended ways for Qdrant users to provide feedback or contribute to the project are:

1. **Feedback on Experience**: Users can provide feedback on their experience with Qdrant by reporting any problems, unexpected behavior, or lack of documentation. This can be done by creating an issue on the Qdrant GitHub repository, starting a discussion, or sending a message through the Discord channel.

2. **Share Stories**: Users who use Qdrant or Metric Learning in their projects are encouraged to share their stories, articles, and demos in the community to contribute to the project's knowledge base.

3. **Contribution Guide**: For users familiar with Rust, there is a specific contribution guide available on the Qdrant GitHub repository that provides guidelines on how to contribute to the project.

4. **Autogen Integration**: Users can leverage the Autogen-Qdrant integration to build retrieval augmented generation services with ease. This integration allows for seamless human participation, enabling users to provide input and feedback to the agents as needed.

5. **Installation and Usage**: Users can install the necessary packages for Autogen and Qdrant integration and refer to the demo application provided for guidance on usage. Additionally, users can reach out for assistance with code or architecture understanding at any time.

6. **Opportunities to Work**: Users feeling confident and wanting to contribute more are encouraged to explore opportunities to work with the Qdrant team by visiting the specified link.

By engaging in these recommended ways, Qdrant users can actively contribute to the project's development and improvement.","---

title: Contribution Guidelines

weight: 35

draft: true

---



# How to contribute



If you are a Qdrant user - Data Scientist, ML Engineer, or MLOps, the best contribution would be the feedback on your experience with Qdrant.

Let us know whenever you have a problem, face an unexpected behavior, or see a lack of documentation.
You can do it in any convenient way - create an [issue](https://github.com/qdrant/qdrant/issues), start a [discussion](https://github.com/qdrant/qdrant/discussions), or drop up a [message](https://discord.gg/tdtYvXjC4h).

If you use Qdrant or Metric Learning in your projects, we'd love to hear your story! Feel free to share articles and demos in our community.



For those familiar with Rust - check out our [contribution guide](https://github.com/qdrant/qdrant/blob/master/CONTRIBUTING.md).
- Human participation: AutoGen seamlessly allows human participation. This means that humans can provide input and feedback to the agents as needed.



With the Autogen-Qdrant integration, you can use the `QdrantRetrieveUserProxyAgent` from autogen to build retrieval augmented generation(RAG) services with ease.



## Installation



```bash

pip install ""pyautogen[retrievechat]"" ""qdrant_client[fastembed]""

```



## Usage



A demo application that generates code based on context w/o human feedback
If you have problems with code or architecture understanding - reach us at any time.

Feeling confident and want to contribute more? - Come to [work with us](https://qdrant.join.com/)!
)

```



#### Positive and negative feedback



Since the [Recommendation API](https://qdrant.tech/documentation/concepts/search/#recommendation-api) requires at least one positive example, we can use it only when 

the user has liked at least one dish. We could theoretically use the same trick as above and negate the disliked dishes, but it would be a bit weird, as Qdrant has","['---\n\ntitle: Contribution Guidelines\n\nweight: 35\n\ndraft: true\n\n---\n\n\n\n# How to contribute\n\n\n\nIf you are a Qdrant user - Data Scientist, ML Engineer, or MLOps, the best contribution would be the feedback on your experience with Qdrant.\n\nLet us know whenever you have a problem, face an unexpected behavior, or see a lack of documentation.', ""You can do it in any convenient way - create an [issue](https://github.com/qdrant/qdrant/issues), start a [discussion](https://github.com/qdrant/qdrant/discussions), or drop up a [message](https://discord.gg/tdtYvXjC4h).\n\nIf you use Qdrant or Metric Learning in your projects, we'd love to hear your story! Feel free to share articles and demos in our community.\n\n\n\nFor those familiar with Rust - check out our [contribution guide](https://github.com/qdrant/qdrant/blob/master/CONTRIBUTING.md)."", '- Human participation: AutoGen seamlessly allows human participation. This means that humans can provide input and feedback to the agents as needed.\n\n\n\nWith the Autogen-Qdrant integration, you can use the `QdrantRetrieveUserProxyAgent` from autogen to build retrieval augmented generation(RAG) services with ease.\n\n\n\n## Installation\n\n\n\n```bash\n\npip install ""pyautogen[retrievechat]"" ""qdrant_client[fastembed]""\n\n```\n\n\n\n## Usage\n\n\n\nA demo application that generates code based on context w/o human feedback', 'If you have problems with code or architecture understanding - reach us at any time.\n\nFeeling confident and want to contribute more? - Come to [work with us](https://qdrant.join.com/)!', ')\n\n```\n\n\n\n#### Positive and negative feedback\n\n\n\nSince the [Recommendation API](https://qdrant.tech/documentation/concepts/search/#recommendation-api) requires at least one positive example, we can use it only when \n\nthe user has liked at least one dish. We could theoretically use the same trick as above and negate the disliked dishes, but it would be a bit weird, as Qdrant has']","Question: What are the recommended ways for Qdrant users to provide feedback or contribute to the project?\n\nContext: ---

title: Contribution Guidelines

weight: 35

draft: true

---



# How to contribute



If you are a Qdrant user - Data Scientist, ML Engineer, or MLOps, the best contribution would be the feedback on your experience with Qdrant.

Let us know whenever you have a problem, face an unexpected behavior, or see a lack of documentation.
You can do it in any convenient way - create an [issue](https://github.com/qdrant/qdrant/issues), start a [discussion](https://github.com/qdrant/qdrant/discussions), or drop up a [message](https://discord.gg/tdtYvXjC4h).

If you use Qdrant or Metric Learning in your projects, we'd love to hear your story! Feel free to share articles and demos in our community.



For those familiar with Rust - check out our [contribution guide](https://github.com/qdrant/qdrant/blob/master/CONTRIBUTING.md).
- Human participation: AutoGen seamlessly allows human participation. This means that humans can provide input and feedback to the agents as needed.



With the Autogen-Qdrant integration, you can use the `QdrantRetrieveUserProxyAgent` from autogen to build retrieval augmented generation(RAG) services with ease.



## Installation



```bash

pip install ""pyautogen[retrievechat]"" ""qdrant_client[fastembed]""

```



## Usage



A demo application that generates code based on context w/o human feedback
If you have problems with code or architecture understanding - reach us at any time.

Feeling confident and want to contribute more? - Come to [work with us](https://qdrant.join.com/)!
)

```



#### Positive and negative feedback



Since the [Recommendation API](https://qdrant.tech/documentation/concepts/search/#recommendation-api) requires at least one positive example, we can use it only when 

the user has liked at least one dish. We could theoretically use the same trick as above and negate the disliked dishes, but it would be a bit weird, as Qdrant has\n\nAnswer:",What are the recommended ways for Qdrant users to provide feedback or contribute to the project?,"Qdrant encourages feedback and contributions from its users, specifically Data Scientists, ML Engineers, and MLOps professionals. The best way for Qdrant users to contribute is by providing feedback on their experience with Qdrant. This can include reporting problems, unexpected behaviors, or suggesting improvements in documentation. Users can share their feedback through various channels such as creating an issue on the Qdrant GitHub repository, starting a discussion on GitHub, or sending a message on Discord. Additionally, users who use Qdrant or Metric Learning in their projects are encouraged to share their stories, articles, and demos with the Qdrant community. For those familiar with Rust, there is a specific contribution guide available on the Qdrant GitHub repository. If users encounter problems with understanding the code or architecture, they can reach out to the Qdrant team at any time for assistance.","['---\n\ntitle: Contribution Guidelines\n\nweight: 35\n\ndraft: true\n\n---\n\n\n\n# How to contribute\n\n\n\nIf you are a Qdrant user - Data Scientist, ML Engineer, or MLOps, the best contribution would be the feedback on your experience with Qdrant.\n\nLet us know whenever you have a problem, face an unexpected behavior, or see a lack of documentation.', ""You can do it in any convenient way - create an [issue](https://github.com/qdrant/qdrant/issues), start a [discussion](https://github.com/qdrant/qdrant/discussions), or drop up a [message](https://discord.gg/tdtYvXjC4h).\n\nIf you use Qdrant or Metric Learning in your projects, we'd love to hear your story! Feel free to share articles and demos in our community.\n\n\n\nFor those familiar with Rust - check out our [contribution guide](https://github.com/qdrant/qdrant/blob/master/CONTRIBUTING.md)."", '- Human participation: AutoGen seamlessly allows human participation. This means that humans can provide input and feedback to the agents as needed.\n\n\n\nWith the Autogen-Qdrant integration, you can use the `QdrantRetrieveUserProxyAgent` from autogen to build retrieval augmented generation(RAG) services with ease.\n\n\n\n## Installation\n\n\n\n```bash\n\npip install ""pyautogen[retrievechat]"" ""qdrant_client[fastembed]""\n\n```\n\n\n\n## Usage\n\n\n\nA demo application that generates code based on context w/o human feedback', 'If you have problems with code or architecture understanding - reach us at any time.\n\nFeeling confident and want to contribute more? - Come to [work with us](https://qdrant.join.com/)!', ')\n\n```\n\n\n\n#### Positive and negative feedback\n\n\n\nSince the [Recommendation API](https://qdrant.tech/documentation/concepts/search/#recommendation-api) requires at least one positive example, we can use it only when \n\nthe user has liked at least one dish. We could theoretically use the same trick as above and negate the disliked dishes, but it would be a bit weird, as Qdrant has']","---

title: Contribution Guidelines

weight: 35

draft: true

---



# How to contribute



If you are a Qdrant user - Data Scientist, ML Engineer, or MLOps, the best contribution would be the feedback on your experience with Qdrant.

Let us know whenever you have a problem, face an unexpected behavior, or see a lack of documentation.
You can do it in any convenient way - create an [issue](https://github.com/qdrant/qdrant/issues), start a [discussion](https://github.com/qdrant/qdrant/discussions), or drop up a [message](https://discord.gg/tdtYvXjC4h).

If you use Qdrant or Metric Learning in your projects, we'd love to hear your story! Feel free to share articles and demos in our community.



For those familiar with Rust - check out our [contribution guide](https://github.com/qdrant/qdrant/blob/master/CONTRIBUTING.md).
- Human participation: AutoGen seamlessly allows human participation. This means that humans can provide input and feedback to the agents as needed.



With the Autogen-Qdrant integration, you can use the `QdrantRetrieveUserProxyAgent` from autogen to build retrieval augmented generation(RAG) services with ease.



## Installation



```bash

pip install ""pyautogen[retrievechat]"" ""qdrant_client[fastembed]""

```



## Usage



A demo application that generates code based on context w/o human feedback
If you have problems with code or architecture understanding - reach us at any time.

Feeling confident and want to contribute more? - Come to [work with us](https://qdrant.join.com/)!
)

```



#### Positive and negative feedback



Since the [Recommendation API](https://qdrant.tech/documentation/concepts/search/#recommendation-api) requires at least one positive example, we can use it only when 

the user has liked at least one dish. We could theoretically use the same trick as above and negate the disliked dishes, but it would be a bit weird, as Qdrant has",0.4608695652173913,0.828125,0.5921787709497207,0.2838427947598253,0.5118110236220472,0.3651685393258427,0.3217391304347826,0.578125,0.41340782122905023,0.43043478260869567,0.7734375,0.553072625698324,0.9242590665817261,0.7086514830589294,0.7967549562454224,0.7501251101493835,0.6692013337888056,0.9983411700814031,248,1.7971014492753623,0.6688206602799811
True,,2463.808,41a69de6,What is Aleph Alpha and what capabilities does their API offer?,"Aleph Alpha is a multimodal and multilingual embeddings provider. Their API allows for the creation of embeddings for both text and images, placing them in the same latent space. They offer an official Python client that can be installed using pip. The API provides both synchronous and asynchronous clients. Users can obtain embeddings for images and store them into Qdrant by utilizing the provided Python client and following the specified steps in the documentation.","Aleph Alpha is a multimodal and multilingual embeddings provider. Their API allows users to create embeddings for both text and images in the same latent space. Some capabilities offered by their API include:

1. Creating embeddings for text and images in a multimodal and multilingual manner.
2. Maintaining an official Python client for easy integration with the API.
3. Providing the ability to vectorize datasets for semantic search systems.
4. Offering different models to choose from for generating embeddings, such as the ""luminous-base"" model.

Overall, Aleph Alpha's API enables users to easily generate embeddings for text and images, facilitating tasks like semantic search without the need for manual annotation or network training.","---

title: Aleph Alpha

weight: 900

aliases: [ ../integrations/aleph-alpha/ ]

---



Aleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both 

in the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be 

installed with pip:



```bash

pip install aleph-alpha-client

```
[Docker is the fastest way to do that](https://qdrant.tech/documentation/quick_start/#installation).

2. You need to have a registered [Aleph Alpha account](https://app.aleph-alpha.com/). 

3. Upon registration, create an API key (see: [API Tokens](https://app.aleph-alpha.com/profile)).



Now you can store the Aleph Alpha API key in a variable and choose the model your are going to use.



```python

aa_token = ""<< your_token >>""

model = ""luminous-base""

```



## Vectorize the dataset
---

title: Aleph Alpha Search

weight: 16

---



# Multimodal Semantic Search with Aleph Alpha



| Time: 30 min | Level: Beginner |  |    |

| --- | ----------- | ----------- |----------- |



This tutorial shows you how to run a proper multimodal semantic search system with a few lines of code, without the need to annotate the data or train your networks.
for i, image_path in enumerate(glob(""./val2017/*.jpg"")):

        # Convert the JPEG file into the embedding by calling

        # Aleph Alpha API

        prompt = Image.from_file(image_path)

        prompt = Prompt.from_image(prompt)

        query_params = {

            ""prompt"": prompt,

            ""representation"": SemanticRepresentation.Symmetric,

            ""compress_to_size"": 128,

        }

        query_request = SemanticEmbeddingRequest(**query_params)
.tech/documentation/embeddings/aleph-alpha/), [Fastembed](https://github.com/qdrant/fastembed), and [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/).","[""---\n\ntitle: Aleph Alpha\n\nweight: 900\n\naliases: [ ../integrations/aleph-alpha/ ]\n\n---\n\n\n\nAleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both \n\nin the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be \n\ninstalled with pip:\n\n\n\n```bash\n\npip install aleph-alpha-client\n\n```"", '[Docker is the fastest way to do that](https://qdrant.tech/documentation/quick_start/#installation).\n\n2. You need to have a registered [Aleph Alpha account](https://app.aleph-alpha.com/). \n\n3. Upon registration, create an API key (see: [API Tokens](https://app.aleph-alpha.com/profile)).\n\n\n\nNow you can store the Aleph Alpha API key in a variable and choose the model your are going to use.\n\n\n\n```python\n\naa_token = ""<< your_token >>""\n\nmodel = ""luminous-base""\n\n```\n\n\n\n## Vectorize the dataset', '---\n\ntitle: Aleph Alpha Search\n\nweight: 16\n\n---\n\n\n\n# Multimodal Semantic Search with Aleph Alpha\n\n\n\n| Time: 30 min | Level: Beginner |  |    |\n\n| --- | ----------- | ----------- |----------- |\n\n\n\nThis tutorial shows you how to run a proper multimodal semantic search system with a few lines of code, without the need to annotate the data or train your networks.', 'for i, image_path in enumerate(glob(""./val2017/*.jpg"")):\n\n        # Convert the JPEG file into the embedding by calling\n\n        # Aleph Alpha API\n\n        prompt = Image.from_file(image_path)\n\n        prompt = Prompt.from_image(prompt)\n\n        query_params = {\n\n            ""prompt"": prompt,\n\n            ""representation"": SemanticRepresentation.Symmetric,\n\n            ""compress_to_size"": 128,\n\n        }\n\n        query_request = SemanticEmbeddingRequest(**query_params)', '.tech/documentation/embeddings/aleph-alpha/), [Fastembed](https://github.com/qdrant/fastembed), and [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/).']","Question: What is Aleph Alpha and what capabilities does their API offer?\n\nContext: ---

title: Aleph Alpha

weight: 900

aliases: [ ../integrations/aleph-alpha/ ]

---



Aleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both 

in the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be 

installed with pip:



```bash

pip install aleph-alpha-client

```
[Docker is the fastest way to do that](https://qdrant.tech/documentation/quick_start/#installation).

2. You need to have a registered [Aleph Alpha account](https://app.aleph-alpha.com/). 

3. Upon registration, create an API key (see: [API Tokens](https://app.aleph-alpha.com/profile)).



Now you can store the Aleph Alpha API key in a variable and choose the model your are going to use.



```python

aa_token = ""<< your_token >>""

model = ""luminous-base""

```



## Vectorize the dataset
---

title: Aleph Alpha Search

weight: 16

---



# Multimodal Semantic Search with Aleph Alpha



| Time: 30 min | Level: Beginner |  |    |

| --- | ----------- | ----------- |----------- |



This tutorial shows you how to run a proper multimodal semantic search system with a few lines of code, without the need to annotate the data or train your networks.
for i, image_path in enumerate(glob(""./val2017/*.jpg"")):

# Convert the JPEG file into the embedding by calling

# Aleph Alpha API

prompt = Image.from_file(image_path)

prompt = Prompt.from_image(prompt)

query_params = {

""prompt"": prompt,

""representation"": SemanticRepresentation.Symmetric,

""compress_to_size"": 128,

}

query_request = SemanticEmbeddingRequest(**query_params)
.tech/documentation/embeddings/aleph-alpha/), [Fastembed](https://github.com/qdrant/fastembed), and [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/).\n\nAnswer:",What is Aleph Alpha and what capabilities does their API offer?,"Aleph Alpha is a multimodal and multilingual embeddings provider. Their API allows for the creation of embeddings for both text and images, placing them in the same latent space. They offer an official Python client that can be installed using pip. The API provides both synchronous and asynchronous clients. Users can obtain embeddings for images and store them into Qdrant by utilizing the provided Python client and following the specified steps in the documentation.","[""---\n\ntitle: Aleph Alpha\n\nweight: 900\n\naliases: [ ../integrations/aleph-alpha/ ]\n\n---\n\n\n\nAleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both \n\nin the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be \n\ninstalled with pip:\n\n\n\n```bash\n\npip install aleph-alpha-client\n\n```"", '[Docker is the fastest way to do that](https://qdrant.tech/documentation/quick_start/#installation).\n\n2. You need to have a registered [Aleph Alpha account](https://app.aleph-alpha.com/). \n\n3. Upon registration, create an API key (see: [API Tokens](https://app.aleph-alpha.com/profile)).\n\n\n\nNow you can store the Aleph Alpha API key in a variable and choose the model your are going to use.\n\n\n\n```python\n\naa_token = ""<< your_token >>""\n\nmodel = ""luminous-base""\n\n```\n\n\n\n## Vectorize the dataset', '---\n\ntitle: Aleph Alpha Search\n\nweight: 16\n\n---\n\n\n\n# Multimodal Semantic Search with Aleph Alpha\n\n\n\n| Time: 30 min | Level: Beginner |  |    |\n\n| --- | ----------- | ----------- |----------- |\n\n\n\nThis tutorial shows you how to run a proper multimodal semantic search system with a few lines of code, without the need to annotate the data or train your networks.', 'for i, image_path in enumerate(glob(""./val2017/*.jpg"")):\n\n        # Convert the JPEG file into the embedding by calling\n\n        # Aleph Alpha API\n\n        prompt = Image.from_file(image_path)\n\n        prompt = Prompt.from_image(prompt)\n\n        query_params = {\n\n            ""prompt"": prompt,\n\n            ""representation"": SemanticRepresentation.Symmetric,\n\n            ""compress_to_size"": 128,\n\n        }\n\n        query_request = SemanticEmbeddingRequest(**query_params)', '.tech/documentation/embeddings/aleph-alpha/), [Fastembed](https://github.com/qdrant/fastembed), and [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/).']","---

title: Aleph Alpha

weight: 900

aliases: [ ../integrations/aleph-alpha/ ]

---



Aleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both 

in the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be 

installed with pip:



```bash

pip install aleph-alpha-client

```
[Docker is the fastest way to do that](https://qdrant.tech/documentation/quick_start/#installation).

2. You need to have a registered [Aleph Alpha account](https://app.aleph-alpha.com/). 

3. Upon registration, create an API key (see: [API Tokens](https://app.aleph-alpha.com/profile)).



Now you can store the Aleph Alpha API key in a variable and choose the model your are going to use.



```python

aa_token = ""<< your_token >>""

model = ""luminous-base""

```



## Vectorize the dataset
---

title: Aleph Alpha Search

weight: 16

---



# Multimodal Semantic Search with Aleph Alpha



| Time: 30 min | Level: Beginner |  |    |

| --- | ----------- | ----------- |----------- |



This tutorial shows you how to run a proper multimodal semantic search system with a few lines of code, without the need to annotate the data or train your networks.
for i, image_path in enumerate(glob(""./val2017/*.jpg"")):

        # Convert the JPEG file into the embedding by calling

        # Aleph Alpha API

        prompt = Image.from_file(image_path)

        prompt = Prompt.from_image(prompt)

        query_params = {

            ""prompt"": prompt,

            ""representation"": SemanticRepresentation.Symmetric,

            ""compress_to_size"": 128,

        }

        query_request = SemanticEmbeddingRequest(**query_params)
.tech/documentation/embeddings/aleph-alpha/), [Fastembed](https://github.com/qdrant/fastembed), and [AWS Bedrock](https://qdrant.tech/documentation/embeddings/bedrock/).",0.3490566037735849,0.5606060606060606,0.4302325581395348,0.2,0.3230769230769231,0.24705882352941175,0.29245283018867924,0.4696969696969697,0.36046511627906974,0.330188679245283,0.5303030303030303,0.40697674418604646,0.9169585704803467,0.6616962552070618,0.7131350040435791,0.6864534020423889,0.938057484163437,0.8094564426224679,113,1.527027027027027,0.5114543784060746
True,,3287.929,7dd71199,What is the purpose of the `task_type` parameter when obtaining Nomic embeddings?,"The `task_type` parameter in the `nomic-embed-text-v1` model is used to define the type of embeddings that are obtained for documents. When obtaining Nomic embeddings for documents, you need to set the `task_type` parameter to `search_document`. This ensures that the embeddings generated are specifically tailored for document search purposes. By specifying the `task_type` as `search_document`, you are instructing the model to encode the text in a way that is optimized for searching and retrieving relevant documents based on the input query. This parameter helps customize the embeddings to suit the specific requirements of document retrieval tasks within the Nomic framework.","The purpose of the `task_type` parameter when obtaining Nomic embeddings is to specify the type of task for which you want to use the embeddings. Different values of the `task_type` parameter correspond to different tasks or uses of the embeddings. 

In the context of obtaining Nomic embeddings, the `task_type` parameter is used to differentiate between different tasks such as storing documents in Qdrant, performing search queries, classification tasks, or text clustering. 

Specifically, when setting the `task_type` to `search_document`, it indicates that you are obtaining embeddings for documents to store in Qdrant. On the other hand, setting the `task_type` to `search_query` indicates that you are obtaining embeddings for search queries to find the most relevant documents.

Therefore, by specifying the appropriate `task_type` parameter, you can retrieve the embeddings tailored for the specific task or application you are working on, such as semantic search applications like RAG.","Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.



- `input_type=""search_document""` - for documents to store in Qdrant

- `input_type=""search_query""` - for search queries to find the most relevant documents

- `input_type=""classification""` - for classification tasks

- `input_type=""clustering""` - for text clustering



While implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the
```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

    texts=[""Qdrant is the best vector database!""],

    model=""nomic-embed-text-v1"",

    task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

    collection_name=""my-collection"",

    points=models.Batch(

        ids=[1],

        vectors=output[""embeddings""],

    ),

)

```



To query the collection, set the `task_type` to `search_query`:



```python
```python

output = embed.text(

    texts=[""What is the best vector database?""],

    model=""nomic-embed-text-v1"",

    task_type=""search_query"",

)



qdrant_client.search(

    collection_name=""my-collection"",

    query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
6. `task_type_unspecified`: Unset value, which will default to one of the other values.





If you're building a semantic search application, such as RAG, you should use `task_type=""retrieval_document""` for the indexed documents and `task_type=""retrieval_query""` for the search queries. 



The following example shows how to do this with Qdrant:



## Setup



```bash

pip install google-generativeai

```



Let's see how to use the Embedding Model API to embed a document for retrieval.","['Once installed, you can configure it with the official Python client or through direct HTTP requests.\n\n\n\n<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>\n\n\n\nYou can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings\n\nare obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.\n\nFor documents, set the `task_type` to `search_document`:\n\n\n\n```python', 'parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.\n\n\n\n- `input_type=""search_document""` - for documents to store in Qdrant\n\n- `input_type=""search_query""` - for search queries to find the most relevant documents\n\n- `input_type=""classification""` - for classification tasks\n\n- `input_type=""clustering""` - for text clustering\n\n\n\nWhile implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the', '```python\n\nfrom qdrant_client import QdrantClient, models\n\nfrom nomic import embed\n\n\n\noutput = embed.text(\n\n    texts=[""Qdrant is the best vector database!""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_document"",\n\n)\n\n\n\nqdrant_client = QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""my-collection"",\n\n    points=models.Batch(\n\n        ids=[1],\n\n        vectors=output[""embeddings""],\n\n    ),\n\n)\n\n```\n\n\n\nTo query the collection, set the `task_type` to `search_query`:\n\n\n\n```python', '```python\n\noutput = embed.text(\n\n    texts=[""What is the best vector database?""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_query"",\n\n)\n\n\n\nqdrant_client.search(\n\n    collection_name=""my-collection"",\n\n    query=output[""embeddings""][0],\n\n)\n\n```\n\n\n\nFor more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).', '6. `task_type_unspecified`: Unset value, which will default to one of the other values.\n\n\n\n\n\nIf you\'re building a semantic search application, such as RAG, you should use `task_type=""retrieval_document""` for the indexed documents and `task_type=""retrieval_query""` for the search queries. \n\n\n\nThe following example shows how to do this with Qdrant:\n\n\n\n## Setup\n\n\n\n```bash\n\npip install google-generativeai\n\n```\n\n\n\nLet\'s see how to use the Embedding Model API to embed a document for retrieval.']","Question: What is the purpose of the `task_type` parameter when obtaining Nomic embeddings?\n\nContext: Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.



- `input_type=""search_document""` - for documents to store in Qdrant

- `input_type=""search_query""` - for search queries to find the most relevant documents

- `input_type=""classification""` - for classification tasks

- `input_type=""clustering""` - for text clustering



While implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the
```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

texts=[""Qdrant is the best vector database!""],

model=""nomic-embed-text-v1"",

task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

collection_name=""my-collection"",

points=models.Batch(

ids=[1],

vectors=output[""embeddings""],

),

)

```



To query the collection, set the `task_type` to `search_query`:



```python
```python

output = embed.text(

texts=[""What is the best vector database?""],

model=""nomic-embed-text-v1"",

task_type=""search_query"",

)



qdrant_client.search(

collection_name=""my-collection"",

query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
6. `task_type_unspecified`: Unset value, which will default to one of the other values.





If you're building a semantic search application, such as RAG, you should use `task_type=""retrieval_document""` for the indexed documents and `task_type=""retrieval_query""` for the search queries. 



The following example shows how to do this with Qdrant:



## Setup



```bash

pip install google-generativeai

```



Let's see how to use the Embedding Model API to embed a document for retrieval.\n\nAnswer:",What is the purpose of the `task_type` parameter when obtaining Nomic embeddings?,"The `task_type` parameter in the `nomic-embed-text-v1` model is used to define the type of embeddings that are obtained for documents. When obtaining Nomic embeddings for documents, you need to set the `task_type` parameter to `search_document`. This ensures that the embeddings generated are specifically tailored for document search purposes. By specifying the `task_type` as `search_document`, you are instructing the model to encode the text in a way that is optimized for searching and retrieving relevant documents based on the input query. This parameter helps customize the embeddings to suit the specific requirements of document retrieval tasks within the Nomic framework.","['Once installed, you can configure it with the official Python client or through direct HTTP requests.\n\n\n\n<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>\n\n\n\nYou can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings\n\nare obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.\n\nFor documents, set the `task_type` to `search_document`:\n\n\n\n```python', 'parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.\n\n\n\n- `input_type=""search_document""` - for documents to store in Qdrant\n\n- `input_type=""search_query""` - for search queries to find the most relevant documents\n\n- `input_type=""classification""` - for classification tasks\n\n- `input_type=""clustering""` - for text clustering\n\n\n\nWhile implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the', '```python\n\nfrom qdrant_client import QdrantClient, models\n\nfrom nomic import embed\n\n\n\noutput = embed.text(\n\n    texts=[""Qdrant is the best vector database!""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_document"",\n\n)\n\n\n\nqdrant_client = QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""my-collection"",\n\n    points=models.Batch(\n\n        ids=[1],\n\n        vectors=output[""embeddings""],\n\n    ),\n\n)\n\n```\n\n\n\nTo query the collection, set the `task_type` to `search_query`:\n\n\n\n```python', '```python\n\noutput = embed.text(\n\n    texts=[""What is the best vector database?""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_query"",\n\n)\n\n\n\nqdrant_client.search(\n\n    collection_name=""my-collection"",\n\n    query=output[""embeddings""][0],\n\n)\n\n```\n\n\n\nFor more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).', '6. `task_type_unspecified`: Unset value, which will default to one of the other values.\n\n\n\n\n\nIf you\'re building a semantic search application, such as RAG, you should use `task_type=""retrieval_document""` for the indexed documents and `task_type=""retrieval_query""` for the search queries. \n\n\n\nThe following example shows how to do this with Qdrant:\n\n\n\n## Setup\n\n\n\n```bash\n\npip install google-generativeai\n\n```\n\n\n\nLet\'s see how to use the Embedding Model API to embed a document for retrieval.']","Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.



- `input_type=""search_document""` - for documents to store in Qdrant

- `input_type=""search_query""` - for search queries to find the most relevant documents

- `input_type=""classification""` - for classification tasks

- `input_type=""clustering""` - for text clustering



While implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the
```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

    texts=[""Qdrant is the best vector database!""],

    model=""nomic-embed-text-v1"",

    task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

    collection_name=""my-collection"",

    points=models.Batch(

        ids=[1],

        vectors=output[""embeddings""],

    ),

)

```



To query the collection, set the `task_type` to `search_query`:



```python
```python

output = embed.text(

    texts=[""What is the best vector database?""],

    model=""nomic-embed-text-v1"",

    task_type=""search_query"",

)



qdrant_client.search(

    collection_name=""my-collection"",

    query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
6. `task_type_unspecified`: Unset value, which will default to one of the other values.





If you're building a semantic search application, such as RAG, you should use `task_type=""retrieval_document""` for the indexed documents and `task_type=""retrieval_query""` for the search queries. 



The following example shows how to do this with Qdrant:



## Setup



```bash

pip install google-generativeai

```



Let's see how to use the Embedding Model API to embed a document for retrieval.",0.463768115942029,0.6808510638297872,0.5517241379310345,0.1678832116788321,0.24731182795698925,0.19999999999999998,0.2608695652173913,0.3829787234042553,0.3103448275862069,0.4057971014492754,0.5957446808510638,0.48275862068965514,0.9061824679374695,0.6553058624267578,0.6699628233909607,0.6625532507896423,0.9690676958610615,0.9327596161514521,145,1.4646464646464648,0.4380807740171547
True,,8442.392,2ab5bc8b,How can you use the Nomic library to upsert data into a collection and query it using text embeddings?,"To upsert data into a collection and query it using text embeddings with the Nomic library, you first need to use the `embed.text` method with the `task_type` set to `search_document` to generate embeddings for the text data you want to upsert. After obtaining the embeddings, you can use the `QdrantClient` to upsert the data into a specific collection by providing the collection name and the batch of points containing the IDs and corresponding vectors.

To query the collection, you need to set the `task_type` to `search_query` in the `embed.text` method with the query text. Once you have the query embeddings, you can use the `QdrantClient` to search the collection by providing the collection name and the query embedding to retrieve relevant results based on similarity.

For more detailed information and examples on using text , refer to : https://qdrant.tech/documentation/embeddings/nomic/","To use the Nomic library to upsert data into a collection and query it using text embeddings, you can follow these steps:

1. Import the necessary modules and functions from the Nomic library and Qdrant client.
2. Use the `embed.text` function to generate embeddings for your text data. Ensure to specify the `task_type` as `search_document` when embedding documents and `search_query` when embedding queries.
3. Upsert the embeddings into your Qdrant collection using the Qdrant client's `upsert` method.
4. To query the collection using text embeddings, you need to embed your query text and then pass the resulting embedding to the Qdrant client's `search` method.

Here is an example code snippet demonstrating how to upsert data into a collection and query it using text embeddings with the Nomic library and Qdrant client:

```python
from qdrant_client import QdrantClient, models
from nomic import embed

# Embed text data for upsertion
output = embed.text(
    texts=[""Qdrant is the best vector database!""],
    model=""nomic-embed-text-v1"",
    task_type=""search_document"",
)

# Upsert the embeddings into the collection
qdrant_client = QdrantClient()
qdrant_client.upsert(
    collection_name=""my-collection"",
    points=models.Batch(
        ids=[1],
        vectors=output[""embeddings""],
    ),
)

# Embed query text for searching
output = embed.text(
    texts=[""What is the best vector database?""],
    model=""nomic-embed-text-v1"",
    task_type=""search_query"",
)

# Query the collection using the query text embeddings
search_results = qdrant_client.search(
    collection_name=""my-collection"",
    query=output[""embeddings""][0],
)
```

Remember to replace placeholders like `my-collection` with your actual collection name and configure the Nomic API token for authentication. This code snippet showcases how to utilize Nomic text embeddings for upserting data into a collection and querying it efficiently using the Qdrant client.","```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

    texts=[""Qdrant is the best vector database!""],

    model=""nomic-embed-text-v1"",

    task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

    collection_name=""my-collection"",

    points=models.Batch(

        ids=[1],

        vectors=output[""embeddings""],

    ),

)

```



To query the collection, set the `task_type` to `search_query`:



```python
```python

output = embed.text(

    texts=[""What is the best vector database?""],

    model=""nomic-embed-text-v1"",

    task_type=""search_query"",

)



qdrant_client.search(

    collection_name=""my-collection"",

    query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
collection_name=""MyCollection"",

    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),

)





qdrant_client.upsert(

    collection_name=""MyCollection"",

    points=Batch(

        ids=list(range(len(embeddings))),

        vectors=embeddings,

    ),

)



```
# https://platform.openai.com/docs/models/embeddings

embedding_model = ""text-embedding-ada-002""



openai_client = openai.Client(

    api_key=""<< your_api_key >>""

)

response = openai_client.embeddings.create(

    input=""The best vector database"",

    model=embedding_model,

)



qdrant_client = qdrant_client.QdrantClient()

qdrant_client.upsert(

    collection_name=""MyCollection"",

    points=Batch(

        ids=[1],

        vectors=[response.data[0].embedding],

    ),

)

```","['```python\n\nfrom qdrant_client import QdrantClient, models\n\nfrom nomic import embed\n\n\n\noutput = embed.text(\n\n    texts=[""Qdrant is the best vector database!""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_document"",\n\n)\n\n\n\nqdrant_client = QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""my-collection"",\n\n    points=models.Batch(\n\n        ids=[1],\n\n        vectors=output[""embeddings""],\n\n    ),\n\n)\n\n```\n\n\n\nTo query the collection, set the `task_type` to `search_query`:\n\n\n\n```python', '```python\n\noutput = embed.text(\n\n    texts=[""What is the best vector database?""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_query"",\n\n)\n\n\n\nqdrant_client.search(\n\n    collection_name=""my-collection"",\n\n    query=output[""embeddings""][0],\n\n)\n\n```\n\n\n\nFor more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).', 'Once installed, you can configure it with the official Python client or through direct HTTP requests.\n\n\n\n<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>\n\n\n\nYou can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings\n\nare obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.\n\nFor documents, set the `task_type` to `search_document`:\n\n\n\n```python', 'collection_name=""MyCollection"",\n\n    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),\n\n)\n\n\n\n\n\nqdrant_client.upsert(\n\n    collection_name=""MyCollection"",\n\n    points=Batch(\n\n        ids=list(range(len(embeddings))),\n\n        vectors=embeddings,\n\n    ),\n\n)\n\n\n\n```', '# https://platform.openai.com/docs/models/embeddings\n\nembedding_model = ""text-embedding-ada-002""\n\n\n\nopenai_client = openai.Client(\n\n    api_key=""<< your_api_key >>""\n\n)\n\nresponse = openai_client.embeddings.create(\n\n    input=""The best vector database"",\n\n    model=embedding_model,\n\n)\n\n\n\nqdrant_client = qdrant_client.QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""MyCollection"",\n\n    points=Batch(\n\n        ids=[1],\n\n        vectors=[response.data[0].embedding],\n\n    ),\n\n)\n\n```']","Question: How can you use the Nomic library to upsert data into a collection and query it using text embeddings?\n\nContext: ```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

texts=[""Qdrant is the best vector database!""],

model=""nomic-embed-text-v1"",

task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

collection_name=""my-collection"",

points=models.Batch(

ids=[1],

vectors=output[""embeddings""],

),

)

```



To query the collection, set the `task_type` to `search_query`:



```python
```python

output = embed.text(

texts=[""What is the best vector database?""],

model=""nomic-embed-text-v1"",

task_type=""search_query"",

)



qdrant_client.search(

collection_name=""my-collection"",

query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
collection_name=""MyCollection"",

vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),

)





qdrant_client.upsert(

collection_name=""MyCollection"",

points=Batch(

ids=list(range(len(embeddings))),

vectors=embeddings,

),

)



```
# https://platform.openai.com/docs/models/embeddings

embedding_model = ""text-embedding-ada-002""



openai_client = openai.Client(

api_key=""<< your_api_key >>""

)

response = openai_client.embeddings.create(

input=""The best vector database"",

model=embedding_model,

)



qdrant_client = qdrant_client.QdrantClient()

qdrant_client.upsert(

collection_name=""MyCollection"",

points=Batch(

ids=[1],

vectors=[response.data[0].embedding],

),

)

```\n\nAnswer:",How can you use the Nomic library to upsert data into a collection and query it using text embeddings?,"To upsert data into a collection and query it using text embeddings with the Nomic library, you first need to use the `embed.text` method with the `task_type` set to `search_document` to generate embeddings for the text data you want to upsert. After obtaining the embeddings, you can use the `QdrantClient` to upsert the data into a specific collection by providing the collection name and the batch of points containing the IDs and corresponding vectors.

To query the collection, you need to set the `task_type` to `search_query` in the `embed.text` method with the query text. Once you have the query embeddings, you can use the `QdrantClient` to search the collection by providing the collection name and the query embedding to retrieve relevant results based on similarity.

For more detailed information and examples on using text , refer to : https://qdrant.tech/documentation/embeddings/nomic/","['```python\n\nfrom qdrant_client import QdrantClient, models\n\nfrom nomic import embed\n\n\n\noutput = embed.text(\n\n    texts=[""Qdrant is the best vector database!""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_document"",\n\n)\n\n\n\nqdrant_client = QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""my-collection"",\n\n    points=models.Batch(\n\n        ids=[1],\n\n        vectors=output[""embeddings""],\n\n    ),\n\n)\n\n```\n\n\n\nTo query the collection, set the `task_type` to `search_query`:\n\n\n\n```python', '```python\n\noutput = embed.text(\n\n    texts=[""What is the best vector database?""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_query"",\n\n)\n\n\n\nqdrant_client.search(\n\n    collection_name=""my-collection"",\n\n    query=output[""embeddings""][0],\n\n)\n\n```\n\n\n\nFor more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).', 'Once installed, you can configure it with the official Python client or through direct HTTP requests.\n\n\n\n<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>\n\n\n\nYou can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings\n\nare obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.\n\nFor documents, set the `task_type` to `search_document`:\n\n\n\n```python', 'collection_name=""MyCollection"",\n\n    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),\n\n)\n\n\n\n\n\nqdrant_client.upsert(\n\n    collection_name=""MyCollection"",\n\n    points=Batch(\n\n        ids=list(range(len(embeddings))),\n\n        vectors=embeddings,\n\n    ),\n\n)\n\n\n\n```', '# https://platform.openai.com/docs/models/embeddings\n\nembedding_model = ""text-embedding-ada-002""\n\n\n\nopenai_client = openai.Client(\n\n    api_key=""<< your_api_key >>""\n\n)\n\nresponse = openai_client.embeddings.create(\n\n    input=""The best vector database"",\n\n    model=embedding_model,\n\n)\n\n\n\nqdrant_client = qdrant_client.QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""MyCollection"",\n\n    points=Batch(\n\n        ids=[1],\n\n        vectors=[response.data[0].embedding],\n\n    ),\n\n)\n\n```']","```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

    texts=[""Qdrant is the best vector database!""],

    model=""nomic-embed-text-v1"",

    task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

    collection_name=""my-collection"",

    points=models.Batch(

        ids=[1],

        vectors=output[""embeddings""],

    ),

)

```



To query the collection, set the `task_type` to `search_query`:



```python
```python

output = embed.text(

    texts=[""What is the best vector database?""],

    model=""nomic-embed-text-v1"",

    task_type=""search_query"",

)



qdrant_client.search(

    collection_name=""my-collection"",

    query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
collection_name=""MyCollection"",

    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),

)





qdrant_client.upsert(

    collection_name=""MyCollection"",

    points=Batch(

        ids=list(range(len(embeddings))),

        vectors=embeddings,

    ),

)



```
# https://platform.openai.com/docs/models/embeddings

embedding_model = ""text-embedding-ada-002""



openai_client = openai.Client(

    api_key=""<< your_api_key >>""

)

response = openai_client.embeddings.create(

    input=""The best vector database"",

    model=embedding_model,

)



qdrant_client = qdrant_client.QdrantClient()

qdrant_client.upsert(

    collection_name=""MyCollection"",

    points=Batch(

        ids=[1],

        vectors=[response.data[0].embedding],

    ),

)

```",0.33969465648854963,0.7007874015748031,0.45758354755784064,0.16475095785440613,0.3412698412698413,0.22222222222222224,0.2099236641221374,0.4330708661417323,0.2827763496143959,0.31679389312977096,0.6535433070866141,0.4267352185089974,0.9487036466598511,0.6548474431037903,0.6772168278694153,0.6658443212509155,0.6155105852263255,0.8841950930655003,253,1.8333333333333333,0.38606482546206555
True,,3908.06,d96e0fc7,What is the purpose of the `task_type` parameter in the Gemini Embedding Model API?,"The `task_type` parameter in the Gemini Embedding Model API serves to designate the intended purpose for the embeddings utilized. It allows users to specify the type of task they want to perform with the given text. The supported task types include `retrieval_query` for search/retrieval queries, `retrieval_document` for documents from the corpus being searched, `semantic_similarity` for Semantic Text Similarity, and `classification` for text classification. This parameter helps in customizing the functionality of the Gemini Embedding Model API based on the specific task requirements.","The purpose of the `task_type` parameter in the Gemini Embedding Model API is to designate the intended purpose for the embeddings being utilized. It helps specify how the given text will be used or processed within the context of the API call. The `task_type` parameter supports various task types, such as:

1. `retrieval_query`: Indicates that the given text is a query in a search or retrieval setting.
2. `retrieval_document`: Indicates that the given text is a document from the corpus being searched.
3. `semantic_similarity`: Specifies that the given text will be used for Semantic Text Similarity.
4. `classification`: Specifies that the given text will be classified.
5. `clustering`: Specifies that the embeddings will be used for clustering.

By providing the `task_type` parameter with the appropriate value, users can tailor the behavior of the Gemini Embedding Model API to suit the specific task they are working on, whether it involves search, classification, semantic similarity, or clustering.","---

title: Gemini

weight: 700

---



# Gemini



Qdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:



Gemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. 



In the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.
The Embedding Model API supports various task types, outlined as follows:



1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.

2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.

3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.

4. `classification`: Specifies that the given text will be classified.

5. `clustering`: Specifies that the embeddings will be used for clustering.
ids=[1],

        vectors=genai.embed_content(

            model=""models/embedding-001"",

            content=""Qdrant is the best vector search engine to use with Gemini"",

            task_type=""retrieval_document"",

            title=""Qdrant x Gemini"",

        )[""embedding""],

    ),

)

```



## Searching for documents with Qdrant



Once the documents are indexed, you can search for the most relevant documents using the same model with the `retrieval_query` task type:



```python
The following example shows how to embed a document with the `models/embedding-001` with the `retrieval_document` task type:



## Embedding a document



```python

import pathlib

import google.generativeai as genai

import qdrant_client



GEMINI_API_KEY = ""YOUR GEMINI API KEY""  # add your key here



genai.configure(api_key=GEMINI_API_KEY)



result = genai.embed_content(

    model=""models/embedding-001"",

    content=""Qdrant is the best vector search engine to use with Gemini"",
```python

qdrant_client.search(

    collection_name=""GeminiCollection"",

    query=genai.embed_content(

        model=""models/embedding-001"",

        content=""What is the best vector database to use with Gemini?"",

        task_type=""retrieval_query"",

    )[""embedding""],

)

```



## Using Gemini Embedding Models with Binary Quantization","['---\n\ntitle: Gemini\n\nweight: 700\n\n---\n\n\n\n# Gemini\n\n\n\nQdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:\n\n\n\nGemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. \n\n\n\nIn the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.', 'The Embedding Model API supports various task types, outlined as follows:\n\n\n\n1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.\n\n2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.\n\n3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.\n\n4. `classification`: Specifies that the given text will be classified.\n\n5. `clustering`: Specifies that the embeddings will be used for clustering.', 'ids=[1],\n\n        vectors=genai.embed_content(\n\n            model=""models/embedding-001"",\n\n            content=""Qdrant is the best vector search engine to use with Gemini"",\n\n            task_type=""retrieval_document"",\n\n            title=""Qdrant x Gemini"",\n\n        )[""embedding""],\n\n    ),\n\n)\n\n```\n\n\n\n## Searching for documents with Qdrant\n\n\n\nOnce the documents are indexed, you can search for the most relevant documents using the same model with the `retrieval_query` task type:\n\n\n\n```python', 'The following example shows how to embed a document with the `models/embedding-001` with the `retrieval_document` task type:\n\n\n\n## Embedding a document\n\n\n\n```python\n\nimport pathlib\n\nimport google.generativeai as genai\n\nimport qdrant_client\n\n\n\nGEMINI_API_KEY = ""YOUR GEMINI API KEY""  # add your key here\n\n\n\ngenai.configure(api_key=GEMINI_API_KEY)\n\n\n\nresult = genai.embed_content(\n\n    model=""models/embedding-001"",\n\n    content=""Qdrant is the best vector search engine to use with Gemini"",', '```python\n\nqdrant_client.search(\n\n    collection_name=""GeminiCollection"",\n\n    query=genai.embed_content(\n\n        model=""models/embedding-001"",\n\n        content=""What is the best vector database to use with Gemini?"",\n\n        task_type=""retrieval_query"",\n\n    )[""embedding""],\n\n)\n\n```\n\n\n\n## Using Gemini Embedding Models with Binary Quantization']","Question: What is the purpose of the `task_type` parameter in the Gemini Embedding Model API?\n\nContext: ---

title: Gemini

weight: 700

---



# Gemini



Qdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:



Gemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. 



In the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.
The Embedding Model API supports various task types, outlined as follows:



1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.

2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.

3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.

4. `classification`: Specifies that the given text will be classified.

5. `clustering`: Specifies that the embeddings will be used for clustering.
ids=[1],

vectors=genai.embed_content(

model=""models/embedding-001"",

content=""Qdrant is the best vector search engine to use with Gemini"",

task_type=""retrieval_document"",

title=""Qdrant x Gemini"",

)[""embedding""],

),

)

```



## Searching for documents with Qdrant



Once the documents are indexed, you can search for the most relevant documents using the same model with the `retrieval_query` task type:



```python
The following example shows how to embed a document with the `models/embedding-001` with the `retrieval_document` task type:



## Embedding a document



```python

import pathlib

import google.generativeai as genai

import qdrant_client



GEMINI_API_KEY = ""YOUR GEMINI API KEY""  # add your key here



genai.configure(api_key=GEMINI_API_KEY)



result = genai.embed_content(

model=""models/embedding-001"",

content=""Qdrant is the best vector search engine to use with Gemini"",
```python

qdrant_client.search(

collection_name=""GeminiCollection"",

query=genai.embed_content(

model=""models/embedding-001"",

content=""What is the best vector database to use with Gemini?"",

task_type=""retrieval_query"",

)[""embedding""],

)

```



## Using Gemini Embedding Models with Binary Quantization\n\nAnswer:",What is the purpose of the `task_type` parameter in the Gemini Embedding Model API?,"The `task_type` parameter in the Gemini Embedding Model API serves to designate the intended purpose for the embeddings utilized. It allows users to specify the type of task they want to perform with the given text. The supported task types include `retrieval_query` for search/retrieval queries, `retrieval_document` for documents from the corpus being searched, `semantic_similarity` for Semantic Text Similarity, and `classification` for text classification. This parameter helps in customizing the functionality of the Gemini Embedding Model API based on the specific task requirements.","['---\n\ntitle: Gemini\n\nweight: 700\n\n---\n\n\n\n# Gemini\n\n\n\nQdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:\n\n\n\nGemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. \n\n\n\nIn the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.', 'The Embedding Model API supports various task types, outlined as follows:\n\n\n\n1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.\n\n2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.\n\n3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.\n\n4. `classification`: Specifies that the given text will be classified.\n\n5. `clustering`: Specifies that the embeddings will be used for clustering.', 'ids=[1],\n\n        vectors=genai.embed_content(\n\n            model=""models/embedding-001"",\n\n            content=""Qdrant is the best vector search engine to use with Gemini"",\n\n            task_type=""retrieval_document"",\n\n            title=""Qdrant x Gemini"",\n\n        )[""embedding""],\n\n    ),\n\n)\n\n```\n\n\n\n## Searching for documents with Qdrant\n\n\n\nOnce the documents are indexed, you can search for the most relevant documents using the same model with the `retrieval_query` task type:\n\n\n\n```python', 'The following example shows how to embed a document with the `models/embedding-001` with the `retrieval_document` task type:\n\n\n\n## Embedding a document\n\n\n\n```python\n\nimport pathlib\n\nimport google.generativeai as genai\n\nimport qdrant_client\n\n\n\nGEMINI_API_KEY = ""YOUR GEMINI API KEY""  # add your key here\n\n\n\ngenai.configure(api_key=GEMINI_API_KEY)\n\n\n\nresult = genai.embed_content(\n\n    model=""models/embedding-001"",\n\n    content=""Qdrant is the best vector search engine to use with Gemini"",', '```python\n\nqdrant_client.search(\n\n    collection_name=""GeminiCollection"",\n\n    query=genai.embed_content(\n\n        model=""models/embedding-001"",\n\n        content=""What is the best vector database to use with Gemini?"",\n\n        task_type=""retrieval_query"",\n\n    )[""embedding""],\n\n)\n\n```\n\n\n\n## Using Gemini Embedding Models with Binary Quantization']","---

title: Gemini

weight: 700

---



# Gemini



Qdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:



Gemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. 



In the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.
The Embedding Model API supports various task types, outlined as follows:



1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.

2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.

3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.

4. `classification`: Specifies that the given text will be classified.

5. `clustering`: Specifies that the embeddings will be used for clustering.
ids=[1],

        vectors=genai.embed_content(

            model=""models/embedding-001"",

            content=""Qdrant is the best vector search engine to use with Gemini"",

            task_type=""retrieval_document"",

            title=""Qdrant x Gemini"",

        )[""embedding""],

    ),

)

```



## Searching for documents with Qdrant



Once the documents are indexed, you can search for the most relevant documents using the same model with the `retrieval_query` task type:



```python
The following example shows how to embed a document with the `models/embedding-001` with the `retrieval_document` task type:



## Embedding a document



```python

import pathlib

import google.generativeai as genai

import qdrant_client



GEMINI_API_KEY = ""YOUR GEMINI API KEY""  # add your key here



genai.configure(api_key=GEMINI_API_KEY)



result = genai.embed_content(

    model=""models/embedding-001"",

    content=""Qdrant is the best vector search engine to use with Gemini"",
```python

qdrant_client.search(

    collection_name=""GeminiCollection"",

    query=genai.embed_content(

        model=""models/embedding-001"",

        content=""What is the best vector database to use with Gemini?"",

        task_type=""retrieval_query"",

    )[""embedding""],

)

```



## Using Gemini Embedding Models with Binary Quantization",0.4492753623188406,0.8157894736842105,0.5794392523364487,0.23357664233576642,0.4266666666666667,0.3018867924528302,0.35507246376811596,0.6447368421052632,0.4579439252336449,0.42028985507246375,0.7631578947368421,0.5420560747663552,0.9326784610748291,0.6559774875640869,0.7478737235069275,0.698917806148529,0.6998969847873713,0.9842372473794967,155,1.8902439024390243,0.7161808803464088
True,,690.789,a85a3807,What is the maximum number of tokens that Jina embeddings allow for model input lengths?,"Jina embeddings allow for model input lengths of up to 8192 tokens. This means that the models utilizing Jina embeddings can process sequences with a maximum length of 8192 tokens, providing flexibility and capability for handling large amounts of text data efficiently.",The maximum number of tokens that Jina embeddings allow for model input lengths is up to 8192 tokens.,"---

title: Jina Embeddings

weight: 800

aliases: [ ../integrations/jina-embeddings/ ]

---



# Jina Embeddings



Qdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.
The default model and several other models have a context window of maximum 512 tokens. This maximum limit comes from the embedding model training and design itself.If you'd like to embed sequences larger than that, we'd recommend using some pooling strategy to get a single vector out of the sequence. For example, you can use the mean of the embeddings of different chunks of a document. This is also what the [SBERT Paper recommends](https://lilianweng.github.io/posts/2021-05-31-contrastive/#sentence-bert)
. The other caveat is that we pass on the token limits or context windows from the embedding model creators themselves. So in the case of this model, which is BGE base, that is 512 BGE tokens.
# Provide Jina API key and choose one of the available models.

# You can get a free trial key here: https://jina.ai/embeddings/

JINA_API_KEY = ""jina_xxxxxxxxxxx""

MODEL = ""jina-embeddings-v2-base-en""  # or ""jina-embeddings-v2-base-en""

EMBEDDING_SIZE = 768  # 512 for small variant



# Get embeddings from the API

url = ""https://api.jina.ai/v1/embeddings""



headers = {

    ""Content-Type"": ""application/json"",

    ""Authorization"": f""Bearer {JINA_API_KEY}"",

}



data = {
This expansion has a direct relationship with what we can control when making a SPLADE model: Sparsity via Regularisation. The number of tokens (BERT wordpieces) we use to represent each document. If we use more tokens, we can represent more terms, but the vectors become denser. This number is typically between 20 to 200 per document. As a reference point, the dense BERT vector is 768 dimensions, OpenAI Embedding is 1536 dimensions, and the sparse vector is 30 dimensions.","['---\n\ntitle: Jina Embeddings\n\nweight: 800\n\naliases: [ ../integrations/jina-embeddings/ ]\n\n---\n\n\n\n# Jina Embeddings\n\n\n\nQdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.', ""The default model and several other models have a context window of maximum 512 tokens. This maximum limit comes from the embedding model training and design itself.If you'd like to embed sequences larger than that, we'd recommend using some pooling strategy to get a single vector out of the sequence. For example, you can use the mean of the embeddings of different chunks of a document. This is also what the [SBERT Paper recommends](https://lilianweng.github.io/posts/2021-05-31-contrastive/#sentence-bert)"", '. The other caveat is that we pass on the token limits or context windows from the embedding model creators themselves. So in the case of this model, which is BGE base, that is 512 BGE tokens.', '# Provide Jina API key and choose one of the available models.\n\n# You can get a free trial key here: https://jina.ai/embeddings/\n\nJINA_API_KEY = ""jina_xxxxxxxxxxx""\n\nMODEL = ""jina-embeddings-v2-base-en""  # or ""jina-embeddings-v2-base-en""\n\nEMBEDDING_SIZE = 768  # 512 for small variant\n\n\n\n# Get embeddings from the API\n\nurl = ""https://api.jina.ai/v1/embeddings""\n\n\n\nheaders = {\n\n    ""Content-Type"": ""application/json"",\n\n    ""Authorization"": f""Bearer {JINA_API_KEY}"",\n\n}\n\n\n\ndata = {', 'This expansion has a direct relationship with what we can control when making a SPLADE model: Sparsity via Regularisation. The number of tokens (BERT wordpieces) we use to represent each document. If we use more tokens, we can represent more terms, but the vectors become denser. This number is typically between 20 to 200 per document. As a reference point, the dense BERT vector is 768 dimensions, OpenAI Embedding is 1536 dimensions, and the sparse vector is 30 dimensions.']","Question: What is the maximum number of tokens that Jina embeddings allow for model input lengths?\n\nContext: ---

title: Jina Embeddings

weight: 800

aliases: [ ../integrations/jina-embeddings/ ]

---



# Jina Embeddings



Qdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.
The default model and several other models have a context window of maximum 512 tokens. This maximum limit comes from the embedding model training and design itself.If you'd like to embed sequences larger than that, we'd recommend using some pooling strategy to get a single vector out of the sequence. For example, you can use the mean of the embeddings of different chunks of a document. This is also what the [SBERT Paper recommends](https://lilianweng.github.io/posts/2021-05-31-contrastive/#sentence-bert)
. The other caveat is that we pass on the token limits or context windows from the embedding model creators themselves. So in the case of this model, which is BGE base, that is 512 BGE tokens.
# Provide Jina API key and choose one of the available models.

# You can get a free trial key here: https://jina.ai/embeddings/

JINA_API_KEY = ""jina_xxxxxxxxxxx""

MODEL = ""jina-embeddings-v2-base-en""  # or ""jina-embeddings-v2-base-en""

EMBEDDING_SIZE = 768  # 512 for small variant



# Get embeddings from the API

url = ""https://api.jina.ai/v1/embeddings""



headers = {

""Content-Type"": ""application/json"",

""Authorization"": f""Bearer {JINA_API_KEY}"",

}



data = {
This expansion has a direct relationship with what we can control when making a SPLADE model: Sparsity via Regularisation. The number of tokens (BERT wordpieces) we use to represent each document. If we use more tokens, we can represent more terms, but the vectors become denser. This number is typically between 20 to 200 per document. As a reference point, the dense BERT vector is 768 dimensions, OpenAI Embedding is 1536 dimensions, and the sparse vector is 30 dimensions.\n\nAnswer:",What is the maximum number of tokens that Jina embeddings allow for model input lengths?,"Jina embeddings allow for model input lengths of up to 8192 tokens. This means that the models utilizing Jina embeddings can process sequences with a maximum length of 8192 tokens, providing flexibility and capability for handling large amounts of text data efficiently.","['---\n\ntitle: Jina Embeddings\n\nweight: 800\n\naliases: [ ../integrations/jina-embeddings/ ]\n\n---\n\n\n\n# Jina Embeddings\n\n\n\nQdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.', ""The default model and several other models have a context window of maximum 512 tokens. This maximum limit comes from the embedding model training and design itself.If you'd like to embed sequences larger than that, we'd recommend using some pooling strategy to get a single vector out of the sequence. For example, you can use the mean of the embeddings of different chunks of a document. This is also what the [SBERT Paper recommends](https://lilianweng.github.io/posts/2021-05-31-contrastive/#sentence-bert)"", '. The other caveat is that we pass on the token limits or context windows from the embedding model creators themselves. So in the case of this model, which is BGE base, that is 512 BGE tokens.', '# Provide Jina API key and choose one of the available models.\n\n# You can get a free trial key here: https://jina.ai/embeddings/\n\nJINA_API_KEY = ""jina_xxxxxxxxxxx""\n\nMODEL = ""jina-embeddings-v2-base-en""  # or ""jina-embeddings-v2-base-en""\n\nEMBEDDING_SIZE = 768  # 512 for small variant\n\n\n\n# Get embeddings from the API\n\nurl = ""https://api.jina.ai/v1/embeddings""\n\n\n\nheaders = {\n\n    ""Content-Type"": ""application/json"",\n\n    ""Authorization"": f""Bearer {JINA_API_KEY}"",\n\n}\n\n\n\ndata = {', 'This expansion has a direct relationship with what we can control when making a SPLADE model: Sparsity via Regularisation. The number of tokens (BERT wordpieces) we use to represent each document. If we use more tokens, we can represent more terms, but the vectors become denser. This number is typically between 20 to 200 per document. As a reference point, the dense BERT vector is 768 dimensions, OpenAI Embedding is 1536 dimensions, and the sparse vector is 30 dimensions.']","---

title: Jina Embeddings

weight: 800

aliases: [ ../integrations/jina-embeddings/ ]

---



# Jina Embeddings



Qdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.
The default model and several other models have a context window of maximum 512 tokens. This maximum limit comes from the embedding model training and design itself.If you'd like to embed sequences larger than that, we'd recommend using some pooling strategy to get a single vector out of the sequence. For example, you can use the mean of the embeddings of different chunks of a document. This is also what the [SBERT Paper recommends](https://lilianweng.github.io/posts/2021-05-31-contrastive/#sentence-bert)
. The other caveat is that we pass on the token limits or context windows from the embedding model creators themselves. So in the case of this model, which is BGE base, that is 512 BGE tokens.
# Provide Jina API key and choose one of the available models.

# You can get a free trial key here: https://jina.ai/embeddings/

JINA_API_KEY = ""jina_xxxxxxxxxxx""

MODEL = ""jina-embeddings-v2-base-en""  # or ""jina-embeddings-v2-base-en""

EMBEDDING_SIZE = 768  # 512 for small variant



# Get embeddings from the API

url = ""https://api.jina.ai/v1/embeddings""



headers = {

    ""Content-Type"": ""application/json"",

    ""Authorization"": f""Bearer {JINA_API_KEY}"",

}



data = {
This expansion has a direct relationship with what we can control when making a SPLADE model: Sparsity via Regularisation. The number of tokens (BERT wordpieces) we use to represent each document. If we use more tokens, we can represent more terms, but the vectors become denser. This number is typically between 20 to 200 per document. As a reference point, the dense BERT vector is 768 dimensions, OpenAI Embedding is 1536 dimensions, and the sparse vector is 30 dimensions.",0.8823529411764706,0.375,0.5263157894736842,0.5625,0.23076923076923078,0.32727272727272727,0.6470588235294118,0.275,0.3859649122807018,0.7647058823529411,0.325,0.456140350877193,0.9264309406280518,0.6830343008041382,0.6052727103233337,0.6418067216873169,0.9966134079732001,0.9928856168407947,18,0.42857142857142855,0.2696752266529241
True,,2508.03,668f4de2,Why does Qdrant by default not return vectors in search results?,"By default, Qdrant tries to minimize network traffic and doesn't return vectors in search results. This default behavior is in place to optimize performance and reduce unnecessary data transfer. However, if you specifically require the vectors to be included in the search results, you can override this default setting by setting the `with_vector` parameter of the Search/Scroll to `true`. This allows you to retrieve the vectors along with other search results, providing you with the option to access the vector data when needed.","Qdrant by default does not return vectors in search results to minimize network traffic. However, you can force Qdrant to return vectors by setting the `with_vector` parameter to `true` in the Search/Scroll request. If you are still getting `""vector"": null` in your results, it could be due to issues with the format of the vector or how you are calling the upsert method.

If you need to search without a vector, you can use the scroll method, which allows you to retrieve records based on filters or iterate over all the records in the collection.

Qdrant supports different search types and strategies, such as the average vector strategy, to provide recommendations based on the input examples.","Please read more about collections, isolation, and multiple users in our [Multitenancy](../../tutorials/multiple-partitions/) tutorial.



### My search results contain vectors with null values. Why?



By default, Qdrant tries to minimize network traffic and doesn't return vectors in search results.

But you can force Qdrant to do so by setting the `with_vector` parameter of the Search/Scroll to `true`.
If you're still seeing `""vector"": null` in your results, it might be that the vector you're passing is not in the correct format, or there's an issue with how you're calling the upsert method.



### How can I search without a vector?



You are likely looking for the [scroll](../../concepts/points/#scroll-points) method. It allows you to retrieve the records based on filters or even iterate over all the records in the collection.



### Does Qdrant support a full-text search or a hybrid search?
}

```



## Pagination



*Available as of v0.8.3*



Search and [recommendation](../explore/#recommendation-api) APIs allow to skip first results of the search and return only the result starting from some specified offset:



Example:



```http

POST /collections/{collection_name}/points/search

{

    ""vector"": [0.2, 0.1, 0.9, 0.7],

    ""with_vectors"": true,

    ""with_payload"": true,

    ""limit"": 10,

    ""offset"": 100

}

```



```python

from qdrant_client import QdrantClient
| Index| Sparse Query | Dense Query |

| --- | --- | --- | 

| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |

| Search Type | Always exact in Qdrant | HNSW is an approximate NN |

| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |



In general, the speed of the search is proportional to the number of non-zero values in the query vector.



```http
### Average vector strategy



The default and first strategy added to Qdrant is called `average_vector`. It preprocesses the input examples to create a single vector that is used for the search. Since the preprocessing step happens very fast, the performance of this strategy is on-par with regular search. The intuition behind this kind of recommendation is that each vector component represents an independent feature of the data, so, by averaging the examples, we should get a good recommendation.","[""Please read more about collections, isolation, and multiple users in our [Multitenancy](../../tutorials/multiple-partitions/) tutorial.\n\n\n\n### My search results contain vectors with null values. Why?\n\n\n\nBy default, Qdrant tries to minimize network traffic and doesn't return vectors in search results.\n\nBut you can force Qdrant to do so by setting the `with_vector` parameter of the Search/Scroll to `true`."", 'If you\'re still seeing `""vector"": null` in your results, it might be that the vector you\'re passing is not in the correct format, or there\'s an issue with how you\'re calling the upsert method.\n\n\n\n### How can I search without a vector?\n\n\n\nYou are likely looking for the [scroll](../../concepts/points/#scroll-points) method. It allows you to retrieve the records based on filters or even iterate over all the records in the collection.\n\n\n\n### Does Qdrant support a full-text search or a hybrid search?', '}\n\n```\n\n\n\n## Pagination\n\n\n\n*Available as of v0.8.3*\n\n\n\nSearch and [recommendation](../explore/#recommendation-api) APIs allow to skip first results of the search and return only the result starting from some specified offset:\n\n\n\nExample:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{\n\n    ""vector"": [0.2, 0.1, 0.9, 0.7],\n\n    ""with_vectors"": true,\n\n    ""with_payload"": true,\n\n    ""limit"": 10,\n\n    ""offset"": 100\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient', '| Index| Sparse Query | Dense Query |\n\n| --- | --- | --- | \n\n| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |\n\n| Search Type | Always exact in Qdrant | HNSW is an approximate NN |\n\n| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |\n\n\n\nIn general, the speed of the search is proportional to the number of non-zero values in the query vector.\n\n\n\n```http', '### Average vector strategy\n\n\n\nThe default and first strategy added to Qdrant is called `average_vector`. It preprocesses the input examples to create a single vector that is used for the search. Since the preprocessing step happens very fast, the performance of this strategy is on-par with regular search. The intuition behind this kind of recommendation is that each vector component represents an independent feature of the data, so, by averaging the examples, we should get a good recommendation.']","Question: Why does Qdrant by default not return vectors in search results?\n\nContext: Please read more about collections, isolation, and multiple users in our [Multitenancy](../../tutorials/multiple-partitions/) tutorial.



### My search results contain vectors with null values. Why?



By default, Qdrant tries to minimize network traffic and doesn't return vectors in search results.

But you can force Qdrant to do so by setting the `with_vector` parameter of the Search/Scroll to `true`.
If you're still seeing `""vector"": null` in your results, it might be that the vector you're passing is not in the correct format, or there's an issue with how you're calling the upsert method.



### How can I search without a vector?



You are likely looking for the [scroll](../../concepts/points/#scroll-points) method. It allows you to retrieve the records based on filters or even iterate over all the records in the collection.



### Does Qdrant support a full-text search or a hybrid search?
}

```



## Pagination



*Available as of v0.8.3*



Search and [recommendation](../explore/#recommendation-api) APIs allow to skip first results of the search and return only the result starting from some specified offset:



Example:



```http

POST /collections/{collection_name}/points/search

{

""vector"": [0.2, 0.1, 0.9, 0.7],

""with_vectors"": true,

""with_payload"": true,

""limit"": 10,

""offset"": 100

}

```



```python

from qdrant_client import QdrantClient
| Index| Sparse Query | Dense Query |

| --- | --- | --- | 

| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |

| Search Type | Always exact in Qdrant | HNSW is an approximate NN |

| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |



In general, the speed of the search is proportional to the number of non-zero values in the query vector.



```http
### Average vector strategy



The default and first strategy added to Qdrant is called `average_vector`. It preprocesses the input examples to create a single vector that is used for the search. Since the preprocessing step happens very fast, the performance of this strategy is on-par with regular search. The intuition behind this kind of recommendation is that each vector component represents an independent feature of the data, so, by averaging the examples, we should get a good recommendation.\n\nAnswer:",Why does Qdrant by default not return vectors in search results?,"By default, Qdrant tries to minimize network traffic and doesn't return vectors in search results. This default behavior is in place to optimize performance and reduce unnecessary data transfer. However, if you specifically require the vectors to be included in the search results, you can override this default setting by setting the `with_vector` parameter of the Search/Scroll to `true`. This allows you to retrieve the vectors along with other search results, providing you with the option to access the vector data when needed.","[""Please read more about collections, isolation, and multiple users in our [Multitenancy](../../tutorials/multiple-partitions/) tutorial.\n\n\n\n### My search results contain vectors with null values. Why?\n\n\n\nBy default, Qdrant tries to minimize network traffic and doesn't return vectors in search results.\n\nBut you can force Qdrant to do so by setting the `with_vector` parameter of the Search/Scroll to `true`."", 'If you\'re still seeing `""vector"": null` in your results, it might be that the vector you\'re passing is not in the correct format, or there\'s an issue with how you\'re calling the upsert method.\n\n\n\n### How can I search without a vector?\n\n\n\nYou are likely looking for the [scroll](../../concepts/points/#scroll-points) method. It allows you to retrieve the records based on filters or even iterate over all the records in the collection.\n\n\n\n### Does Qdrant support a full-text search or a hybrid search?', '}\n\n```\n\n\n\n## Pagination\n\n\n\n*Available as of v0.8.3*\n\n\n\nSearch and [recommendation](../explore/#recommendation-api) APIs allow to skip first results of the search and return only the result starting from some specified offset:\n\n\n\nExample:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{\n\n    ""vector"": [0.2, 0.1, 0.9, 0.7],\n\n    ""with_vectors"": true,\n\n    ""with_payload"": true,\n\n    ""limit"": 10,\n\n    ""offset"": 100\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient', '| Index| Sparse Query | Dense Query |\n\n| --- | --- | --- | \n\n| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |\n\n| Search Type | Always exact in Qdrant | HNSW is an approximate NN |\n\n| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |\n\n\n\nIn general, the speed of the search is proportional to the number of non-zero values in the query vector.\n\n\n\n```http', '### Average vector strategy\n\n\n\nThe default and first strategy added to Qdrant is called `average_vector`. It preprocesses the input examples to create a single vector that is used for the search. Since the preprocessing step happens very fast, the performance of this strategy is on-par with regular search. The intuition behind this kind of recommendation is that each vector component represents an independent feature of the data, so, by averaging the examples, we should get a good recommendation.']","Please read more about collections, isolation, and multiple users in our [Multitenancy](../../tutorials/multiple-partitions/) tutorial.



### My search results contain vectors with null values. Why?



By default, Qdrant tries to minimize network traffic and doesn't return vectors in search results.

But you can force Qdrant to do so by setting the `with_vector` parameter of the Search/Scroll to `true`.
If you're still seeing `""vector"": null` in your results, it might be that the vector you're passing is not in the correct format, or there's an issue with how you're calling the upsert method.



### How can I search without a vector?



You are likely looking for the [scroll](../../concepts/points/#scroll-points) method. It allows you to retrieve the records based on filters or even iterate over all the records in the collection.



### Does Qdrant support a full-text search or a hybrid search?
}

```



## Pagination



*Available as of v0.8.3*



Search and [recommendation](../explore/#recommendation-api) APIs allow to skip first results of the search and return only the result starting from some specified offset:



Example:



```http

POST /collections/{collection_name}/points/search

{

    ""vector"": [0.2, 0.1, 0.9, 0.7],

    ""with_vectors"": true,

    ""with_payload"": true,

    ""limit"": 10,

    ""offset"": 100

}

```



```python

from qdrant_client import QdrantClient
| Index| Sparse Query | Dense Query |

| --- | --- | --- | 

| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |

| Search Type | Always exact in Qdrant | HNSW is an approximate NN |

| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |



In general, the speed of the search is proportional to the number of non-zero values in the query vector.



```http
### Average vector strategy



The default and first strategy added to Qdrant is called `average_vector`. It preprocesses the input examples to create a single vector that is used for the search. Since the preprocessing step happens very fast, the performance of this strategy is on-par with regular search. The intuition behind this kind of recommendation is that each vector component represents an independent feature of the data, so, by averaging the examples, we should get a good recommendation.",0.4485981308411215,0.6075949367088608,0.5161290322580645,0.18867924528301888,0.2564102564102564,0.21739130434782608,0.2336448598130841,0.31645569620253167,0.26881720430107525,0.3644859813084112,0.4936708860759494,0.41935483870967744,0.9197760224342346,0.6543468236923218,0.6965067982673645,0.67476886510849,0.9656069879420102,0.9329653959721327,116,1.380952380952381,0.3929191417749824
True,,944.288,3e44823b,What does Qdrant not plan to support in the future?,"In the future, Qdrant does not plan to support BM25 or other non-vector-based retrieval or ranking functions, built-in ontologies or knowledge graphs, and query analyzers and other NLP tools. Qdrant aims to introduce support for sparse vectors, as used in SPLADE or similar models, but does not intend to incorporate the aforementioned features into its functionality. Users can combine Qdrant with specialized tools, including full-text search engines, to complement its capabilities.","Qdrant does not plan to support the following in the future:

- BM25 or other non-vector-based retrieval or ranking functions
- Built-in ontologies or knowledge graphs
- Query analyzers and other NLP tools","What Qdrant plans to introduce in the future:



- Support for sparse vectors, as used in [SPLADE](https://github.com/naver/splade) or similar models



What Qdrant doesn't plan to support:



- BM25 or other non-vector-based retrieval or ranking functions

- Built-in ontologies or knowledge graphs

- Query analyzers and other NLP tools



Of course, you can always combine Qdrant with any specialized tool you need, including full-text search engines.
. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.
Other than as expressly set out in these terms, Qdrant does not make any commitments about the Solution and Website’s availability or ability to meet your expectations.





### 15. Liability



In no event shall Qdrant be liable for:





* any indirect damages of any kind, including any potential loss of business;

* any damage or loss which is not caused by a breach of its obligations under the T&Cs;

* disruptions or damage inherent in an electronic communications network;
## Known limitations



1. Pinecone does not support arbitrary JSON metadata, but a flat structure with strings, numbers, booleans, or lists of strings used as values. Qdrant accepts any JSON object as a payload, even nested structures.

2. NULL values are not supported in Pinecone metadata but are handled properly by Qdrant.

3. The maximum size of Pinecone metadata is 40kb per vector. 

4. Pinecone, unlike Qdrant, does not support geolocation and filtering based on geographical criteria.
You acknowledge that you are informed that the unavailability of the Solution may be the result of (a) a maintenance operation, (b) an urgent operation relating in particular to security, (c) a case of “force majeure” or (d) the malfunctioning of computer applications of Qdrant's third-party partners. Qdrant undertakes to restore the availability of the Solution as soon as possible once the problem causing the unavailability has been resolved.","[""What Qdrant plans to introduce in the future:\n\n\n\n- Support for sparse vectors, as used in [SPLADE](https://github.com/naver/splade) or similar models\n\n\n\nWhat Qdrant doesn't plan to support:\n\n\n\n- BM25 or other non-vector-based retrieval or ranking functions\n\n- Built-in ontologies or knowledge graphs\n\n- Query analyzers and other NLP tools\n\n\n\nOf course, you can always combine Qdrant with any specialized tool you need, including full-text search engines."", '. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.', 'Other than as expressly set out in these terms, Qdrant does not make any commitments about the Solution and Website’s availability or ability to meet your expectations.\n\n\n\n\n\n### 15. Liability\n\n\n\nIn no event shall Qdrant be liable for:\n\n\n\n\n\n* any indirect damages of any kind, including any potential loss of business;\n\n* any damage or loss which is not caused by a breach of its obligations under the T&Cs;\n\n* disruptions or damage inherent in an electronic communications network;', '## Known limitations\n\n\n\n1. Pinecone does not support arbitrary JSON metadata, but a flat structure with strings, numbers, booleans, or lists of strings used as values. Qdrant accepts any JSON object as a payload, even nested structures.\n\n2. NULL values are not supported in Pinecone metadata but are handled properly by Qdrant.\n\n3. The maximum size of Pinecone metadata is 40kb per vector. \n\n4. Pinecone, unlike Qdrant, does not support geolocation and filtering based on geographical criteria.', ""You acknowledge that you are informed that the unavailability of the Solution may be the result of (a) a maintenance operation, (b) an urgent operation relating in particular to security, (c) a case of “force majeure” or (d) the malfunctioning of computer applications of Qdrant's third-party partners. Qdrant undertakes to restore the availability of the Solution as soon as possible once the problem causing the unavailability has been resolved.""]","Question: What does Qdrant not plan to support in the future?\n\nContext: What Qdrant plans to introduce in the future:



- Support for sparse vectors, as used in [SPLADE](https://github.com/naver/splade) or similar models



What Qdrant doesn't plan to support:



- BM25 or other non-vector-based retrieval or ranking functions

- Built-in ontologies or knowledge graphs

- Query analyzers and other NLP tools



Of course, you can always combine Qdrant with any specialized tool you need, including full-text search engines.
. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.
Other than as expressly set out in these terms, Qdrant does not make any commitments about the Solution and Website’s availability or ability to meet your expectations.





### 15. Liability



In no event shall Qdrant be liable for:





* any indirect damages of any kind, including any potential loss of business;

* any damage or loss which is not caused by a breach of its obligations under the T&Cs;

* disruptions or damage inherent in an electronic communications network;
## Known limitations



1. Pinecone does not support arbitrary JSON metadata, but a flat structure with strings, numbers, booleans, or lists of strings used as values. Qdrant accepts any JSON object as a payload, even nested structures.

2. NULL values are not supported in Pinecone metadata but are handled properly by Qdrant.

3. The maximum size of Pinecone metadata is 40kb per vector. 

4. Pinecone, unlike Qdrant, does not support geolocation and filtering based on geographical criteria.
You acknowledge that you are informed that the unavailability of the Solution may be the result of (a) a maintenance operation, (b) an urgent operation relating in particular to security, (c) a case of “force majeure” or (d) the malfunctioning of computer applications of Qdrant's third-party partners. Qdrant undertakes to restore the availability of the Solution as soon as possible once the problem causing the unavailability has been resolved.\n\nAnswer:",What does Qdrant not plan to support in the future?,"In the future, Qdrant does not plan to support BM25 or other non-vector-based retrieval or ranking functions, built-in ontologies or knowledge graphs, and query analyzers and other NLP tools. Qdrant aims to introduce support for sparse vectors, as used in SPLADE or similar models, but does not intend to incorporate the aforementioned features into its functionality. Users can combine Qdrant with specialized tools, including full-text search engines, to complement its capabilities.","[""What Qdrant plans to introduce in the future:\n\n\n\n- Support for sparse vectors, as used in [SPLADE](https://github.com/naver/splade) or similar models\n\n\n\nWhat Qdrant doesn't plan to support:\n\n\n\n- BM25 or other non-vector-based retrieval or ranking functions\n\n- Built-in ontologies or knowledge graphs\n\n- Query analyzers and other NLP tools\n\n\n\nOf course, you can always combine Qdrant with any specialized tool you need, including full-text search engines."", '. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.', 'Other than as expressly set out in these terms, Qdrant does not make any commitments about the Solution and Website’s availability or ability to meet your expectations.\n\n\n\n\n\n### 15. Liability\n\n\n\nIn no event shall Qdrant be liable for:\n\n\n\n\n\n* any indirect damages of any kind, including any potential loss of business;\n\n* any damage or loss which is not caused by a breach of its obligations under the T&Cs;\n\n* disruptions or damage inherent in an electronic communications network;', '## Known limitations\n\n\n\n1. Pinecone does not support arbitrary JSON metadata, but a flat structure with strings, numbers, booleans, or lists of strings used as values. Qdrant accepts any JSON object as a payload, even nested structures.\n\n2. NULL values are not supported in Pinecone metadata but are handled properly by Qdrant.\n\n3. The maximum size of Pinecone metadata is 40kb per vector. \n\n4. Pinecone, unlike Qdrant, does not support geolocation and filtering based on geographical criteria.', ""You acknowledge that you are informed that the unavailability of the Solution may be the result of (a) a maintenance operation, (b) an urgent operation relating in particular to security, (c) a case of “force majeure” or (d) the malfunctioning of computer applications of Qdrant's third-party partners. Qdrant undertakes to restore the availability of the Solution as soon as possible once the problem causing the unavailability has been resolved.""]","What Qdrant plans to introduce in the future:



- Support for sparse vectors, as used in [SPLADE](https://github.com/naver/splade) or similar models



What Qdrant doesn't plan to support:



- BM25 or other non-vector-based retrieval or ranking functions

- Built-in ontologies or knowledge graphs

- Query analyzers and other NLP tools



Of course, you can always combine Qdrant with any specialized tool you need, including full-text search engines.
. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.
Other than as expressly set out in these terms, Qdrant does not make any commitments about the Solution and Website’s availability or ability to meet your expectations.





### 15. Liability



In no event shall Qdrant be liable for:





* any indirect damages of any kind, including any potential loss of business;

* any damage or loss which is not caused by a breach of its obligations under the T&Cs;

* disruptions or damage inherent in an electronic communications network;
## Known limitations



1. Pinecone does not support arbitrary JSON metadata, but a flat structure with strings, numbers, booleans, or lists of strings used as values. Qdrant accepts any JSON object as a payload, even nested structures.

2. NULL values are not supported in Pinecone metadata but are handled properly by Qdrant.

3. The maximum size of Pinecone metadata is 40kb per vector. 

4. Pinecone, unlike Qdrant, does not support geolocation and filtering based on geographical criteria.
You acknowledge that you are informed that the unavailability of the Solution may be the result of (a) a maintenance operation, (b) an urgent operation relating in particular to security, (c) a case of “force majeure” or (d) the malfunctioning of computer applications of Qdrant's third-party partners. Qdrant undertakes to restore the availability of the Solution as soon as possible once the problem causing the unavailability has been resolved.",0.967741935483871,0.410958904109589,0.5769230769230769,0.8666666666666667,0.3611111111111111,0.5098039215686274,0.9032258064516129,0.3835616438356164,0.5384615384615385,0.9354838709677419,0.3972602739726027,0.5576923076923077,0.8704211711883545,0.850044310092926,0.6999370455741882,0.7677221298217773,0.8315955549478531,0.7971253256643346,30,0.4225352112676056,0.49753411697598393
True,,2878.557,bc949876, What is the recommended approach for configuring multitenancy in terms of the number of collections to create?,"In most cases, the recommended approach for configuring multitenancy is to create only a single collection with payload-based partitioning. This approach, known as multitenancy, is efficient for the majority of users. By using a single collection with payload-based partitioning, each user can access their own vectors while ensuring that they cannot see the vectors of other users. However, it is important to note that this approach requires additional configuration to set up properly.","When setting up multitenancy, it is generally recommended to start with a single collection using payload-based partitioning. This approach is efficient for most users and can be sufficient in many cases.

However, if you have a limited number of users and require isolation between them, you may consider creating multiple collections. This approach provides more flexibility but may come with additional costs due to resource overhead. It's important to ensure that the multiple collections do not negatively impact each other's performance.

In summary:

1. **Single Collection**: Use payload-based partitioning for most cases.
2. **Multiple Collections**: Consider creating multiple collections for isolation when needed, but be mindful of potential resource overhead and performance implications.

Ultimately, the decision on the number of collections to create should be based on your specific requirements for data isolation, performance, and resource utilization.","## Setting up multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called [multitenancy](https://en.wikipedia.org/wiki/Multitenancy). It is efficient for most of users, but it requires additional configuration. [Learn how to set it up](../../tutorials/multiple-partitions/)
---

title: Multitenancy

weight: 12

aliases:

  - ../tutorials/multiple-partitions

---

# Configure Multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.
**When should you create multiple collections?** When you have a limited number of users and you need isolation. This approach is flexible, but it may be more costly, since creating numerous collections may result in resource overhead. Also, you need to ensure that they do not affect each other in any way, including performance-wise. 



> Note: If you're running `curl` from the command line, the following commands

assume that you have a running instance of Qdrant on `http://localhost:6333`.
In addition to metrics and vector size, each collection uses its own set of parameters that controls collection optimization, index construction, and vacuum.

These settings can be changed at any time by a corresponding request.



## Setting up multitenancy
Qdrant is built to excel in a single collection with a vast number of tenants. You should only create multiple collections when your data is not homogenous or if users' vectors are created by different embedding models. Creating too many collections may result in resource overhead and cause dependencies. This can increase costs and affect overall performance. 



## Sharding your database","['## Setting up multitenancy\n\n\n\n**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called [multitenancy](https://en.wikipedia.org/wiki/Multitenancy). It is efficient for most of users, but it requires additional configuration. [Learn how to set it up](../../tutorials/multiple-partitions/)', '---\n\ntitle: Multitenancy\n\nweight: 12\n\naliases:\n\n  - ../tutorials/multiple-partitions\n\n---\n\n# Configure Multitenancy\n\n\n\n**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.', ""**When should you create multiple collections?** When you have a limited number of users and you need isolation. This approach is flexible, but it may be more costly, since creating numerous collections may result in resource overhead. Also, you need to ensure that they do not affect each other in any way, including performance-wise. \n\n\n\n> Note: If you're running `curl` from the command line, the following commands\n\nassume that you have a running instance of Qdrant on `http://localhost:6333`."", 'In addition to metrics and vector size, each collection uses its own set of parameters that controls collection optimization, index construction, and vacuum.\n\nThese settings can be changed at any time by a corresponding request.\n\n\n\n## Setting up multitenancy', ""Qdrant is built to excel in a single collection with a vast number of tenants. You should only create multiple collections when your data is not homogenous or if users' vectors are created by different embedding models. Creating too many collections may result in resource overhead and cause dependencies. This can increase costs and affect overall performance. \n\n\n\n## Sharding your database""]","Question:  What is the recommended approach for configuring multitenancy in terms of the number of collections to create?\n\nContext: ## Setting up multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called [multitenancy](https://en.wikipedia.org/wiki/Multitenancy). It is efficient for most of users, but it requires additional configuration. [Learn how to set it up](../../tutorials/multiple-partitions/)
---

title: Multitenancy

weight: 12

aliases:

- ../tutorials/multiple-partitions

---

# Configure Multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.
**When should you create multiple collections?** When you have a limited number of users and you need isolation. This approach is flexible, but it may be more costly, since creating numerous collections may result in resource overhead. Also, you need to ensure that they do not affect each other in any way, including performance-wise. 



> Note: If you're running `curl` from the command line, the following commands

assume that you have a running instance of Qdrant on `http://localhost:6333`.
In addition to metrics and vector size, each collection uses its own set of parameters that controls collection optimization, index construction, and vacuum.

These settings can be changed at any time by a corresponding request.



## Setting up multitenancy
Qdrant is built to excel in a single collection with a vast number of tenants. You should only create multiple collections when your data is not homogenous or if users' vectors are created by different embedding models. Creating too many collections may result in resource overhead and cause dependencies. This can increase costs and affect overall performance. 



## Sharding your database\n\nAnswer:", What is the recommended approach for configuring multitenancy in terms of the number of collections to create?,"In most cases, the recommended approach for configuring multitenancy is to create only a single collection with payload-based partitioning. This approach, known as multitenancy, is efficient for the majority of users. By using a single collection with payload-based partitioning, each user can access their own vectors while ensuring that they cannot see the vectors of other users. However, it is important to note that this approach requires additional configuration to set up properly.","['## Setting up multitenancy\n\n\n\n**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called [multitenancy](https://en.wikipedia.org/wiki/Multitenancy). It is efficient for most of users, but it requires additional configuration. [Learn how to set it up](../../tutorials/multiple-partitions/)', '---\n\ntitle: Multitenancy\n\nweight: 12\n\naliases:\n\n  - ../tutorials/multiple-partitions\n\n---\n\n# Configure Multitenancy\n\n\n\n**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.', ""**When should you create multiple collections?** When you have a limited number of users and you need isolation. This approach is flexible, but it may be more costly, since creating numerous collections may result in resource overhead. Also, you need to ensure that they do not affect each other in any way, including performance-wise. \n\n\n\n> Note: If you're running `curl` from the command line, the following commands\n\nassume that you have a running instance of Qdrant on `http://localhost:6333`."", 'In addition to metrics and vector size, each collection uses its own set of parameters that controls collection optimization, index construction, and vacuum.\n\nThese settings can be changed at any time by a corresponding request.\n\n\n\n## Setting up multitenancy', ""Qdrant is built to excel in a single collection with a vast number of tenants. You should only create multiple collections when your data is not homogenous or if users' vectors are created by different embedding models. Creating too many collections may result in resource overhead and cause dependencies. This can increase costs and affect overall performance. \n\n\n\n## Sharding your database""]","## Setting up multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called [multitenancy](https://en.wikipedia.org/wiki/Multitenancy). It is efficient for most of users, but it requires additional configuration. [Learn how to set it up](../../tutorials/multiple-partitions/)
---

title: Multitenancy

weight: 12

aliases:

  - ../tutorials/multiple-partitions

---

# Configure Multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.
**When should you create multiple collections?** When you have a limited number of users and you need isolation. This approach is flexible, but it may be more costly, since creating numerous collections may result in resource overhead. Also, you need to ensure that they do not affect each other in any way, including performance-wise. 



> Note: If you're running `curl` from the command line, the following commands

assume that you have a running instance of Qdrant on `http://localhost:6333`.
In addition to metrics and vector size, each collection uses its own set of parameters that controls collection optimization, index construction, and vacuum.

These settings can be changed at any time by a corresponding request.



## Setting up multitenancy
Qdrant is built to excel in a single collection with a vast number of tenants. You should only create multiple collections when your data is not homogenous or if users' vectors are created by different embedding models. Creating too many collections may result in resource overhead and cause dependencies. This can increase costs and affect overall performance. 



## Sharding your database",0.34558823529411764,0.6714285714285714,0.45631067961165045,0.13333333333333333,0.2608695652173913,0.1764705882352941,0.16911764705882354,0.32857142857142857,0.22330097087378642,0.2867647058823529,0.5571428571428572,0.37864077669902907,0.8363739848136902,0.5947833061218262,0.6745120286941528,0.6321436762809753,0.8605438576152664,0.9753836070449324,139,1.8783783783783783,0.5378137283626401
True,,2053.962,a6f43392,What is the purpose of the locking feature in Qdrant administration tools?,"The locking feature in Qdrant administration tools serves the purpose of restricting the possible operations on a Qdrant process. It allows users to control the behavior of a Qdrant instance at runtime without manually changing its configuration. When a lock is applied, specific operations such as creating new collections or adding new data to the existing storage can be disabled. It is important to note that the locking configuration is not persistent, so it needs to be reapplied after a restart. Additionally, the locking feature applies to a single node only, requiring the lock to be set on all desired nodes in a distributed deployment setup. The locking API in Qdrant enables users to manage and control the access and operations on the Qdrant instance effectively.","The purpose of the locking feature in Qdrant administration tools is to restrict the possible operations on a Qdrant process. By using the locking API, users can control the behavior of the Qdrant instance at runtime, preventing certain operations from being carried out. For example, setting a write lock can prevent the creation of new collections or the addition of new data to the existing storage, while still allowing deletion operations or updates. This feature provides a way to manage and control the actions that can be performed on the Qdrant instance, ensuring data integrity and security.","---

title: Administration

weight: 10

aliases:

  - ../administration

---



# Administration



Qdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.



## Locking



A locking API enables users to restrict the possible operations on a qdrant process.

It is important to mention that:

- The configuration is not persistent therefore it is necessary to lock again following a restart.
- Locking applies to a single node only. It is necessary to call lock on all the desired nodes in a distributed deployment setup.



Lock request sample:



```http

POST /locks

{

    ""error_message"": ""write is forbidden"",

    ""write"": true

}

```



Write flags enables/disables write lock.

If the write lock is set to true, qdrant doesn't allow creating new collections or adding new data to the existing storage.

However, deletion operations or updates are not forbidden under the write lock.
Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.
setup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale 

both the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle 

large amounts of data without sacrificing performance or reliability.



## Administration API



Another new feature is the administration API, which allows you to disable write operations to the service. This is
## Why Qdrant?","['---\n\ntitle: Administration\n\nweight: 10\n\naliases:\n\n  - ../administration\n\n---\n\n\n\n# Administration\n\n\n\nQdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.\n\n\n\n## Locking\n\n\n\nA locking API enables users to restrict the possible operations on a qdrant process.\n\nIt is important to mention that:\n\n- The configuration is not persistent therefore it is necessary to lock again following a restart.', '- Locking applies to a single node only. It is necessary to call lock on all the desired nodes in a distributed deployment setup.\n\n\n\nLock request sample:\n\n\n\n```http\n\nPOST /locks\n\n{\n\n    ""error_message"": ""write is forbidden"",\n\n    ""write"": true\n\n}\n\n```\n\n\n\nWrite flags enables/disables write lock.\n\nIf the write lock is set to true, qdrant doesn\'t allow creating new collections or adding new data to the existing storage.\n\nHowever, deletion operations or updates are not forbidden under the write lock.', 'Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.', 'setup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale \n\nboth the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle \n\nlarge amounts of data without sacrificing performance or reliability.\n\n\n\n## Administration API\n\n\n\nAnother new feature is the administration API, which allows you to disable write operations to the service. This is', '## Why Qdrant?']","Question: What is the purpose of the locking feature in Qdrant administration tools?\n\nContext: ---

title: Administration

weight: 10

aliases:

- ../administration

---



# Administration



Qdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.



## Locking



A locking API enables users to restrict the possible operations on a qdrant process.

It is important to mention that:

- The configuration is not persistent therefore it is necessary to lock again following a restart.
- Locking applies to a single node only. It is necessary to call lock on all the desired nodes in a distributed deployment setup.



Lock request sample:



```http

POST /locks

{

""error_message"": ""write is forbidden"",

""write"": true

}

```



Write flags enables/disables write lock.

If the write lock is set to true, qdrant doesn't allow creating new collections or adding new data to the existing storage.

However, deletion operations or updates are not forbidden under the write lock.
Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.
setup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale 

both the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle 

large amounts of data without sacrificing performance or reliability.



## Administration API



Another new feature is the administration API, which allows you to disable write operations to the service. This is
## Why Qdrant?\n\nAnswer:",What is the purpose of the locking feature in Qdrant administration tools?,"The locking feature in Qdrant administration tools serves the purpose of restricting the possible operations on a Qdrant process. It allows users to control the behavior of a Qdrant instance at runtime without manually changing its configuration. When a lock is applied, specific operations such as creating new collections or adding new data to the existing storage can be disabled. It is important to note that the locking configuration is not persistent, so it needs to be reapplied after a restart. Additionally, the locking feature applies to a single node only, requiring the lock to be set on all desired nodes in a distributed deployment setup. The locking API in Qdrant enables users to manage and control the access and operations on the Qdrant instance effectively.","['---\n\ntitle: Administration\n\nweight: 10\n\naliases:\n\n  - ../administration\n\n---\n\n\n\n# Administration\n\n\n\nQdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.\n\n\n\n## Locking\n\n\n\nA locking API enables users to restrict the possible operations on a qdrant process.\n\nIt is important to mention that:\n\n- The configuration is not persistent therefore it is necessary to lock again following a restart.', '- Locking applies to a single node only. It is necessary to call lock on all the desired nodes in a distributed deployment setup.\n\n\n\nLock request sample:\n\n\n\n```http\n\nPOST /locks\n\n{\n\n    ""error_message"": ""write is forbidden"",\n\n    ""write"": true\n\n}\n\n```\n\n\n\nWrite flags enables/disables write lock.\n\nIf the write lock is set to true, qdrant doesn\'t allow creating new collections or adding new data to the existing storage.\n\nHowever, deletion operations or updates are not forbidden under the write lock.', 'Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.', 'setup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale \n\nboth the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle \n\nlarge amounts of data without sacrificing performance or reliability.\n\n\n\n## Administration API\n\n\n\nAnother new feature is the administration API, which allows you to disable write operations to the service. This is', '## Why Qdrant?']","---

title: Administration

weight: 10

aliases:

  - ../administration

---



# Administration



Qdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.



## Locking



A locking API enables users to restrict the possible operations on a qdrant process.

It is important to mention that:

- The configuration is not persistent therefore it is necessary to lock again following a restart.
- Locking applies to a single node only. It is necessary to call lock on all the desired nodes in a distributed deployment setup.



Lock request sample:



```http

POST /locks

{

    ""error_message"": ""write is forbidden"",

    ""write"": true

}

```



Write flags enables/disables write lock.

If the write lock is set to true, qdrant doesn't allow creating new collections or adding new data to the existing storage.

However, deletion operations or updates are not forbidden under the write lock.
Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.
setup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale 

both the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle 

large amounts of data without sacrificing performance or reliability.



## Administration API



Another new feature is the administration API, which allows you to disable write operations to the service. This is
## Why Qdrant?",0.6385542168674698,0.48623853211009177,0.5520833333333334,0.36585365853658536,0.2777777777777778,0.3157894736842105,0.4457831325301205,0.3394495412844037,0.3854166666666667,0.6024096385542169,0.45871559633027525,0.5208333333333334,0.9279898405075073,0.7614399194717407,0.6992033123970032,0.728995680809021,0.9984299731404462,0.995027522338205,97,0.7698412698412699,0.5564712299708153
True,,4840.284,a83e2be1,What is the significance of the `on_disk_payload` setting?,"The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes.","The `on_disk_payload` setting in Qdrant has the significance of determining where to store payload data. When set to `true`, the payload data will be stored on disk only, rather than in memory. This setting can be useful for limiting RAM usage, especially in cases where there is a large payload.

By storing the payload on disk only, it helps save RAM while slightly increasing the response time because the payload will be read from the disk every time it is requested. It's important to note that payload values involved in filtering and indexed remain in RAM, even when `on_disk_payload` is set to `true`.

In summary, setting `on_disk_payload` to `true` can be beneficial for managing memory usage, especially when dealing with large payload data in Qdrant.","* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.

* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.
temp_path: null



  # If true - point's payload will not be stored in memory.

  # It will be read from the disk every time it is requested.

  # This setting saves RAM by (slightly) increasing the response time.

  # Note: those payload values that are involved in filtering and are indexed - remain in RAM.

  on_disk_payload: true



  # Maximum number of concurrent updates to shard replicas

  # If `null` - maximum concurrency is used.

  update_concurrency: null
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
InMemory payload storage is organized in the same way as in-memory vectors.

The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.

This type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.



In the case of large payload values, it might be better to use OnDisk payload storage.
""on_disk"": true

    }

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),

    hnsw_config=models.HnswConfigDiff(on_disk=True),

)

```



```typescript","['* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.\n\n* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\n\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.', ""temp_path: null\n\n\n\n  # If true - point's payload will not be stored in memory.\n\n  # It will be read from the disk every time it is requested.\n\n  # This setting saves RAM by (slightly) increasing the response time.\n\n  # Note: those payload values that are involved in filtering and are indexed - remain in RAM.\n\n  on_disk_payload: true\n\n\n\n  # Maximum number of concurrent updates to shard replicas\n\n  # If `null` - maximum concurrency is used.\n\n  update_concurrency: null"", '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', 'InMemory payload storage is organized in the same way as in-memory vectors.\n\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\n\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\n\n\n\nIn the case of large payload values, it might be better to use OnDisk payload storage.', '""on_disk"": true\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),\n\n    hnsw_config=models.HnswConfigDiff(on_disk=True),\n\n)\n\n```\n\n\n\n```typescript']","Question: What is the significance of the `on_disk_payload` setting?\n\nContext: * `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.

* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.
temp_path: null



# If true - point's payload will not be stored in memory.

# It will be read from the disk every time it is requested.

# This setting saves RAM by (slightly) increasing the response time.

# Note: those payload values that are involved in filtering and are indexed - remain in RAM.

on_disk_payload: true



# Maximum number of concurrent updates to shard replicas

# If `null` - maximum concurrency is used.

update_concurrency: null
""on_disk_payload"": false

},

""hnsw_config"": {

""m"": 16,

""ef_construct"": 100,

""full_scan_threshold"": 10000,

""max_indexing_threads"": 0

},

""optimizer_config"": {

""deleted_threshold"": 0.2,

""vacuum_min_vector_number"": 1000,

""default_segment_number"": 0,

""max_segment_size"": null,
InMemory payload storage is organized in the same way as in-memory vectors.

The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.

This type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.



In the case of large payload values, it might be better to use OnDisk payload storage.
""on_disk"": true

}

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),

hnsw_config=models.HnswConfigDiff(on_disk=True),

)

```



```typescript\n\nAnswer:",What is the significance of the `on_disk_payload` setting?,"The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes.","['* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.\n\n* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\n\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.', ""temp_path: null\n\n\n\n  # If true - point's payload will not be stored in memory.\n\n  # It will be read from the disk every time it is requested.\n\n  # This setting saves RAM by (slightly) increasing the response time.\n\n  # Note: those payload values that are involved in filtering and are indexed - remain in RAM.\n\n  on_disk_payload: true\n\n\n\n  # Maximum number of concurrent updates to shard replicas\n\n  # If `null` - maximum concurrency is used.\n\n  update_concurrency: null"", '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', 'InMemory payload storage is organized in the same way as in-memory vectors.\n\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\n\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\n\n\n\nIn the case of large payload values, it might be better to use OnDisk payload storage.', '""on_disk"": true\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),\n\n    hnsw_config=models.HnswConfigDiff(on_disk=True),\n\n)\n\n```\n\n\n\n```typescript']","* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.

* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.
temp_path: null



  # If true - point's payload will not be stored in memory.

  # It will be read from the disk every time it is requested.

  # This setting saves RAM by (slightly) increasing the response time.

  # Note: those payload values that are involved in filtering and are indexed - remain in RAM.

  on_disk_payload: true



  # Maximum number of concurrent updates to shard replicas

  # If `null` - maximum concurrency is used.

  update_concurrency: null
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
InMemory payload storage is organized in the same way as in-memory vectors.

The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.

This type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.



In the case of large payload values, it might be better to use OnDisk payload storage.
""on_disk"": true

    }

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),

    hnsw_config=models.HnswConfigDiff(on_disk=True),

)

```



```typescript",0.5241935483870968,0.6914893617021277,0.5963302752293578,0.2926829268292683,0.3870967741935484,0.3333333333333333,0.3709677419354839,0.48936170212765956,0.4220183486238532,0.5080645161290323,0.6702127659574468,0.5779816513761468,0.7224053144454956,0.6650106906890869,0.7178534269332886,0.6904224753379822,0.9323829570785165,0.9006570316851139,126,1.2727272727272727,0.4710657473354399
True,,3308.668,19d44dd1,What is the significance of the parameter `max_segment_size_kb` in the context of vector indexation?,"The `max_segment_size_kb` parameter in the context of vector indexation determines the maximum size, in kilobytes, that a segment can have. Segments larger than this specified size might lead to disproportionately long indexation times. Therefore, it is essential to limit the size of segments to optimize the indexation process. The choice of this parameter should be based on the priority between indexation speed and search speed. If indexation speed is more critical, the parameter should be set lower. Conversely, if search speed is more important, the parameter should be set higher. It is important to note that 1Kb is equivalent to 1 vector of size 256. If the `max_segment_size_kb` parameter is not explicitly set, it will be automatically selected based on the number of available CPUs.","In the context of vector indexation, the `max_segment_size_kb` parameter defines the maximum size (in kilobytes) of vectors to store in-memory per segment. Segments larger than this threshold will be stored as read-only memmaped files. 

This parameter is significant because it allows for efficient storage and retrieval of vectors during the indexing process. By setting an appropriate value for `max_segment_size_kb`, you can balance the trade-off between search speed and memory consumption. 

If the parameter is set too low, more segments will be created, potentially slowing down the search speed. On the other hand, if it is set too high, it may consume more memory than necessary. 

Therefore, adjusting the `max_segment_size_kb` parameter based on the specific requirements of your application can help optimize the performance of vector indexation.","# If search speed is more important - make this parameter higher.

    # Note: 1Kb = 1 vector of size 256

    # If not set, will be automatically selected considering the number of available CPUs.

    max_segment_size_kb: null



    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.

    # Segments larger than this threshold will be stored as read-only memmaped file.

    # To enable memmap storage, lower the threshold

    # Note: 1Kb = 1 vector of size 256
# Segments larger than this threshold will be stored as read-only memmaped file.

    # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value.

    # To disable memmap storage, set this to `0`.

    # Note: 1Kb = 1 vector of size 256

    memmap_threshold_kb: 200000



    # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing
### Indexing vectors in HNSW



In some cases, you might be surprised the value of `indexed_vectors_count` is lower than `vectors_count`. This is an intended behaviour and

depends on the [optimizer configuration](../optimizer). A new index segment is built if the size of non-indexed vectors is higher than the

value of `indexing_threshold`(in kB).  If your collection is very small or the dimensionality of the vectors is low, there might be no HNSW segment
We're changing the `default_segment_number` to 5. Segment numbers influence the number of graph nodes in the underlying HNSW index, thereby indirectly influencing the memory efficiency. 



#### Next, we upload our vectors to this and then enable indexing: 



```python

batch_size = 10000

client.upload_collection(

    collection_name=collection_name,

    ids=range(len(dataset)),

    vectors=dataset[""openai""],

    payload=[

        {""text"": x} for x in dataset[""text""]

    ],

    parallel=10,

)
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,","['# If search speed is more important - make this parameter higher.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    # If not set, will be automatically selected considering the number of available CPUs.\n\n    max_segment_size_kb: null\n\n\n\n    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.\n\n    # Segments larger than this threshold will be stored as read-only memmaped file.\n\n    # To enable memmap storage, lower the threshold\n\n    # Note: 1Kb = 1 vector of size 256', '# Segments larger than this threshold will be stored as read-only memmaped file.\n\n    # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value.\n\n    # To disable memmap storage, set this to `0`.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    memmap_threshold_kb: 200000\n\n\n\n    # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing', '### Indexing vectors in HNSW\n\n\n\nIn some cases, you might be surprised the value of `indexed_vectors_count` is lower than `vectors_count`. This is an intended behaviour and\n\ndepends on the [optimizer configuration](../optimizer). A new index segment is built if the size of non-indexed vectors is higher than the\n\nvalue of `indexing_threshold`(in kB).  If your collection is very small or the dimensionality of the vectors is low, there might be no HNSW segment', 'We\'re changing the `default_segment_number` to 5. Segment numbers influence the number of graph nodes in the underlying HNSW index, thereby indirectly influencing the memory efficiency. \n\n\n\n#### Next, we upload our vectors to this and then enable indexing: \n\n\n\n```python\n\nbatch_size = 10000\n\nclient.upload_collection(\n\n    collection_name=collection_name,\n\n    ids=range(len(dataset)),\n\n    vectors=dataset[""openai""],\n\n    payload=[\n\n        {""text"": x} for x in dataset[""text""]\n\n    ],\n\n    parallel=10,\n\n)', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,']","Question: What is the significance of the parameter `max_segment_size_kb` in the context of vector indexation?\n\nContext: # If search speed is more important - make this parameter higher.

# Note: 1Kb = 1 vector of size 256

# If not set, will be automatically selected considering the number of available CPUs.

max_segment_size_kb: null



# Maximum size (in KiloBytes) of vectors to store in-memory per segment.

# Segments larger than this threshold will be stored as read-only memmaped file.

# To enable memmap storage, lower the threshold

# Note: 1Kb = 1 vector of size 256
# Segments larger than this threshold will be stored as read-only memmaped file.

# Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value.

# To disable memmap storage, set this to `0`.

# Note: 1Kb = 1 vector of size 256

memmap_threshold_kb: 200000



# Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing
### Indexing vectors in HNSW



In some cases, you might be surprised the value of `indexed_vectors_count` is lower than `vectors_count`. This is an intended behaviour and

depends on the [optimizer configuration](../optimizer). A new index segment is built if the size of non-indexed vectors is higher than the

value of `indexing_threshold`(in kB).  If your collection is very small or the dimensionality of the vectors is low, there might be no HNSW segment
We're changing the `default_segment_number` to 5. Segment numbers influence the number of graph nodes in the underlying HNSW index, thereby indirectly influencing the memory efficiency. 



#### Next, we upload our vectors to this and then enable indexing: 



```python

batch_size = 10000

client.upload_collection(

collection_name=collection_name,

ids=range(len(dataset)),

vectors=dataset[""openai""],

payload=[

{""text"": x} for x in dataset[""text""]

],

parallel=10,

)
""on_disk_payload"": false

},

""hnsw_config"": {

""m"": 16,

""ef_construct"": 100,

""full_scan_threshold"": 10000,

""max_indexing_threads"": 0

},

""optimizer_config"": {

""deleted_threshold"": 0.2,

""vacuum_min_vector_number"": 1000,

""default_segment_number"": 0,

""max_segment_size"": null,\n\nAnswer:",What is the significance of the parameter `max_segment_size_kb` in the context of vector indexation?,"The `max_segment_size_kb` parameter in the context of vector indexation determines the maximum size, in kilobytes, that a segment can have. Segments larger than this specified size might lead to disproportionately long indexation times. Therefore, it is essential to limit the size of segments to optimize the indexation process. The choice of this parameter should be based on the priority between indexation speed and search speed. If indexation speed is more critical, the parameter should be set lower. Conversely, if search speed is more important, the parameter should be set higher. It is important to note that 1Kb is equivalent to 1 vector of size 256. If the `max_segment_size_kb` parameter is not explicitly set, it will be automatically selected based on the number of available CPUs.","['# If search speed is more important - make this parameter higher.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    # If not set, will be automatically selected considering the number of available CPUs.\n\n    max_segment_size_kb: null\n\n\n\n    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.\n\n    # Segments larger than this threshold will be stored as read-only memmaped file.\n\n    # To enable memmap storage, lower the threshold\n\n    # Note: 1Kb = 1 vector of size 256', '# Segments larger than this threshold will be stored as read-only memmaped file.\n\n    # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value.\n\n    # To disable memmap storage, set this to `0`.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    memmap_threshold_kb: 200000\n\n\n\n    # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing', '### Indexing vectors in HNSW\n\n\n\nIn some cases, you might be surprised the value of `indexed_vectors_count` is lower than `vectors_count`. This is an intended behaviour and\n\ndepends on the [optimizer configuration](../optimizer). A new index segment is built if the size of non-indexed vectors is higher than the\n\nvalue of `indexing_threshold`(in kB).  If your collection is very small or the dimensionality of the vectors is low, there might be no HNSW segment', 'We\'re changing the `default_segment_number` to 5. Segment numbers influence the number of graph nodes in the underlying HNSW index, thereby indirectly influencing the memory efficiency. \n\n\n\n#### Next, we upload our vectors to this and then enable indexing: \n\n\n\n```python\n\nbatch_size = 10000\n\nclient.upload_collection(\n\n    collection_name=collection_name,\n\n    ids=range(len(dataset)),\n\n    vectors=dataset[""openai""],\n\n    payload=[\n\n        {""text"": x} for x in dataset[""text""]\n\n    ],\n\n    parallel=10,\n\n)', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,']","# If search speed is more important - make this parameter higher.

    # Note: 1Kb = 1 vector of size 256

    # If not set, will be automatically selected considering the number of available CPUs.

    max_segment_size_kb: null



    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.

    # Segments larger than this threshold will be stored as read-only memmaped file.

    # To enable memmap storage, lower the threshold

    # Note: 1Kb = 1 vector of size 256
# Segments larger than this threshold will be stored as read-only memmaped file.

    # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value.

    # To disable memmap storage, set this to `0`.

    # Note: 1Kb = 1 vector of size 256

    memmap_threshold_kb: 200000



    # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing
### Indexing vectors in HNSW



In some cases, you might be surprised the value of `indexed_vectors_count` is lower than `vectors_count`. This is an intended behaviour and

depends on the [optimizer configuration](../optimizer). A new index segment is built if the size of non-indexed vectors is higher than the

value of `indexing_threshold`(in kB).  If your collection is very small or the dimensionality of the vectors is low, there might be no HNSW segment
We're changing the `default_segment_number` to 5. Segment numbers influence the number of graph nodes in the underlying HNSW index, thereby indirectly influencing the memory efficiency. 



#### Next, we upload our vectors to this and then enable indexing: 



```python

batch_size = 10000

client.upload_collection(

    collection_name=collection_name,

    ids=range(len(dataset)),

    vectors=dataset[""openai""],

    payload=[

        {""text"": x} for x in dataset[""text""]

    ],

    parallel=10,

)
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,",0.5238095238095238,0.5546218487394958,0.5387755102040817,0.216,0.2288135593220339,0.2222222222222222,0.30158730158730157,0.31932773109243695,0.310204081632653,0.47619047619047616,0.5042016806722689,0.48979591836734687,0.8924208879470825,0.669573187828064,0.6684519648551941,0.6690121293067932,0.712144826804953,0.6954114254830139,126,1.008,0.2690533597935287
True,,5268.705,38f85f4a,What is the significance of the `indexing_threshold_kb` parameter?,"The `indexing_threshold_kb` parameter specifies the maximum size, in Kilobytes, allowed for vectors for plain index. The default value for this parameter is set to 20000 KB. It is important to note that 1 KB is equivalent to 1 vector of size 256. This parameter plays a crucial role in determining the maximum size of vectors that can be used for indexing. If the `indexing_threshold_kb` parameter is not explicitly set, the default value will be used. Additionally, setting this parameter to `0` will explicitly disable vector indexing. This parameter directly impacts the efficiency and performance of the indexing process for the given vectors.","The `indexing_threshold_kb` parameter is used to set the threshold for when a segment will be converted to memmap storage. When the size of the segment exceeds this threshold, it will be stored as a read-only memmaped file. This parameter is important for optimizing memory usage and performance when working with large amounts of data.

In the context of setting up the `memmap_threshold_kb` option, the `indexing_threshold_kb` parameter plays a role in determining when segments should be converted to memmap storage, which can be beneficial for efficient memory management. It is recommended to set the memmap threshold parameter based on the use scenario and the importance of search speed.

In the provided code snippet, the `indexing_threshold` parameter is set to 0, which disables indexing to zero. This allows for faster uploads of vectors and payloads. Once all the data is loaded, the indexing will be turned back on. Additionally, the `score_threshold` parameter can be used in the search query to exclude results with scores below a certain threshold.

Overall, the `indexing_threshold_kb` parameter, along with other related parameters, helps in optimizing memory usage, search speed, and performance when working with vector data in the context of retrieval methods.","- Set up `memmap_threshold_kb` option. This option will set the threshold after which the segment will be converted to memmap storage.



There are two ways to do this:



1. You can set the threshold globally in the [configuration file](../../guides/configuration/). The parameter is called `memmap_threshold_kb`.

2. You can set the threshold for each collection separately during [creation](../collections/#create-collection) or [update](../collections/#update-collection-parameters).



```http
);

```



The rule of thumb to set the memmap threshold parameter is simple:



- if you have a balanced use scenario - set memmap threshold the same as `indexing_threshold` (default is 20000). In this case the optimizer will not make any extra runs and will optimize all thresholds at once.
# If search speed is more important - make this parameter higher.

    # Note: 1Kb = 1 vector of size 256

    # If not set, will be automatically selected considering the number of available CPUs.

    max_segment_size_kb: null



    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.

    # Segments larger than this threshold will be stored as read-only memmaped file.

    # To enable memmap storage, lower the threshold

    # Note: 1Kb = 1 vector of size 256
default_segment_number=5,

        indexing_threshold=0,

    ),

    quantization_config=models.BinaryQuantization(

        binary=models.BinaryQuantizationConfig(always_ram=True),

    ),

)

```



#### What is happening in the OptimizerConfig? 



We're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded
In this case, you can use `score_threshold` parameter of the search query.

It will exclude all results with a score worse than the given.



<aside role=""status"">This parameter may exclude lower or higher scores depending on the used metric. For example, higher scores of Euclidean metric are considered more distant and, therefore, will be excluded.</aside>



### Payload and vector in the result



By default, retrieval methods do not return any stored information such as","['- Set up `memmap_threshold_kb` option. This option will set the threshold after which the segment will be converted to memmap storage.\n\n\n\nThere are two ways to do this:\n\n\n\n1. You can set the threshold globally in the [configuration file](../../guides/configuration/). The parameter is called `memmap_threshold_kb`.\n\n2. You can set the threshold for each collection separately during [creation](../collections/#create-collection) or [update](../collections/#update-collection-parameters).\n\n\n\n```http', ');\n\n```\n\n\n\nThe rule of thumb to set the memmap threshold parameter is simple:\n\n\n\n- if you have a balanced use scenario - set memmap threshold the same as `indexing_threshold` (default is 20000). In this case the optimizer will not make any extra runs and will optimize all thresholds at once.', '# If search speed is more important - make this parameter higher.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    # If not set, will be automatically selected considering the number of available CPUs.\n\n    max_segment_size_kb: null\n\n\n\n    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.\n\n    # Segments larger than this threshold will be stored as read-only memmaped file.\n\n    # To enable memmap storage, lower the threshold\n\n    # Note: 1Kb = 1 vector of size 256', ""default_segment_number=5,\n\n        indexing_threshold=0,\n\n    ),\n\n    quantization_config=models.BinaryQuantization(\n\n        binary=models.BinaryQuantizationConfig(always_ram=True),\n\n    ),\n\n)\n\n```\n\n\n\n#### What is happening in the OptimizerConfig? \n\n\n\nWe're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded"", 'In this case, you can use `score_threshold` parameter of the search query.\n\nIt will exclude all results with a score worse than the given.\n\n\n\n<aside role=""status"">This parameter may exclude lower or higher scores depending on the used metric. For example, higher scores of Euclidean metric are considered more distant and, therefore, will be excluded.</aside>\n\n\n\n### Payload and vector in the result\n\n\n\nBy default, retrieval methods do not return any stored information such as']","Question: What is the significance of the `indexing_threshold_kb` parameter?\n\nContext: - Set up `memmap_threshold_kb` option. This option will set the threshold after which the segment will be converted to memmap storage.



There are two ways to do this:



1. You can set the threshold globally in the [configuration file](../../guides/configuration/). The parameter is called `memmap_threshold_kb`.

2. You can set the threshold for each collection separately during [creation](../collections/#create-collection) or [update](../collections/#update-collection-parameters).



```http
);

```



The rule of thumb to set the memmap threshold parameter is simple:



- if you have a balanced use scenario - set memmap threshold the same as `indexing_threshold` (default is 20000). In this case the optimizer will not make any extra runs and will optimize all thresholds at once.
# If search speed is more important - make this parameter higher.

# Note: 1Kb = 1 vector of size 256

# If not set, will be automatically selected considering the number of available CPUs.

max_segment_size_kb: null



# Maximum size (in KiloBytes) of vectors to store in-memory per segment.

# Segments larger than this threshold will be stored as read-only memmaped file.

# To enable memmap storage, lower the threshold

# Note: 1Kb = 1 vector of size 256
default_segment_number=5,

indexing_threshold=0,

),

quantization_config=models.BinaryQuantization(

binary=models.BinaryQuantizationConfig(always_ram=True),

),

)

```



#### What is happening in the OptimizerConfig? 



We're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded
In this case, you can use `score_threshold` parameter of the search query.

It will exclude all results with a score worse than the given.



<aside role=""status"">This parameter may exclude lower or higher scores depending on the used metric. For example, higher scores of Euclidean metric are considered more distant and, therefore, will be excluded.</aside>



### Payload and vector in the result



By default, retrieval methods do not return any stored information such as\n\nAnswer:",What is the significance of the `indexing_threshold_kb` parameter?,"The `indexing_threshold_kb` parameter specifies the maximum size, in Kilobytes, allowed for vectors for plain index. The default value for this parameter is set to 20000 KB. It is important to note that 1 KB is equivalent to 1 vector of size 256. This parameter plays a crucial role in determining the maximum size of vectors that can be used for indexing. If the `indexing_threshold_kb` parameter is not explicitly set, the default value will be used. Additionally, setting this parameter to `0` will explicitly disable vector indexing. This parameter directly impacts the efficiency and performance of the indexing process for the given vectors.","['- Set up `memmap_threshold_kb` option. This option will set the threshold after which the segment will be converted to memmap storage.\n\n\n\nThere are two ways to do this:\n\n\n\n1. You can set the threshold globally in the [configuration file](../../guides/configuration/). The parameter is called `memmap_threshold_kb`.\n\n2. You can set the threshold for each collection separately during [creation](../collections/#create-collection) or [update](../collections/#update-collection-parameters).\n\n\n\n```http', ');\n\n```\n\n\n\nThe rule of thumb to set the memmap threshold parameter is simple:\n\n\n\n- if you have a balanced use scenario - set memmap threshold the same as `indexing_threshold` (default is 20000). In this case the optimizer will not make any extra runs and will optimize all thresholds at once.', '# If search speed is more important - make this parameter higher.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    # If not set, will be automatically selected considering the number of available CPUs.\n\n    max_segment_size_kb: null\n\n\n\n    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.\n\n    # Segments larger than this threshold will be stored as read-only memmaped file.\n\n    # To enable memmap storage, lower the threshold\n\n    # Note: 1Kb = 1 vector of size 256', ""default_segment_number=5,\n\n        indexing_threshold=0,\n\n    ),\n\n    quantization_config=models.BinaryQuantization(\n\n        binary=models.BinaryQuantizationConfig(always_ram=True),\n\n    ),\n\n)\n\n```\n\n\n\n#### What is happening in the OptimizerConfig? \n\n\n\nWe're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded"", 'In this case, you can use `score_threshold` parameter of the search query.\n\nIt will exclude all results with a score worse than the given.\n\n\n\n<aside role=""status"">This parameter may exclude lower or higher scores depending on the used metric. For example, higher scores of Euclidean metric are considered more distant and, therefore, will be excluded.</aside>\n\n\n\n### Payload and vector in the result\n\n\n\nBy default, retrieval methods do not return any stored information such as']","- Set up `memmap_threshold_kb` option. This option will set the threshold after which the segment will be converted to memmap storage.



There are two ways to do this:



1. You can set the threshold globally in the [configuration file](../../guides/configuration/). The parameter is called `memmap_threshold_kb`.

2. You can set the threshold for each collection separately during [creation](../collections/#create-collection) or [update](../collections/#update-collection-parameters).



```http
);

```



The rule of thumb to set the memmap threshold parameter is simple:



- if you have a balanced use scenario - set memmap threshold the same as `indexing_threshold` (default is 20000). In this case the optimizer will not make any extra runs and will optimize all thresholds at once.
# If search speed is more important - make this parameter higher.

    # Note: 1Kb = 1 vector of size 256

    # If not set, will be automatically selected considering the number of available CPUs.

    max_segment_size_kb: null



    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.

    # Segments larger than this threshold will be stored as read-only memmaped file.

    # To enable memmap storage, lower the threshold

    # Note: 1Kb = 1 vector of size 256
default_segment_number=5,

        indexing_threshold=0,

    ),

    quantization_config=models.BinaryQuantization(

        binary=models.BinaryQuantizationConfig(always_ram=True),

    ),

)

```



#### What is happening in the OptimizerConfig? 



We're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded
In this case, you can use `score_threshold` parameter of the search query.

It will exclude all results with a score worse than the given.



<aside role=""status"">This parameter may exclude lower or higher scores depending on the used metric. For example, higher scores of Euclidean metric are considered more distant and, therefore, will be excluded.</aside>



### Payload and vector in the result



By default, retrieval methods do not return any stored information such as",0.3423913043478261,0.65625,0.45,0.13114754098360656,0.25263157894736843,0.17266187050359713,0.18478260869565216,0.3541666666666667,0.24285714285714285,0.32065217391304346,0.6145833333333334,0.42142857142857143,0.6431847810745239,0.6121870279312134,0.6444047689437866,0.6278828978538513,0.9618591049918905,0.7449019857061405,195,1.911764705882353,0.15751642780763764
True,,5705.625999999999,55b83181,How can you optimize Qdrant for minimizing latency in search requests?,"To optimize Qdrant for minimizing latency in search requests, you can set up the system to use as many cores as possible for a single request. This can be achieved by setting the number of segments in the collection to be equal to the number of cores in the system. By doing this, each segment will be processed in parallel, leading to a faster final result. This approach allows for the efficient utilization of system resources and can significantly reduce the time taken from the moment a request is submitted to the moment a response is received. By optimizing for latency in this manner, you can enhance the overall speed and responsiveness of the search functionality in Qdrant.","To optimize Qdrant for minimizing latency in search requests, consider the following strategies:

1. Utilize Multiple Cores: Set up Qdrant to use as many cores as possible for a single request by configuring the number of segments in the collection to be equal to the number of cores in the system. This allows for parallel processing of segments, resulting in faster retrieval of the final search results.

2. Configurable Timeout for Searches: Enable users to configure the timeout for searches, providing greater flexibility and helping optimize system performance under various operational conditions. This feature can be particularly useful in ensuring that searches do not exceed a specified time limit, thereby improving overall latency.

3. Batch Search Feature: Take advantage of Qdrant's batch search feature, which allows for handling multiple search requests efficiently with just one API call. This feature minimizes network overhead and enhances performance by enabling simultaneous execution of multiple tasks.

4. ARM Support: Qdrant includes built-in support for ARM, enabling efficient processing of search requests on ARM-based systems. This support ensures that even with increased load on the Qdrant vector search engine, latency remains low, especially when relevant data is cached, leading to improved performance.

By implementing these strategies and features, you can optimize Qdrant for minimal latency in search requests, ensuring quicker response times and enhanced system performance.","Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
5. Configurable Timeout for Searches: allowing users to configure the timeout for searches provides greater flexibility and can help optimize system performance under different operational conditions ([PR#2748](https://github.com/qdrant/qdrant/pull/2748), [PR#2771](https://github.com/qdrant/qdrant/pull/2771)).



## Release notes
Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows 

you to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way 

possible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead

or slow performance.



## Built-in ARM support
![sequential vs. concurrent flow](/articles_data/search-as-you-type/sayt_concurrency.png)



While this means more load for the Qdrant vector search engine, this is not the limiting factor. The relevant data is already in cache in many cases, so the overhead stays within acceptable bounds, and the maximum latency in case of prefix cache misses is measurably reduced.



The code is available on the [Qdrant github](https://github.com/qdrant/page-search)
- title: Minimal Latency

          text: ""Qdrant consistently achieves the lowest latency, ensuring quicker response times in data retrieval: 3ms response for 1M Open AI embeddings, outpacing alternatives by 50x-100x.""

          icon: /marketing/mozilla/latency.svg

        - title: Fast Indexing

          text: Qdrant’s indexing time for large-scale, high-dimensional datasets is notably faster than alternative options.

          icon: /marketing/mozilla/indexing.svg","['Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.\n\n\n\nTo prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{', '5. Configurable Timeout for Searches: allowing users to configure the timeout for searches provides greater flexibility and can help optimize system performance under different operational conditions ([PR#2748](https://github.com/qdrant/qdrant/pull/2748), [PR#2771](https://github.com/qdrant/qdrant/pull/2771)).\n\n\n\n## Release notes', 'Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows \n\nyou to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way \n\npossible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead\n\nor slow performance.\n\n\n\n## Built-in ARM support', '![sequential vs. concurrent flow](/articles_data/search-as-you-type/sayt_concurrency.png)\n\n\n\nWhile this means more load for the Qdrant vector search engine, this is not the limiting factor. The relevant data is already in cache in many cases, so the overhead stays within acceptable bounds, and the maximum latency in case of prefix cache misses is measurably reduced.\n\n\n\nThe code is available on the [Qdrant github](https://github.com/qdrant/page-search)', '- title: Minimal Latency\n\n          text: ""Qdrant consistently achieves the lowest latency, ensuring quicker response times in data retrieval: 3ms response for 1M Open AI embeddings, outpacing alternatives by 50x-100x.""\n\n          icon: /marketing/mozilla/latency.svg\n\n        - title: Fast Indexing\n\n          text: Qdrant’s indexing time for large-scale, high-dimensional datasets is notably faster than alternative options.\n\n          icon: /marketing/mozilla/indexing.svg']","Question: How can you optimize Qdrant for minimizing latency in search requests?\n\nContext: Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
5. Configurable Timeout for Searches: allowing users to configure the timeout for searches provides greater flexibility and can help optimize system performance under different operational conditions ([PR#2748](https://github.com/qdrant/qdrant/pull/2748), [PR#2771](https://github.com/qdrant/qdrant/pull/2771)).



## Release notes
Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows 

you to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way 

possible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead

or slow performance.



## Built-in ARM support
![sequential vs. concurrent flow](/articles_data/search-as-you-type/sayt_concurrency.png)



While this means more load for the Qdrant vector search engine, this is not the limiting factor. The relevant data is already in cache in many cases, so the overhead stays within acceptable bounds, and the maximum latency in case of prefix cache misses is measurably reduced.



The code is available on the [Qdrant github](https://github.com/qdrant/page-search)
- title: Minimal Latency

text: ""Qdrant consistently achieves the lowest latency, ensuring quicker response times in data retrieval: 3ms response for 1M Open AI embeddings, outpacing alternatives by 50x-100x.""

icon: /marketing/mozilla/latency.svg

- title: Fast Indexing

text: Qdrant’s indexing time for large-scale, high-dimensional datasets is notably faster than alternative options.

icon: /marketing/mozilla/indexing.svg\n\nAnswer:",How can you optimize Qdrant for minimizing latency in search requests?,"To optimize Qdrant for minimizing latency in search requests, you can set up the system to use as many cores as possible for a single request. This can be achieved by setting the number of segments in the collection to be equal to the number of cores in the system. By doing this, each segment will be processed in parallel, leading to a faster final result. This approach allows for the efficient utilization of system resources and can significantly reduce the time taken from the moment a request is submitted to the moment a response is received. By optimizing for latency in this manner, you can enhance the overall speed and responsiveness of the search functionality in Qdrant.","['Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.\n\n\n\nTo prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{', '5. Configurable Timeout for Searches: allowing users to configure the timeout for searches provides greater flexibility and can help optimize system performance under different operational conditions ([PR#2748](https://github.com/qdrant/qdrant/pull/2748), [PR#2771](https://github.com/qdrant/qdrant/pull/2771)).\n\n\n\n## Release notes', 'Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows \n\nyou to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way \n\npossible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead\n\nor slow performance.\n\n\n\n## Built-in ARM support', '![sequential vs. concurrent flow](/articles_data/search-as-you-type/sayt_concurrency.png)\n\n\n\nWhile this means more load for the Qdrant vector search engine, this is not the limiting factor. The relevant data is already in cache in many cases, so the overhead stays within acceptable bounds, and the maximum latency in case of prefix cache misses is measurably reduced.\n\n\n\nThe code is available on the [Qdrant github](https://github.com/qdrant/page-search)', '- title: Minimal Latency\n\n          text: ""Qdrant consistently achieves the lowest latency, ensuring quicker response times in data retrieval: 3ms response for 1M Open AI embeddings, outpacing alternatives by 50x-100x.""\n\n          icon: /marketing/mozilla/latency.svg\n\n        - title: Fast Indexing\n\n          text: Qdrant’s indexing time for large-scale, high-dimensional datasets is notably faster than alternative options.\n\n          icon: /marketing/mozilla/indexing.svg']","Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
5. Configurable Timeout for Searches: allowing users to configure the timeout for searches provides greater flexibility and can help optimize system performance under different operational conditions ([PR#2748](https://github.com/qdrant/qdrant/pull/2748), [PR#2771](https://github.com/qdrant/qdrant/pull/2771)).



## Release notes
Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows 

you to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way 

possible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead

or slow performance.



## Built-in ARM support
![sequential vs. concurrent flow](/articles_data/search-as-you-type/sayt_concurrency.png)



While this means more load for the Qdrant vector search engine, this is not the limiting factor. The relevant data is already in cache in many cases, so the overhead stays within acceptable bounds, and the maximum latency in case of prefix cache misses is measurably reduced.



The code is available on the [Qdrant github](https://github.com/qdrant/page-search)
- title: Minimal Latency

          text: ""Qdrant consistently achieves the lowest latency, ensuring quicker response times in data retrieval: 3ms response for 1M Open AI embeddings, outpacing alternatives by 50x-100x.""

          icon: /marketing/mozilla/latency.svg

        - title: Fast Indexing

          text: Qdrant’s indexing time for large-scale, high-dimensional datasets is notably faster than alternative options.

          icon: /marketing/mozilla/indexing.svg",0.37383177570093457,0.7766990291262136,0.5047318611987383,0.17370892018779344,0.3627450980392157,0.23492063492063492,0.2616822429906542,0.5436893203883495,0.3533123028391167,0.35514018691588783,0.7378640776699029,0.4794952681388012,0.966508686542511,0.6471681594848633,0.7287735342979431,0.6855508685112,0.8194901592408618,0.9722036050632596,222,1.88135593220339,0.510338017588968
True,,3575.3759999999997,5432c854,How can Qdrant be configured to optimize throughput by utilizing multiple cores for parallel processing?,"To optimize throughput in Qdrant and utilize multiple cores for parallel processing, you can configure the system to use minimal number of segments, typically set to 2. By setting the default_segment_number to 2 in the optimizers_config section of the PUT request for creating a collection, you can instruct Qdrant to use a smaller number of segments. This approach benefits from the size of the index and reduces the overall number of vector comparisons needed to find the nearest neighbors. However, it is important to note that larger segments require more time to build the index. By adjusting this configuration parameter, you can strike a balance between maximizing throughput and optimizing the efficiency of vector search operations in Qdrant.","To configure Qdrant to optimize throughput by utilizing multiple cores for parallel processing, you can follow these steps:

1. Set up Qdrant to use minimal number of segments: Configure Qdrant to use a minimal number of segments, usually 2. Large segments benefit from the size of the index and require fewer vector comparisons to find nearest neighbors. This approach helps in processing multiple requests in parallel.

2. Set the number of segments in the collection to match the number of cores: If you prefer to minimize latency for a single request, set the number of segments in the collection to be equal to the number of cores in the system. Each segment will be processed in parallel, leading to faster results for the final query.

3. Utilize quantization techniques: Qdrant offers quantization techniques like scalar or product-based quantization. Implementing quantization can help optimize search performance for high-dimensional vectors, reducing memory consumption and improving retrieval speeds significantly.

4. Benchmark and optimize based on usage: Collect information about how Qdrant is used in your specific environment. Conduct benchmarking tests to identify bottlenecks and areas for improvement. Use internal heuristics to optimize performance and set parameters based on collected timings and counters.

By following these steps and leveraging Qdrant's features for parallel processing, quantization, and optimization based on usage insights, you can configure Qdrant to maximize throughput and performance across multiple cores.",");

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,
Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
led to some modest overhead in case of disk latency. The kernel may

stop a user thread trying to access a mapped region, which incurs some context

switching overhead plus the wait time until the disk IO is finished. Ultimately,

this works very well with the asynchronous nature of Qdrant's core.



One of the great optimizations Qdrant offers is quantization (either

[scalar](https://qdrant.tech/articles/scalar-quantization/) or 

[product](https://qdrant.tech/articles/product-quantization/)-based).
---



#### Optimizing high-dimensional vectors 



Qdrant is built to handle typical scaling challenges: high throughput, low latency and efficient indexing. **Binary quantization (BQ)** is our latest attempt to give our customers the edge they need to scale efficiently. This feature is particularly excellent for collections with large vector lengths and a large number of points. 



Our results are dramatic: Using BQ will reduce your memory consumption and improve retrieval speeds by up to 40x.
We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.



In order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.



Additionally, Qdrant uses a bunch of internal heuristics to optimize the performance.

To better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.

With this information, we can make Qdrant faster for everyone.","[');\n\n```\n\n\n\nTo prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.\n\nTo do that, you can configure qdrant to use minimal number of segments, which is usually 2.\n\nLarge segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,', 'Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.\n\n\n\nTo prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{', ""led to some modest overhead in case of disk latency. The kernel may\n\nstop a user thread trying to access a mapped region, which incurs some context\n\nswitching overhead plus the wait time until the disk IO is finished. Ultimately,\n\nthis works very well with the asynchronous nature of Qdrant's core.\n\n\n\nOne of the great optimizations Qdrant offers is quantization (either\n\n[scalar](https://qdrant.tech/articles/scalar-quantization/) or \n\n[product](https://qdrant.tech/articles/product-quantization/)-based)."", '---\n\n\n\n#### Optimizing high-dimensional vectors \n\n\n\nQdrant is built to handle typical scaling challenges: high throughput, low latency and efficient indexing. **Binary quantization (BQ)** is our latest attempt to give our customers the edge they need to scale efficiently. This feature is particularly excellent for collections with large vector lengths and a large number of points. \n\n\n\nOur results are dramatic: Using BQ will reduce your memory consumption and improve retrieval speeds by up to 40x.', 'We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.\n\n\n\nIn order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.\n\n\n\nAdditionally, Qdrant uses a bunch of internal heuristics to optimize the performance.\n\nTo better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.\n\nWith this information, we can make Qdrant faster for everyone.']","Question: How can Qdrant be configured to optimize throughput by utilizing multiple cores for parallel processing?\n\nContext: );

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,
Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
led to some modest overhead in case of disk latency. The kernel may

stop a user thread trying to access a mapped region, which incurs some context

switching overhead plus the wait time until the disk IO is finished. Ultimately,

this works very well with the asynchronous nature of Qdrant's core.



One of the great optimizations Qdrant offers is quantization (either

[scalar](https://qdrant.tech/articles/scalar-quantization/) or 

[product](https://qdrant.tech/articles/product-quantization/)-based).
---



#### Optimizing high-dimensional vectors 



Qdrant is built to handle typical scaling challenges: high throughput, low latency and efficient indexing. **Binary quantization (BQ)** is our latest attempt to give our customers the edge they need to scale efficiently. This feature is particularly excellent for collections with large vector lengths and a large number of points. 



Our results are dramatic: Using BQ will reduce your memory consumption and improve retrieval speeds by up to 40x.
We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.



In order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.



Additionally, Qdrant uses a bunch of internal heuristics to optimize the performance.

To better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.

With this information, we can make Qdrant faster for everyone.\n\nAnswer:",How can Qdrant be configured to optimize throughput by utilizing multiple cores for parallel processing?,"To optimize throughput in Qdrant and utilize multiple cores for parallel processing, you can configure the system to use minimal number of segments, typically set to 2. By setting the default_segment_number to 2 in the optimizers_config section of the PUT request for creating a collection, you can instruct Qdrant to use a smaller number of segments. This approach benefits from the size of the index and reduces the overall number of vector comparisons needed to find the nearest neighbors. However, it is important to note that larger segments require more time to build the index. By adjusting this configuration parameter, you can strike a balance between maximizing throughput and optimizing the efficiency of vector search operations in Qdrant.","[');\n\n```\n\n\n\nTo prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.\n\nTo do that, you can configure qdrant to use minimal number of segments, which is usually 2.\n\nLarge segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,', 'Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.\n\n\n\nTo prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{', ""led to some modest overhead in case of disk latency. The kernel may\n\nstop a user thread trying to access a mapped region, which incurs some context\n\nswitching overhead plus the wait time until the disk IO is finished. Ultimately,\n\nthis works very well with the asynchronous nature of Qdrant's core.\n\n\n\nOne of the great optimizations Qdrant offers is quantization (either\n\n[scalar](https://qdrant.tech/articles/scalar-quantization/) or \n\n[product](https://qdrant.tech/articles/product-quantization/)-based)."", '---\n\n\n\n#### Optimizing high-dimensional vectors \n\n\n\nQdrant is built to handle typical scaling challenges: high throughput, low latency and efficient indexing. **Binary quantization (BQ)** is our latest attempt to give our customers the edge they need to scale efficiently. This feature is particularly excellent for collections with large vector lengths and a large number of points. \n\n\n\nOur results are dramatic: Using BQ will reduce your memory consumption and improve retrieval speeds by up to 40x.', 'We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.\n\n\n\nIn order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.\n\n\n\nAdditionally, Qdrant uses a bunch of internal heuristics to optimize the performance.\n\nTo better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.\n\nWith this information, we can make Qdrant faster for everyone.']",");

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,
Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
led to some modest overhead in case of disk latency. The kernel may

stop a user thread trying to access a mapped region, which incurs some context

switching overhead plus the wait time until the disk IO is finished. Ultimately,

this works very well with the asynchronous nature of Qdrant's core.



One of the great optimizations Qdrant offers is quantization (either

[scalar](https://qdrant.tech/articles/scalar-quantization/) or 

[product](https://qdrant.tech/articles/product-quantization/)-based).
---



#### Optimizing high-dimensional vectors 



Qdrant is built to handle typical scaling challenges: high throughput, low latency and efficient indexing. **Binary quantization (BQ)** is our latest attempt to give our customers the edge they need to scale efficiently. This feature is particularly excellent for collections with large vector lengths and a large number of points. 



Our results are dramatic: Using BQ will reduce your memory consumption and improve retrieval speeds by up to 40x.
We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.



In order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.



Additionally, Qdrant uses a bunch of internal heuristics to optimize the performance.

To better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.

With this information, we can make Qdrant faster for everyone.",0.37272727272727274,0.7592592592592593,0.5,0.1552511415525114,0.3177570093457944,0.2085889570552147,0.2,0.4074074074074074,0.2682926829268293,0.30454545454545456,0.6203703703703703,0.4085365853658537,0.8617979288101196,0.6188780069351196,0.6701788306236267,0.6435076594352722,0.8045407547615469,0.8685894596274011,230,1.9491525423728813,0.460281459023919
True,,3650.706,6a9b0517,How can one enable distributed deployment mode in Qdrant?,"To enable distributed deployment mode in Qdrant, one needs to enable the cluster mode in the configuration or set the ENV variable `QDRANT__CLUSTER__ENABLED=true`. This allows multiple Qdrant services to communicate with each other, distributing data across peers to extend storage capabilities and increase stability. In the configuration file, under the `cluster` section, set `enabled: true` to run Qdrant in distributed deployment mode. Additionally, configure the internal communication port between peers using the `p2p` section with the specified port number. Furthermore, adjust the parameters related to the distributed consensus algorithm as needed for optimal performance.","To enable distributed deployment mode in Qdrant, you can follow these steps:

1. Update the configuration file by setting `enabled: true` under the `cluster` section:

```yaml
cluster:
  enabled: true
  p2p:
    port: 6335
  consensus:
```

2. Alternatively, you can set the `QDRANT__CLUSTER__ENABLED=true` environment variable to enable cluster mode.

By enabling distributed deployment mode in Qdrant, you allow multiple Qdrant services to communicate with each other to distribute data across peers, extending storage capabilities and increasing stability. This mode is beneficial for large enterprises dealing with massive data volumes, ensuring scalability and performance are not hindered as data volumes increase. 

Additionally, make sure to:

- Use a performant persistent storage for your data.
- Configure security settings for your deployment.
- Set up and configure Qdrant on multiple nodes for a highly available distributed deployment.
- Set up a load balancer for your Qdrant cluster.
- Create a backup and disaster recovery strategy for your data.","To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.



```yaml

cluster:

  # Use `enabled: true` to run Qdrant in distributed deployment mode

  enabled: true

  # Configuration of the inter-cluster communication

  p2p:

    # Port for internal communication between peers

    port: 6335



  # Configuration related to distributed consensus algorithm

  consensus:
---

title: Distributed Deployment

weight: 100

aliases:

  - ../distributed_deployment

---



# Distributed deployment



Since version v0.8.0 Qdrant supports a distributed deployment mode.

In this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.
Qdrant’s distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model’s capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes
Qdrant's distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model's capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes
In addition, you have to make sure:



* To use a performant [persistent storage](#storage) for your data

* To configure the [security settings](/documentation/guides/security/) for your deployment

* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)

* To set up a load balancer for your Qdrant cluster

* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data","['To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.\n\n\n\n```yaml\n\ncluster:\n\n  # Use `enabled: true` to run Qdrant in distributed deployment mode\n\n  enabled: true\n\n  # Configuration of the inter-cluster communication\n\n  p2p:\n\n    # Port for internal communication between peers\n\n    port: 6335\n\n\n\n  # Configuration related to distributed consensus algorithm\n\n  consensus:', '---\n\ntitle: Distributed Deployment\n\nweight: 100\n\naliases:\n\n  - ../distributed_deployment\n\n---\n\n\n\n# Distributed deployment\n\n\n\nSince version v0.8.0 Qdrant supports a distributed deployment mode.\n\nIn this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.', 'Qdrant’s distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model’s capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes', ""Qdrant's distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model's capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes"", 'In addition, you have to make sure:\n\n\n\n* To use a performant [persistent storage](#storage) for your data\n\n* To configure the [security settings](/documentation/guides/security/) for your deployment\n\n* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)\n\n* To set up a load balancer for your Qdrant cluster\n\n* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data']","Question: How can one enable distributed deployment mode in Qdrant?\n\nContext: To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.



```yaml

cluster:

# Use `enabled: true` to run Qdrant in distributed deployment mode

enabled: true

# Configuration of the inter-cluster communication

p2p:

# Port for internal communication between peers

port: 6335



# Configuration related to distributed consensus algorithm

consensus:
---

title: Distributed Deployment

weight: 100

aliases:

- ../distributed_deployment

---



# Distributed deployment



Since version v0.8.0 Qdrant supports a distributed deployment mode.

In this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.
Qdrant’s distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model’s capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes
Qdrant's distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model's capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes
In addition, you have to make sure:



* To use a performant [persistent storage](#storage) for your data

* To configure the [security settings](/documentation/guides/security/) for your deployment

* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)

* To set up a load balancer for your Qdrant cluster

* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data\n\nAnswer:",How can one enable distributed deployment mode in Qdrant?,"To enable distributed deployment mode in Qdrant, one needs to enable the cluster mode in the configuration or set the ENV variable `QDRANT__CLUSTER__ENABLED=true`. This allows multiple Qdrant services to communicate with each other, distributing data across peers to extend storage capabilities and increase stability. In the configuration file, under the `cluster` section, set `enabled: true` to run Qdrant in distributed deployment mode. Additionally, configure the internal communication port between peers using the `p2p` section with the specified port number. Furthermore, adjust the parameters related to the distributed consensus algorithm as needed for optimal performance.","['To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.\n\n\n\n```yaml\n\ncluster:\n\n  # Use `enabled: true` to run Qdrant in distributed deployment mode\n\n  enabled: true\n\n  # Configuration of the inter-cluster communication\n\n  p2p:\n\n    # Port for internal communication between peers\n\n    port: 6335\n\n\n\n  # Configuration related to distributed consensus algorithm\n\n  consensus:', '---\n\ntitle: Distributed Deployment\n\nweight: 100\n\naliases:\n\n  - ../distributed_deployment\n\n---\n\n\n\n# Distributed deployment\n\n\n\nSince version v0.8.0 Qdrant supports a distributed deployment mode.\n\nIn this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.', 'Qdrant’s distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model’s capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes', ""Qdrant's distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model's capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes"", 'In addition, you have to make sure:\n\n\n\n* To use a performant [persistent storage](#storage) for your data\n\n* To configure the [security settings](/documentation/guides/security/) for your deployment\n\n* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)\n\n* To set up a load balancer for your Qdrant cluster\n\n* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data']","To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.



```yaml

cluster:

  # Use `enabled: true` to run Qdrant in distributed deployment mode

  enabled: true

  # Configuration of the inter-cluster communication

  p2p:

    # Port for internal communication between peers

    port: 6335



  # Configuration related to distributed consensus algorithm

  consensus:
---

title: Distributed Deployment

weight: 100

aliases:

  - ../distributed_deployment

---



# Distributed deployment



Since version v0.8.0 Qdrant supports a distributed deployment mode.

In this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.
Qdrant’s distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model’s capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes
Qdrant's distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model's capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes
In addition, you have to make sure:



* To use a performant [persistent storage](#storage) for your data

* To configure the [security settings](/documentation/guides/security/) for your deployment

* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)

* To set up a load balancer for your Qdrant cluster

* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data",0.4444444444444444,0.735632183908046,0.5541125541125541,0.25874125874125875,0.43023255813953487,0.3231441048034935,0.2708333333333333,0.4482758620689655,0.33766233766233766,0.4236111111111111,0.7011494252873564,0.5281385281385281,0.9723547697067261,0.6949315071105957,0.7470349073410034,0.7200418710708618,0.9878560853685485,0.9844548519933596,148,1.574468085106383,0.602290217892096
True,,4317.316,e9e3bff5,What is the recommended approach for determining the number of shards when creating a collection using Qdrant?,"When creating a collection using Qdrant, it is recommended to set the number of shards to be a multiple of the number of nodes currently running in your cluster. For example, if you have 3 nodes in your cluster, setting the number of shards to 6 could be a good option. Shards are evenly distributed across all existing nodes when a collection is first created. It is important to note that Qdrant does not automatically rebalance shards if your cluster size or replication factor changes, as this operation can be expensive on large clusters. If scaling operations require moving shards, there are specific procedures outlined for this purpose, available as of version 0.9.0 of Qdrant.","The recommended approach for determining the number of shards when creating a collection using Qdrant is to consider the following factors:

1. Number of nodes in your cluster: By default, if you do not specify the shard number, Qdrant will set it to the number of nodes in your cluster. This helps in distributing the data evenly across the cluster.

2. Size of the dataset: If you have a large dataset, creating multiple shards can help parallelize the upload process. It is suggested to have 2 to 4 shards per machine for efficient processing.

3. Sharding method: Qdrant allows you to specify a custom sharding method if needed. You can choose the sharding method based on your specific requirements and workload.

4. Consider the hardware resources: You should also consider the hardware resources available on each node in your cluster when determining the number of shards. Ensure that each shard has sufficient resources to handle the workload efficiently.

In summary, the number of shards should be determined based on the size of the dataset, the number of nodes in the cluster, the sharding method, and the available hardware resources to achieve optimal performance and scalability.","},

    ""shard_number"": 2

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    shard_number=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });
Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.



When you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 300,
.get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",

	// ... other collection parameters

	shardNumber: 1,

	shardingMethod: ShardingMethod.Custom

);

```
## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""shard_number"": 2

}

```



```python
client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".into(),

        shard_number: Some(1),

        sharding_method: Some(ShardingMethod::Custom),

        // ... other collection parameters

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.ShardingMethod;","['},\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    shard_number=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });', 'Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.\n\n\n\nWhen you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 300,', '.get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\t// ... other collection parameters\n\n\tshardNumber: 1,\n\n\tshardingMethod: ShardingMethod.Custom\n\n);\n\n```', '## Parallel upload into multiple shards\n\n\n\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\n\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python', 'client\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".into(),\n\n        shard_number: Some(1),\n\n        sharding_method: Some(ShardingMethod::Custom),\n\n        // ... other collection parameters\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.ShardingMethod;']","Question: What is the recommended approach for determining the number of shards when creating a collection using Qdrant?\n\nContext: },

""shard_number"": 2

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

shard_number=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });
Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.



When you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 300,
.get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

collectionName: ""{collection_name}"",

// ... other collection parameters

shardNumber: 1,

shardingMethod: ShardingMethod.Custom

);

```
## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""shard_number"": 2

}

```



```python
client

.create_collection(&CreateCollection {

collection_name: ""{collection_name}"".into(),

shard_number: Some(1),

sharding_method: Some(ShardingMethod::Custom),

// ... other collection parameters

..Default::default()

})

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.ShardingMethod;\n\nAnswer:",What is the recommended approach for determining the number of shards when creating a collection using Qdrant?,"When creating a collection using Qdrant, it is recommended to set the number of shards to be a multiple of the number of nodes currently running in your cluster. For example, if you have 3 nodes in your cluster, setting the number of shards to 6 could be a good option. Shards are evenly distributed across all existing nodes when a collection is first created. It is important to note that Qdrant does not automatically rebalance shards if your cluster size or replication factor changes, as this operation can be expensive on large clusters. If scaling operations require moving shards, there are specific procedures outlined for this purpose, available as of version 0.9.0 of Qdrant.","['},\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    shard_number=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });', 'Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.\n\n\n\nWhen you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 300,', '.get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\t// ... other collection parameters\n\n\tshardNumber: 1,\n\n\tshardingMethod: ShardingMethod.Custom\n\n);\n\n```', '## Parallel upload into multiple shards\n\n\n\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\n\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python', 'client\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".into(),\n\n        shard_number: Some(1),\n\n        sharding_method: Some(ShardingMethod::Custom),\n\n        // ... other collection parameters\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.ShardingMethod;']","},

    ""shard_number"": 2

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    shard_number=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });
Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.



When you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 300,
.get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",

	// ... other collection parameters

	shardNumber: 1,

	shardingMethod: ShardingMethod.Custom

);

```
## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""shard_number"": 2

}

```



```python
client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".into(),

        shard_number: Some(1),

        sharding_method: Some(ShardingMethod::Custom),

        // ... other collection parameters

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.ShardingMethod;",0.40350877192982454,0.6272727272727273,0.49110320284697506,0.11176470588235295,0.1743119266055046,0.13620071684587817,0.21052631578947367,0.32727272727272727,0.25622775800711745,0.3508771929824561,0.5454545454545454,0.4270462633451957,0.8709246516227722,0.5948349833488464,0.6309277415275574,0.6123499870300293,0.7344796592628376,0.2940244525671005,195,1.6956521739130435,0.40427115457129914
True,,3958.4570000000003,d0187728,How is the discovery score computed in the context provided?,"In the context provided, the discovery score is computed using the formula: 

Discovery Score = sigmoid(s(v_t)) + Σ rank(v_i^+, v_i^-)

Here, s(v_t) represents the similarity function of the target vector, and v_i^+ and v_i^- represent the positive and negative examples, respectively. The sigmoid function is utilized to normalize the score within the range of 0 to 1. The sum of ranks is used to penalize vectors that are closer to negative examples than to positive ones. This means that the sum of individual ranks determines how many positive zones a point is in, while the closeness hierarchy serves as a secondary factor in the computation of the discovery score.","The discovery score in the provided context is computed using the sigmoid function of the similarity score of the target vector $v_t$, along with the sum of the ranks of positive-negative example pairs. 

Specifically, the discovery score is calculated as follows:

1. Compute the rank between each positive example $v_i^+$ and negative example $v_i^-$ using the formula provided:

\[
\text{rank}(v_i^+, v_i^-) = \begin{cases}
1, & \text{if } s(v_i^+) \geq s(v_i^-) \\
-1, & \text{if } s(v_i^+) < s(v_i^-)
\end{cases}
\]

2. Sum up the ranks of all positive-negative pairs.

3. Add the sigmoid function of the similarity score of the target vector $v_t$ to the sum of ranks.

Thus, the final formula for the discovery score is:

\[
\text{discovery score} = \text{sigmoid}(s(v_t)) + \sum \text{rank}(v_i^+, v_i^-)
\]

This calculation allows for a comprehensive evaluation of the similarity between the target vector and the positive and negative examples within the specified context.","$$

\text{rank}(v^+, v^-) = \begin{cases}

    1, &\quad s(v^+) \geq s(v^-) \\\\

    -1, &\quad s(v^+) < s(v^-)

\end{cases}

$$

where $v^+$ represents a positive example, $v^-$ represents a negative example, and $s(v)$ is the similarity score of a vector $v$ to the target vector. The discovery score is then computed as:

$$

 \text{discovery score} = \text{sigmoid}(s(v_t))+ \sum \text{rank}(v_i^+, v_i^-),

$$
The interface for providing context is similar to the recommendation API (ids or raw vectors). Still, in this case, they need to be provided in the form of positive-negative pairs.



Discovery API lets you do two new types of search:

- **Discovery search**: Uses the context (the pairs of positive-negative vectors) and a target to return the points more similar to the target, but constrained by the context.
},

	context:

	[

		new()

		{

			Positive = new VectorExample { Id = 100 },

			Negative = new VectorExample { Id = 718 }

		},

		new()

		{

			Positive = new VectorExample { Id = 200 },

			Negative = new VectorExample { Id = 300 }

		}

	],

	limit: 10

);

```



<aside role=""status"">

Notes about discovery search:



* When providing ids as examples, they will be excluded from the results.

* Score is always in descending order (larger is better), regardless of the metric used.
client

    .discover(&DiscoverPoints {

        collection_name: ""{collection_name}"".to_string(),

        context: vec![

            ContextExamplePair {

                positive: Some(VectorExample {

                    example: Some(Example::Id(100.into())),

                }),

                negative: Some(VectorExample {

                    example: Some(Example::Id(718.into())),

                }),

            },

            ContextExamplePair {
The Discovery API can be used in two ways - either with or without the target point. The first case is called a **discovery search**, while the second is called a **context search**.



#### Discovery search



*Discovery search* is an operation that uses a target point to find the most relevant points in the collection, while performing the search in the preferred areas only. That is basically a search operation with more control over the search space.","['$$\n\n\\text{rank}(v^+, v^-) = \\begin{cases}\n\n    1, &\\quad s(v^+) \\geq s(v^-) \\\\\\\\\n\n    -1, &\\quad s(v^+) < s(v^-)\n\n\\end{cases}\n\n$$\n\nwhere $v^+$ represents a positive example, $v^-$ represents a negative example, and $s(v)$ is the similarity score of a vector $v$ to the target vector. The discovery score is then computed as:\n\n$$\n\n \\text{discovery score} = \\text{sigmoid}(s(v_t))+ \\sum \\text{rank}(v_i^+, v_i^-),\n\n$$', 'The interface for providing context is similar to the recommendation API (ids or raw vectors). Still, in this case, they need to be provided in the form of positive-negative pairs.\n\n\n\nDiscovery API lets you do two new types of search:\n\n- **Discovery search**: Uses the context (the pairs of positive-negative vectors) and a target to return the points more similar to the target, but constrained by the context.', '},\n\n\tcontext:\n\n\t[\n\n\t\tnew()\n\n\t\t{\n\n\t\t\tPositive = new VectorExample { Id = 100 },\n\n\t\t\tNegative = new VectorExample { Id = 718 }\n\n\t\t},\n\n\t\tnew()\n\n\t\t{\n\n\t\t\tPositive = new VectorExample { Id = 200 },\n\n\t\t\tNegative = new VectorExample { Id = 300 }\n\n\t\t}\n\n\t],\n\n\tlimit: 10\n\n);\n\n```\n\n\n\n<aside role=""status"">\n\nNotes about discovery search:\n\n\n\n* When providing ids as examples, they will be excluded from the results.\n\n* Score is always in descending order (larger is better), regardless of the metric used.', 'client\n\n    .discover(&DiscoverPoints {\n\n        collection_name: ""{collection_name}"".to_string(),\n\n        context: vec![\n\n            ContextExamplePair {\n\n                positive: Some(VectorExample {\n\n                    example: Some(Example::Id(100.into())),\n\n                }),\n\n                negative: Some(VectorExample {\n\n                    example: Some(Example::Id(718.into())),\n\n                }),\n\n            },\n\n            ContextExamplePair {', 'The Discovery API can be used in two ways - either with or without the target point. The first case is called a **discovery search**, while the second is called a **context search**.\n\n\n\n#### Discovery search\n\n\n\n*Discovery search* is an operation that uses a target point to find the most relevant points in the collection, while performing the search in the preferred areas only. That is basically a search operation with more control over the search space.']","Question: How is the discovery score computed in the context provided?\n\nContext: $$

\text{rank}(v^+, v^-) = \begin{cases}

1, &\quad s(v^+) \geq s(v^-) \\\\

-1, &\quad s(v^+) < s(v^-)

\end{cases}

$$

where $v^+$ represents a positive example, $v^-$ represents a negative example, and $s(v)$ is the similarity score of a vector $v$ to the target vector. The discovery score is then computed as:

$$

\text{discovery score} = \text{sigmoid}(s(v_t))+ \sum \text{rank}(v_i^+, v_i^-),

$$
The interface for providing context is similar to the recommendation API (ids or raw vectors). Still, in this case, they need to be provided in the form of positive-negative pairs.



Discovery API lets you do two new types of search:

- **Discovery search**: Uses the context (the pairs of positive-negative vectors) and a target to return the points more similar to the target, but constrained by the context.
},

context:

[

new()

{

Positive = new VectorExample { Id = 100 },

Negative = new VectorExample { Id = 718 }

},

new()

{

Positive = new VectorExample { Id = 200 },

Negative = new VectorExample { Id = 300 }

}

],

limit: 10

);

```



<aside role=""status"">

Notes about discovery search:



* When providing ids as examples, they will be excluded from the results.

* Score is always in descending order (larger is better), regardless of the metric used.
client

.discover(&DiscoverPoints {

collection_name: ""{collection_name}"".to_string(),

context: vec![

ContextExamplePair {

positive: Some(VectorExample {

example: Some(Example::Id(100.into())),

}),

negative: Some(VectorExample {

example: Some(Example::Id(718.into())),

}),

},

ContextExamplePair {
The Discovery API can be used in two ways - either with or without the target point. The first case is called a **discovery search**, while the second is called a **context search**.



#### Discovery search



*Discovery search* is an operation that uses a target point to find the most relevant points in the collection, while performing the search in the preferred areas only. That is basically a search operation with more control over the search space.\n\nAnswer:",How is the discovery score computed in the context provided?,"In the context provided, the discovery score is computed using the formula: 

Discovery Score = sigmoid(s(v_t)) + Σ rank(v_i^+, v_i^-)

Here, s(v_t) represents the similarity function of the target vector, and v_i^+ and v_i^- represent the positive and negative examples, respectively. The sigmoid function is utilized to normalize the score within the range of 0 to 1. The sum of ranks is used to penalize vectors that are closer to negative examples than to positive ones. This means that the sum of individual ranks determines how many positive zones a point is in, while the closeness hierarchy serves as a secondary factor in the computation of the discovery score.","['$$\n\n\\text{rank}(v^+, v^-) = \\begin{cases}\n\n    1, &\\quad s(v^+) \\geq s(v^-) \\\\\\\\\n\n    -1, &\\quad s(v^+) < s(v^-)\n\n\\end{cases}\n\n$$\n\nwhere $v^+$ represents a positive example, $v^-$ represents a negative example, and $s(v)$ is the similarity score of a vector $v$ to the target vector. The discovery score is then computed as:\n\n$$\n\n \\text{discovery score} = \\text{sigmoid}(s(v_t))+ \\sum \\text{rank}(v_i^+, v_i^-),\n\n$$', 'The interface for providing context is similar to the recommendation API (ids or raw vectors). Still, in this case, they need to be provided in the form of positive-negative pairs.\n\n\n\nDiscovery API lets you do two new types of search:\n\n- **Discovery search**: Uses the context (the pairs of positive-negative vectors) and a target to return the points more similar to the target, but constrained by the context.', '},\n\n\tcontext:\n\n\t[\n\n\t\tnew()\n\n\t\t{\n\n\t\t\tPositive = new VectorExample { Id = 100 },\n\n\t\t\tNegative = new VectorExample { Id = 718 }\n\n\t\t},\n\n\t\tnew()\n\n\t\t{\n\n\t\t\tPositive = new VectorExample { Id = 200 },\n\n\t\t\tNegative = new VectorExample { Id = 300 }\n\n\t\t}\n\n\t],\n\n\tlimit: 10\n\n);\n\n```\n\n\n\n<aside role=""status"">\n\nNotes about discovery search:\n\n\n\n* When providing ids as examples, they will be excluded from the results.\n\n* Score is always in descending order (larger is better), regardless of the metric used.', 'client\n\n    .discover(&DiscoverPoints {\n\n        collection_name: ""{collection_name}"".to_string(),\n\n        context: vec![\n\n            ContextExamplePair {\n\n                positive: Some(VectorExample {\n\n                    example: Some(Example::Id(100.into())),\n\n                }),\n\n                negative: Some(VectorExample {\n\n                    example: Some(Example::Id(718.into())),\n\n                }),\n\n            },\n\n            ContextExamplePair {', 'The Discovery API can be used in two ways - either with or without the target point. The first case is called a **discovery search**, while the second is called a **context search**.\n\n\n\n#### Discovery search\n\n\n\n*Discovery search* is an operation that uses a target point to find the most relevant points in the collection, while performing the search in the preferred areas only. That is basically a search operation with more control over the search space.']","$$

\text{rank}(v^+, v^-) = \begin{cases}

    1, &\quad s(v^+) \geq s(v^-) \\\\

    -1, &\quad s(v^+) < s(v^-)

\end{cases}

$$

where $v^+$ represents a positive example, $v^-$ represents a negative example, and $s(v)$ is the similarity score of a vector $v$ to the target vector. The discovery score is then computed as:

$$

 \text{discovery score} = \text{sigmoid}(s(v_t))+ \sum \text{rank}(v_i^+, v_i^-),

$$
The interface for providing context is similar to the recommendation API (ids or raw vectors). Still, in this case, they need to be provided in the form of positive-negative pairs.



Discovery API lets you do two new types of search:

- **Discovery search**: Uses the context (the pairs of positive-negative vectors) and a target to return the points more similar to the target, but constrained by the context.
},

	context:

	[

		new()

		{

			Positive = new VectorExample { Id = 100 },

			Negative = new VectorExample { Id = 718 }

		},

		new()

		{

			Positive = new VectorExample { Id = 200 },

			Negative = new VectorExample { Id = 300 }

		}

	],

	limit: 10

);

```



<aside role=""status"">

Notes about discovery search:



* When providing ids as examples, they will be excluded from the results.

* Score is always in descending order (larger is better), regardless of the metric used.
client

    .discover(&DiscoverPoints {

        collection_name: ""{collection_name}"".to_string(),

        context: vec![

            ContextExamplePair {

                positive: Some(VectorExample {

                    example: Some(Example::Id(100.into())),

                }),

                negative: Some(VectorExample {

                    example: Some(Example::Id(718.into())),

                }),

            },

            ContextExamplePair {
The Discovery API can be used in two ways - either with or without the target point. The first case is called a **discovery search**, while the second is called a **context search**.



#### Discovery search



*Discovery search* is an operation that uses a target point to find the most relevant points in the collection, while performing the search in the preferred areas only. That is basically a search operation with more control over the search space.",0.4421768707482993,0.65,0.5263157894736842,0.21232876712328766,0.31313131313131315,0.25306122448979596,0.22448979591836735,0.33,0.26720647773279355,0.3877551020408163,0.57,0.46153846153846156,0.9393033981323242,0.5937203764915466,0.5952067971229553,0.5944626331329346,0.8065656400285661,0.9204640587170919,159,1.4324324324324325,0.5399454047811446
True,,2472.19,5d55c90d,How does Qdrant optimize storage at the segment level?,"Qdrant optimizes storage at the segment level by applying changes in batches rather than individually. When optimization is needed, the segment to be optimized remains readable during the rebuild process. This is achieved by wrapping the segment into a proxy that handles data changes transparently. Changed data is placed in a copy-on-write segment, prioritizing retrieval and subsequent updates. This approach ensures efficiency in storage optimization within Qdrant.","Qdrant optimizes storage at the segment level by implementing several strategies such as merging small segments into larger ones using the Merge Optimizer. The criteria for initiating optimization are defined in the configuration file, such as specifying the maximum number of segments before optimization is triggered. Qdrant also allows for the creation of temporary segments like copy-on-write segments during optimization. Additionally, it is important to balance the number of small segments for frequently updated data with search performance optimization. By setting up Qdrant to use multiple cores for processing segments in parallel, latency can be minimized and query results obtained faster. Another optimization technique offered by Qdrant is quantization, which can be either scalar or product-based.","---

title: Optimizer

weight: 70

aliases:

  - ../optimizer

---



# Optimizer



It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.



Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).
The criteria for starting the optimizer are defined in the configuration file.



Here is an example of parameter values:



```yaml

storage:

  optimizers:

    # If the number of segments exceeds this value, the optimizer will merge the smallest segments.

    max_segment_number: 5

```



## Indexing Optimizer



Qdrant allows you to choose the type of indexes and data storage methods used depending on the number of records.
The service may require the creation of temporary segments.

Such segments, for example, are created as copy-on-write segments during optimization itself.



It is also essential to have at least one small segment that Qdrant will use to store frequently updated data.

On the other hand, too many small segments lead to suboptimal search performance.



There is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.
Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
led to some modest overhead in case of disk latency. The kernel may

stop a user thread trying to access a mapped region, which incurs some context

switching overhead plus the wait time until the disk IO is finished. Ultimately,

this works very well with the asynchronous nature of Qdrant's core.



One of the great optimizations Qdrant offers is quantization (either

[scalar](https://qdrant.tech/articles/scalar-quantization/) or 

[product](https://qdrant.tech/articles/product-quantization/)-based).","['---\n\ntitle: Optimizer\n\nweight: 70\n\naliases:\n\n  - ../optimizer\n\n---\n\n\n\n# Optimizer\n\n\n\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\n\n\n\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).', 'The criteria for starting the optimizer are defined in the configuration file.\n\n\n\nHere is an example of parameter values:\n\n\n\n```yaml\n\nstorage:\n\n  optimizers:\n\n    # If the number of segments exceeds this value, the optimizer will merge the smallest segments.\n\n    max_segment_number: 5\n\n```\n\n\n\n## Indexing Optimizer\n\n\n\nQdrant allows you to choose the type of indexes and data storage methods used depending on the number of records.', 'The service may require the creation of temporary segments.\n\nSuch segments, for example, are created as copy-on-write segments during optimization itself.\n\n\n\nIt is also essential to have at least one small segment that Qdrant will use to store frequently updated data.\n\nOn the other hand, too many small segments lead to suboptimal search performance.\n\n\n\nThere is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.', 'Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.\n\n\n\nTo prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{', ""led to some modest overhead in case of disk latency. The kernel may\n\nstop a user thread trying to access a mapped region, which incurs some context\n\nswitching overhead plus the wait time until the disk IO is finished. Ultimately,\n\nthis works very well with the asynchronous nature of Qdrant's core.\n\n\n\nOne of the great optimizations Qdrant offers is quantization (either\n\n[scalar](https://qdrant.tech/articles/scalar-quantization/) or \n\n[product](https://qdrant.tech/articles/product-quantization/)-based).""]","Question: How does Qdrant optimize storage at the segment level?\n\nContext: ---

title: Optimizer

weight: 70

aliases:

- ../optimizer

---



# Optimizer



It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.



Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).
The criteria for starting the optimizer are defined in the configuration file.



Here is an example of parameter values:



```yaml

storage:

optimizers:

# If the number of segments exceeds this value, the optimizer will merge the smallest segments.

max_segment_number: 5

```



## Indexing Optimizer



Qdrant allows you to choose the type of indexes and data storage methods used depending on the number of records.
The service may require the creation of temporary segments.

Such segments, for example, are created as copy-on-write segments during optimization itself.



It is also essential to have at least one small segment that Qdrant will use to store frequently updated data.

On the other hand, too many small segments lead to suboptimal search performance.



There is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.
Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
led to some modest overhead in case of disk latency. The kernel may

stop a user thread trying to access a mapped region, which incurs some context

switching overhead plus the wait time until the disk IO is finished. Ultimately,

this works very well with the asynchronous nature of Qdrant's core.



One of the great optimizations Qdrant offers is quantization (either

[scalar](https://qdrant.tech/articles/scalar-quantization/) or 

[product](https://qdrant.tech/articles/product-quantization/)-based).\n\nAnswer:",How does Qdrant optimize storage at the segment level?,"Qdrant optimizes storage at the segment level by applying changes in batches rather than individually. When optimization is needed, the segment to be optimized remains readable during the rebuild process. This is achieved by wrapping the segment into a proxy that handles data changes transparently. Changed data is placed in a copy-on-write segment, prioritizing retrieval and subsequent updates. This approach ensures efficiency in storage optimization within Qdrant.","['---\n\ntitle: Optimizer\n\nweight: 70\n\naliases:\n\n  - ../optimizer\n\n---\n\n\n\n# Optimizer\n\n\n\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\n\n\n\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).', 'The criteria for starting the optimizer are defined in the configuration file.\n\n\n\nHere is an example of parameter values:\n\n\n\n```yaml\n\nstorage:\n\n  optimizers:\n\n    # If the number of segments exceeds this value, the optimizer will merge the smallest segments.\n\n    max_segment_number: 5\n\n```\n\n\n\n## Indexing Optimizer\n\n\n\nQdrant allows you to choose the type of indexes and data storage methods used depending on the number of records.', 'The service may require the creation of temporary segments.\n\nSuch segments, for example, are created as copy-on-write segments during optimization itself.\n\n\n\nIt is also essential to have at least one small segment that Qdrant will use to store frequently updated data.\n\nOn the other hand, too many small segments lead to suboptimal search performance.\n\n\n\nThere is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.', 'Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.\n\n\n\nTo prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{', ""led to some modest overhead in case of disk latency. The kernel may\n\nstop a user thread trying to access a mapped region, which incurs some context\n\nswitching overhead plus the wait time until the disk IO is finished. Ultimately,\n\nthis works very well with the asynchronous nature of Qdrant's core.\n\n\n\nOne of the great optimizations Qdrant offers is quantization (either\n\n[scalar](https://qdrant.tech/articles/scalar-quantization/) or \n\n[product](https://qdrant.tech/articles/product-quantization/)-based).""]","---

title: Optimizer

weight: 70

aliases:

  - ../optimizer

---



# Optimizer



It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.



Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).
The criteria for starting the optimizer are defined in the configuration file.



Here is an example of parameter values:



```yaml

storage:

  optimizers:

    # If the number of segments exceeds this value, the optimizer will merge the smallest segments.

    max_segment_number: 5

```



## Indexing Optimizer



Qdrant allows you to choose the type of indexes and data storage methods used depending on the number of records.
The service may require the creation of temporary segments.

Such segments, for example, are created as copy-on-write segments during optimization itself.



It is also essential to have at least one small segment that Qdrant will use to store frequently updated data.

On the other hand, too many small segments lead to suboptimal search performance.



There is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.
Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
led to some modest overhead in case of disk latency. The kernel may

stop a user thread trying to access a mapped region, which incurs some context

switching overhead plus the wait time until the disk IO is finished. Ultimately,

this works very well with the asynchronous nature of Qdrant's core.



One of the great optimizations Qdrant offers is quantization (either

[scalar](https://qdrant.tech/articles/scalar-quantization/) or 

[product](https://qdrant.tech/articles/product-quantization/)-based).",0.2767857142857143,0.49206349206349204,0.3542857142857143,0.0990990990990991,0.1774193548387097,0.12716763005780346,0.17857142857142858,0.31746031746031744,0.22857142857142854,0.25,0.4444444444444444,0.32,0.8417129516601562,0.6033640503883362,0.6448196172714233,0.6234033703804016,0.9954892387710667,0.6883703277446329,116,1.7313432835820894,0.32912626054637983
True,,1387.046,cd542cb5,What is the purpose of the Vacuum Optimizer in the Qdrant database system?,"The Vacuum Optimizer in the Qdrant database system is utilized to address the issue of accumulated deleted records within segments. When records are marked as deleted instead of being immediately removed, they can accumulate over time, occupying memory and slowing down the system. The Vacuum Optimizer is triggered when a segment has accumulated a significant number of deleted records, as defined by the criteria set in the configuration file. This optimizer helps in optimizing segments by removing these accumulated deleted records, thereby improving system performance and efficiency. The configuration file specifies parameters such as the minimal fraction of deleted vectors and the minimal number of vectors in a segment required to trigger the Vacuum Optimizer. By running the Vacuum Optimizer, the system can effectively manage and optimize segments to prevent performance degradation due to accumulated deleted records.","The purpose of the Vacuum Optimizer in the Qdrant database system is to manage accumulated deleted records within segments. It is used to optimize storage and prevent the accumulation of deleted records from slowing down the system over time. By utilizing the Vacuum Optimizer, Qdrant can efficiently handle deleted records and maintain system performance by periodically cleaning up and optimizing storage at the segment level.","Like many other databases, Qdrant does not delete entries immediately after a query.

Instead, it marks records as deleted and ignores them for future queries.



This strategy allows us to minimize disk access - one of the slowest operations.

However, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.



To avoid these adverse effects, Vacuum Optimizer is used.

It is used if the segment has accumulated too many deleted records.
---

title: Optimizer

weight: 70

aliases:

  - ../optimizer

---



# Optimizer



It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.



Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).
> Fun Fact: The Qdrant system is capable of in-place filtering during graph traversal, which is a novel approach compared to traditional post-filtering methods, ensuring the correct quantity of results that meet the filtering conditions.

> 



## Timestamps:



00:00 Search professional with expertise in vectors and engines.\

09:59 Elasticsearch: scalable, weak consistency, prefer vector search.\

12:53 Optimize data structures for faster processing efficiency.\
Conversely, the only up-front investment with vector databases is the indexing (which requires more compute). After this step, everything else is a breeze. Once setup, Qdrant easily scales via [features like Multitenancy and Sharding](https://qdrant.tech/articles/multitenancy/). This lets you scale up your reliance on the vector retrieval process and minimize your use of the compute-heavy LLMs. As an optimization  measure, Qdrant is irreplaceable. 



Julien Simon from HuggingFace says it best:
And I think we've been able to.

Do with Qdrant some stuff that really are possible only because of the very good performance of the database.

As an example, we're not using your clustered setup. We have n number of independent nodes.

And as we scale, we kind of.

Reshuffle which users go on which nodes.

As we need, trying to keep our largest users and most paying users on.

Very well identified nodes. We have a kind of a garbage.","['Like many other databases, Qdrant does not delete entries immediately after a query.\n\nInstead, it marks records as deleted and ignores them for future queries.\n\n\n\nThis strategy allows us to minimize disk access - one of the slowest operations.\n\nHowever, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.\n\n\n\nTo avoid these adverse effects, Vacuum Optimizer is used.\n\nIt is used if the segment has accumulated too many deleted records.', '---\n\ntitle: Optimizer\n\nweight: 70\n\naliases:\n\n  - ../optimizer\n\n---\n\n\n\n# Optimizer\n\n\n\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\n\n\n\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).', '> Fun Fact: The Qdrant system is capable of in-place filtering during graph traversal, which is a novel approach compared to traditional post-filtering methods, ensuring the correct quantity of results that meet the filtering conditions.\n\n> \n\n\n\n## Timestamps:\n\n\n\n00:00 Search professional with expertise in vectors and engines.\\\n\n09:59 Elasticsearch: scalable, weak consistency, prefer vector search.\\\n\n12:53 Optimize data structures for faster processing efficiency.\\', 'Conversely, the only up-front investment with vector databases is the indexing (which requires more compute). After this step, everything else is a breeze. Once setup, Qdrant easily scales via [features like Multitenancy and Sharding](https://qdrant.tech/articles/multitenancy/). This lets you scale up your reliance on the vector retrieval process and minimize your use of the compute-heavy LLMs. As an optimization  measure, Qdrant is irreplaceable. \n\n\n\nJulien Simon from HuggingFace says it best:', ""And I think we've been able to.\n\nDo with Qdrant some stuff that really are possible only because of the very good performance of the database.\n\nAs an example, we're not using your clustered setup. We have n number of independent nodes.\n\nAnd as we scale, we kind of.\n\nReshuffle which users go on which nodes.\n\nAs we need, trying to keep our largest users and most paying users on.\n\nVery well identified nodes. We have a kind of a garbage.""]","Question: What is the purpose of the Vacuum Optimizer in the Qdrant database system?\n\nContext: Like many other databases, Qdrant does not delete entries immediately after a query.

Instead, it marks records as deleted and ignores them for future queries.



This strategy allows us to minimize disk access - one of the slowest operations.

However, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.



To avoid these adverse effects, Vacuum Optimizer is used.

It is used if the segment has accumulated too many deleted records.
---

title: Optimizer

weight: 70

aliases:

- ../optimizer

---



# Optimizer



It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.



Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).
> Fun Fact: The Qdrant system is capable of in-place filtering during graph traversal, which is a novel approach compared to traditional post-filtering methods, ensuring the correct quantity of results that meet the filtering conditions.

> 



## Timestamps:



00:00 Search professional with expertise in vectors and engines.\

09:59 Elasticsearch: scalable, weak consistency, prefer vector search.\

12:53 Optimize data structures for faster processing efficiency.\
Conversely, the only up-front investment with vector databases is the indexing (which requires more compute). After this step, everything else is a breeze. Once setup, Qdrant easily scales via [features like Multitenancy and Sharding](https://qdrant.tech/articles/multitenancy/). This lets you scale up your reliance on the vector retrieval process and minimize your use of the compute-heavy LLMs. As an optimization  measure, Qdrant is irreplaceable. 



Julien Simon from HuggingFace says it best:
And I think we've been able to.

Do with Qdrant some stuff that really are possible only because of the very good performance of the database.

As an example, we're not using your clustered setup. We have n number of independent nodes.

And as we scale, we kind of.

Reshuffle which users go on which nodes.

As we need, trying to keep our largest users and most paying users on.

Very well identified nodes. We have a kind of a garbage.\n\nAnswer:",What is the purpose of the Vacuum Optimizer in the Qdrant database system?,"The Vacuum Optimizer in the Qdrant database system is utilized to address the issue of accumulated deleted records within segments. When records are marked as deleted instead of being immediately removed, they can accumulate over time, occupying memory and slowing down the system. The Vacuum Optimizer is triggered when a segment has accumulated a significant number of deleted records, as defined by the criteria set in the configuration file. This optimizer helps in optimizing segments by removing these accumulated deleted records, thereby improving system performance and efficiency. The configuration file specifies parameters such as the minimal fraction of deleted vectors and the minimal number of vectors in a segment required to trigger the Vacuum Optimizer. By running the Vacuum Optimizer, the system can effectively manage and optimize segments to prevent performance degradation due to accumulated deleted records.","['Like many other databases, Qdrant does not delete entries immediately after a query.\n\nInstead, it marks records as deleted and ignores them for future queries.\n\n\n\nThis strategy allows us to minimize disk access - one of the slowest operations.\n\nHowever, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.\n\n\n\nTo avoid these adverse effects, Vacuum Optimizer is used.\n\nIt is used if the segment has accumulated too many deleted records.', '---\n\ntitle: Optimizer\n\nweight: 70\n\naliases:\n\n  - ../optimizer\n\n---\n\n\n\n# Optimizer\n\n\n\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\n\n\n\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).', '> Fun Fact: The Qdrant system is capable of in-place filtering during graph traversal, which is a novel approach compared to traditional post-filtering methods, ensuring the correct quantity of results that meet the filtering conditions.\n\n> \n\n\n\n## Timestamps:\n\n\n\n00:00 Search professional with expertise in vectors and engines.\\\n\n09:59 Elasticsearch: scalable, weak consistency, prefer vector search.\\\n\n12:53 Optimize data structures for faster processing efficiency.\\', 'Conversely, the only up-front investment with vector databases is the indexing (which requires more compute). After this step, everything else is a breeze. Once setup, Qdrant easily scales via [features like Multitenancy and Sharding](https://qdrant.tech/articles/multitenancy/). This lets you scale up your reliance on the vector retrieval process and minimize your use of the compute-heavy LLMs. As an optimization  measure, Qdrant is irreplaceable. \n\n\n\nJulien Simon from HuggingFace says it best:', ""And I think we've been able to.\n\nDo with Qdrant some stuff that really are possible only because of the very good performance of the database.\n\nAs an example, we're not using your clustered setup. We have n number of independent nodes.\n\nAnd as we scale, we kind of.\n\nReshuffle which users go on which nodes.\n\nAs we need, trying to keep our largest users and most paying users on.\n\nVery well identified nodes. We have a kind of a garbage.""]","Like many other databases, Qdrant does not delete entries immediately after a query.

Instead, it marks records as deleted and ignores them for future queries.



This strategy allows us to minimize disk access - one of the slowest operations.

However, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.



To avoid these adverse effects, Vacuum Optimizer is used.

It is used if the segment has accumulated too many deleted records.
---

title: Optimizer

weight: 70

aliases:

  - ../optimizer

---



# Optimizer



It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.



Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).
> Fun Fact: The Qdrant system is capable of in-place filtering during graph traversal, which is a novel approach compared to traditional post-filtering methods, ensuring the correct quantity of results that meet the filtering conditions.

> 



## Timestamps:



00:00 Search professional with expertise in vectors and engines.\

09:59 Elasticsearch: scalable, weak consistency, prefer vector search.\

12:53 Optimize data structures for faster processing efficiency.\
Conversely, the only up-front investment with vector databases is the indexing (which requires more compute). After this step, everything else is a breeze. Once setup, Qdrant easily scales via [features like Multitenancy and Sharding](https://qdrant.tech/articles/multitenancy/). This lets you scale up your reliance on the vector retrieval process and minimize your use of the compute-heavy LLMs. As an optimization  measure, Qdrant is irreplaceable. 



Julien Simon from HuggingFace says it best:
And I think we've been able to.

Do with Qdrant some stuff that really are possible only because of the very good performance of the database.

As an example, we're not using your clustered setup. We have n number of independent nodes.

And as we scale, we kind of.

Reshuffle which users go on which nodes.

As we need, trying to keep our largest users and most paying users on.

Very well identified nodes. We have a kind of a garbage.",0.7586206896551724,0.36363636363636365,0.4916201117318436,0.3333333333333333,0.15833333333333333,0.21468926553672316,0.4827586206896552,0.23140495867768596,0.31284916201117324,0.6551724137931034,0.3140495867768595,0.4245810055865922,0.9571446776390076,0.732465386390686,0.6592513918876648,0.6939326524734497,0.9746494956004123,0.7877109555556672,65,0.4744525547445255,0.3906792637891853
True,,1599.54,82e0b1b9,What is the purpose of the `payload` field in the data points?,"The `payload` field in the data points being upserted using the QdrantClient serves as a container for additional metadata or information related to the vector data being stored. It allows users to associate supplementary details with each data point, such as the city name, price, or any other custom attributes that provide context or additional insights into the vector data. This metadata can be used for filtering, searching, or categorizing the vectors during retrieval or query operations, enabling more efficient and targeted data analysis and retrieval processes. In the examples given, the `payload` field includes information like city names, prices, and other relevant details specific to each data point, enhancing the overall utility and relevance of the stored vector data.","The `payload` field in the data points serves the purpose of storing additional information or metadata associated with the vector data. It can include key-value pairs that provide context or details about the vector. This information can be used for filtering, searching, or any other operations that require accessing the metadata associated with the vector data. In the provided examples, the payload contains details like the city and price associated with a particular vector.","```http

POST /collections/{collection_name}/points/payload

{

    ""payload"": {

        ""property1"": ""string"",

        ""property2"": ""string""

    },

    ""filter"": {

        ""must"": [

            {

                ""key"": ""color"",

                ""match"": {

                    ""value"": ""red""

                }

            }

        ]

    }

}

```



```python

client.set_payload(

    collection_name=""{collection_name}"",

    payload={

        ""property1"": ""string"",
### Full-text index



*Available as of v0.10.0*



Qdrant supports full-text search for string payload.

Full-text index allows you to filter points by the presence of a word or a phrase in the payload field.



Full-text index configuration is a bit more complex than other indexes, as you can specify the tokenization parameters.

Tokenization is the process of splitting a string into tokens, which are then indexed in the inverted index.



To create a full-text index, you can use the following:
payload={

        ""property1"": ""string"",

        ""property2"": ""string"",

    },

    points=models.Filter(

        must=[

            models.FieldCondition(

                key=""color"",

                match=models.MatchValue(value=""red""),

            ),

        ],

    ),

)

```



```typescript

client.setPayload(""{collection_name}"", {

  payload: {

    property1: ""string"",

    property2: ""string"",

  },

  filter: {

    must: [

      {

        key: ""color"",

        match: {
```http

PUT /collections/{collection_name}/points

{

    ""points"": [

        {

            ""id"": 1,

            ""vector"": [0.05, 0.61, 0.76, 0.74],

            ""payload"": {""city"": ""Berlin"", ""price"": 1.99}

        },

        {

            ""id"": 2,

            ""vector"": [0.19, 0.81, 0.75, 0.11],

            ""payload"": {""city"": [""Berlin"", ""London""], ""price"": 1.99}

        },

        {

            ""id"": 3,

            ""vector"": [0.36, 0.55, 0.47, 0.94],
Payload = { [""city""] = ""Berlin"", [""price""] = 1.99 }

		},

		new PointStruct

		{

			Id = 2,

			Vectors = new[] { 0.19f, 0.81f, 0.75f, 0.11f },

			Payload = { [""city""] = new[] { ""Berlin"", ""London"" } }

		},

		new PointStruct

		{

			Id = 3,

			Vectors = new[] { 0.36f, 0.55f, 0.47f, 0.94f },

			Payload =

			{

				[""city""] = new[] { ""Berlin"", ""Moscow"" },

				[""price""] = new Value

				{

					ListValue = new ListValue { Values = { new Value[] { 1.99, 2.99 } } }

				}

			}

		}

	}

);

```","['```http\n\nPOST /collections/{collection_name}/points/payload\n\n{\n\n    ""payload"": {\n\n        ""property1"": ""string"",\n\n        ""property2"": ""string""\n\n    },\n\n    ""filter"": {\n\n        ""must"": [\n\n            {\n\n                ""key"": ""color"",\n\n                ""match"": {\n\n                    ""value"": ""red""\n\n                }\n\n            }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.set_payload(\n\n    collection_name=""{collection_name}"",\n\n    payload={\n\n        ""property1"": ""string"",', '### Full-text index\n\n\n\n*Available as of v0.10.0*\n\n\n\nQdrant supports full-text search for string payload.\n\nFull-text index allows you to filter points by the presence of a word or a phrase in the payload field.\n\n\n\nFull-text index configuration is a bit more complex than other indexes, as you can specify the tokenization parameters.\n\nTokenization is the process of splitting a string into tokens, which are then indexed in the inverted index.\n\n\n\nTo create a full-text index, you can use the following:', 'payload={\n\n        ""property1"": ""string"",\n\n        ""property2"": ""string"",\n\n    },\n\n    points=models.Filter(\n\n        must=[\n\n            models.FieldCondition(\n\n                key=""color"",\n\n                match=models.MatchValue(value=""red""),\n\n            ),\n\n        ],\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nclient.setPayload(""{collection_name}"", {\n\n  payload: {\n\n    property1: ""string"",\n\n    property2: ""string"",\n\n  },\n\n  filter: {\n\n    must: [\n\n      {\n\n        key: ""color"",\n\n        match: {', '```http\n\nPUT /collections/{collection_name}/points\n\n{\n\n    ""points"": [\n\n        {\n\n            ""id"": 1,\n\n            ""vector"": [0.05, 0.61, 0.76, 0.74],\n\n            ""payload"": {""city"": ""Berlin"", ""price"": 1.99}\n\n        },\n\n        {\n\n            ""id"": 2,\n\n            ""vector"": [0.19, 0.81, 0.75, 0.11],\n\n            ""payload"": {""city"": [""Berlin"", ""London""], ""price"": 1.99}\n\n        },\n\n        {\n\n            ""id"": 3,\n\n            ""vector"": [0.36, 0.55, 0.47, 0.94],', 'Payload = { [""city""] = ""Berlin"", [""price""] = 1.99 }\n\n\t\t},\n\n\t\tnew PointStruct\n\n\t\t{\n\n\t\t\tId = 2,\n\n\t\t\tVectors = new[] { 0.19f, 0.81f, 0.75f, 0.11f },\n\n\t\t\tPayload = { [""city""] = new[] { ""Berlin"", ""London"" } }\n\n\t\t},\n\n\t\tnew PointStruct\n\n\t\t{\n\n\t\t\tId = 3,\n\n\t\t\tVectors = new[] { 0.36f, 0.55f, 0.47f, 0.94f },\n\n\t\t\tPayload =\n\n\t\t\t{\n\n\t\t\t\t[""city""] = new[] { ""Berlin"", ""Moscow"" },\n\n\t\t\t\t[""price""] = new Value\n\n\t\t\t\t{\n\n\t\t\t\t\tListValue = new ListValue { Values = { new Value[] { 1.99, 2.99 } } }\n\n\t\t\t\t}\n\n\t\t\t}\n\n\t\t}\n\n\t}\n\n);\n\n```']","Question: What is the purpose of the `payload` field in the data points?\n\nContext: ```http

POST /collections/{collection_name}/points/payload

{

""payload"": {

""property1"": ""string"",

""property2"": ""string""

},

""filter"": {

""must"": [

{

""key"": ""color"",

""match"": {

""value"": ""red""

}

}

]

}

}

```



```python

client.set_payload(

collection_name=""{collection_name}"",

payload={

""property1"": ""string"",
### Full-text index



*Available as of v0.10.0*



Qdrant supports full-text search for string payload.

Full-text index allows you to filter points by the presence of a word or a phrase in the payload field.



Full-text index configuration is a bit more complex than other indexes, as you can specify the tokenization parameters.

Tokenization is the process of splitting a string into tokens, which are then indexed in the inverted index.



To create a full-text index, you can use the following:
payload={

""property1"": ""string"",

""property2"": ""string"",

},

points=models.Filter(

must=[

models.FieldCondition(

key=""color"",

match=models.MatchValue(value=""red""),

),

],

),

)

```



```typescript

client.setPayload(""{collection_name}"", {

payload: {

property1: ""string"",

property2: ""string"",

},

filter: {

must: [

{

key: ""color"",

match: {
```http

PUT /collections/{collection_name}/points

{

""points"": [

{

""id"": 1,

""vector"": [0.05, 0.61, 0.76, 0.74],

""payload"": {""city"": ""Berlin"", ""price"": 1.99}

},

{

""id"": 2,

""vector"": [0.19, 0.81, 0.75, 0.11],

""payload"": {""city"": [""Berlin"", ""London""], ""price"": 1.99}

},

{

""id"": 3,

""vector"": [0.36, 0.55, 0.47, 0.94],
Payload = { [""city""] = ""Berlin"", [""price""] = 1.99 }

},

new PointStruct

{

Id = 2,

Vectors = new[] { 0.19f, 0.81f, 0.75f, 0.11f },

Payload = { [""city""] = new[] { ""Berlin"", ""London"" } }

},

new PointStruct

{

Id = 3,

Vectors = new[] { 0.36f, 0.55f, 0.47f, 0.94f },

Payload =

{

[""city""] = new[] { ""Berlin"", ""Moscow"" },

[""price""] = new Value

{

ListValue = new ListValue { Values = { new Value[] { 1.99, 2.99 } } }

}

}

}

}

);

```\n\nAnswer:",What is the purpose of the `payload` field in the data points?,"The `payload` field in the data points being upserted using the QdrantClient serves as a container for additional metadata or information related to the vector data being stored. It allows users to associate supplementary details with each data point, such as the city name, price, or any other custom attributes that provide context or additional insights into the vector data. This metadata can be used for filtering, searching, or categorizing the vectors during retrieval or query operations, enabling more efficient and targeted data analysis and retrieval processes. In the examples given, the `payload` field includes information like city names, prices, and other relevant details specific to each data point, enhancing the overall utility and relevance of the stored vector data.","['```http\n\nPOST /collections/{collection_name}/points/payload\n\n{\n\n    ""payload"": {\n\n        ""property1"": ""string"",\n\n        ""property2"": ""string""\n\n    },\n\n    ""filter"": {\n\n        ""must"": [\n\n            {\n\n                ""key"": ""color"",\n\n                ""match"": {\n\n                    ""value"": ""red""\n\n                }\n\n            }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.set_payload(\n\n    collection_name=""{collection_name}"",\n\n    payload={\n\n        ""property1"": ""string"",', '### Full-text index\n\n\n\n*Available as of v0.10.0*\n\n\n\nQdrant supports full-text search for string payload.\n\nFull-text index allows you to filter points by the presence of a word or a phrase in the payload field.\n\n\n\nFull-text index configuration is a bit more complex than other indexes, as you can specify the tokenization parameters.\n\nTokenization is the process of splitting a string into tokens, which are then indexed in the inverted index.\n\n\n\nTo create a full-text index, you can use the following:', 'payload={\n\n        ""property1"": ""string"",\n\n        ""property2"": ""string"",\n\n    },\n\n    points=models.Filter(\n\n        must=[\n\n            models.FieldCondition(\n\n                key=""color"",\n\n                match=models.MatchValue(value=""red""),\n\n            ),\n\n        ],\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nclient.setPayload(""{collection_name}"", {\n\n  payload: {\n\n    property1: ""string"",\n\n    property2: ""string"",\n\n  },\n\n  filter: {\n\n    must: [\n\n      {\n\n        key: ""color"",\n\n        match: {', '```http\n\nPUT /collections/{collection_name}/points\n\n{\n\n    ""points"": [\n\n        {\n\n            ""id"": 1,\n\n            ""vector"": [0.05, 0.61, 0.76, 0.74],\n\n            ""payload"": {""city"": ""Berlin"", ""price"": 1.99}\n\n        },\n\n        {\n\n            ""id"": 2,\n\n            ""vector"": [0.19, 0.81, 0.75, 0.11],\n\n            ""payload"": {""city"": [""Berlin"", ""London""], ""price"": 1.99}\n\n        },\n\n        {\n\n            ""id"": 3,\n\n            ""vector"": [0.36, 0.55, 0.47, 0.94],', 'Payload = { [""city""] = ""Berlin"", [""price""] = 1.99 }\n\n\t\t},\n\n\t\tnew PointStruct\n\n\t\t{\n\n\t\t\tId = 2,\n\n\t\t\tVectors = new[] { 0.19f, 0.81f, 0.75f, 0.11f },\n\n\t\t\tPayload = { [""city""] = new[] { ""Berlin"", ""London"" } }\n\n\t\t},\n\n\t\tnew PointStruct\n\n\t\t{\n\n\t\t\tId = 3,\n\n\t\t\tVectors = new[] { 0.36f, 0.55f, 0.47f, 0.94f },\n\n\t\t\tPayload =\n\n\t\t\t{\n\n\t\t\t\t[""city""] = new[] { ""Berlin"", ""Moscow"" },\n\n\t\t\t\t[""price""] = new Value\n\n\t\t\t\t{\n\n\t\t\t\t\tListValue = new ListValue { Values = { new Value[] { 1.99, 2.99 } } }\n\n\t\t\t\t}\n\n\t\t\t}\n\n\t\t}\n\n\t}\n\n);\n\n```']","```http

POST /collections/{collection_name}/points/payload

{

    ""payload"": {

        ""property1"": ""string"",

        ""property2"": ""string""

    },

    ""filter"": {

        ""must"": [

            {

                ""key"": ""color"",

                ""match"": {

                    ""value"": ""red""

                }

            }

        ]

    }

}

```



```python

client.set_payload(

    collection_name=""{collection_name}"",

    payload={

        ""property1"": ""string"",
### Full-text index



*Available as of v0.10.0*



Qdrant supports full-text search for string payload.

Full-text index allows you to filter points by the presence of a word or a phrase in the payload field.



Full-text index configuration is a bit more complex than other indexes, as you can specify the tokenization parameters.

Tokenization is the process of splitting a string into tokens, which are then indexed in the inverted index.



To create a full-text index, you can use the following:
payload={

        ""property1"": ""string"",

        ""property2"": ""string"",

    },

    points=models.Filter(

        must=[

            models.FieldCondition(

                key=""color"",

                match=models.MatchValue(value=""red""),

            ),

        ],

    ),

)

```



```typescript

client.setPayload(""{collection_name}"", {

  payload: {

    property1: ""string"",

    property2: ""string"",

  },

  filter: {

    must: [

      {

        key: ""color"",

        match: {
```http

PUT /collections/{collection_name}/points

{

    ""points"": [

        {

            ""id"": 1,

            ""vector"": [0.05, 0.61, 0.76, 0.74],

            ""payload"": {""city"": ""Berlin"", ""price"": 1.99}

        },

        {

            ""id"": 2,

            ""vector"": [0.19, 0.81, 0.75, 0.11],

            ""payload"": {""city"": [""Berlin"", ""London""], ""price"": 1.99}

        },

        {

            ""id"": 3,

            ""vector"": [0.36, 0.55, 0.47, 0.94],
Payload = { [""city""] = ""Berlin"", [""price""] = 1.99 }

		},

		new PointStruct

		{

			Id = 2,

			Vectors = new[] { 0.19f, 0.81f, 0.75f, 0.11f },

			Payload = { [""city""] = new[] { ""Berlin"", ""London"" } }

		},

		new PointStruct

		{

			Id = 3,

			Vectors = new[] { 0.36f, 0.55f, 0.47f, 0.94f },

			Payload =

			{

				[""city""] = new[] { ""Berlin"", ""Moscow"" },

				[""price""] = new Value

				{

					ListValue = new ListValue { Values = { new Value[] { 1.99, 2.99 } } }

				}

			}

		}

	}

);

```",0.765625,0.4537037037037037,0.5697674418604651,0.30158730158730157,0.17757009345794392,0.2235294117647059,0.515625,0.3055555555555556,0.38372093023255816,0.6875,0.4074074074074074,0.5116279069767441,0.8004177808761597,0.7742115259170532,0.7048677802085876,0.7379141449928284,0.9004949172958732,0.7311538048088551,74,0.6166666666666667,0.5897634604806051
True,,2488.8309999999997,97e49139,What is the purpose of shard replication in Qdrant?,"Shard replication in Qdrant increases the reliability of the cluster by keeping several copies of a shard spread across the cluster. This ensures the availability of the data in case of node failures, except if all replicas are lost.","The purpose of shard replication in Qdrant is to increase the reliability of the cluster by keeping several copies of a shard spread across multiple nodes. Replicating shards ensures that data is consistent and available on multiple nodes, reducing the risk of data loss or performance degradation in case of node failures. Additionally, shard replication transfers index and quantization data to the target shard, making the data immediately available without the need for re-optimization. This process can significantly improve performance, especially for large shards. It is important to note that replication factors should be carefully considered to ensure that there is enough storage space to accommodate the additional shard replicas.","Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.

If there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.



### Recover from a URL or local file



*Available as of v0.11.3*
ordering guarantees can be `strong`[^ordered], required for some applications.



The `stream_records` method is currently used as default. This may change in the

future.



## Replication



*Available as of v0.11.0*



Qdrant allows you to replicate shards between nodes in the cluster.



Shard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.
order to the target shard to bring it into the same state as the source. There

are two important benefits: 1. It transfers index and quantization data, so that

the shard does not have to be optimized again on the target node, making them

immediately available. This way, Qdrant ensures that there will be no

degradation in performance at the end of the transfer. Especially on large

shards, this can give a huge performance improvement. 2. The consistency and
Since a replication factor of ""2"" would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.



### Creating new shard replicas



It is possible to create or delete replicas manually on an existing collection using the [Update collection cluster setup API](https://qdrant.github.io/qdrant/redoc/index.html?v=v0.11.0#tag/cluster/operation/update_collection_cluster).
- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents","['Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.\n\nIf there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.\n\n\n\n### Recover from a URL or local file\n\n\n\n*Available as of v0.11.3*', 'ordering guarantees can be `strong`[^ordered], required for some applications.\n\n\n\nThe `stream_records` method is currently used as default. This may change in the\n\nfuture.\n\n\n\n## Replication\n\n\n\n*Available as of v0.11.0*\n\n\n\nQdrant allows you to replicate shards between nodes in the cluster.\n\n\n\nShard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.', 'order to the target shard to bring it into the same state as the source. There\n\nare two important benefits: 1. It transfers index and quantization data, so that\n\nthe shard does not have to be optimized again on the target node, making them\n\nimmediately available. This way, Qdrant ensures that there will be no\n\ndegradation in performance at the end of the transfer. Especially on large\n\nshards, this can give a huge performance improvement. 2. The consistency and', 'Since a replication factor of ""2"" would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.\n\n\n\n### Creating new shard replicas\n\n\n\nIt is possible to create or delete replicas manually on an existing collection using the [Update collection cluster setup API](https://qdrant.github.io/qdrant/redoc/index.html?v=v0.11.0#tag/cluster/operation/update_collection_cluster).', '- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents']","Question: What is the purpose of shard replication in Qdrant?\n\nContext: Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.

If there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.



### Recover from a URL or local file



*Available as of v0.11.3*
ordering guarantees can be `strong`[^ordered], required for some applications.



The `stream_records` method is currently used as default. This may change in the

future.



## Replication



*Available as of v0.11.0*



Qdrant allows you to replicate shards between nodes in the cluster.



Shard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.
order to the target shard to bring it into the same state as the source. There

are two important benefits: 1. It transfers index and quantization data, so that

the shard does not have to be optimized again on the target node, making them

immediately available. This way, Qdrant ensures that there will be no

degradation in performance at the end of the transfer. Especially on large

shards, this can give a huge performance improvement. 2. The consistency and
Since a replication factor of ""2"" would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.



### Creating new shard replicas



It is possible to create or delete replicas manually on an existing collection using the [Update collection cluster setup API](https://qdrant.github.io/qdrant/redoc/index.html?v=v0.11.0#tag/cluster/operation/update_collection_cluster).
- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents\n\nAnswer:",What is the purpose of shard replication in Qdrant?,"Shard replication in Qdrant increases the reliability of the cluster by keeping several copies of a shard spread across the cluster. This ensures the availability of the data in case of node failures, except if all replicas are lost.","['Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.\n\nIf there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.\n\n\n\n### Recover from a URL or local file\n\n\n\n*Available as of v0.11.3*', 'ordering guarantees can be `strong`[^ordered], required for some applications.\n\n\n\nThe `stream_records` method is currently used as default. This may change in the\n\nfuture.\n\n\n\n## Replication\n\n\n\n*Available as of v0.11.0*\n\n\n\nQdrant allows you to replicate shards between nodes in the cluster.\n\n\n\nShard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.', 'order to the target shard to bring it into the same state as the source. There\n\nare two important benefits: 1. It transfers index and quantization data, so that\n\nthe shard does not have to be optimized again on the target node, making them\n\nimmediately available. This way, Qdrant ensures that there will be no\n\ndegradation in performance at the end of the transfer. Especially on large\n\nshards, this can give a huge performance improvement. 2. The consistency and', 'Since a replication factor of ""2"" would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.\n\n\n\n### Creating new shard replicas\n\n\n\nIt is possible to create or delete replicas manually on an existing collection using the [Update collection cluster setup API](https://qdrant.github.io/qdrant/redoc/index.html?v=v0.11.0#tag/cluster/operation/update_collection_cluster).', '- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents']","Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.

If there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.



### Recover from a URL or local file



*Available as of v0.11.3*
ordering guarantees can be `strong`[^ordered], required for some applications.



The `stream_records` method is currently used as default. This may change in the

future.



## Replication



*Available as of v0.11.0*



Qdrant allows you to replicate shards between nodes in the cluster.



Shard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.
order to the target shard to bring it into the same state as the source. There

are two important benefits: 1. It transfers index and quantization data, so that

the shard does not have to be optimized again on the target node, making them

immediately available. This way, Qdrant ensures that there will be no

degradation in performance at the end of the transfer. Especially on large

shards, this can give a huge performance improvement. 2. The consistency and
Since a replication factor of ""2"" would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.



### Creating new shard replicas



It is possible to create or delete replicas manually on an existing collection using the [Update collection cluster setup API](https://qdrant.github.io/qdrant/redoc/index.html?v=v0.11.0#tag/cluster/operation/update_collection_cluster).
- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents",0.2647058823529412,0.8181818181818182,0.39999999999999997,0.18811881188118812,0.59375,0.28571428571428575,0.2549019607843137,0.7878787878787878,0.3851851851851852,0.2647058823529412,0.8181818181818182,0.39999999999999997,0.8946146965026855,0.6516718864440918,0.8007599115371704,0.7185641527175903,0.9978215204493608,0.596093183150515,110,2.8205128205128207,0.4121451922342063
True,,5224.942,0cc914cb,What are the steps to set up product quantization in QdrantClient?,"To set up product quantization in QdrantClient, you need to specify the quantization parameters in the quantization_config section of the collection configuration. The compression ratio can be set to x16 and the always_ram parameter can be set to true to store quantized vectors in RAM. The vectors_config section should also be specified with the desired vector size and distance metric. Finally, you can use the create_collection method of the QdrantClient to create the collection with the specified configurations.","To set up product quantization in QdrantClient, you can follow the steps below:

1. Import the necessary modules and classes for Product Quantization configuration:

```python
from qdrant_client import QdrantClient
from qdrant_client.http import models
```

2. Create a QdrantClient instance and specify the host and port for the Qdrant server:

```python
client = QdrantClient(""localhost"", port=6333)
```

3. Use the `create_collection` method on the client instance to create a new collection with Product Quantization configuration:

```python
client.create_collection(
    collection_name=""{collection_name}"",
    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),
    quantization_config=models.ProductQuantization(
        product=models.ProductQuantizationConfig(
            compression=models.CompressionRatio.X16,
            always_ram=True
        ),
        quantization_config=models.BinaryQuantization(
            binary=models.BinaryQuantizationConfig(
                always_ram=True
            )
        )
    )
)
```

In the above code snippet:
- `collection_name` is the name of the collection you want to create.
- `VectorParams` specifies the size of vectors and the distance metric to use.
- `ProductQuantization` configures the product quantization settings, such as compression ratio and RAM usage.
- `BinaryQuantization` configures the binary quantization settings, including RAM usage.

By following these steps, you can set up product quantization in QdrantClient for your collection.","ProductQuantization.newBuilder()

                            .setCompression(CompressionRatio.x16)

                            .setAlwaysRam(true)

                            .build())

                    .build())

            .build())

    .get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",
import io.qdrant.client.grpc.Collections.ProductQuantization;

import io.qdrant.client.grpc.Collections.QuantizationConfig;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(
customizable, so you can find the sweet spot between memory usage and search precision. This article 

covers all the steps required to perform Product Quantization and the way it's implemented in Qdrant.



Let’s assume we have a few vectors being added to the collection and that our optimizer decided 

to start creating a new segment.



![A list of raw vectors](/articles_data/product-quantization/raw-vectors.png)



### Cutting the vector into pieces
}

    }

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    quantization_config=models.ProductQuantization(

        product=models.ProductQuantizationConfig(

            compression=models.CompressionRatio.X16,

            always_ram=True,
quantization_config=models.BinaryQuantization(

        binary=models.BinaryQuantizationConfig(

            always_ram=True,

        ),

    ),

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createCollection(""{collection_name}"", {

  vectors: {

    size: 1536,

    distance: ""Cosine"",

  },

  quantization_config: {

    binary: {

      always_ram: true,

    },

  },

});

```","['ProductQuantization.newBuilder()\n\n                            .setCompression(CompressionRatio.x16)\n\n                            .setAlwaysRam(true)\n\n                            .build())\n\n                    .build())\n\n            .build())\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",', 'import io.qdrant.client.grpc.Collections.ProductQuantization;\n\nimport io.qdrant.client.grpc.Collections.QuantizationConfig;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(', ""customizable, so you can find the sweet spot between memory usage and search precision. This article \n\ncovers all the steps required to perform Product Quantization and the way it's implemented in Qdrant.\n\n\n\nLet’s assume we have a few vectors being added to the collection and that our optimizer decided \n\nto start creating a new segment.\n\n\n\n![A list of raw vectors](/articles_data/product-quantization/raw-vectors.png)\n\n\n\n### Cutting the vector into pieces"", '}\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    quantization_config=models.ProductQuantization(\n\n        product=models.ProductQuantizationConfig(\n\n            compression=models.CompressionRatio.X16,\n\n            always_ram=True,', 'quantization_config=models.BinaryQuantization(\n\n        binary=models.BinaryQuantizationConfig(\n\n            always_ram=True,\n\n        ),\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createCollection(""{collection_name}"", {\n\n  vectors: {\n\n    size: 1536,\n\n    distance: ""Cosine"",\n\n  },\n\n  quantization_config: {\n\n    binary: {\n\n      always_ram: true,\n\n    },\n\n  },\n\n});\n\n```']","Question: What are the steps to set up product quantization in QdrantClient?\n\nContext: ProductQuantization.newBuilder()

.setCompression(CompressionRatio.x16)

.setAlwaysRam(true)

.build())

.build())

.build())

.get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

collectionName: ""{collection_name}"",
import io.qdrant.client.grpc.Collections.ProductQuantization;

import io.qdrant.client.grpc.Collections.QuantizationConfig;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createCollectionAsync(

CreateCollection.newBuilder()

.setCollectionName(""{collection_name}"")

.setVectorsConfig(
customizable, so you can find the sweet spot between memory usage and search precision. This article 

covers all the steps required to perform Product Quantization and the way it's implemented in Qdrant.



Let’s assume we have a few vectors being added to the collection and that our optimizer decided 

to start creating a new segment.



![A list of raw vectors](/articles_data/product-quantization/raw-vectors.png)



### Cutting the vector into pieces
}

}

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

quantization_config=models.ProductQuantization(

product=models.ProductQuantizationConfig(

compression=models.CompressionRatio.X16,

always_ram=True,
quantization_config=models.BinaryQuantization(

binary=models.BinaryQuantizationConfig(

always_ram=True,

),

),

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createCollection(""{collection_name}"", {

vectors: {

size: 1536,

distance: ""Cosine"",

},

quantization_config: {

binary: {

always_ram: true,

},

},

});

```\n\nAnswer:",What are the steps to set up product quantization in QdrantClient?,"To set up product quantization in QdrantClient, you need to specify the quantization parameters in the quantization_config section of the collection configuration. The compression ratio can be set to x16 and the always_ram parameter can be set to true to store quantized vectors in RAM. The vectors_config section should also be specified with the desired vector size and distance metric. Finally, you can use the create_collection method of the QdrantClient to create the collection with the specified configurations.","['ProductQuantization.newBuilder()\n\n                            .setCompression(CompressionRatio.x16)\n\n                            .setAlwaysRam(true)\n\n                            .build())\n\n                    .build())\n\n            .build())\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",', 'import io.qdrant.client.grpc.Collections.ProductQuantization;\n\nimport io.qdrant.client.grpc.Collections.QuantizationConfig;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(', ""customizable, so you can find the sweet spot between memory usage and search precision. This article \n\ncovers all the steps required to perform Product Quantization and the way it's implemented in Qdrant.\n\n\n\nLet’s assume we have a few vectors being added to the collection and that our optimizer decided \n\nto start creating a new segment.\n\n\n\n![A list of raw vectors](/articles_data/product-quantization/raw-vectors.png)\n\n\n\n### Cutting the vector into pieces"", '}\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    quantization_config=models.ProductQuantization(\n\n        product=models.ProductQuantizationConfig(\n\n            compression=models.CompressionRatio.X16,\n\n            always_ram=True,', 'quantization_config=models.BinaryQuantization(\n\n        binary=models.BinaryQuantizationConfig(\n\n            always_ram=True,\n\n        ),\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createCollection(""{collection_name}"", {\n\n  vectors: {\n\n    size: 1536,\n\n    distance: ""Cosine"",\n\n  },\n\n  quantization_config: {\n\n    binary: {\n\n      always_ram: true,\n\n    },\n\n  },\n\n});\n\n```']","ProductQuantization.newBuilder()

                            .setCompression(CompressionRatio.x16)

                            .setAlwaysRam(true)

                            .build())

                    .build())

            .build())

    .get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",
import io.qdrant.client.grpc.Collections.ProductQuantization;

import io.qdrant.client.grpc.Collections.QuantizationConfig;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(
customizable, so you can find the sweet spot between memory usage and search precision. This article 

covers all the steps required to perform Product Quantization and the way it's implemented in Qdrant.



Let’s assume we have a few vectors being added to the collection and that our optimizer decided 

to start creating a new segment.



![A list of raw vectors](/articles_data/product-quantization/raw-vectors.png)



### Cutting the vector into pieces
}

    }

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    quantization_config=models.ProductQuantization(

        product=models.ProductQuantizationConfig(

            compression=models.CompressionRatio.X16,

            always_ram=True,
quantization_config=models.BinaryQuantization(

        binary=models.BinaryQuantizationConfig(

            always_ram=True,

        ),

    ),

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createCollection(""{collection_name}"", {

  vectors: {

    size: 1536,

    distance: ""Cosine"",

  },

  quantization_config: {

    binary: {

      always_ram: true,

    },

  },

});

```",0.3076923076923077,0.7323943661971831,0.43333333333333335,0.125,0.3,0.17647058823529413,0.16568047337278108,0.39436619718309857,0.23333333333333334,0.27218934911242604,0.647887323943662,0.38333333333333336,0.8318413496017456,0.581062376499176,0.652736246585846,0.6148175001144409,0.7257177874445915,0.760143144056201,150,1.9230769230769231,0.5296942718145519
True,,1492.928,1c6b817c,What are the different write ordering options available in Qdrant?,"Qdrant provides three different write ordering options: weak, medium, and strong. Weak ordering does not provide any additional guarantees and allows write operations to be freely reordered. Medium ordering serializes write operations through a dynamically elected leader, which may cause minor inconsistencies in case of leader change. Strong ordering serializes write operations through the permanent leader, providing strong consistency but may result in write operations being unavailable if the leader is down.","In Qdrant, the different write ordering options available are:

1. **Strong Ordering**: This option ensures strong ordering guarantees and is required for some applications. When using strong ordering, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This helps to avoid data inconsistency in case of concurrent updates of the same documents.","- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
ordering guarantees can be `strong`[^ordered], required for some applications.



The `stream_records` method is currently used as default. This may change in the

future.



## Replication



*Available as of v0.11.0*



Qdrant allows you to replicate shards between nodes in the cluster.



Shard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.
## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""shard_number"": 2

}

```



```python
location=""http://localhost:6333"",

            collection_name=""test"",

        ),

        write_config=QdrantWriteConfig(batch_size=80),

    )



if __name__ == ""__main__"":

    writer = get_writer()

    runner = LocalRunner(

        processor_config=ProcessorConfig(

            verbose=True,

            output_dir=""local-output-to-qdrant"",

            num_processes=2,

        ),

        connector_config=SimpleLocalConfig(
.                                                                                    |","['- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', 'ordering guarantees can be `strong`[^ordered], required for some applications.\n\n\n\nThe `stream_records` method is currently used as default. This may change in the\n\nfuture.\n\n\n\n## Replication\n\n\n\n*Available as of v0.11.0*\n\n\n\nQdrant allows you to replicate shards between nodes in the cluster.\n\n\n\nShard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.', '## Parallel upload into multiple shards\n\n\n\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\n\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python', 'location=""http://localhost:6333"",\n\n            collection_name=""test"",\n\n        ),\n\n        write_config=QdrantWriteConfig(batch_size=80),\n\n    )\n\n\n\nif __name__ == ""__main__"":\n\n    writer = get_writer()\n\n    runner = LocalRunner(\n\n        processor_config=ProcessorConfig(\n\n            verbose=True,\n\n            output_dir=""local-output-to-qdrant"",\n\n            num_processes=2,\n\n        ),\n\n        connector_config=SimpleLocalConfig(', '.                                                                                    |']","Question: What are the different write ordering options available in Qdrant?\n\nContext: - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
ordering guarantees can be `strong`[^ordered], required for some applications.



The `stream_records` method is currently used as default. This may change in the

future.



## Replication



*Available as of v0.11.0*



Qdrant allows you to replicate shards between nodes in the cluster.



Shard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.
## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""shard_number"": 2

}

```



```python
location=""http://localhost:6333"",

collection_name=""test"",

),

write_config=QdrantWriteConfig(batch_size=80),

)



if __name__ == ""__main__"":

writer = get_writer()

runner = LocalRunner(

processor_config=ProcessorConfig(

verbose=True,

output_dir=""local-output-to-qdrant"",

num_processes=2,

),

connector_config=SimpleLocalConfig(
.                                                                                    |\n\nAnswer:",What are the different write ordering options available in Qdrant?,"Qdrant provides three different write ordering options: weak, medium, and strong. Weak ordering does not provide any additional guarantees and allows write operations to be freely reordered. Medium ordering serializes write operations through a dynamically elected leader, which may cause minor inconsistencies in case of leader change. Strong ordering serializes write operations through the permanent leader, providing strong consistency but may result in write operations being unavailable if the leader is down.","['- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', 'ordering guarantees can be `strong`[^ordered], required for some applications.\n\n\n\nThe `stream_records` method is currently used as default. This may change in the\n\nfuture.\n\n\n\n## Replication\n\n\n\n*Available as of v0.11.0*\n\n\n\nQdrant allows you to replicate shards between nodes in the cluster.\n\n\n\nShard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.', '## Parallel upload into multiple shards\n\n\n\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\n\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python', 'location=""http://localhost:6333"",\n\n            collection_name=""test"",\n\n        ),\n\n        write_config=QdrantWriteConfig(batch_size=80),\n\n    )\n\n\n\nif __name__ == ""__main__"":\n\n    writer = get_writer()\n\n    runner = LocalRunner(\n\n        processor_config=ProcessorConfig(\n\n            verbose=True,\n\n            output_dir=""local-output-to-qdrant"",\n\n            num_processes=2,\n\n        ),\n\n        connector_config=SimpleLocalConfig(', '.                                                                                    |']","- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
ordering guarantees can be `strong`[^ordered], required for some applications.



The `stream_records` method is currently used as default. This may change in the

future.



## Replication



*Available as of v0.11.0*



Qdrant allows you to replicate shards between nodes in the cluster.



Shard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.
## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""shard_number"": 2

}

```



```python
location=""http://localhost:6333"",

            collection_name=""test"",

        ),

        write_config=QdrantWriteConfig(batch_size=80),

    )



if __name__ == ""__main__"":

    writer = get_writer()

    runner = LocalRunner(

        processor_config=ProcessorConfig(

            verbose=True,

            output_dir=""local-output-to-qdrant"",

            num_processes=2,

        ),

        connector_config=SimpleLocalConfig(
.                                                                                    |",0.39655172413793105,0.3333333333333333,0.36220472440944884,0.15789473684210525,0.1323529411764706,0.144,0.27586206896551724,0.2318840579710145,0.25196850393700787,0.3793103448275862,0.3188405797101449,0.3464566929133858,0.8199748992919922,0.5979508757591248,0.5799710750579834,0.5888237357139587,0.9934520247916225,0.25754616409540176,65,0.9027777777777778,0.47516054873234337
True,,5397.588,39a58d2e,How can Qdrant be optimized for low memory footprint and high speed search?,"To optimize Qdrant for low memory footprint and high speed search, you can use vector quantization. This involves converting vectors into a more compact representation that can be stored in memory and used for search. By using smaller vectors, you can cache more in RAM and reduce the number of disk reads. To configure in-memory quantization with on-disk original vectors, you need to create a collection with the specified configuration.","To optimize Qdrant for low memory footprint and high-speed search, consider the following strategies:

1. **Binary Quantization (BQ):** Implement binary quantization to reduce memory consumption and improve retrieval speeds by up to 40 times. Quantization is particularly effective in reducing the memory footprint and accelerating the search process in high-dimensional vector spaces. It allows you to strike a balance between accuracy, storage efficiency, and search speed.

2. **Optimizing High-Dimensional Vectors:** Use BQ to handle scaling challenges like high throughput, low latency, and efficient indexing. By leveraging quantization, you can significantly reduce storage requirements and speed up search times. For example, with a local SSD, you could increase search speed by 10 times, and with production-grade disks, the speed could be even higher.

3. **Utilize SSDs for High IOPS:** Consider using SSDs with configurations that can reach 1M IOPS or more. This can be an interesting option for serving large datasets with low search latency in Qdrant.

4. **Batch Search Feature:** Take advantage of the new batch search feature in Qdrant. With just one API call, Qdrant can handle multiple search requests efficiently, allowing you to perform multiple tasks simultaneously without worrying about network overhead or slow performance.

By incorporating these strategies, you can optimize Qdrant for low memory footprint and high-speed search, making it more efficient and effective for handling large datasets.","---

title: Optimize Resources

weight: 11

aliases:

  - ../tutorials/optimize

---



# Optimize Qdrant



Different use cases have different requirements for balancing between memory, speed, and precision.

Qdrant is designed to be flexible and customizable so you can tune it to your needs.



![Trafeoff](/docs/tradeoff.png)



Let's look deeper into each of those possible optimization scenarios.



## Prefer low memory footprint with high speed search
---



#### Optimizing high-dimensional vectors 



Qdrant is built to handle typical scaling challenges: high throughput, low latency and efficient indexing. **Binary quantization (BQ)** is our latest attempt to give our customers the edge they need to scale efficiently. This feature is particularly excellent for collections with large vector lengths and a large number of points. 



Our results are dramatic: Using BQ will reduce your memory consumption and improve retrieval speeds by up to 40x.
Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.

In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.



There are tradeoffs associated with quantization.

On the one hand, quantization allows for significant reductions in storage requirements and faster search times.
With a local SSD, we were able to increase the search speed by 10x!



With the production-grade disk, the search speed could be even higher. 

Some configurations of the SSDs can reach 1M IOPS and more.



Which might be an interesting option to serve large datasets with low search latency in Qdrant.





## Conclusion



In this article, we showed that Qdrant have flexibility in terms of RAM usage and can be used to serve large datasets.
Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows 

you to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way 

possible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead

or slow performance.



## Built-in ARM support","[""---\n\ntitle: Optimize Resources\n\nweight: 11\n\naliases:\n\n  - ../tutorials/optimize\n\n---\n\n\n\n# Optimize Qdrant\n\n\n\nDifferent use cases have different requirements for balancing between memory, speed, and precision.\n\nQdrant is designed to be flexible and customizable so you can tune it to your needs.\n\n\n\n![Trafeoff](/docs/tradeoff.png)\n\n\n\nLet's look deeper into each of those possible optimization scenarios.\n\n\n\n## Prefer low memory footprint with high speed search"", '---\n\n\n\n#### Optimizing high-dimensional vectors \n\n\n\nQdrant is built to handle typical scaling challenges: high throughput, low latency and efficient indexing. **Binary quantization (BQ)** is our latest attempt to give our customers the edge they need to scale efficiently. This feature is particularly excellent for collections with large vector lengths and a large number of points. \n\n\n\nOur results are dramatic: Using BQ will reduce your memory consumption and improve retrieval speeds by up to 40x.', 'Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\n\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\n\n\n\nThere are tradeoffs associated with quantization.\n\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.', 'With a local SSD, we were able to increase the search speed by 10x!\n\n\n\nWith the production-grade disk, the search speed could be even higher. \n\nSome configurations of the SSDs can reach 1M IOPS and more.\n\n\n\nWhich might be an interesting option to serve large datasets with low search latency in Qdrant.\n\n\n\n\n\n## Conclusion\n\n\n\nIn this article, we showed that Qdrant have flexibility in terms of RAM usage and can be used to serve large datasets.', 'Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows \n\nyou to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way \n\npossible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead\n\nor slow performance.\n\n\n\n## Built-in ARM support']","Question: How can Qdrant be optimized for low memory footprint and high speed search?\n\nContext: ---

title: Optimize Resources

weight: 11

aliases:

- ../tutorials/optimize

---



# Optimize Qdrant



Different use cases have different requirements for balancing between memory, speed, and precision.

Qdrant is designed to be flexible and customizable so you can tune it to your needs.



![Trafeoff](/docs/tradeoff.png)



Let's look deeper into each of those possible optimization scenarios.



## Prefer low memory footprint with high speed search
---



#### Optimizing high-dimensional vectors 



Qdrant is built to handle typical scaling challenges: high throughput, low latency and efficient indexing. **Binary quantization (BQ)** is our latest attempt to give our customers the edge they need to scale efficiently. This feature is particularly excellent for collections with large vector lengths and a large number of points. 



Our results are dramatic: Using BQ will reduce your memory consumption and improve retrieval speeds by up to 40x.
Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.

In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.



There are tradeoffs associated with quantization.

On the one hand, quantization allows for significant reductions in storage requirements and faster search times.
With a local SSD, we were able to increase the search speed by 10x!



With the production-grade disk, the search speed could be even higher. 

Some configurations of the SSDs can reach 1M IOPS and more.



Which might be an interesting option to serve large datasets with low search latency in Qdrant.





## Conclusion



In this article, we showed that Qdrant have flexibility in terms of RAM usage and can be used to serve large datasets.
Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows 

you to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way 

possible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead

or slow performance.



## Built-in ARM support\n\nAnswer:",How can Qdrant be optimized for low memory footprint and high speed search?,"To optimize Qdrant for low memory footprint and high speed search, you can use vector quantization. This involves converting vectors into a more compact representation that can be stored in memory and used for search. By using smaller vectors, you can cache more in RAM and reduce the number of disk reads. To configure in-memory quantization with on-disk original vectors, you need to create a collection with the specified configuration.","[""---\n\ntitle: Optimize Resources\n\nweight: 11\n\naliases:\n\n  - ../tutorials/optimize\n\n---\n\n\n\n# Optimize Qdrant\n\n\n\nDifferent use cases have different requirements for balancing between memory, speed, and precision.\n\nQdrant is designed to be flexible and customizable so you can tune it to your needs.\n\n\n\n![Trafeoff](/docs/tradeoff.png)\n\n\n\nLet's look deeper into each of those possible optimization scenarios.\n\n\n\n## Prefer low memory footprint with high speed search"", '---\n\n\n\n#### Optimizing high-dimensional vectors \n\n\n\nQdrant is built to handle typical scaling challenges: high throughput, low latency and efficient indexing. **Binary quantization (BQ)** is our latest attempt to give our customers the edge they need to scale efficiently. This feature is particularly excellent for collections with large vector lengths and a large number of points. \n\n\n\nOur results are dramatic: Using BQ will reduce your memory consumption and improve retrieval speeds by up to 40x.', 'Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\n\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\n\n\n\nThere are tradeoffs associated with quantization.\n\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.', 'With a local SSD, we were able to increase the search speed by 10x!\n\n\n\nWith the production-grade disk, the search speed could be even higher. \n\nSome configurations of the SSDs can reach 1M IOPS and more.\n\n\n\nWhich might be an interesting option to serve large datasets with low search latency in Qdrant.\n\n\n\n\n\n## Conclusion\n\n\n\nIn this article, we showed that Qdrant have flexibility in terms of RAM usage and can be used to serve large datasets.', 'Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows \n\nyou to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way \n\npossible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead\n\nor slow performance.\n\n\n\n## Built-in ARM support']","---

title: Optimize Resources

weight: 11

aliases:

  - ../tutorials/optimize

---



# Optimize Qdrant



Different use cases have different requirements for balancing between memory, speed, and precision.

Qdrant is designed to be flexible and customizable so you can tune it to your needs.



![Trafeoff](/docs/tradeoff.png)



Let's look deeper into each of those possible optimization scenarios.



## Prefer low memory footprint with high speed search
---



#### Optimizing high-dimensional vectors 



Qdrant is built to handle typical scaling challenges: high throughput, low latency and efficient indexing. **Binary quantization (BQ)** is our latest attempt to give our customers the edge they need to scale efficiently. This feature is particularly excellent for collections with large vector lengths and a large number of points. 



Our results are dramatic: Using BQ will reduce your memory consumption and improve retrieval speeds by up to 40x.
Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.

In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.



There are tradeoffs associated with quantization.

On the one hand, quantization allows for significant reductions in storage requirements and faster search times.
With a local SSD, we were able to increase the search speed by 10x!



With the production-grade disk, the search speed could be even higher. 

Some configurations of the SSDs can reach 1M IOPS and more.



Which might be an interesting option to serve large datasets with low search latency in Qdrant.





## Conclusion



In this article, we showed that Qdrant have flexibility in terms of RAM usage and can be used to serve large datasets.
Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows 

you to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way 

possible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead

or slow performance.



## Built-in ARM support",0.2100456621004566,0.6764705882352942,0.3205574912891986,0.06422018348623854,0.208955223880597,0.09824561403508772,0.1278538812785388,0.4117647058823529,0.19512195121951217,0.1872146118721461,0.6029411764705882,0.28571428571428564,0.9443399906158447,0.5470943450927734,0.6160268783569336,0.5795179605484009,0.7870070599145644,0.6426959857344627,222,3.1714285714285713,0.5506858581745866
True,,3665.402,d5fbf904,How does Qdrant optimize memory and search speed for sparse vectors?,"Qdrant optimizes memory and search speed for sparse vectors by utilizing an inverted index structure to store vectors for each non-zero dimension. This approach allows Qdrant to efficiently represent sparse vectors, which are characterized by a high proportion of zeroes. By only storing information about non-zero dimensions, Qdrant reduces the memory footprint required to store sparse vectors and also speeds up search operations by focusing only on relevant dimensions during indexing and querying processes. This optimization ensures that Qdrant can handle sparse vectors effectively while maintaining efficient memory usage and search performance.","Qdrant optimizes memory and search speed for sparse vectors by utilizing efficient indexing techniques and specialized CPU instructions. Here's how it achieves this:

1. **Support for Sparse Vectors:** Qdrant is designed to handle sparse vectors, which have a high proportion of zero values.

2. **Efficient Indexing:** Qdrant utilizes an inverted index structure to store vectors for each non-zero dimension. This approach helps optimize memory usage and search speed by reducing the amount of memory required to store a vector.

3. **Scalar Quantization:** Qdrant uses scalar quantization to further reduce the memory footprint of sparse vectors. This technique not only makes the vectors memory-efficient but also speeds up the search process.

4. **Specialized CPU Instruction:** Qdrant leverages a special SIMD CPU instruction that works with 8-bit integers to perform fast vector comparison. By converting vectors to `uint8` for comparison, Qdrant can achieve faster and more efficient search operations.

Overall, these optimizations make Qdrant well-suited for handling sparse vectors efficiently in high-dimensional search spaces, contributing to improved search performance and memory usage.","performance.



## Sparse Vector Index



*Available as of v1.7.0*



### Key Features of Sparse Vector Index

- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.

- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.



### Search Mechanism

- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
The resulting sparse vectors are not only memory-efficient but also tailored for precise matching in the high-dimensional space of a search engine like Qdrant.



## Interpreting SPLADE



A downside of dense vectors is that they are not interpretable, making it difficult to understand why a document is relevant to a query.
Our plan for the current [open-source roadmap](https://github.com/qdrant/qdrant/blob/master/docs/roadmap/README.md) is to make billion-scale vector search affordable. Our recent release of the [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/) improves both memory usage (x4) as well as speed (x2). Upcoming [Product Quantization](https://www.irisa.fr/texmex/people/jegou/papers/jegou_searching_with_quantization.pdf) will introduce even another option with more memory saving. Stay tuned
## Conclusion



Alright, folks, let's wrap it up. Better search isn't a 'nice-to-have,' it's a game-changer, and Qdrant can get you there.



Got questions? Our [Discord community](https://qdrant.to/discord?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors) is teeming with answers.","['performance.\n\n\n\n## Sparse Vector Index\n\n\n\n*Available as of v1.7.0*\n\n\n\n### Key Features of Sparse Vector Index\n\n- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.\n\n- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.\n\n\n\n### Search Mechanism\n\n- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.', 'Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.\n\n\n\nIn addition to reducing the memory footprint, scalar quantization also speeds up the search process.\n\nQdrant uses a special SIMD CPU instruction to perform fast vector comparison.\n\nThis instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.', 'The resulting sparse vectors are not only memory-efficient but also tailored for precise matching in the high-dimensional space of a search engine like Qdrant.\n\n\n\n## Interpreting SPLADE\n\n\n\nA downside of dense vectors is that they are not interpretable, making it difficult to understand why a document is relevant to a query.', 'Our plan for the current [open-source roadmap](https://github.com/qdrant/qdrant/blob/master/docs/roadmap/README.md) is to make billion-scale vector search affordable. Our recent release of the [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/) improves both memory usage (x4) as well as speed (x2). Upcoming [Product Quantization](https://www.irisa.fr/texmex/people/jegou/papers/jegou_searching_with_quantization.pdf) will introduce even another option with more memory saving. Stay tuned', ""## Conclusion\n\n\n\nAlright, folks, let's wrap it up. Better search isn't a 'nice-to-have,' it's a game-changer, and Qdrant can get you there.\n\n\n\nGot questions? Our [Discord community](https://qdrant.to/discord?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors) is teeming with answers.""]","Question: How does Qdrant optimize memory and search speed for sparse vectors?\n\nContext: performance.



## Sparse Vector Index



*Available as of v1.7.0*



### Key Features of Sparse Vector Index

- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.

- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.



### Search Mechanism

- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
The resulting sparse vectors are not only memory-efficient but also tailored for precise matching in the high-dimensional space of a search engine like Qdrant.



## Interpreting SPLADE



A downside of dense vectors is that they are not interpretable, making it difficult to understand why a document is relevant to a query.
Our plan for the current [open-source roadmap](https://github.com/qdrant/qdrant/blob/master/docs/roadmap/README.md) is to make billion-scale vector search affordable. Our recent release of the [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/) improves both memory usage (x4) as well as speed (x2). Upcoming [Product Quantization](https://www.irisa.fr/texmex/people/jegou/papers/jegou_searching_with_quantization.pdf) will introduce even another option with more memory saving. Stay tuned
## Conclusion



Alright, folks, let's wrap it up. Better search isn't a 'nice-to-have,' it's a game-changer, and Qdrant can get you there.



Got questions? Our [Discord community](https://qdrant.to/discord?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors) is teeming with answers.\n\nAnswer:",How does Qdrant optimize memory and search speed for sparse vectors?,"Qdrant optimizes memory and search speed for sparse vectors by utilizing an inverted index structure to store vectors for each non-zero dimension. This approach allows Qdrant to efficiently represent sparse vectors, which are characterized by a high proportion of zeroes. By only storing information about non-zero dimensions, Qdrant reduces the memory footprint required to store sparse vectors and also speeds up search operations by focusing only on relevant dimensions during indexing and querying processes. This optimization ensures that Qdrant can handle sparse vectors effectively while maintaining efficient memory usage and search performance.","['performance.\n\n\n\n## Sparse Vector Index\n\n\n\n*Available as of v1.7.0*\n\n\n\n### Key Features of Sparse Vector Index\n\n- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.\n\n- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.\n\n\n\n### Search Mechanism\n\n- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.', 'Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.\n\n\n\nIn addition to reducing the memory footprint, scalar quantization also speeds up the search process.\n\nQdrant uses a special SIMD CPU instruction to perform fast vector comparison.\n\nThis instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.', 'The resulting sparse vectors are not only memory-efficient but also tailored for precise matching in the high-dimensional space of a search engine like Qdrant.\n\n\n\n## Interpreting SPLADE\n\n\n\nA downside of dense vectors is that they are not interpretable, making it difficult to understand why a document is relevant to a query.', 'Our plan for the current [open-source roadmap](https://github.com/qdrant/qdrant/blob/master/docs/roadmap/README.md) is to make billion-scale vector search affordable. Our recent release of the [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/) improves both memory usage (x4) as well as speed (x2). Upcoming [Product Quantization](https://www.irisa.fr/texmex/people/jegou/papers/jegou_searching_with_quantization.pdf) will introduce even another option with more memory saving. Stay tuned', ""## Conclusion\n\n\n\nAlright, folks, let's wrap it up. Better search isn't a 'nice-to-have,' it's a game-changer, and Qdrant can get you there.\n\n\n\nGot questions? Our [Discord community](https://qdrant.to/discord?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors) is teeming with answers.""]","performance.



## Sparse Vector Index



*Available as of v1.7.0*



### Key Features of Sparse Vector Index

- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.

- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.



### Search Mechanism

- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
The resulting sparse vectors are not only memory-efficient but also tailored for precise matching in the high-dimensional space of a search engine like Qdrant.



## Interpreting SPLADE



A downside of dense vectors is that they are not interpretable, making it difficult to understand why a document is relevant to a query.
Our plan for the current [open-source roadmap](https://github.com/qdrant/qdrant/blob/master/docs/roadmap/README.md) is to make billion-scale vector search affordable. Our recent release of the [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/) improves both memory usage (x4) as well as speed (x2). Upcoming [Product Quantization](https://www.irisa.fr/texmex/people/jegou/papers/jegou_searching_with_quantization.pdf) will introduce even another option with more memory saving. Stay tuned
## Conclusion



Alright, folks, let's wrap it up. Better search isn't a 'nice-to-have,' it's a game-changer, and Qdrant can get you there.



Got questions? Our [Discord community](https://qdrant.to/discord?utm_source=qdrant&utm_medium=website&utm_campaign=sparse-vectors&utm_content=article&utm_term=sparse-vectors) is teeming with answers.",0.4107142857142857,0.7582417582417582,0.5328185328185329,0.2634730538922156,0.4888888888888889,0.34241245136186776,0.27976190476190477,0.5164835164835165,0.362934362934363,0.40476190476190477,0.7472527472527473,0.5250965250965252,0.8981521129608154,0.6837928295135498,0.767207145690918,0.7231023907661438,0.6602567602307178,0.9424522314220667,171,1.858695652173913,0.4548594976148493
True,,2334.062,1b9d8f7c,How does Qdrant address the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters?,"Qdrant addresses the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters by extending the HNSW graph with additional edges based on the stored payload values. This extension allows for more efficient searching of nearby vectors using the HNSW index while applying filters during the search in the graph. By adding extra edges, Qdrant minimizes the overhead on condition checks since the conditions only need to be calculated for a small fraction of the points involved in the search. This approach improves the performance of the HNSW index in cases where the traditional methods of weak filters or complete rescore are not optimal, providing a more balanced solution for scenarios that fall in between.","Qdrant addresses the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters by extending the HNSW graph with additional edges based on the stored payload values. This approach allows Qdrant to avoid the need for pre-filtering or post-filtering while also addressing the accuracy problem that arises when too many vectors are filtered out, causing the HNSW graph to become disconnected. Qdrant leverages the compatibility of HNSW with modifications that enable the use of filters during a search, making it a suitable choice for achieving both accuracy and speed. Additionally, Qdrant allows for the configuration of HNSW parameters on a collection and named vector level, providing the flexibility to fine-tune search performance according to specific use cases.","On the one hand, we cannot apply a full scan on too many vectors. On the other hand, the HNSW graph starts to fall apart when using too strict filters.



![HNSW fail](/docs/precision_by_m.png)



![hnsw graph](/docs/graph.gif)



You can find more information on why this happens in our [blog post](https://blog.vasnetsov.com/posts/categorical-hnsw/).

Qdrant solves this problem by extending the HNSW graph with additional edges based on the stored payload values.
Read more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.
- Others use **pre-filtering**, which requires a binary mask of the whole dataset to be passed into the ANN algorithm. It is also not scalable, as the mask size grows linearly with the dataset size.



On top of it, there is also a problem with search accuracy.

It appears if too many vectors are filtered out, so the HNSW graph becomes disconnected.



Qdrant uses a different approach, not requiring pre- or post-filtering while addressing the accuracy problem.
HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
All right, keep going. I like it.



Rishabh Bhardwaj:

Yeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.



Demetrios:

Right.



Rishabh Bhardwaj:","['On the one hand, we cannot apply a full scan on too many vectors. On the other hand, the HNSW graph starts to fall apart when using too strict filters.\n\n\n\n![HNSW fail](/docs/precision_by_m.png)\n\n\n\n![hnsw graph](/docs/graph.gif)\n\n\n\nYou can find more information on why this happens in our [blog post](https://blog.vasnetsov.com/posts/categorical-hnsw/).\n\nQdrant solves this problem by extending the HNSW graph with additional edges based on the stored payload values.', 'Read more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.', '- Others use **pre-filtering**, which requires a binary mask of the whole dataset to be passed into the ANN algorithm. It is also not scalable, as the mask size grows linearly with the dataset size.\n\n\n\nOn top of it, there is also a problem with search accuracy.\n\nIt appears if too many vectors are filtered out, so the HNSW graph becomes disconnected.\n\n\n\nQdrant uses a different approach, not requiring pre- or post-filtering while addressing the accuracy problem.', 'HNSW is chosen for several reasons.\n\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\n\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\n\n\n\n*Available as of v1.1.1*\n\n\n\nThe HNSW parameters can also be configured on a collection and named vector\n\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\n\nperformance.', 'All right, keep going. I like it.\n\n\n\nRishabh Bhardwaj:\n\nYeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.\n\n\n\nDemetrios:\n\nRight.\n\n\n\nRishabh Bhardwaj:']","Question: How does Qdrant address the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters?\n\nContext: On the one hand, we cannot apply a full scan on too many vectors. On the other hand, the HNSW graph starts to fall apart when using too strict filters.



![HNSW fail](/docs/precision_by_m.png)



![hnsw graph](/docs/graph.gif)



You can find more information on why this happens in our [blog post](https://blog.vasnetsov.com/posts/categorical-hnsw/).

Qdrant solves this problem by extending the HNSW graph with additional edges based on the stored payload values.
Read more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.
- Others use **pre-filtering**, which requires a binary mask of the whole dataset to be passed into the ANN algorithm. It is also not scalable, as the mask size grows linearly with the dataset size.



On top of it, there is also a problem with search accuracy.

It appears if too many vectors are filtered out, so the HNSW graph becomes disconnected.



Qdrant uses a different approach, not requiring pre- or post-filtering while addressing the accuracy problem.
HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
All right, keep going. I like it.



Rishabh Bhardwaj:

Yeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.



Demetrios:

Right.



Rishabh Bhardwaj:\n\nAnswer:",How does Qdrant address the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters?,"Qdrant addresses the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters by extending the HNSW graph with additional edges based on the stored payload values. This extension allows for more efficient searching of nearby vectors using the HNSW index while applying filters during the search in the graph. By adding extra edges, Qdrant minimizes the overhead on condition checks since the conditions only need to be calculated for a small fraction of the points involved in the search. This approach improves the performance of the HNSW index in cases where the traditional methods of weak filters or complete rescore are not optimal, providing a more balanced solution for scenarios that fall in between.","['On the one hand, we cannot apply a full scan on too many vectors. On the other hand, the HNSW graph starts to fall apart when using too strict filters.\n\n\n\n![HNSW fail](/docs/precision_by_m.png)\n\n\n\n![hnsw graph](/docs/graph.gif)\n\n\n\nYou can find more information on why this happens in our [blog post](https://blog.vasnetsov.com/posts/categorical-hnsw/).\n\nQdrant solves this problem by extending the HNSW graph with additional edges based on the stored payload values.', 'Read more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.', '- Others use **pre-filtering**, which requires a binary mask of the whole dataset to be passed into the ANN algorithm. It is also not scalable, as the mask size grows linearly with the dataset size.\n\n\n\nOn top of it, there is also a problem with search accuracy.\n\nIt appears if too many vectors are filtered out, so the HNSW graph becomes disconnected.\n\n\n\nQdrant uses a different approach, not requiring pre- or post-filtering while addressing the accuracy problem.', 'HNSW is chosen for several reasons.\n\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\n\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\n\n\n\n*Available as of v1.1.1*\n\n\n\nThe HNSW parameters can also be configured on a collection and named vector\n\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\n\nperformance.', 'All right, keep going. I like it.\n\n\n\nRishabh Bhardwaj:\n\nYeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.\n\n\n\nDemetrios:\n\nRight.\n\n\n\nRishabh Bhardwaj:']","On the one hand, we cannot apply a full scan on too many vectors. On the other hand, the HNSW graph starts to fall apart when using too strict filters.



![HNSW fail](/docs/precision_by_m.png)



![hnsw graph](/docs/graph.gif)



You can find more information on why this happens in our [blog post](https://blog.vasnetsov.com/posts/categorical-hnsw/).

Qdrant solves this problem by extending the HNSW graph with additional edges based on the stored payload values.
Read more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.
- Others use **pre-filtering**, which requires a binary mask of the whole dataset to be passed into the ANN algorithm. It is also not scalable, as the mask size grows linearly with the dataset size.



On top of it, there is also a problem with search accuracy.

It appears if too many vectors are filtered out, so the HNSW graph becomes disconnected.



Qdrant uses a different approach, not requiring pre- or post-filtering while addressing the accuracy problem.
HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
All right, keep going. I like it.



Rishabh Bhardwaj:

Yeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.



Demetrios:

Right.



Rishabh Bhardwaj:",0.5309734513274337,0.5660377358490566,0.547945205479452,0.32142857142857145,0.34285714285714286,0.3317972350230414,0.3893805309734513,0.41509433962264153,0.4018264840182649,0.48672566371681414,0.5188679245283019,0.5022831050228311,0.933314323425293,0.7032071948051453,0.7083436250686646,0.7057660818099976,0.9976115269237198,0.9807934046839364,125,1.016260162601626,0.48079521742412584
True,,1720.2839999999999,ecf47648,How is metric learning utilized in addressing the challenge of detecting anomalies in coffee beans?,"Metric learning is utilized in addressing the challenge of detecting anomalies in coffee beans by encoding images in an n-dimensional vector space and using learned similarities to label images during the inference process. This approach involves representing the images in a vector space where similar images are closer together in the space. By doing so, the model can effectively classify and detect anomalies in coffee beans based on the similarities learned during training. The KNN (K-Nearest Neighbors) classification method is commonly used in this approach as it simplifies the process of determining the class label of an image based on the labels of its nearest neighbors in the vector space. This method allows for the adaptation to new types of defects and changing shooting conditions, making the model more robust and accurate in detecting anomalies in coffee beans.","Metric learning is utilized in addressing the challenge of detecting anomalies in coffee beans by enabling the system to learn a suitable distance metric to distinguish between normal and anomalous samples. This approach allows for the detection of anomalies with just a small set of labeled data, making it more efficient and cost-effective compared to traditional methods. Additionally, metric learning can adapt to changing conditions and new types of defects without the need for extensive re-training, providing flexibility and scalability in anomaly detection for coffee bean quality assessment.","---

title: Metric Learning for Anomaly Detection

short_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""

description: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.

social_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg

preview_dir: /articles_data/detecting-coffee-anomalies/preview
{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}



We should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it's own.

In the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.



Let's find out how metric learning might help to address this challenge.



## Metric Learning Approach
{{< figure src=/articles_data/detecting-coffee-anomalies/anomalies_detection.png caption=""Production deployment"" >}}



This approach has the following advantages:



* We can benefit from unlabeled data, considering labeling is time-consuming and expensive.

* The relevant metric, e.g., precision or recall, can be tuned according to changing requirements during the inference without re-training.

* Queries labeled with a high score can be added to the KNN classifier on the fly as new data points.
We will describe our approach in the further articles.



This time it converged smoothly, and our evaluation metrics also improved considerably to match the supervised classification approach.



{{< figure src=/articles_data/detecting-coffee-anomalies/ae_report_knn.png caption=""Metrics for the autoencoder model with KNN classifier"" >}}



{{< figure src=/articles_data/detecting-coffee-anomalies/ft_report_knn.png caption=""Metrics for the finetuned model with KNN classifier"" >}}
We are actively looking into these, and we will continue to publish our findings in this challenge and other use cases of metric learning.","['---\n\ntitle: Metric Learning for Anomaly Detection\n\nshort_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""\n\ndescription: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.\n\nsocial_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg\n\npreview_dir: /articles_data/detecting-coffee-anomalies/preview', '{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}\n\n\n\nWe should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it\'s own.\n\nIn the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.\n\n\n\nLet\'s find out how metric learning might help to address this challenge.\n\n\n\n## Metric Learning Approach', '{{< figure src=/articles_data/detecting-coffee-anomalies/anomalies_detection.png caption=""Production deployment"" >}}\n\n\n\nThis approach has the following advantages:\n\n\n\n* We can benefit from unlabeled data, considering labeling is time-consuming and expensive.\n\n* The relevant metric, e.g., precision or recall, can be tuned according to changing requirements during the inference without re-training.\n\n* Queries labeled with a high score can be added to the KNN classifier on the fly as new data points.', 'We will describe our approach in the further articles.\n\n\n\nThis time it converged smoothly, and our evaluation metrics also improved considerably to match the supervised classification approach.\n\n\n\n{{< figure src=/articles_data/detecting-coffee-anomalies/ae_report_knn.png caption=""Metrics for the autoencoder model with KNN classifier"" >}}\n\n\n\n{{< figure src=/articles_data/detecting-coffee-anomalies/ft_report_knn.png caption=""Metrics for the finetuned model with KNN classifier"" >}}', 'We are actively looking into these, and we will continue to publish our findings in this challenge and other use cases of metric learning.']","Question: How is metric learning utilized in addressing the challenge of detecting anomalies in coffee beans?\n\nContext: ---

title: Metric Learning for Anomaly Detection

short_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""

description: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.

social_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg

preview_dir: /articles_data/detecting-coffee-anomalies/preview
{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}



We should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it's own.

In the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.



Let's find out how metric learning might help to address this challenge.



## Metric Learning Approach
{{< figure src=/articles_data/detecting-coffee-anomalies/anomalies_detection.png caption=""Production deployment"" >}}



This approach has the following advantages:



* We can benefit from unlabeled data, considering labeling is time-consuming and expensive.

* The relevant metric, e.g., precision or recall, can be tuned according to changing requirements during the inference without re-training.

* Queries labeled with a high score can be added to the KNN classifier on the fly as new data points.
We will describe our approach in the further articles.



This time it converged smoothly, and our evaluation metrics also improved considerably to match the supervised classification approach.



{{< figure src=/articles_data/detecting-coffee-anomalies/ae_report_knn.png caption=""Metrics for the autoencoder model with KNN classifier"" >}}



{{< figure src=/articles_data/detecting-coffee-anomalies/ft_report_knn.png caption=""Metrics for the finetuned model with KNN classifier"" >}}
We are actively looking into these, and we will continue to publish our findings in this challenge and other use cases of metric learning.\n\nAnswer:",How is metric learning utilized in addressing the challenge of detecting anomalies in coffee beans?,"Metric learning is utilized in addressing the challenge of detecting anomalies in coffee beans by encoding images in an n-dimensional vector space and using learned similarities to label images during the inference process. This approach involves representing the images in a vector space where similar images are closer together in the space. By doing so, the model can effectively classify and detect anomalies in coffee beans based on the similarities learned during training. The KNN (K-Nearest Neighbors) classification method is commonly used in this approach as it simplifies the process of determining the class label of an image based on the labels of its nearest neighbors in the vector space. This method allows for the adaptation to new types of defects and changing shooting conditions, making the model more robust and accurate in detecting anomalies in coffee beans.","['---\n\ntitle: Metric Learning for Anomaly Detection\n\nshort_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""\n\ndescription: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.\n\nsocial_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg\n\npreview_dir: /articles_data/detecting-coffee-anomalies/preview', '{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}\n\n\n\nWe should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it\'s own.\n\nIn the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.\n\n\n\nLet\'s find out how metric learning might help to address this challenge.\n\n\n\n## Metric Learning Approach', '{{< figure src=/articles_data/detecting-coffee-anomalies/anomalies_detection.png caption=""Production deployment"" >}}\n\n\n\nThis approach has the following advantages:\n\n\n\n* We can benefit from unlabeled data, considering labeling is time-consuming and expensive.\n\n* The relevant metric, e.g., precision or recall, can be tuned according to changing requirements during the inference without re-training.\n\n* Queries labeled with a high score can be added to the KNN classifier on the fly as new data points.', 'We will describe our approach in the further articles.\n\n\n\nThis time it converged smoothly, and our evaluation metrics also improved considerably to match the supervised classification approach.\n\n\n\n{{< figure src=/articles_data/detecting-coffee-anomalies/ae_report_knn.png caption=""Metrics for the autoencoder model with KNN classifier"" >}}\n\n\n\n{{< figure src=/articles_data/detecting-coffee-anomalies/ft_report_knn.png caption=""Metrics for the finetuned model with KNN classifier"" >}}', 'We are actively looking into these, and we will continue to publish our findings in this challenge and other use cases of metric learning.']","---

title: Metric Learning for Anomaly Detection

short_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""

description: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.

social_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg

preview_dir: /articles_data/detecting-coffee-anomalies/preview
{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}



We should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it's own.

In the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.



Let's find out how metric learning might help to address this challenge.



## Metric Learning Approach
{{< figure src=/articles_data/detecting-coffee-anomalies/anomalies_detection.png caption=""Production deployment"" >}}



This approach has the following advantages:



* We can benefit from unlabeled data, considering labeling is time-consuming and expensive.

* The relevant metric, e.g., precision or recall, can be tuned according to changing requirements during the inference without re-training.

* Queries labeled with a high score can be added to the KNN classifier on the fly as new data points.
We will describe our approach in the further articles.



This time it converged smoothly, and our evaluation metrics also improved considerably to match the supervised classification approach.



{{< figure src=/articles_data/detecting-coffee-anomalies/ae_report_knn.png caption=""Metrics for the autoencoder model with KNN classifier"" >}}



{{< figure src=/articles_data/detecting-coffee-anomalies/ft_report_knn.png caption=""Metrics for the finetuned model with KNN classifier"" >}}
We are actively looking into these, and we will continue to publish our findings in this challenge and other use cases of metric learning.",0.5952380952380952,0.4032258064516129,0.4807692307692307,0.24096385542168675,0.16260162601626016,0.1941747572815534,0.40476190476190477,0.27419354838709675,0.32692307692307687,0.5238095238095238,0.3548387096774194,0.4230769230769231,0.8049716949462891,0.6929640769958496,0.6579487919807434,0.6750026941299438,0.9853843324429666,0.827370466850698,88,0.6376811594202898,0.5088243961222039
True,,5603.5380000000005,b5035aad,How can one reproduce the benchmark for Open Source vector databases?,"To reproduce the benchmark for Open Source vector databases, one can access the source code available on Github at https://github.com/qdrant/vector-db-benchmark. The repository contains a `README.md` file that provides detailed instructions on how to run the benchmark for a specific engine. By following the guidelines outlined in the README file, individuals can easily replicate the benchmarks conducted on the Open Source vector databases. Additionally, the document emphasizes the importance of transparency in the benchmarking process and invites contributions from individuals who may be able to provide insights, identify misconfigurations, or suggest improvements. Interested parties can contribute to the benchmarking efforts by accessing the benchmark repository at https://github.com/qdrant/vector-db-benchmark.","To reproduce the benchmark for Open Source vector databases, follow these steps:

1. **Access the Source Code**: The benchmark source code is available on [Github](https://github.com/qdrant/vector-db-benchmark). Make sure to download or clone the repository to your local machine.

2. **Read the README.md**: The repository should contain a `README.md` file that describes the process of running the benchmark for a specific engine. This file will provide detailed instructions on how to set up and run the benchmark.

3. **Follow Benchmarking Principles**:
   - Focus on relative numbers: Compare the results based on relative performance rather than absolute numbers.
   - Use affordable hardware: Ensure that you are using hardware that is easily accessible and affordable.
   - Run benchmarks on the same machines: To avoid hardware bias, run the benchmarks on the same exact machines as used in the original benchmark.
   - Contribute and improve: The benchmarks are open-sourced, so feel free to contribute and improve them based on your findings.

4. **Conduct the Benchmark**: Once you have set up the benchmark environment according to the instructions provided, run the benchmark for the specific engine you are interested in testing.

5. **Analyze the Results**: After running the benchmark, analyze the results to compare the performance of the Open Source vector databases based on the metrics provided in the benchmark.

6. **Contribute and Share**: If you have any insights or improvements to the benchmarking process, consider contributing back to the open-source community. Sharing your findings can help others in reproducing and validating the benchmarks.

By following these steps, you can reproduce the benchmark for Open Source vector databases and contribute to the ongoing effort of testing and comparing different vector database engines.","That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.



This is not the final list, and we’ll continue benchmarking as many different engines as possible.



## How to reproduce the benchmark?



The source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.
Here are the principles we followed while designing these benchmarks:



- We do comparative benchmarks, which means we focus on **relative numbers** rather than absolute numbers.

- We use affordable hardware, so that you can reproduce the results easily.

- We run benchmarks on the same exact machines to avoid any possible hardware bias.

- All the benchmarks are [open-sourced](https://github.com/qdrant/vector-db-benchmark), so you can contribute and improve them.



<details>
---

title: Vector Database Benchmarks

description: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.

keywords:

  - vector databases comparative benchmark

  - ANN Benchmark

  - Qdrant vs Milvus

  - Qdrant vs Weaviate

  - Qdrant vs Redis

  - Qdrant vs ElasticSearch

  - benchmark

  - performance

  - latency

  - RPS

  - comparison

  - vector search

  - embedding

preview_image: /benchmarks/benchmark-1.png
description: |

    We benchmarked several vector databases using various configurations of them on different datasets to check how the results may vary. Those datasets may have different vector dimensionality but also vary in terms of the distance function being used. We also tried to capture the difference we can expect while using some different configuration parameters, for both the engine itself and the search operation separately. </br> </br> <b> Updated: January 2024 </b>
---

draft: false

id: 2

title: How vector search should be benchmarked?

weight: 1

---



# Benchmarking Vector Databases","['That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.\n\n\n\nThis is not the final list, and we’ll continue benchmarking as many different engines as possible.\n\n\n\n## How to reproduce the benchmark?\n\n\n\nThe source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.', 'Here are the principles we followed while designing these benchmarks:\n\n\n\n- We do comparative benchmarks, which means we focus on **relative numbers** rather than absolute numbers.\n\n- We use affordable hardware, so that you can reproduce the results easily.\n\n- We run benchmarks on the same exact machines to avoid any possible hardware bias.\n\n- All the benchmarks are [open-sourced](https://github.com/qdrant/vector-db-benchmark), so you can contribute and improve them.\n\n\n\n<details>', '---\n\ntitle: Vector Database Benchmarks\n\ndescription: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.\n\nkeywords:\n\n  - vector databases comparative benchmark\n\n  - ANN Benchmark\n\n  - Qdrant vs Milvus\n\n  - Qdrant vs Weaviate\n\n  - Qdrant vs Redis\n\n  - Qdrant vs ElasticSearch\n\n  - benchmark\n\n  - performance\n\n  - latency\n\n  - RPS\n\n  - comparison\n\n  - vector search\n\n  - embedding\n\npreview_image: /benchmarks/benchmark-1.png', 'description: |\n\n    We benchmarked several vector databases using various configurations of them on different datasets to check how the results may vary. Those datasets may have different vector dimensionality but also vary in terms of the distance function being used. We also tried to capture the difference we can expect while using some different configuration parameters, for both the engine itself and the search operation separately. </br> </br> <b> Updated: January 2024 </b>', '---\n\ndraft: false\n\nid: 2\n\ntitle: How vector search should be benchmarked?\n\nweight: 1\n\n---\n\n\n\n# Benchmarking Vector Databases']","Question: How can one reproduce the benchmark for Open Source vector databases?\n\nContext: That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.



This is not the final list, and we’ll continue benchmarking as many different engines as possible.



## How to reproduce the benchmark?



The source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.
Here are the principles we followed while designing these benchmarks:



- We do comparative benchmarks, which means we focus on **relative numbers** rather than absolute numbers.

- We use affordable hardware, so that you can reproduce the results easily.

- We run benchmarks on the same exact machines to avoid any possible hardware bias.

- All the benchmarks are [open-sourced](https://github.com/qdrant/vector-db-benchmark), so you can contribute and improve them.



<details>
---

title: Vector Database Benchmarks

description: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.

keywords:

- vector databases comparative benchmark

- ANN Benchmark

- Qdrant vs Milvus

- Qdrant vs Weaviate

- Qdrant vs Redis

- Qdrant vs ElasticSearch

- benchmark

- performance

- latency

- RPS

- comparison

- vector search

- embedding

preview_image: /benchmarks/benchmark-1.png
description: |

We benchmarked several vector databases using various configurations of them on different datasets to check how the results may vary. Those datasets may have different vector dimensionality but also vary in terms of the distance function being used. We also tried to capture the difference we can expect while using some different configuration parameters, for both the engine itself and the search operation separately. </br> </br> <b> Updated: January 2024 </b>
---

draft: false

id: 2

title: How vector search should be benchmarked?

weight: 1

---



# Benchmarking Vector Databases\n\nAnswer:",How can one reproduce the benchmark for Open Source vector databases?,"To reproduce the benchmark for Open Source vector databases, one can access the source code available on Github at https://github.com/qdrant/vector-db-benchmark. The repository contains a `README.md` file that provides detailed instructions on how to run the benchmark for a specific engine. By following the guidelines outlined in the README file, individuals can easily replicate the benchmarks conducted on the Open Source vector databases. Additionally, the document emphasizes the importance of transparency in the benchmarking process and invites contributions from individuals who may be able to provide insights, identify misconfigurations, or suggest improvements. Interested parties can contribute to the benchmarking efforts by accessing the benchmark repository at https://github.com/qdrant/vector-db-benchmark.","['That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.\n\n\n\nThis is not the final list, and we’ll continue benchmarking as many different engines as possible.\n\n\n\n## How to reproduce the benchmark?\n\n\n\nThe source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.', 'Here are the principles we followed while designing these benchmarks:\n\n\n\n- We do comparative benchmarks, which means we focus on **relative numbers** rather than absolute numbers.\n\n- We use affordable hardware, so that you can reproduce the results easily.\n\n- We run benchmarks on the same exact machines to avoid any possible hardware bias.\n\n- All the benchmarks are [open-sourced](https://github.com/qdrant/vector-db-benchmark), so you can contribute and improve them.\n\n\n\n<details>', '---\n\ntitle: Vector Database Benchmarks\n\ndescription: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.\n\nkeywords:\n\n  - vector databases comparative benchmark\n\n  - ANN Benchmark\n\n  - Qdrant vs Milvus\n\n  - Qdrant vs Weaviate\n\n  - Qdrant vs Redis\n\n  - Qdrant vs ElasticSearch\n\n  - benchmark\n\n  - performance\n\n  - latency\n\n  - RPS\n\n  - comparison\n\n  - vector search\n\n  - embedding\n\npreview_image: /benchmarks/benchmark-1.png', 'description: |\n\n    We benchmarked several vector databases using various configurations of them on different datasets to check how the results may vary. Those datasets may have different vector dimensionality but also vary in terms of the distance function being used. We also tried to capture the difference we can expect while using some different configuration parameters, for both the engine itself and the search operation separately. </br> </br> <b> Updated: January 2024 </b>', '---\n\ndraft: false\n\nid: 2\n\ntitle: How vector search should be benchmarked?\n\nweight: 1\n\n---\n\n\n\n# Benchmarking Vector Databases']","That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.



This is not the final list, and we’ll continue benchmarking as many different engines as possible.



## How to reproduce the benchmark?



The source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.
Here are the principles we followed while designing these benchmarks:



- We do comparative benchmarks, which means we focus on **relative numbers** rather than absolute numbers.

- We use affordable hardware, so that you can reproduce the results easily.

- We run benchmarks on the same exact machines to avoid any possible hardware bias.

- All the benchmarks are [open-sourced](https://github.com/qdrant/vector-db-benchmark), so you can contribute and improve them.



<details>
---

title: Vector Database Benchmarks

description: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.

keywords:

  - vector databases comparative benchmark

  - ANN Benchmark

  - Qdrant vs Milvus

  - Qdrant vs Weaviate

  - Qdrant vs Redis

  - Qdrant vs ElasticSearch

  - benchmark

  - performance

  - latency

  - RPS

  - comparison

  - vector search

  - embedding

preview_image: /benchmarks/benchmark-1.png
description: |

    We benchmarked several vector databases using various configurations of them on different datasets to check how the results may vary. Those datasets may have different vector dimensionality but also vary in terms of the distance function being used. We also tried to capture the difference we can expect while using some different configuration parameters, for both the engine itself and the search operation separately. </br> </br> <b> Updated: January 2024 </b>
---

draft: false

id: 2

title: How vector search should be benchmarked?

weight: 1

---



# Benchmarking Vector Databases",0.2971887550200803,0.7115384615384616,0.4192634560906515,0.1532258064516129,0.36893203883495146,0.21652421652421652,0.2248995983935743,0.5384615384615384,0.3172804532577903,0.285140562248996,0.6826923076923077,0.40226628895184136,0.9337660670280457,0.6211186051368713,0.7004634737968445,0.6584091782569885,0.5509197596693411,0.9869584514410235,274,2.537037037037037,0.5034760960791334
True,,2621.499,69c8994c,What is the purpose of the `ordering` parameter in Qdrant and when should it be used?,"The `ordering` parameter in Qdrant is used with update and delete operations to ensure that the operations are executed in the same order on all replicas. When this option is enabled, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This is beneficial in preventing data inconsistency that may arise from concurrent updates of the same documents. The `ordering` parameter is particularly recommended when read operations are more frequent than updates and when search performance is critical. By enforcing a specific order of operations across replicas, the `ordering` parameter helps maintain data consistency and integrity in distributed environments.","The purpose of the `ordering` parameter in Qdrant is to ensure that update and delete operations are executed in the same order on all replicas. When this parameter is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This helps to avoid data inconsistency in cases of concurrent updates of the same documents. 

The `ordering` guarantee can be set to `strong`, which is required for some applications where maintaining the order of operations is crucial.

In summary, the `ordering` parameter in Qdrant is used to maintain consistency and avoid data conflicts by ensuring that operations are executed in a specific order across all replicas.","- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
ordering guarantees can be `strong`[^ordered], required for some applications.



The `stream_records` method is currently used as default. This may change in the

future.



## Replication



*Available as of v0.11.0*



Qdrant allows you to replicate shards between nodes in the cluster.



Shard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.
WITH ENGINE = ""qdrant"",

PARAMETERS = {

    ""location"": "":memory:"",

    ""collection_config"": {

        ""size"": 386,

        ""distance"": ""Cosine""

    }

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
## Why Qdrant?","['- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', 'ordering guarantees can be `strong`[^ordered], required for some applications.\n\n\n\nThe `stream_records` method is currently used as default. This may change in the\n\nfuture.\n\n\n\n## Replication\n\n\n\n*Available as of v0.11.0*\n\n\n\nQdrant allows you to replicate shards between nodes in the cluster.\n\n\n\nShard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.', 'WITH ENGINE = ""qdrant"",\n\nPARAMETERS = {\n\n    ""location"": "":memory:"",\n\n    ""collection_config"": {\n\n        ""size"": 386,\n\n        ""distance"": ""Cosine""\n\n    }\n\n}\n\n```\n\n\n\nThe available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).\n\n\n\n## Creating a new table', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", '## Why Qdrant?']","Question: What is the purpose of the `ordering` parameter in Qdrant and when should it be used?\n\nContext: - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
ordering guarantees can be `strong`[^ordered], required for some applications.



The `stream_records` method is currently used as default. This may change in the

future.



## Replication



*Available as of v0.11.0*



Qdrant allows you to replicate shards between nodes in the cluster.



Shard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.
WITH ENGINE = ""qdrant"",

PARAMETERS = {

""location"": "":memory:"",

""collection_config"": {

""size"": 386,

""distance"": ""Cosine""

}

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
## Why Qdrant?\n\nAnswer:",What is the purpose of the `ordering` parameter in Qdrant and when should it be used?,"The `ordering` parameter in Qdrant is used with update and delete operations to ensure that the operations are executed in the same order on all replicas. When this option is enabled, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This is beneficial in preventing data inconsistency that may arise from concurrent updates of the same documents. The `ordering` parameter is particularly recommended when read operations are more frequent than updates and when search performance is critical. By enforcing a specific order of operations across replicas, the `ordering` parameter helps maintain data consistency and integrity in distributed environments.","['- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', 'ordering guarantees can be `strong`[^ordered], required for some applications.\n\n\n\nThe `stream_records` method is currently used as default. This may change in the\n\nfuture.\n\n\n\n## Replication\n\n\n\n*Available as of v0.11.0*\n\n\n\nQdrant allows you to replicate shards between nodes in the cluster.\n\n\n\nShard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.', 'WITH ENGINE = ""qdrant"",\n\nPARAMETERS = {\n\n    ""location"": "":memory:"",\n\n    ""collection_config"": {\n\n        ""size"": 386,\n\n        ""distance"": ""Cosine""\n\n    }\n\n}\n\n```\n\n\n\nThe available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).\n\n\n\n## Creating a new table', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", '## Why Qdrant?']","- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
ordering guarantees can be `strong`[^ordered], required for some applications.



The `stream_records` method is currently used as default. This may change in the

future.



## Replication



*Available as of v0.11.0*



Qdrant allows you to replicate shards between nodes in the cluster.



Shard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.
WITH ENGINE = ""qdrant"",

PARAMETERS = {

    ""location"": "":memory:"",

    ""collection_config"": {

        ""size"": 386,

        ""distance"": ""Cosine""

    }

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
## Why Qdrant?",0.6952380952380952,0.7373737373737373,0.7156862745098039,0.49038461538461536,0.5204081632653061,0.504950495049505,0.5333333333333333,0.5656565656565656,0.5490196078431372,0.6571428571428571,0.696969696969697,0.676470588235294,0.9625539779663086,0.7904313802719116,0.80301833152771,0.7966751456260681,0.9969935169676318,0.7993308540229919,118,1.063063063063063,0.5588860437599185
True,,3744.2850000000003,4ca5dbaf,How can you enable API key based authentication in Qdrant instances for security purposes?,"To enable API key based authentication in Qdrant instances for security purposes, you must specify a key in the configuration. This can be done by adding the following code snippet to your configuration file:

```yaml
service:
  # Set an api-key.
  # If set, all requests must include a header with the api-key.
  # example header: `api-key: <API-KEY>`
  #
  # If you enable this you should also enable TLS.
  # (Either above or via an external service like nginx.)
  # Sending an api-key over an unencrypted channel is insecure.
  api_key: your_secret_api_key_here
```

By adding this configuration, all requests to your Qdrant instance must include a header with the specified API key. This simple form of client authentication helps secure your instance and is available starting from version 1.2.0","To enable API key based authentication in Qdrant instances for security purposes, you can follow these steps:

1. Specify the API key in the Qdrant configuration file:

```yaml
service:
  api_key: your_secret_api_key_here
```

2. Alternatively, you can set the API key using an environment variable:

```bash
export QDRANT__SERVICE__API_KEY=your_secret_api_key_here
```

Remember to use TLS (Transport Layer Security) to encrypt the communication channel and prevent the API key from being leaked over an unencrypted connection.

By enabling API key based authentication, all requests to your Qdrant instance must include a header with the API key. This adds an extra layer of security to your instance.

Additionally, if you are using Qdrant Cloud, you can refer to the Authentication section in the Qdrant Cloud documentation for information on using API key based authentication.

Remember that security measures need to be enabled before using Qdrant instances in production to prevent unauthorized access and protect sensitive data.","This can be used to secure your instance.



To enable API key based authentication in your own Qdrant instance you must

specify a key in the configuration:



```yaml

service:

  # Set an api-key.

  # If set, all requests must include a header with the api-key.

  # example header: `api-key: <API-KEY>`

  #

  # If you enable this you should also enable TLS.

  # (Either above or via an external service like nginx.)

  # Sending an api-key over an unencrypted channel is insecure.
api_key: your_secret_api_key_here

```



Or alternatively, you can use the environment variable:



```bash

export QDRANT__SERVICE__API_KEY=your_secret_api_key_here

```



<aside role=""alert""><a href=""#tls"">TLS</a> must be used to prevent leaking the API key over an unencrypted connection.</aside>



For using API key based authentication in Qdrant cloud see the cloud

[Authentication](https://qdrant.tech/documentation/cloud/authentication)

section.
## Authenticate via SDK



Now that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.

Our official Qdrant clients for Python, TypeScript, Go, Rust, .NET and Java all support the API key parameter. 



```bash

curl \

  -X GET https://xyz-example.eu-central.aws.cloud.qdrant.io:6333 \

  --header 'api-key: <provide-your-own-key>'



# Alternatively, you can use the `Authorization` header with the `Bearer` prefix

curl \
---

title: Security

weight: 165

aliases:

  - ../security

---



# Security







Please read this page carefully. Although there are various ways to secure your Qdrant instances, **they are unsecured by default**. 

You need to enable security measures before production use. Otherwise, they are completely open to anyone



## Authentication



*Available as of v1.2.0*



Qdrant supports a simple form of client authentication using a static API key.

This can be used to secure your instance.
section.



The API key then needs to be present in all REST or gRPC requests to your instance.

All official Qdrant clients for Python, Go, Rust, .NET and Java support the API key parameter.



<!---

Examples with clients

-->



```bash

curl \

  -X GET https://localhost:6333 \

  --header 'api-key: your_secret_api_key_here'

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(

    url=""https://localhost"",

    port=6333,

    api_key=""your_secret_api_key_here"",

)

```","['This can be used to secure your instance.\n\n\n\nTo enable API key based authentication in your own Qdrant instance you must\n\nspecify a key in the configuration:\n\n\n\n```yaml\n\nservice:\n\n  # Set an api-key.\n\n  # If set, all requests must include a header with the api-key.\n\n  # example header: `api-key: <API-KEY>`\n\n  #\n\n  # If you enable this you should also enable TLS.\n\n  # (Either above or via an external service like nginx.)\n\n  # Sending an api-key over an unencrypted channel is insecure.', 'api_key: your_secret_api_key_here\n\n```\n\n\n\nOr alternatively, you can use the environment variable:\n\n\n\n```bash\n\nexport QDRANT__SERVICE__API_KEY=your_secret_api_key_here\n\n```\n\n\n\n<aside role=""alert""><a href=""#tls"">TLS</a> must be used to prevent leaking the API key over an unencrypted connection.</aside>\n\n\n\nFor using API key based authentication in Qdrant cloud see the cloud\n\n[Authentication](https://qdrant.tech/documentation/cloud/authentication)\n\nsection.', ""## Authenticate via SDK\n\n\n\nNow that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.\n\nOur official Qdrant clients for Python, TypeScript, Go, Rust, .NET and Java all support the API key parameter. \n\n\n\n```bash\n\ncurl \\\n\n  -X GET https://xyz-example.eu-central.aws.cloud.qdrant.io:6333 \\\n\n  --header 'api-key: <provide-your-own-key>'\n\n\n\n# Alternatively, you can use the `Authorization` header with the `Bearer` prefix\n\ncurl \\"", '---\n\ntitle: Security\n\nweight: 165\n\naliases:\n\n  - ../security\n\n---\n\n\n\n# Security\n\n\n\n\n\n\n\nPlease read this page carefully. Although there are various ways to secure your Qdrant instances, **they are unsecured by default**. \n\nYou need to enable security measures before production use. Otherwise, they are completely open to anyone\n\n\n\n## Authentication\n\n\n\n*Available as of v1.2.0*\n\n\n\nQdrant supports a simple form of client authentication using a static API key.\n\nThis can be used to secure your instance.', 'section.\n\n\n\nThe API key then needs to be present in all REST or gRPC requests to your instance.\n\nAll official Qdrant clients for Python, Go, Rust, .NET and Java support the API key parameter.\n\n\n\n<!---\n\nExamples with clients\n\n-->\n\n\n\n```bash\n\ncurl \\\n\n  -X GET https://localhost:6333 \\\n\n  --header \'api-key: your_secret_api_key_here\'\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(\n\n    url=""https://localhost"",\n\n    port=6333,\n\n    api_key=""your_secret_api_key_here"",\n\n)\n\n```']","Question: How can you enable API key based authentication in Qdrant instances for security purposes?\n\nContext: This can be used to secure your instance.



To enable API key based authentication in your own Qdrant instance you must

specify a key in the configuration:



```yaml

service:

# Set an api-key.

# If set, all requests must include a header with the api-key.

# example header: `api-key: <API-KEY>`

#

# If you enable this you should also enable TLS.

# (Either above or via an external service like nginx.)

# Sending an api-key over an unencrypted channel is insecure.
api_key: your_secret_api_key_here

```



Or alternatively, you can use the environment variable:



```bash

export QDRANT__SERVICE__API_KEY=your_secret_api_key_here

```



<aside role=""alert""><a href=""#tls"">TLS</a> must be used to prevent leaking the API key over an unencrypted connection.</aside>



For using API key based authentication in Qdrant cloud see the cloud

[Authentication](https://qdrant.tech/documentation/cloud/authentication)

section.
## Authenticate via SDK



Now that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.

Our official Qdrant clients for Python, TypeScript, Go, Rust, .NET and Java all support the API key parameter. 



```bash

curl \

-X GET https://xyz-example.eu-central.aws.cloud.qdrant.io:6333 \

--header 'api-key: <provide-your-own-key>'



# Alternatively, you can use the `Authorization` header with the `Bearer` prefix

curl \
---

title: Security

weight: 165

aliases:

- ../security

---



# Security







Please read this page carefully. Although there are various ways to secure your Qdrant instances, **they are unsecured by default**. 

You need to enable security measures before production use. Otherwise, they are completely open to anyone



## Authentication



*Available as of v1.2.0*



Qdrant supports a simple form of client authentication using a static API key.

This can be used to secure your instance.
section.



The API key then needs to be present in all REST or gRPC requests to your instance.

All official Qdrant clients for Python, Go, Rust, .NET and Java support the API key parameter.



<!---

Examples with clients

-->



```bash

curl \

-X GET https://localhost:6333 \

--header 'api-key: your_secret_api_key_here'

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(

url=""https://localhost"",

port=6333,

api_key=""your_secret_api_key_here"",

)

```\n\nAnswer:",How can you enable API key based authentication in Qdrant instances for security purposes?,"To enable API key based authentication in Qdrant instances for security purposes, you must specify a key in the configuration. This can be done by adding the following code snippet to your configuration file:

```yaml
service:
  # Set an api-key.
  # If set, all requests must include a header with the api-key.
  # example header: `api-key: <API-KEY>`
  #
  # If you enable this you should also enable TLS.
  # (Either above or via an external service like nginx.)
  # Sending an api-key over an unencrypted channel is insecure.
  api_key: your_secret_api_key_here
```

By adding this configuration, all requests to your Qdrant instance must include a header with the specified API key. This simple form of client authentication helps secure your instance and is available starting from version 1.2.0","['This can be used to secure your instance.\n\n\n\nTo enable API key based authentication in your own Qdrant instance you must\n\nspecify a key in the configuration:\n\n\n\n```yaml\n\nservice:\n\n  # Set an api-key.\n\n  # If set, all requests must include a header with the api-key.\n\n  # example header: `api-key: <API-KEY>`\n\n  #\n\n  # If you enable this you should also enable TLS.\n\n  # (Either above or via an external service like nginx.)\n\n  # Sending an api-key over an unencrypted channel is insecure.', 'api_key: your_secret_api_key_here\n\n```\n\n\n\nOr alternatively, you can use the environment variable:\n\n\n\n```bash\n\nexport QDRANT__SERVICE__API_KEY=your_secret_api_key_here\n\n```\n\n\n\n<aside role=""alert""><a href=""#tls"">TLS</a> must be used to prevent leaking the API key over an unencrypted connection.</aside>\n\n\n\nFor using API key based authentication in Qdrant cloud see the cloud\n\n[Authentication](https://qdrant.tech/documentation/cloud/authentication)\n\nsection.', ""## Authenticate via SDK\n\n\n\nNow that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.\n\nOur official Qdrant clients for Python, TypeScript, Go, Rust, .NET and Java all support the API key parameter. \n\n\n\n```bash\n\ncurl \\\n\n  -X GET https://xyz-example.eu-central.aws.cloud.qdrant.io:6333 \\\n\n  --header 'api-key: <provide-your-own-key>'\n\n\n\n# Alternatively, you can use the `Authorization` header with the `Bearer` prefix\n\ncurl \\"", '---\n\ntitle: Security\n\nweight: 165\n\naliases:\n\n  - ../security\n\n---\n\n\n\n# Security\n\n\n\n\n\n\n\nPlease read this page carefully. Although there are various ways to secure your Qdrant instances, **they are unsecured by default**. \n\nYou need to enable security measures before production use. Otherwise, they are completely open to anyone\n\n\n\n## Authentication\n\n\n\n*Available as of v1.2.0*\n\n\n\nQdrant supports a simple form of client authentication using a static API key.\n\nThis can be used to secure your instance.', 'section.\n\n\n\nThe API key then needs to be present in all REST or gRPC requests to your instance.\n\nAll official Qdrant clients for Python, Go, Rust, .NET and Java support the API key parameter.\n\n\n\n<!---\n\nExamples with clients\n\n-->\n\n\n\n```bash\n\ncurl \\\n\n  -X GET https://localhost:6333 \\\n\n  --header \'api-key: your_secret_api_key_here\'\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(\n\n    url=""https://localhost"",\n\n    port=6333,\n\n    api_key=""your_secret_api_key_here"",\n\n)\n\n```']","This can be used to secure your instance.



To enable API key based authentication in your own Qdrant instance you must

specify a key in the configuration:



```yaml

service:

  # Set an api-key.

  # If set, all requests must include a header with the api-key.

  # example header: `api-key: <API-KEY>`

  #

  # If you enable this you should also enable TLS.

  # (Either above or via an external service like nginx.)

  # Sending an api-key over an unencrypted channel is insecure.
api_key: your_secret_api_key_here

```



Or alternatively, you can use the environment variable:



```bash

export QDRANT__SERVICE__API_KEY=your_secret_api_key_here

```



<aside role=""alert""><a href=""#tls"">TLS</a> must be used to prevent leaking the API key over an unencrypted connection.</aside>



For using API key based authentication in Qdrant cloud see the cloud

[Authentication](https://qdrant.tech/documentation/cloud/authentication)

section.
## Authenticate via SDK



Now that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.

Our official Qdrant clients for Python, TypeScript, Go, Rust, .NET and Java all support the API key parameter. 



```bash

curl \

  -X GET https://xyz-example.eu-central.aws.cloud.qdrant.io:6333 \

  --header 'api-key: <provide-your-own-key>'



# Alternatively, you can use the `Authorization` header with the `Bearer` prefix

curl \
---

title: Security

weight: 165

aliases:

  - ../security

---



# Security







Please read this page carefully. Although there are various ways to secure your Qdrant instances, **they are unsecured by default**. 

You need to enable security measures before production use. Otherwise, they are completely open to anyone



## Authentication



*Available as of v1.2.0*



Qdrant supports a simple form of client authentication using a static API key.

This can be used to secure your instance.
section.



The API key then needs to be present in all REST or gRPC requests to your instance.

All official Qdrant clients for Python, Go, Rust, .NET and Java support the API key parameter.



<!---

Examples with clients

-->



```bash

curl \

  -X GET https://localhost:6333 \

  --header 'api-key: your_secret_api_key_here'

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(

    url=""https://localhost"",

    port=6333,

    api_key=""your_secret_api_key_here"",

)

```",0.5066666666666667,0.6333333333333333,0.5629629629629629,0.30201342281879195,0.37815126050420167,0.33582089552238803,0.36,0.45,0.39999999999999997,0.48,0.6,0.5333333333333332,0.9513025283813477,0.671640157699585,0.6934205889701843,0.682356595993042,0.9659937136340886,0.9804156211903319,149,1.2521008403361344,0.7601666613311837
