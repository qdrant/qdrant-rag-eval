input_text,answer,documents,context
What is vaccum optimizer ?,"The Vacuum Optimizer is a feature in the Qdrant database designed to manage and improve system performance by removing obsolete data. In Qdrant, when records are deleted, they are not immediately removed from the database. Instead, they are marked as deleted and subsequently ignored in future queries. This approach helps minimize disk access but results in accumulated deleted records that can occupy valuable memory and degrade performance over time.
The Vacuum Optimizer addresses this issue by systematically clearing out these marked-as-deleted records once they reach a certain threshold. This threshold is configurable and includes conditions such as the minimum fraction of deleted vectors in a segment and the minimum number of vectors present in a segment. ","['""indexed_vectors_count"": 1024232,\n\n        ""points_count"": 1068786,\n\n        ""segments_count"": 31,\n\n        ""config"": {\n\n            ""params"": {\n\n                ""vectors"": {\n\n                    ""size"": 384,\n\n                    ""distance"": ""Cosine""\n\n                },\n\n                ""shard_number"": 1,\n\n                ""replication_factor"": 1,\n\n                ""write_consistency_factor"": 1,\n\n                ""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,\n\n                ""memmap_threshold"": null,\n\n                ""indexing_threshold"": 20000,\n\n                ""flush_interval_sec"": 5,', 'client.updateCollectionAsync(\n\n    UpdateCollection.newBuilder()\n\n        .setCollectionName(""{collection_name}"")\n\n        .setOptimizersConfig(\n\n            OptimizersConfigDiff.newBuilder().setIndexingThreshold(10000).build())\n\n        .build());\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.UpdateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\toptimizersConfig: new OptimizersConfigDiff { IndexingThreshold = 10000 }\n\n);\n\n```\n\n\n\nThe following parameters can be updated:\n\n\n\n* `optimizers_config` - see [optimizer](../optimizer/) for details.\n\n* `hnsw_config` - see [indexing](../indexing/#vector-index) for details.\n\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\n\n* `vectors` - vector-specific configuration, including individual `hnsw_config`, `quantization_config` and `on_disk` settings.', 'optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 },\n\n\tquantizationConfig: new QuantizationConfig\n\n\t{\n\n\t\tScalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }\n\n\t}\n\n);\n\n```\n\n\n\nThere are also some search-time parameters you can use to tune the search accuracy and speed:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{\n\n    ""params"": {\n\n        ""hnsw_ef"": 128,\n\n        ""exact"": false\n\n    },\n\n    ""vector"": [0.2, 0.1, 0.9, 0.7],\n\n    ""limit"": 3\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.search(\n\n    collection_name=""{collection_name}"",\n\n    search_params=models.SearchParams(hnsw_ef=128, exact=False),\n\n    query_vector=[0.2, 0.1, 0.9, 0.7],\n\n    limit=3,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.search(""{collection_name}"", {']","""indexed_vectors_count"": 1024232,

        ""points_count"": 1068786,

        ""segments_count"": 31,

        ""config"": {

            ""params"": {

                ""vectors"": {

                    ""size"": 384,

                    ""distance"": ""Cosine""

                },

                ""shard_number"": 1,

                ""replication_factor"": 1,

                ""write_consistency_factor"": 1,

                ""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,

                ""memmap_threshold"": null,

                ""indexing_threshold"": 20000,

                ""flush_interval_sec"": 5,
client.updateCollectionAsync(

    UpdateCollection.newBuilder()

        .setCollectionName(""{collection_name}"")

        .setOptimizersConfig(

            OptimizersConfigDiff.newBuilder().setIndexingThreshold(10000).build())

        .build());

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.UpdateCollectionAsync(

	collectionName: ""{collection_name}"",

	optimizersConfig: new OptimizersConfigDiff { IndexingThreshold = 10000 }

);

```



The following parameters can be updated:



* `optimizers_config` - see [optimizer](../optimizer/) for details.

* `hnsw_config` - see [indexing](../indexing/#vector-index) for details.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.

* `vectors` - vector-specific configuration, including individual `hnsw_config`, `quantization_config` and `on_disk` settings.
optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 },

	quantizationConfig: new QuantizationConfig

	{

		Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }

	}

);

```



There are also some search-time parameters you can use to tune the search accuracy and speed:



```http

POST /collections/{collection_name}/points/search

{

    ""params"": {

        ""hnsw_ef"": 128,

        ""exact"": false

    },

    ""vector"": [0.2, 0.1, 0.9, 0.7],

    ""limit"": 3

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.search(

    collection_name=""{collection_name}"",

    search_params=models.SearchParams(hnsw_ef=128, exact=False),

    query_vector=[0.2, 0.1, 0.9, 0.7],

    limit=3,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.search(""{collection_name}"", {"
Tell me about ‘always_ram’ parameter?,"In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times.","['""indexed_vectors_count"": 1024232,\n\n        ""points_count"": 1068786,\n\n        ""segments_count"": 31,\n\n        ""config"": {\n\n            ""params"": {\n\n                ""vectors"": {\n\n                    ""size"": 384,\n\n                    ""distance"": ""Cosine""\n\n                },\n\n                ""shard_number"": 1,\n\n                ""replication_factor"": 1,\n\n                ""write_consistency_factor"": 1,\n\n                ""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,\n\n                ""memmap_threshold"": null,\n\n                ""indexing_threshold"": 20000,\n\n                ""flush_interval_sec"": 5,', 'This mode is enabled by setting `always_ram` to `true` in the quantization config while using memmap storage:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),\n\n    quantization_config=models.ScalarQuantization(\n\n        scalar=models.ScalarQuantizationConfig(\n\n            type=models.ScalarType.INT8,\n\n            always_ram=True,\n\n        ),\n\n    ),\n\n)\n\n```\n\n\n\n```typescript', 'always_ram: Some(true),\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.BinaryQuantization;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.QuantizationConfig;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()\n\n                    .setParams(\n\n                        VectorParams.newBuilder()\n\n                            .setSize(1536)']","""indexed_vectors_count"": 1024232,

        ""points_count"": 1068786,

        ""segments_count"": 31,

        ""config"": {

            ""params"": {

                ""vectors"": {

                    ""size"": 384,

                    ""distance"": ""Cosine""

                },

                ""shard_number"": 1,

                ""replication_factor"": 1,

                ""write_consistency_factor"": 1,

                ""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,

                ""memmap_threshold"": null,

                ""indexing_threshold"": 20000,

                ""flush_interval_sec"": 5,
This mode is enabled by setting `always_ram` to `true` in the quantization config while using memmap storage:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": true

        }

    }

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),

    quantization_config=models.ScalarQuantization(

        scalar=models.ScalarQuantizationConfig(

            type=models.ScalarType.INT8,

            always_ram=True,

        ),

    ),

)

```



```typescript
always_ram: Some(true),

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.BinaryQuantization;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.QuantizationConfig;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()

                    .setParams(

                        VectorParams.newBuilder()

                            .setSize(1536)"
What is difference between scalar and product quantization?,"While both methods aim to reduce the memory footprint and potentially speed up operations, scalar quantization offers faster processing with SIMD-friendly operations and minimal accuracy loss, making it suitable for scenarios where speed is critical. Product quantization achieves higher compression rates and is better suited for handling very high-dimensional vectors where memory space is more of a concern than computation speed.","['Product quantization can compress by a more prominent factor than a scalar one.\n\nBut there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\n\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\n\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n## How to choose the right quantization method\n\n\n\nHere is a brief table of the pros and cons of each quantization method:\n\n\n\n| Quantization method | Accuracy | Speed        | Compression |\n\n|---------------------|----------|--------------|-------------|\n\n| Scalar              | 0.99     | up to x2     | 4           |\n\n| Product             | 0.7      | 0.5          | up to 64    |\n\n| Binary              | 0.95*    | up to x40    | 32          |\n\n\n\n`*` - for compatible models', 'but also the search time.\n\n\n\n## Good practices\n\n\n\nCompared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\n\n\n\nProduct Quantization tends to be favored in certain specific scenarios:\n\n\n\n- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself\n\n- Situations where the dimensionality of the original vectors is sufficiently high\n\n- Cases where indexing speed is not a critical factor\n\n\n\nIn circumstances that do not align with the above, Scalar Quantization should be the preferred choice.\n\n\n\nQdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization) \n\nwill help you to set and configure the new quantization for your data and achieve even \n\nup to 64x memory reduction.', '. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.']","Product quantization can compress by a more prominent factor than a scalar one.

But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method



Here is a brief table of the pros and cons of each quantization method:



| Quantization method | Accuracy | Speed        | Compression |

|---------------------|----------|--------------|-------------|

| Scalar              | 0.99     | up to x2     | 4           |

| Product             | 0.7      | 0.5          | up to 64    |

| Binary              | 0.95*    | up to x40    | 32          |



`*` - for compatible models
but also the search time.



## Good practices



Compared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.



Product Quantization tends to be favored in certain specific scenarios:



- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself

- Situations where the dimensionality of the original vectors is sufficiently high

- Cases where indexing speed is not a critical factor



In circumstances that do not align with the above, Scalar Quantization should be the preferred choice.



Qdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization) 

will help you to set and configure the new quantization for your data and achieve even 

up to 64x memory reduction.
. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors."
What is ‘best_score’ strategy?,"The `best_score` strategy is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided.","['```rust\n\nlet score = if best_positive_score > best_negative_score {\n\n    best_positive_score;\n\n} else {\n\n    -(best_negative_score * best_negative_score);\n\n};\n\n```\n\n\n\n<aside role=""alert"">\n\nThe performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.\n\n</aside>\n\n\n\nSince we are computing similarities to every example at each step of the search, the performance of this strategy will be linearly impacted by the amount of examples. This means that the more examples you provide, the slower the search will be. However, this strategy can be very powerful and should be more embedding-agnostic.\n\n\n\n<aside role=""status"">\n\nAccuracy may be impacted with this strategy. To improve it, increasing the <code>ef</code> search parameter to something above 32 will already be much better than the default 16, e.g: <code>""params"": { ""ef"": 64 }</code>\n\n</aside>\n\n\n\nTo use this algorithm, you need to set `""strategy"": ""best_score""` in the recommendation request.', '</aside>\n\n\n\nTo use this algorithm, you need to set `""strategy"": ""best_score""` in the recommendation request.\n\n\n\n#### Using only negative examples\n\n\n\nA beneficial side-effect of `best_score` strategy is that you can use it with only negative examples. This will allow you to find the most dissimilar vectors to the ones you provide. This can be useful for finding outliers in your data, or for finding the most dissimilar vectors to a given one.\n\n\n\nCombining negative-only examples with filtering can be a powerful tool for data exploration and cleaning.\n\n\n\n### Multiple vectors\n\n\n\n*Available as of v0.10.0*\n\n\n\nIf the collection was created with multiple vectors, the name of the vector should be specified in the recommendation request:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/recommend\n\n{\n\n  ""positive"": [100, 231],\n\n  ""negative"": [718],\n\n  ""using"": ""image"",\n\n  ""limit"": 10\n\n }\n\n```\n\n\n\n```python\n\nclient.recommend(\n\n    collection_name=""{collection_name}"",\n\n    positive=[100, 231],\n\n    negative=[718],', '{ ""id"": 14, ""score"": 0.75 },\n\n    { ""id"": 11, ""score"": 0.73 }\n\n  ],\n\n  ""status"": ""ok"",\n\n  ""time"": 0.001\n\n}\n\n```\n\n\n\nThe algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.\n\n\n\n### Average vector strategy\n\n\n\nThe default and first strategy added to Qdrant is called `average_vector`. It preprocesses the input examples to create a single vector that is used for the search. Since the preprocessing step happens very fast, the performance of this strategy is on-par with regular search. The intuition behind this kind of recommendation is that each vector component represents an independent feature of the data, so, by averaging the examples, we should get a good recommendation.']","```rust

let score = if best_positive_score > best_negative_score {

    best_positive_score;

} else {

    -(best_negative_score * best_negative_score);

};

```



<aside role=""alert"">

The performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.

</aside>



Since we are computing similarities to every example at each step of the search, the performance of this strategy will be linearly impacted by the amount of examples. This means that the more examples you provide, the slower the search will be. However, this strategy can be very powerful and should be more embedding-agnostic.



<aside role=""status"">

Accuracy may be impacted with this strategy. To improve it, increasing the <code>ef</code> search parameter to something above 32 will already be much better than the default 16, e.g: <code>""params"": { ""ef"": 64 }</code>

</aside>



To use this algorithm, you need to set `""strategy"": ""best_score""` in the recommendation request.
</aside>



To use this algorithm, you need to set `""strategy"": ""best_score""` in the recommendation request.



#### Using only negative examples



A beneficial side-effect of `best_score` strategy is that you can use it with only negative examples. This will allow you to find the most dissimilar vectors to the ones you provide. This can be useful for finding outliers in your data, or for finding the most dissimilar vectors to a given one.



Combining negative-only examples with filtering can be a powerful tool for data exploration and cleaning.



### Multiple vectors



*Available as of v0.10.0*



If the collection was created with multiple vectors, the name of the vector should be specified in the recommendation request:



```http

POST /collections/{collection_name}/points/recommend

{

  ""positive"": [100, 231],

  ""negative"": [718],

  ""using"": ""image"",

  ""limit"": 10

 }

```



```python

client.recommend(

    collection_name=""{collection_name}"",

    positive=[100, 231],

    negative=[718],
{ ""id"": 14, ""score"": 0.75 },

    { ""id"": 11, ""score"": 0.73 }

  ],

  ""status"": ""ok"",

  ""time"": 0.001

}

```



The algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.



### Average vector strategy



The default and first strategy added to Qdrant is called `average_vector`. It preprocesses the input examples to create a single vector that is used for the search. Since the preprocessing step happens very fast, the performance of this strategy is on-par with regular search. The intuition behind this kind of recommendation is that each vector component represents an independent feature of the data, so, by averaging the examples, we should get a good recommendation."
How does oversampling helps?,"Defines how many extra vectors should be pre-selected using quantized index, and then re-scored using original vectors. For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring. Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.","['index=""limit"", columns=[""oversampling"", ""rescore""], values=""accuracy""\n\n    )\n\n    print(acc)\n\n```\n\n\n\n#### Impact of Oversampling\n\n\n\nYou can use oversampling in machine learning to counteract imbalances in datasets.\n\nIt works well when one class significantly outnumbers others. This imbalance\n\ncan skew the performance of models, which favors the majority class at the\n\nexpense of others. By creating additional samples from the minority classes,\n\noversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.\n\n\n\nThe screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren\'t shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one.', '**Oversampling:**\n\nIn the figure below, we illustrate the relationship between recall and number of candidates:\n\n\n\n![Correct vs candidates](/articles_data/binary-quantization/bq-5.png)\n\n\n\nWe see that ""correct"" results i.e. recall increases as the number of potential ""candidates"" increase (limit x oversampling). To highlight the impact of changing the `limit`, different limit values are broken apart into different curves. For example, we see that the lowest recall for limit 50 is around 94 correct, with 100 candidates. This also implies we used an oversampling of 2.0\n\n\n\nAs oversampling increases, we see a general improvement in results – but that does not hold in every case. \n\n\n\n**Rescore:**\n\nAs expected, rescoring increases the time it takes to return a query. \n\nWe also repeated the experiment with oversampling except this time we looked at how rescore impacted result accuracy. \n\n\n\n![Relationship between limit and rescore on correct](/articles_data/binary-quantization/bq-7.png)\n\n\n\n**Limit:**', 'from qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.search(\n\n    collection_name=""{collection_name}"",\n\n    query_vector=[0.2, 0.1, 0.9, 0.7],\n\n    search_params=models.SearchParams(\n\n        quantization=models.QuantizationSearchParams(\n\n            ignore=False,\n\n            rescore=True,\n\n            oversampling=2.4\n\n        )\n\n    )\n\n)\n\n```\n\n\n\nIn this case, if `oversampling` is 2.4 and `limit` is 100, then 240 vectors will be pre-selected using quantized index, and then the top 100 points will be returned after re-scoring with the unquantized vectors.\n\n\n\nAs you can see from the example above, this parameter is set during the query. This is a flexible method that will let you tune query accuracy. While the index is not changed, you can decide how many points you want to retrieve using quantized vectors.\n\n\n\n### Grouping API lookup']","index=""limit"", columns=[""oversampling"", ""rescore""], values=""accuracy""

    )

    print(acc)

```



#### Impact of Oversampling



You can use oversampling in machine learning to counteract imbalances in datasets.

It works well when one class significantly outnumbers others. This imbalance

can skew the performance of models, which favors the majority class at the

expense of others. By creating additional samples from the minority classes,

oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.



The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one.
**Oversampling:**

In the figure below, we illustrate the relationship between recall and number of candidates:



![Correct vs candidates](/articles_data/binary-quantization/bq-5.png)



We see that ""correct"" results i.e. recall increases as the number of potential ""candidates"" increase (limit x oversampling). To highlight the impact of changing the `limit`, different limit values are broken apart into different curves. For example, we see that the lowest recall for limit 50 is around 94 correct, with 100 candidates. This also implies we used an oversampling of 2.0



As oversampling increases, we see a general improvement in results – but that does not hold in every case. 



**Rescore:**

As expected, rescoring increases the time it takes to return a query. 

We also repeated the experiment with oversampling except this time we looked at how rescore impacted result accuracy. 



![Relationship between limit and rescore on correct](/articles_data/binary-quantization/bq-7.png)



**Limit:**
from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.search(

    collection_name=""{collection_name}"",

    query_vector=[0.2, 0.1, 0.9, 0.7],

    search_params=models.SearchParams(

        quantization=models.QuantizationSearchParams(

            ignore=False,

            rescore=True,

            oversampling=2.4

        )

    )

)

```



In this case, if `oversampling` is 2.4 and `limit` is 100, then 240 vectors will be pre-selected using quantized index, and then the top 100 points will be returned after re-scoring with the unquantized vectors.



As you can see from the example above, this parameter is set during the query. This is a flexible method that will let you tune query accuracy. While the index is not changed, you can decide how many points you want to retrieve using quantized vectors.



### Grouping API lookup"
What is the purpose of ‘CreatePayloadIndexAsync’?,CreatePayloadIndexAsync is a method in the Qdrant library that enables the creation of a payload index in Qdrant. A payload index is a data structure designed to store supplemental information about the data stored in Qdrant. This method requires a Qdrant instance and a payload index name as input parameters.,"['client\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        PayloadSchemaType.Text,\n\n        PayloadIndexParams.newBuilder()\n\n            .setTextIndexParams(\n\n                TextIndexParams.newBuilder()\n\n                    .setTokenizer(TokenizerType.Word)\n\n                    .setMinTokenLen(2)\n\n                    .setMaxTokenLen(10)\n\n                    .setLowercase(true)\n\n                    .build())\n\n            .build(),\n\n        null,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tfieldName: ""name_of_the_field_to_index"",\n\n\tschemaType: PayloadSchemaType.Text,\n\n\tindexParams: new PayloadIndexParams\n\n\t{\n\n\t\tTextIndexParams = new TextIndexParams\n\n\t\t{\n\n\t\t\tTokenizer = TokenizerType.Word,\n\n\t\t\tMinTokenLen = 2,\n\n\t\t\tMaxTokenLen = 10,\n\n\t\t\tLowercase = true', 'const client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createPayloadIndex(""{collection_name}"", {\n\n  field_name: ""name_of_the_field_to_index"",\n\n  field_schema: ""keyword"",\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{client::QdrantClient, qdrant::FieldType};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_field_index(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        PayloadSchemaType.Keyword,\n\n        null,\n\n        null,\n\n        null,\n\n        null)', '{\n\n                        ""id"": 1,\n\n                        ""vector"": [1.0, 2.0, 3.0, 4.0],\n\n                        ""payload"": {}\n\n                    }\n\n                ]\n\n            }\n\n        },\n\n        {\n\n            ""update_vectors"": {\n\n                ""points"": [\n\n                    {\n\n                        ""id"": 1,\n\n                        ""vector"": [1.0, 2.0, 3.0, 4.0]\n\n                    }\n\n                ]\n\n            }\n\n        },\n\n        {\n\n            ""delete_vectors"": {\n\n                ""points"": [1],\n\n                ""vector"": [""""]\n\n            }\n\n        },\n\n        {\n\n            ""overwrite_payload"": {\n\n                ""payload"": {\n\n                    ""test_payload"": ""1""\n\n                },\n\n                ""points"": [1]\n\n            }\n\n        },\n\n        {\n\n            ""set_payload"": {\n\n                ""payload"": {\n\n                    ""test_payload_2"": ""2"",\n\n                    ""test_payload_3"": ""3""\n\n                },\n\n                ""points"": [1]']","client

    .createPayloadIndexAsync(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        PayloadSchemaType.Text,

        PayloadIndexParams.newBuilder()

            .setTextIndexParams(

                TextIndexParams.newBuilder()

                    .setTokenizer(TokenizerType.Word)

                    .setMinTokenLen(2)

                    .setMaxTokenLen(10)

                    .setLowercase(true)

                    .build())

            .build(),

        null,

        null,

        null)

    .get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(

	collectionName: ""{collection_name}"",

	fieldName: ""name_of_the_field_to_index"",

	schemaType: PayloadSchemaType.Text,

	indexParams: new PayloadIndexParams

	{

		TextIndexParams = new TextIndexParams

		{

			Tokenizer = TokenizerType.Word,

			MinTokenLen = 2,

			MaxTokenLen = 10,

			Lowercase = true
const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createPayloadIndex(""{collection_name}"", {

  field_name: ""name_of_the_field_to_index"",

  field_schema: ""keyword"",

});

```



```rust

use qdrant_client::{client::QdrantClient, qdrant::FieldType};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_field_index(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        FieldType::Keyword,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createPayloadIndexAsync(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        PayloadSchemaType.Keyword,

        null,

        null,

        null,

        null)
{

                        ""id"": 1,

                        ""vector"": [1.0, 2.0, 3.0, 4.0],

                        ""payload"": {}

                    }

                ]

            }

        },

        {

            ""update_vectors"": {

                ""points"": [

                    {

                        ""id"": 1,

                        ""vector"": [1.0, 2.0, 3.0, 4.0]

                    }

                ]

            }

        },

        {

            ""delete_vectors"": {

                ""points"": [1],

                ""vector"": [""""]

            }

        },

        {

            ""overwrite_payload"": {

                ""payload"": {

                    ""test_payload"": ""1""

                },

                ""points"": [1]

            }

        },

        {

            ""set_payload"": {

                ""payload"": {

                    ""test_payload_2"": ""2"",

                    ""test_payload_3"": ""3""

                },

                ""points"": [1]"
What is the purpose of ef_construct in HNSW ?,"In HNSW algorithm the ef_construct parameter is the number of neighbours to consider during the index building. The larger the value, the higher the precision, but the longer the indexing time. The default values of this parameters 100","['hnsw_ef: Some(128),\n\n            exact: Some(false),\n\n            ..Default::default()\n\n        }),\n\n        vector: vec![0.2, 0.1, 0.9, 0.7],\n\n        limit: 3,\n\n        read_consistency: Some(ReadConsistency {\n\n            value: Some(Value::Type(ReadConsistencyType::Majority.into())),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport java.util.List;\n\n\n\nimport static io.qdrant.client.ConditionFactory.matchKeyword;\n\n\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Points.Filter;\n\nimport io.qdrant.client.grpc.Points.ReadConsistency;\n\nimport io.qdrant.client.grpc.Points.ReadConsistencyType;\n\nimport io.qdrant.client.grpc.Points.SearchParams;\n\nimport io.qdrant.client.grpc.Points.SearchPoints;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .searchAsync(\n\n        SearchPoints.newBuilder()', ""ef_construct: 100\n\n    # Minimal size (in KiloBytes) of vectors for additional payload-based indexing.\n\n    # If payload chunk is smaller than `full_scan_threshold_kb` additional indexing won't be used -\n\n    # in this case full-scan search should be preferred by query planner and additional indexing is not required.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    full_scan_threshold_kb: 10000\n\n    # Number of parallel threads used for background index building. If 0 - auto selection.\n\n    max_indexing_threads: 0\n\n    # Store HNSW index on disk. If set to false, index will be stored in RAM. Default: false\n\n    on_disk: false\n\n    # Custom M param for hnsw graph built for payload index. If not set, default M will be used.\n\n    payload_m: null\n\n\n\n\n\nservice:\n\n\n\n  # Maximum size of POST data in a single request in megabytes\n\n  max_request_size_mb: 32\n\n\n\n  # Number of parallel workers used for serving the api. If 0 - equal to the number of available cores.\n\n  # If missing - Same as storage.max_search_threads"", '## Tweaking the HNSW parameters\n\n\n\nHNSW is a hierarchical graph, where each node has a set of links to other nodes. The number of edges per node is called the `m` parameter. \n\nThe larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \n\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\n\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let\'s try to increase them to `m=32` and `ef_construct=200` and\n\nsee how it affects the precision. Of course, we need to wait until the indexing is finished before we can perform the search.\n\n\n\n```python\n\nclient.update_collection(\n\n    collection_name=""arxiv-titles-instructorxl-embeddings"",\n\n    hnsw_config=models.HnswConfigDiff(\n\n        m=32,  # Increase the number of edges per node from the default 16 to 32']","hnsw_ef: Some(128),

            exact: Some(false),

            ..Default::default()

        }),

        vector: vec![0.2, 0.1, 0.9, 0.7],

        limit: 3,

        read_consistency: Some(ReadConsistency {

            value: Some(Value::Type(ReadConsistencyType::Majority.into())),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import java.util.List;



import static io.qdrant.client.ConditionFactory.matchKeyword;



import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Points.Filter;

import io.qdrant.client.grpc.Points.ReadConsistency;

import io.qdrant.client.grpc.Points.ReadConsistencyType;

import io.qdrant.client.grpc.Points.SearchParams;

import io.qdrant.client.grpc.Points.SearchPoints;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .searchAsync(

        SearchPoints.newBuilder()
ef_construct: 100

    # Minimal size (in KiloBytes) of vectors for additional payload-based indexing.

    # If payload chunk is smaller than `full_scan_threshold_kb` additional indexing won't be used -

    # in this case full-scan search should be preferred by query planner and additional indexing is not required.

    # Note: 1Kb = 1 vector of size 256

    full_scan_threshold_kb: 10000

    # Number of parallel threads used for background index building. If 0 - auto selection.

    max_indexing_threads: 0

    # Store HNSW index on disk. If set to false, index will be stored in RAM. Default: false

    on_disk: false

    # Custom M param for hnsw graph built for payload index. If not set, default M will be used.

    payload_m: null





service:



  # Maximum size of POST data in a single request in megabytes

  max_request_size_mb: 32



  # Number of parallel workers used for serving the api. If 0 - equal to the number of available cores.

  # If missing - Same as storage.max_search_threads
## Tweaking the HNSW parameters



HNSW is a hierarchical graph, where each node has a set of links to other nodes. The number of edges per node is called the `m` parameter. 

The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of 

neighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.

The default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and

see how it affects the precision. Of course, we need to wait until the indexing is finished before we can perform the search.



```python

client.update_collection(

    collection_name=""arxiv-titles-instructorxl-embeddings"",

    hnsw_config=models.HnswConfigDiff(

        m=32,  # Increase the number of edges per node from the default 16 to 32"
How do you use ‘ordering’ parameter?,"Write ordering can be specified for any write request to serialize it through a single “leader” node, which ensures that all write operations (issued with the same ordering) are performed and observed sequentially. It is of 3 types weak , medium and strong and is used in python with additional param ordering=models.WriteOrdering.STRONG to upsert request.","['```http\n\nPUT /collections/{collection_name}/points?ordering=strong\n\n{\n\n    ""batch"": {\n\n        ""ids"": [1, 2, 3],\n\n        ""payloads"": [\n\n            {""color"": ""red""},\n\n            {""color"": ""green""},\n\n            {""color"": ""blue""}\n\n        ],\n\n        ""vectors"": [\n\n            [0.9, 0.1, 0.1],\n\n            [0.1, 0.9, 0.1],\n\n            [0.1, 0.1, 0.9]\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.upsert(\n\n    collection_name=""{collection_name}"",\n\n    points=models.Batch(\n\n        ids=[1, 2, 3],\n\n        payloads=[\n\n            {""color"": ""red""},\n\n            {""color"": ""green""},\n\n            {""color"": ""blue""},\n\n        ],\n\n        vectors=[\n\n            [0.9, 0.1, 0.1],\n\n            [0.1, 0.9, 0.1],\n\n            [0.1, 0.1, 0.9],\n\n        ],\n\n    ),\n\n    ordering=""strong"",\n\n)\n\n```\n\n\n\n```typescript\n\nclient.upsert(""{collection_name}"", {\n\n  batch: {\n\n    ids: [1, 2, 3],\n\n    payloads: [{ color: ""red"" }, { color: ""green"" }, { color: ""blue"" }],\n\n    vectors: [\n\n      [0.9, 0.1, 0.1],\n\n      [0.1, 0.9, 0.1],', 'None,\n\n        &PointsSelector {\n\n            points_selector_one_of: Some(PointsSelectorOneOf::Points(PointsIdsList {\n\n                ids: vec![0.into(), 3.into(), 10.into()],\n\n            })),\n\n        },\n\n        json!({\n\n            ""property1"": ""string"",\n\n            ""property2"": ""string"",\n\n        })\n\n        .try_into()\n\n        .unwrap(),\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport java.util.List;\n\n\n\nimport static io.qdrant.client.PointIdFactory.id;\n\nimport static io.qdrant.client.ValueFactory.value;\n\n\n\nclient\n\n    .overwritePayloadAsync(\n\n        ""{collection_name}"",\n\n        Map.of(""property1"", value(""string""), ""property2"", value(""string"")),\n\n        List.of(id(0), id(3), id(10)),\n\n        true,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.OverwritePayloadAsync(\n\n\tcollectionName: ""{collection_name}"",', '```http\n\nPOST /collections/{collection_name}/points/recommend\n\n{\n\n  ""positive"": [100, 231],\n\n  ""negative"": [718, [0.2, 0.3, 0.4, 0.5]],\n\n  ""filter"": {\n\n        ""must"": [\n\n            {\n\n                ""key"": ""city"",\n\n                ""match"": {\n\n                    ""value"": ""London""\n\n                }\n\n            }\n\n        ]\n\n  },\n\n  ""strategy"": ""average_vector"",\n\n  ""limit"": 3\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.recommend(\n\n    collection_name=""{collection_name}"",\n\n    positive=[100, 231],\n\n    negative=[718, [0.2, 0.3, 0.4, 0.5]],\n\n    strategy=models.RecommendStrategy.AVERAGE_VECTOR,\n\n    query_filter=models.Filter(\n\n        must=[\n\n            models.FieldCondition(\n\n                key=""city"",\n\n                match=models.MatchValue(\n\n                    value=""London"",\n\n                ),\n\n            )\n\n        ]\n\n    ),\n\n    limit=3,\n\n)\n\n```\n\n\n\n```typescript']","```http

PUT /collections/{collection_name}/points?ordering=strong

{

    ""batch"": {

        ""ids"": [1, 2, 3],

        ""payloads"": [

            {""color"": ""red""},

            {""color"": ""green""},

            {""color"": ""blue""}

        ],

        ""vectors"": [

            [0.9, 0.1, 0.1],

            [0.1, 0.9, 0.1],

            [0.1, 0.1, 0.9]

        ]

    }

}

```



```python

client.upsert(

    collection_name=""{collection_name}"",

    points=models.Batch(

        ids=[1, 2, 3],

        payloads=[

            {""color"": ""red""},

            {""color"": ""green""},

            {""color"": ""blue""},

        ],

        vectors=[

            [0.9, 0.1, 0.1],

            [0.1, 0.9, 0.1],

            [0.1, 0.1, 0.9],

        ],

    ),

    ordering=""strong"",

)

```



```typescript

client.upsert(""{collection_name}"", {

  batch: {

    ids: [1, 2, 3],

    payloads: [{ color: ""red"" }, { color: ""green"" }, { color: ""blue"" }],

    vectors: [

      [0.9, 0.1, 0.1],

      [0.1, 0.9, 0.1],
None,

        &PointsSelector {

            points_selector_one_of: Some(PointsSelectorOneOf::Points(PointsIdsList {

                ids: vec![0.into(), 3.into(), 10.into()],

            })),

        },

        json!({

            ""property1"": ""string"",

            ""property2"": ""string"",

        })

        .try_into()

        .unwrap(),

        None,

    )

    .await?;

```



```java

import java.util.List;



import static io.qdrant.client.PointIdFactory.id;

import static io.qdrant.client.ValueFactory.value;



client

    .overwritePayloadAsync(

        ""{collection_name}"",

        Map.of(""property1"", value(""string""), ""property2"", value(""string"")),

        List.of(id(0), id(3), id(10)),

        true,

        null,

        null)

    .get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.OverwritePayloadAsync(

	collectionName: ""{collection_name}"",
```http

POST /collections/{collection_name}/points/recommend

{

  ""positive"": [100, 231],

  ""negative"": [718, [0.2, 0.3, 0.4, 0.5]],

  ""filter"": {

        ""must"": [

            {

                ""key"": ""city"",

                ""match"": {

                    ""value"": ""London""

                }

            }

        ]

  },

  ""strategy"": ""average_vector"",

  ""limit"": 3

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.recommend(

    collection_name=""{collection_name}"",

    positive=[100, 231],

    negative=[718, [0.2, 0.3, 0.4, 0.5]],

    strategy=models.RecommendStrategy.AVERAGE_VECTOR,

    query_filter=models.Filter(

        must=[

            models.FieldCondition(

                key=""city"",

                match=models.MatchValue(

                    value=""London"",

                ),

            )

        ]

    ),

    limit=3,

)

```



```typescript"
What is significance of ‘on_disk_payload’ setting?,"The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes.","['{\n\n                        ""id"": 1,\n\n                        ""vector"": [1.0, 2.0, 3.0, 4.0],\n\n                        ""payload"": {}\n\n                    }\n\n                ]\n\n            }\n\n        },\n\n        {\n\n            ""update_vectors"": {\n\n                ""points"": [\n\n                    {\n\n                        ""id"": 1,\n\n                        ""vector"": [1.0, 2.0, 3.0, 4.0]\n\n                    }\n\n                ]\n\n            }\n\n        },\n\n        {\n\n            ""delete_vectors"": {\n\n                ""points"": [1],\n\n                ""vector"": [""""]\n\n            }\n\n        },\n\n        {\n\n            ""overwrite_payload"": {\n\n                ""payload"": {\n\n                    ""test_payload"": ""1""\n\n                },\n\n                ""points"": [1]\n\n            }\n\n        },\n\n        {\n\n            ""set_payload"": {\n\n                ""payload"": {\n\n                    ""test_payload_2"": ""2"",\n\n                    ""test_payload_3"": ""3""\n\n                },\n\n                ""points"": [1]', '""indexed_vectors_count"": 1024232,\n\n        ""points_count"": 1068786,\n\n        ""segments_count"": 31,\n\n        ""config"": {\n\n            ""params"": {\n\n                ""vectors"": {\n\n                    ""size"": 384,\n\n                    ""distance"": ""Cosine""\n\n                },\n\n                ""shard_number"": 1,\n\n                ""replication_factor"": 1,\n\n                ""write_consistency_factor"": 1,\n\n                ""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,\n\n                ""memmap_threshold"": null,\n\n                ""indexing_threshold"": 20000,\n\n                ""flush_interval_sec"": 5,', 'You can specify the desired type of payload storage with [configuration file](../../guides/configuration/) or with collection parameter `on_disk_payload` during [creation](../collections/#create-collection) of the collection.\n\n\n\n## Versioning\n\n\n\nTo ensure data integrity, Qdrant performs all data changes in 2 stages.\n\nIn the first step, the data is written to the Write-ahead-log(WAL), which orders all operations and assigns them a sequential number.\n\n\n\nOnce a change has been added to the WAL, it will not be lost even if a power loss occurs.\n\nThen the changes go into the segments.\n\nEach segment stores the last version of the change applied to it as well as the version of each individual point.\n\nIf the new change has a sequential number less than the current version of the point, the updater will ignore the change.\n\nThis mechanism allows Qdrant to safely and efficiently restore the storage from the WAL in case of an abnormal shutdown.']","{

                        ""id"": 1,

                        ""vector"": [1.0, 2.0, 3.0, 4.0],

                        ""payload"": {}

                    }

                ]

            }

        },

        {

            ""update_vectors"": {

                ""points"": [

                    {

                        ""id"": 1,

                        ""vector"": [1.0, 2.0, 3.0, 4.0]

                    }

                ]

            }

        },

        {

            ""delete_vectors"": {

                ""points"": [1],

                ""vector"": [""""]

            }

        },

        {

            ""overwrite_payload"": {

                ""payload"": {

                    ""test_payload"": ""1""

                },

                ""points"": [1]

            }

        },

        {

            ""set_payload"": {

                ""payload"": {

                    ""test_payload_2"": ""2"",

                    ""test_payload_3"": ""3""

                },

                ""points"": [1]
""indexed_vectors_count"": 1024232,

        ""points_count"": 1068786,

        ""segments_count"": 31,

        ""config"": {

            ""params"": {

                ""vectors"": {

                    ""size"": 384,

                    ""distance"": ""Cosine""

                },

                ""shard_number"": 1,

                ""replication_factor"": 1,

                ""write_consistency_factor"": 1,

                ""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,

                ""memmap_threshold"": null,

                ""indexing_threshold"": 20000,

                ""flush_interval_sec"": 5,
You can specify the desired type of payload storage with [configuration file](../../guides/configuration/) or with collection parameter `on_disk_payload` during [creation](../collections/#create-collection) of the collection.



## Versioning



To ensure data integrity, Qdrant performs all data changes in 2 stages.

In the first step, the data is written to the Write-ahead-log(WAL), which orders all operations and assigns them a sequential number.



Once a change has been added to the WAL, it will not be lost even if a power loss occurs.

Then the changes go into the segments.

Each segment stores the last version of the change applied to it as well as the version of each individual point.

If the new change has a sequential number less than the current version of the point, the updater will ignore the change.

This mechanism allows Qdrant to safely and efficiently restore the storage from the WAL in case of an abnormal shutdown."
What is the impact of ‘write_consistency_factor’ ?,"The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully.","['Qdrant provides a few options to control consistency guarantees:\n\n\n\n- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.\n\n- Read `consistency` param, can be used with search and retrieve operations to ensure that the results obtained from all replicas are the same. If this option is used, Qdrant will perform the read operation on multiple replicas and resolve the result according to the selected strategy. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if the update operations are frequent and the number of replicas is low.', '""indexed_vectors_count"": 1024232,\n\n        ""points_count"": 1068786,\n\n        ""segments_count"": 31,\n\n        ""config"": {\n\n            ""params"": {\n\n                ""vectors"": {\n\n                    ""size"": 384,\n\n                    ""distance"": ""Cosine""\n\n                },\n\n                ""shard_number"": 1,\n\n                ""replication_factor"": 1,\n\n                ""write_consistency_factor"": 1,\n\n                ""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,\n\n                ""memmap_threshold"": null,\n\n                ""indexing_threshold"": 20000,\n\n                ""flush_interval_sec"": 5,', '- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if read operations are more frequent than update and if search performance is critical.\n\n\n\n\n\n### Write consistency factor\n\n\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\n\nIt can be configured at the collection\'s creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python']","Qdrant provides a few options to control consistency guarantees:



- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.

- Read `consistency` param, can be used with search and retrieve operations to ensure that the results obtained from all replicas are the same. If this option is used, Qdrant will perform the read operation on multiple replicas and resolve the result according to the selected strategy. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if the update operations are frequent and the number of replicas is low.
""indexed_vectors_count"": 1024232,

        ""points_count"": 1068786,

        ""segments_count"": 31,

        ""config"": {

            ""params"": {

                ""vectors"": {

                    ""size"": 384,

                    ""distance"": ""Cosine""

                },

                ""shard_number"": 1,

                ""replication_factor"": 1,

                ""write_consistency_factor"": 1,

                ""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,

                ""memmap_threshold"": null,

                ""indexing_threshold"": 20000,

                ""flush_interval_sec"": 5,
- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if read operations are more frequent than update and if search performance is critical.





### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

    ""write_consistency_factor"": 2,

}

```



```python"
What is the purpose of oversampling in Qdrant search process?,Oversampling in Qdrant search process defines how many extra vectors should be pre-selected using quantized index and then re-scored using original vectors to improve search quality.,"['from qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.search(\n\n    collection_name=""{collection_name}"",\n\n    query_vector=[0.2, 0.1, 0.9, 0.7],\n\n    search_params=models.SearchParams(\n\n        quantization=models.QuantizationSearchParams(\n\n            ignore=False,\n\n            rescore=True,\n\n            oversampling=2.4\n\n        )\n\n    )\n\n)\n\n```\n\n\n\nIn this case, if `oversampling` is 2.4 and `limit` is 100, then 240 vectors will be pre-selected using quantized index, and then the top 100 points will be returned after re-scoring with the unquantized vectors.\n\n\n\nAs you can see from the example above, this parameter is set during the query. This is a flexible method that will let you tune query accuracy. While the index is not changed, you can decide how many points you want to retrieve using quantized vectors.\n\n\n\n### Grouping API lookup', '`rescore` - Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. \n\nThis can improve the search quality, but may slightly decrease the search speed, compared to the search without rescore.\n\nIt is recommended to disable rescore only if the original vectors are stored on a slow storage (e.g. HDD or network storage).\n\nBy default, rescore is enabled.\n\n\n\n**Available as of v1.3.0**\n\n\n\n`oversampling` - Defines how many extra vectors should be pre-selected using quantized index, and then re-scored using original vectors.\n\nFor example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.\n\nOversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.\n\n\n\n## Quantization tips\n\n\n\n#### Accuracy tuning\n\n\n\nIn this section, we will discuss how to tune the search precision.', '<td><span style=""color: green;"">-44,16%</span></td>\n\n         <td><span style=""color: green;"">+0.11%</span></td>\n\n         <td><span style=""color: green;"">-42.96%</span></td>\n\n         <td>0%</td>\n\n         <td><span style=""color: green;"">-41,56%</span></td>\n\n      </tr>\n\n   </tbody>\n\n</table>\n\n\n\nIn all the cases, the decrease in search precision is negligible, but we keep a latency \n\nreduction of at least 28.57%, even up to 60,64%, while searching. As a rule of thumb,\n\nthe higher the dimensionality of the vectors, the lower the precision loss.\n\n\n\n### Oversampling and Rescoring\n\n\n\nA distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.\n\nThis enables the best combination of speed, accuracy, and RAM usage.\n\n\n\nQdrant stores the original vectors, so it is possible to rescore the top-k results with\n\nthe original vectors after doing the neighbours search in quantized space. That obviously']","from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.search(

    collection_name=""{collection_name}"",

    query_vector=[0.2, 0.1, 0.9, 0.7],

    search_params=models.SearchParams(

        quantization=models.QuantizationSearchParams(

            ignore=False,

            rescore=True,

            oversampling=2.4

        )

    )

)

```



In this case, if `oversampling` is 2.4 and `limit` is 100, then 240 vectors will be pre-selected using quantized index, and then the top 100 points will be returned after re-scoring with the unquantized vectors.



As you can see from the example above, this parameter is set during the query. This is a flexible method that will let you tune query accuracy. While the index is not changed, you can decide how many points you want to retrieve using quantized vectors.



### Grouping API lookup
`rescore` - Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. 

This can improve the search quality, but may slightly decrease the search speed, compared to the search without rescore.

It is recommended to disable rescore only if the original vectors are stored on a slow storage (e.g. HDD or network storage).

By default, rescore is enabled.



**Available as of v1.3.0**



`oversampling` - Defines how many extra vectors should be pre-selected using quantized index, and then re-scored using original vectors.

For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.

Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.



## Quantization tips



#### Accuracy tuning



In this section, we will discuss how to tune the search precision.
<td><span style=""color: green;"">-44,16%</span></td>

         <td><span style=""color: green;"">+0.11%</span></td>

         <td><span style=""color: green;"">-42.96%</span></td>

         <td>0%</td>

         <td><span style=""color: green;"">-41,56%</span></td>

      </tr>

   </tbody>

</table>



In all the cases, the decrease in search precision is negligible, but we keep a latency 

reduction of at least 28.57%, even up to 60,64%, while searching. As a rule of thumb,

the higher the dimensionality of the vectors, the lower the precision loss.



### Oversampling and Rescoring



A distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.

This enables the best combination of speed, accuracy, and RAM usage.



Qdrant stores the original vectors, so it is possible to rescore the top-k results with

the original vectors after doing the neighbours search in quantized space. That obviously"
How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?,"Qdrant uses a different approach that does not require pre- or post-filtering, effectively addressing the accuracy problem while maintaining search efficiency.","['to do it. \n\n\n\n## Wrapping up\n\n\n\nAssessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.\n\nyour search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, \n\neven in an automated way, as part of your CI/CD pipeline.\n\n\n\nAgain, **the quality of the embeddings is the most important factor**. HNSW does a pretty good job in terms of precision, and it is\n\nparameterizable and tunable, when required. There are some other ANN algorithms available out there, such as [IVF*](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#cell-probe-methods-indexivf-indexes), \n\nbut they usually [perform worse than HNSW in terms of quality and performance](https://nirantk.com/writing/pgvector-vs-qdrant/#correctness).', 'Rishabh Bhardwaj:\n\nNo, not at all.\n\n\n\nDemetrios:\n\nAll right, keep going. I like it.\n\n\n\nRishabh Bhardwaj:\n\nYeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.\n\n\n\nDemetrios:\n\nRight.\n\n\n\nRishabh Bhardwaj:\n\nSo also the other thing is, Qdrant also provides the functionality to specify those parameters while making the search as well. So it does not mean if we build the index initially, we only have to use those specifications. We can again specify them during the search as well.\n\n\n\nDemetrios:\n\nOkay.\n\n\n\nRishabh Bhardwaj:', 'In this episode, Rishabh dives into the nitty-gritty of creating a high-performance hotel matching solution with Qdrant, covering everything from data inconsistency challenges to the speed and accuracy enhancements achieved through the HNSW algorithm.\n\n\n\n5 Keys to Learning from the Episode:\n\n\n\n1. Discover the importance of data consistency and the challenges it poses when dealing with multiple sources and languages.\n\n2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.\n\n3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.\n\n4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.\n\n5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.']","to do it. 



## Wrapping up



Assessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.

your search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, 

even in an automated way, as part of your CI/CD pipeline.



Again, **the quality of the embeddings is the most important factor**. HNSW does a pretty good job in terms of precision, and it is

parameterizable and tunable, when required. There are some other ANN algorithms available out there, such as [IVF*](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#cell-probe-methods-indexivf-indexes), 

but they usually [perform worse than HNSW in terms of quality and performance](https://nirantk.com/writing/pgvector-vs-qdrant/#correctness).
Rishabh Bhardwaj:

No, not at all.



Demetrios:

All right, keep going. I like it.



Rishabh Bhardwaj:

Yeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.



Demetrios:

Right.



Rishabh Bhardwaj:

So also the other thing is, Qdrant also provides the functionality to specify those parameters while making the search as well. So it does not mean if we build the index initially, we only have to use those specifications. We can again specify them during the search as well.



Demetrios:

Okay.



Rishabh Bhardwaj:
In this episode, Rishabh dives into the nitty-gritty of creating a high-performance hotel matching solution with Qdrant, covering everything from data inconsistency challenges to the speed and accuracy enhancements achieved through the HNSW algorithm.



5 Keys to Learning from the Episode:



1. Discover the importance of data consistency and the challenges it poses when dealing with multiple sources and languages.

2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.

3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.

4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.

5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data."
What is the difference between regular and neural search?,"Regular full-text search involves searching for keywords within a document, while neural search considers the real meaning of the query and documents, allowing for more accurate results.","['In this tutorial we are going to find answers to these questions:\n\n\n\n* What is the difference between regular and neural search?\n\n* What neural networks could be used for search?\n\n* In what tasks is neural network search useful?\n\n* How to build and deploy own neural search service step-by-step?\n\n\n\n**What is neural search?**\n\n\n\nA regular full-text search, such as Google’s, consists of searching for keywords inside a document. For this reason, the algorithm can not take into account the real meaning of the query and documents. Many documents that might be of interest to the user are not found because they use different wording.', ""These days, search technology is the heart of a variety of applications.\n\nFrom web-pages search to product recommendations.\n\nFor many years, this technology didn't get much change until neural networks came into play.\n\n\n\nIn this tutorial we are going to find answers to these questions:\n\n\n\n* What is the difference between regular and neural search?\n\n* What neural networks could be used for search?\n\n* In what tasks is neural network search useful?\n\n* How to build and deploy own neural search service step-by-step?\n\n\n\n## What is neural search?\n\n\n\nA regular full-text search, such as Google's, consists of searching for keywords inside a document.\n\nFor this reason, the algorithm can not take into account the real meaning of the query and documents.\n\nMany documents that might be of interest to the user are not found because they use different wording.\n\n\n\nNeural search tries to solve exactly this problem - it attempts to enable searches not by keywords but by meaning.\n\nTo achieve this, the search works in 2 steps."", 'The demo contains a switch that selects between neural and full-text searches.\n\nYou can turn the neural search on and off to compare your result with a regular full-text search.\n\n\n\n> **Note**: The code for this tutorial can be found here: | [Step 1: Data Preparation Process](https://colab.research.google.com/drive/1kPktoudAP8Tu8n8l-iVMOQhVmHkWV_L9?usp=sharing) | [Step 2: Full Code for Neural Search](https://github.com/qdrant/qdrant_demo/tree/sentense-transformers). |\n\n\n\nJoin our [Discord community](https://qdrant.to/discord), where we talk about vector search and similarity learning, publish other examples of neural networks and neural search applications.']","In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



**What is neural search?**



A regular full-text search, such as Google’s, consists of searching for keywords inside a document. For this reason, the algorithm can not take into account the real meaning of the query and documents. Many documents that might be of interest to the user are not found because they use different wording.
These days, search technology is the heart of a variety of applications.

From web-pages search to product recommendations.

For many years, this technology didn't get much change until neural networks came into play.



In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



## What is neural search?



A regular full-text search, such as Google's, consists of searching for keywords inside a document.

For this reason, the algorithm can not take into account the real meaning of the query and documents.

Many documents that might be of interest to the user are not found because they use different wording.



Neural search tries to solve exactly this problem - it attempts to enable searches not by keywords but by meaning.

To achieve this, the search works in 2 steps.
The demo contains a switch that selects between neural and full-text searches.

You can turn the neural search on and off to compare your result with a regular full-text search.



> **Note**: The code for this tutorial can be found here: | [Step 1: Data Preparation Process](https://colab.research.google.com/drive/1kPktoudAP8Tu8n8l-iVMOQhVmHkWV_L9?usp=sharing) | [Step 2: Full Code for Neural Search](https://github.com/qdrant/qdrant_demo/tree/sentense-transformers). |



Join our [Discord community](https://qdrant.to/discord), where we talk about vector search and similarity learning, publish other examples of neural networks and neural search applications."
How can I use Qdrant as a vector store in Langchain Go?,"You can use Qdrant as a vector store in Langchain Go by installing the `langchain-go` project dependency and customizing the values for your configuration, such as the Qdrant REST URL and collection name.","['---\n\ntitle: Langchain Go\n\nweight: 120\n\n---\n\n\n\n# Langchain Go\n\n\n\n[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.\n\n\n\nYou can use Qdrant as a vector store in Langchain Go.\n\n\n\n## Setup\n\n\n\nInstall the `langchain-go` project dependency\n\n\n\n```bash\n\ngo get -u github.com/tmc/langchaingo\n\n```\n\n\n\n## Usage\n\n\n\nBefore you use the following code sample, customize the following values for your configuration:\n\n\n\n- `YOUR_QDRANT_REST_URL`: If you\'ve set up Qdrant using the [Quick Start](/documentation/quick-start/) guide,\n\n  set this value to `http://localhost:6333`.\n\n- `YOUR_COLLECTION_NAME`: Use our [Collections](/documentation/concepts/collections) guide to create or\n\n  list collections.\n\n\n\n```go\n\nimport (\n\n        ""fmt""\n\n        ""log""\n\n\n\n        ""github.com/tmc/langchaingo/embeddings""\n\n        ""github.com/tmc/langchaingo/llms/openai""\n\n        ""github.com/tmc/langchaingo/vectorstores""', ')\n\n\n\nvector_store = QdrantVectorStore(client=client, collection_name=""documents"")\n\nindex = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n\n\n\n```\n\n\n\nThe library [comes with a notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/vector_stores/QdrantIndexDemo.ipynb) \n\nthat shows an end-to-end example of how to use Qdrant within LlamaIndex.', '## Further reading\n\n\n\n- [n8n Documentation](https://docs.n8n.io/)\n\n- [n8n Qdrant Node documentation](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoreqdrant/#qdrant-vector-store)']","---

title: Langchain Go

weight: 120

---



# Langchain Go



[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.



You can use Qdrant as a vector store in Langchain Go.



## Setup



Install the `langchain-go` project dependency



```bash

go get -u github.com/tmc/langchaingo

```



## Usage



Before you use the following code sample, customize the following values for your configuration:



- `YOUR_QDRANT_REST_URL`: If you've set up Qdrant using the [Quick Start](/documentation/quick-start/) guide,

  set this value to `http://localhost:6333`.

- `YOUR_COLLECTION_NAME`: Use our [Collections](/documentation/concepts/collections) guide to create or

  list collections.



```go

import (

        ""fmt""

        ""log""



        ""github.com/tmc/langchaingo/embeddings""

        ""github.com/tmc/langchaingo/llms/openai""

        ""github.com/tmc/langchaingo/vectorstores""
)



vector_store = QdrantVectorStore(client=client, collection_name=""documents"")

index = VectorStoreIndex.from_vector_store(vector_store=vector_store)



```



The library [comes with a notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/vector_stores/QdrantIndexDemo.ipynb) 

that shows an end-to-end example of how to use Qdrant within LlamaIndex.
## Further reading



- [n8n Documentation](https://docs.n8n.io/)

- [n8n Qdrant Node documentation](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoreqdrant/#qdrant-vector-store)"
How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?,Dust leveraged the control of the MMAP payload threshold and Scalar Quantization in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively.,"['acknowledges: “Qdrant’s ability to handle large-scale models and the flexibility\n\nit offers in terms of data management has been crucial for us. The observability\n\nfeatures, such as historical graphs of RAM, Disk, and CPU, provided by Qdrant are\n\nalso particularly useful, allowing us to plan our scaling strategy effectively.”\n\n\n\n![“We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as\n\nwe don’t have to run lots of nodes in parallel. While being memory-bound, we were\n\nable to push the same instances further with the help of quantization. While you\n\nget pressure on MMAP in this case you maintain very good performance even if the\n\nRAM is fully used. With this we were able to reduce our cost by 2x.” - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)\n\n\n\nDust was able to scale its application with Qdrant while maintaining low latency\n\nacross hundreds of thousands of collections with retrieval only taking', '“The early setup worked out of the box nicely,” Polu says.\n\n\n\n2. **Scale and optimize:** As the load grew, Dust started to take advantage of Qdrant’s\n\nfeatures to tune the setup for optimization and scale. They started to look into\n\nhow they map and cache data, as well as applying some of Qdrant’s [built-in\n\ncompression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP\n\npayload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage\n\nthe balance between storing vectors on disk and keeping quantized vectors in RAM,\n\nmore effectively. “This allowed us to scale smoothly from there,” Polu says.\n\n\n\n## Results\n\n\n\nDust has seen success in using Qdrant as their vector database of choice, as Polu\n\nacknowledges: “Qdrant’s ability to handle large-scale models and the flexibility', ""```text\n\nmemory_size = 1.5 * number_of_vectors * vector_dimension * 4 bytes\n\n```\n\n\n\nWhile Qdrant offers various options to store some parts of the data on disk, starting \n\nfrom version 1.1.0, you can also optimize your memory by compressing the embeddings. \n\nWe've implemented the mechanism of **Scalar Quantization**! It turns out to have not \n\nonly a positive impact on memory but also on the performance. \n\n\n\n## Scalar Quantization\n\n\n\nScalar quantization is a data compression technique that converts floating point values \n\ninto integers. In case of Qdrant `float32` gets converted into `int8`, so a single number \n\nneeds 75% less memory. It's not a simple rounding though! It's a process that makes that\n\ntransformation partially reversible, so we can also revert integers back to floats with \n\na small loss of precision. \n\n\n\n### Theoretical background\n\n\n\nAssume we have a collection of `float32` vectors and denote a single value as `f32`.""]","acknowledges: “Qdrant’s ability to handle large-scale models and the flexibility

it offers in terms of data management has been crucial for us. The observability

features, such as historical graphs of RAM, Disk, and CPU, provided by Qdrant are

also particularly useful, allowing us to plan our scaling strategy effectively.”



![“We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as

we don’t have to run lots of nodes in parallel. While being memory-bound, we were

able to push the same instances further with the help of quantization. While you

get pressure on MMAP in this case you maintain very good performance even if the

RAM is fully used. With this we were able to reduce our cost by 2x.” - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)



Dust was able to scale its application with Qdrant while maintaining low latency

across hundreds of thousands of collections with retrieval only taking
“The early setup worked out of the box nicely,” Polu says.



2. **Scale and optimize:** As the load grew, Dust started to take advantage of Qdrant’s

features to tune the setup for optimization and scale. They started to look into

how they map and cache data, as well as applying some of Qdrant’s [built-in

compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP

payload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage

the balance between storing vectors on disk and keeping quantized vectors in RAM,

more effectively. “This allowed us to scale smoothly from there,” Polu says.



## Results



Dust has seen success in using Qdrant as their vector database of choice, as Polu

acknowledges: “Qdrant’s ability to handle large-scale models and the flexibility
```text

memory_size = 1.5 * number_of_vectors * vector_dimension * 4 bytes

```



While Qdrant offers various options to store some parts of the data on disk, starting 

from version 1.1.0, you can also optimize your memory by compressing the embeddings. 

We've implemented the mechanism of **Scalar Quantization**! It turns out to have not 

only a positive impact on memory but also on the performance. 



## Scalar Quantization



Scalar quantization is a data compression technique that converts floating point values 

into integers. In case of Qdrant `float32` gets converted into `int8`, so a single number 

needs 75% less memory. It's not a simple rounding though! It's a process that makes that

transformation partially reversible, so we can also revert integers back to floats with 

a small loss of precision. 



### Theoretical background



Assume we have a collection of `float32` vectors and denote a single value as `f32`."
Why do we still need keyword search?,"Keyword search is still useful in cases of out-of-domain search, where words are just words regardless of their meaning.","[""You'll never cover all the possible queries with a list of synonyms, so a full-text search may not find all the relevant \n\ndocuments. There are also some cases in which your users use different terminology than the one you have in your database. \n\nThose problems are easily solvable with neural vector embeddings, and combining both approaches with an additional reranking \n\nstep is possible. So you don't need to resign from your well-known full-text search mechanism but extend it with vector \n\nsearch to support the queries you haven't foreseen."", ""2. Vector search with keyword-based search. This one is covered in this article.\n\n3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.\n\n\n\n## Why do we still need keyword search?\n\n\n\nA keyword-based search was the obvious choice for search engines in the past. It struggled with some\n\ncommon issues, but since we didn't have any alternatives, we had to overcome them with additional\n\npreprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has\n\nsome clear advantages in the following scenarios:\n\n\n\n- 🌍 Multi-lingual & multi-modal search\n\n- 🤔 For short texts with typos and ambiguous content-dependent meanings\n\n- 👨\u200d🔬 Specialized domains with tuned encoder models\n\n- 📄 Document-as-a-Query similarity search\n\n\n\nIt doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method\n\nmight be useful:\n\n\n\n- 🌐💭 Out-of-domain search. Words are just words, no matter what they mean. BM25 ranking represents the"", '{{< figure src=/docs/gettingstarted/inverted-index.png caption=""A simplified version of the inverted index."" >}}\n\n\n\nTime passed, and we haven’t had much change in that area for quite a long time. But our textual data collection started to grow at a greater pace. So we also started building up many processes around those inverted indexes. For example, we allowed our users to provide many words and started splitting them into pieces. That allowed finding some documents which do not necessarily contain all the query words, but possibly part of them. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.']","You'll never cover all the possible queries with a list of synonyms, so a full-text search may not find all the relevant 

documents. There are also some cases in which your users use different terminology than the one you have in your database. 

Those problems are easily solvable with neural vector embeddings, and combining both approaches with an additional reranking 

step is possible. So you don't need to resign from your well-known full-text search mechanism but extend it with vector 

search to support the queries you haven't foreseen.
2. Vector search with keyword-based search. This one is covered in this article.

3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.



## Why do we still need keyword search?



A keyword-based search was the obvious choice for search engines in the past. It struggled with some

common issues, but since we didn't have any alternatives, we had to overcome them with additional

preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has

some clear advantages in the following scenarios:



- 🌍 Multi-lingual & multi-modal search

- 🤔 For short texts with typos and ambiguous content-dependent meanings

- 👨‍🔬 Specialized domains with tuned encoder models

- 📄 Document-as-a-Query similarity search



It doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method

might be useful:



- 🌐💭 Out-of-domain search. Words are just words, no matter what they mean. BM25 ranking represents the
{{< figure src=/docs/gettingstarted/inverted-index.png caption=""A simplified version of the inverted index."" >}}



Time passed, and we haven’t had much change in that area for quite a long time. But our textual data collection started to grow at a greater pace. So we also started building up many processes around those inverted indexes. For example, we allowed our users to provide many words and started splitting them into pieces. That allowed finding some documents which do not necessarily contain all the query words, but possibly part of them. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks."
What principles did Qdrant follow while designing benchmarks for vector search engines?,"Qdrant followed the principles of doing comparative benchmarks focusing on relative numbers rather than absolute numbers, and using affordable hardware for easy result reproduction.","['---\n\ntitle: Vector Database Benchmarks\n\ndescription: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.\n\nkeywords:\n\n  - vector databases comparative benchmark\n\n  - ANN Benchmark\n\n  - Qdrant vs Milvus\n\n  - Qdrant vs Weaviate\n\n  - Qdrant vs Redis\n\n  - Qdrant vs ElasticSearch\n\n  - benchmark\n\n  - performance\n\n  - latency\n\n  - RPS\n\n  - comparison\n\n  - vector search\n\n  - embedding\n\npreview_image: /benchmarks/benchmark-1.png\n\nseo_schema: { ""@context"": ""https://schema.org"", ""@type"": ""Article"", ""headline"": ""Vector Search Comparative Benchmarks"", ""image"": [ ""https://qdrant.tech/benchmarks/benchmark-1.png"" ], ""abstract"": ""The first comparative benchmark and benchmarking framework for vector search engines"", ""datePublished"": ""2022-08-23"", ""dateModified"": ""2022-08-23"", ""author"": [{ ""@type"": ""Organization"", ""name"": ""Qdrant"", ""url"": ""https://qdrant.tech"" }] }\n\n \n\n---', '---\n\ndraft: false\n\nid: 2\n\ntitle: How vector search should be benchmarked?\n\nweight: 1\n\n---\n\n\n\n# Benchmarking Vector Databases\n\n\n\nAt Qdrant, performance is the top-most priority. We always make sure that we use system resources efficiently so you get the **fastest and most accurate results at the cheapest cloud costs**. So all of our decisions from [choosing Rust](/articles/why-rust), [io optimisations](/articles/io_uring), [serverless support](/articles/serverless), [binary quantization](/articles/binary-quantization), to our [fastembed library](/articles/fastembed) are all based on our principle. In this article, we will compare how Qdrant performs against the other vector search engines.\n\n\n\nHere are the principles we followed while designing these benchmarks:\n\n\n\n- We do comparative benchmarks, which means we focus on **relative numbers** rather than absolute numbers.\n\n- We use affordable hardware, so that you can reproduce the results easily.', 'author: Demetrios Brinkmann\n\nfeatured: false\n\ntags:\n\n  - Qdrant\n\n  - Vector Search Engine\n\n  - Vector Database\n\n---\n\n> *""For systems like Qdrant, scalability and performance in my opinion, is much more important than transactional consistency, so it should be treated as a search engine rather than database.""*\\\n\n-- Andrey Vasnetsov\n\n> \n\n\n\nDiscussing core differences between search engines and databases, Andrey underlined the importance of application needs and scalability in database selection for vector search tasks.\n\n\n\nAndrey Vasnetsov, CTO at Qdrant is an enthusiast of Open Source, machine learning, and vector search. He works on Open Source projects related to Vector Similarity Search and Similarity Learning. He prefers practical over theoretical, working demo over arXiv paper.\n\n\n\n***You can\xa0watch this episode on [YouTube](https://www.youtube.com/watch?v=bU38Ovdh3NY).***']","---

title: Vector Database Benchmarks

description: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.

keywords:

  - vector databases comparative benchmark

  - ANN Benchmark

  - Qdrant vs Milvus

  - Qdrant vs Weaviate

  - Qdrant vs Redis

  - Qdrant vs ElasticSearch

  - benchmark

  - performance

  - latency

  - RPS

  - comparison

  - vector search

  - embedding

preview_image: /benchmarks/benchmark-1.png

seo_schema: { ""@context"": ""https://schema.org"", ""@type"": ""Article"", ""headline"": ""Vector Search Comparative Benchmarks"", ""image"": [ ""https://qdrant.tech/benchmarks/benchmark-1.png"" ], ""abstract"": ""The first comparative benchmark and benchmarking framework for vector search engines"", ""datePublished"": ""2022-08-23"", ""dateModified"": ""2022-08-23"", ""author"": [{ ""@type"": ""Organization"", ""name"": ""Qdrant"", ""url"": ""https://qdrant.tech"" }] }

 

---
---

draft: false

id: 2

title: How vector search should be benchmarked?

weight: 1

---



# Benchmarking Vector Databases



At Qdrant, performance is the top-most priority. We always make sure that we use system resources efficiently so you get the **fastest and most accurate results at the cheapest cloud costs**. So all of our decisions from [choosing Rust](/articles/why-rust), [io optimisations](/articles/io_uring), [serverless support](/articles/serverless), [binary quantization](/articles/binary-quantization), to our [fastembed library](/articles/fastembed) are all based on our principle. In this article, we will compare how Qdrant performs against the other vector search engines.



Here are the principles we followed while designing these benchmarks:



- We do comparative benchmarks, which means we focus on **relative numbers** rather than absolute numbers.

- We use affordable hardware, so that you can reproduce the results easily.
author: Demetrios Brinkmann

featured: false

tags:

  - Qdrant

  - Vector Search Engine

  - Vector Database

---

> *""For systems like Qdrant, scalability and performance in my opinion, is much more important than transactional consistency, so it should be treated as a search engine rather than database.""*\

-- Andrey Vasnetsov

> 



Discussing core differences between search engines and databases, Andrey underlined the importance of application needs and scalability in database selection for vector search tasks.



Andrey Vasnetsov, CTO at Qdrant is an enthusiast of Open Source, machine learning, and vector search. He works on Open Source projects related to Vector Similarity Search and Similarity Learning. He prefers practical over theoretical, working demo over arXiv paper.



***You can watch this episode on [YouTube](https://www.youtube.com/watch?v=bU38Ovdh3NY).***"
What models does Qdrant support for embedding generation?,"Qdrant supports a reasonable range of models, including a few multilingual ones.","['Since calculating the embeddings takes a while, we precomputed them and exported them as a [snapshot](https://storage.googleapis.com/common-datasets-snapshots/wolt-clip-ViT-B-32.snapshot), \n\nwhich might be easily imported into any Qdrant instance. [Qdrant Cloud is the easiest way to start](https://cloud.qdrant.io/), though!', ""Demetrios:\n\nAnd secondly, I will encourage you if you have something that you've been doing with Qdrant or in the vector database space, or in the AI application space and you want to show it off, we would love to have you talk at the vector space talks. So without further ado, Nirant, my man, I'm going to kick it over to you and I am going to start it off with what are the challenges with embedding creation today?\n\n\n\nNirant Kasliwal:"", 'The rise of generative AI in the last few years has shone a spotlight on vector databases, prized for their ability to power retrieval-augmented generation (RAG) applications. What we are seeing now, both within AI and beyond, is only the beginning of the opportunity for vector databases. Within our Qdrant community, we already see a multitude of unique solutions and applications leveraging our technology for multimodal search, anomaly detection, recommendation systems, complex data analysis, and more. \n\n\n\n## What sets Qdrant apart?']","Since calculating the embeddings takes a while, we precomputed them and exported them as a [snapshot](https://storage.googleapis.com/common-datasets-snapshots/wolt-clip-ViT-B-32.snapshot), 

which might be easily imported into any Qdrant instance. [Qdrant Cloud is the easiest way to start](https://cloud.qdrant.io/), though!
Demetrios:

And secondly, I will encourage you if you have something that you've been doing with Qdrant or in the vector database space, or in the AI application space and you want to show it off, we would love to have you talk at the vector space talks. So without further ado, Nirant, my man, I'm going to kick it over to you and I am going to start it off with what are the challenges with embedding creation today?



Nirant Kasliwal:
The rise of generative AI in the last few years has shone a spotlight on vector databases, prized for their ability to power retrieval-augmented generation (RAG) applications. What we are seeing now, both within AI and beyond, is only the beginning of the opportunity for vector databases. Within our Qdrant community, we already see a multitude of unique solutions and applications leveraging our technology for multimodal search, anomaly detection, recommendation systems, complex data analysis, and more. 



## What sets Qdrant apart?"
How can you parallelize the upload of a large dataset using shards in Qdrant?,"By creating multiple shards in Qdrant, you can parallelize the upload of a large dataset. It is recommended to have 2 to 4 shards per machine for efficient processing. When creating a collection in Qdrant, you can specify the number of shards to use for that collection. ","['[Configuring Memmap Storage](../../concepts/storage/#configuring-memmap-storage).\n\n\n\n## Parallel upload into multiple shards\n\n\n\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\n\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    shard_number=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });', 'To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""default_segment_number"": 16\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    optimizers_config=models.OptimizersConfigDiff(default_segment_number=16),\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', '```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 100, Distance = Distance.Cosine }\n\n);\n\n```\n\n\n\nIn addition to the required options, you can also specify custom values for the following collection options:\n\n\n\n* `hnsw_config` - see [indexing](../indexing/#vector-index) for details.\n\n* `wal_config` - Write-Ahead-Log related configuration. See more details about [WAL](../storage/#versioning)\n\n* `optimizers_config` - see [optimizer](../optimizer) for details.\n\n* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.\n\n* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.']","[Configuring Memmap Storage](../../concepts/storage/#configuring-memmap-storage).



## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""shard_number"": 2

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    shard_number=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });
To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""default_segment_number"": 16

    }

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    optimizers_config=models.OptimizersConfigDiff(default_segment_number=16),

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 100, Distance = Distance.Cosine }

);

```



In addition to the required options, you can also specify custom values for the following collection options:



* `hnsw_config` - see [indexing](../indexing/#vector-index) for details.

* `wal_config` - Write-Ahead-Log related configuration. See more details about [WAL](../storage/#versioning)

* `optimizers_config` - see [optimizer](../optimizer) for details.

* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.

* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload."
What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?,"Maximizing the distance between all points in the response when utilizing vector similarity for diversity search is crucial as it allows for the algorithm to sequentially output dissimilar results. By doing so, a diverse selection of the collection can be made possible without the need for any labeling efforts. This approach ensures that the algorithm can effectively identify and retrieve a wide range of diverse items or data points.","['The power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.\n\nBy maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.\n\n\n\n{{< figure src=/articles_data/vector-similarity-beyond-search/diversity.png caption=""Diversity Search"" >}}\n\n\n\n\n\nSome forms of diversity sampling are already used in the industry and are known as [Maximum Margin Relevance](https://python.langchain.com/docs/integrations/vectorstores/qdrant#maximum-marginal-relevance-search-mmr) (MMR). Techniques like this were developed to enhance similarity on a universal search API.\n\nHowever, there is still room for new ideas, particularly regarding diversity retrieval.\n\nBy utilizing more advanced vector-native engines, it could be possible to take use cases to the next level and achieve even better results.\n\n\n\n\n\n## Recommendations', 'The difference lies in deciding which point should be extracted next.\n\n\n\nLet\'s imagine how to get 3 points with similarity search and then with diversity search.\n\n\n\nSimilarity:\n\n1. Calculate distance matrix\n\n2. Choose your anchor\n\n3. Get a vector corresponding to the distances from the selected anchor from the distance matrix\n\n4. Sort fetched vector\n\n5. Get top-3 embeddings\n\n \n\nDiversity:\n\n1. Calculate distance matrix\n\n2. Initialize starting point (randomly or according to the certain conditions)\n\n3. Get a distance vector for the selected starting point from the distance matrix\n\n4. Find the furthest point\n\n5. Get a distance vector for the new point\n\n6. Find the furthest point from all of already fetched points \n\n\n\n{{< figure src=https://storage.googleapis.com/demo-dataset-quality-public/article/diversity_transparent.png caption=""Diversity search"" >}}\n\n\n\nDiversity search utilizes the very same embeddings, and you can reuse them.', '## Diversity Search\n\n\n\nEven with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.\n\n\n\nThe naive approach is to do random sampling. \n\nHowever, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.\n\n\n\n{{< figure  width=80% src=/articles_data/vector-similarity-beyond-search/diversity-random.png caption=""Example of random sampling"" >}}\n\n\n\n\n\nThe similarity information can increase the diversity of those results and make the first overview more interesting.\n\nThat is especially useful when users do not yet know what they are looking for and want to explore the dataset.\n\n\n\n{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=""Example of similarity-based sampling"" >}}']","The power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.

By maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.



{{< figure src=/articles_data/vector-similarity-beyond-search/diversity.png caption=""Diversity Search"" >}}





Some forms of diversity sampling are already used in the industry and are known as [Maximum Margin Relevance](https://python.langchain.com/docs/integrations/vectorstores/qdrant#maximum-marginal-relevance-search-mmr) (MMR). Techniques like this were developed to enhance similarity on a universal search API.

However, there is still room for new ideas, particularly regarding diversity retrieval.

By utilizing more advanced vector-native engines, it could be possible to take use cases to the next level and achieve even better results.





## Recommendations
The difference lies in deciding which point should be extracted next.



Let's imagine how to get 3 points with similarity search and then with diversity search.



Similarity:

1. Calculate distance matrix

2. Choose your anchor

3. Get a vector corresponding to the distances from the selected anchor from the distance matrix

4. Sort fetched vector

5. Get top-3 embeddings

 

Diversity:

1. Calculate distance matrix

2. Initialize starting point (randomly or according to the certain conditions)

3. Get a distance vector for the selected starting point from the distance matrix

4. Find the furthest point

5. Get a distance vector for the new point

6. Find the furthest point from all of already fetched points 



{{< figure src=https://storage.googleapis.com/demo-dataset-quality-public/article/diversity_transparent.png caption=""Diversity search"" >}}



Diversity search utilizes the very same embeddings, and you can reuse them.
## Diversity Search



Even with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.



The naive approach is to do random sampling. 

However, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.



{{< figure  width=80% src=/articles_data/vector-similarity-beyond-search/diversity-random.png caption=""Example of random sampling"" >}}





The similarity information can increase the diversity of those results and make the first overview more interesting.

That is especially useful when users do not yet know what they are looking for and want to explore the dataset.



{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=""Example of similarity-based sampling"" >}}"
How can you ensure that collection shards are replicated in Qdrant after adding a new node to the cluster?,"To ensure that collection shards are replicated in Qdrant after adding a new node to the cluster, you can use the Replicate Shard Operation. This operation allows you to create another copy of the shard on the newly connected node. It's important to note that Qdrant does not automatically balance shards as it is considered an expensive operation.","[""Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.\n\nUse the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.\n\n\n\nIt's worth mentioning that Qdrant only provides the necessary building blocks to create an automated failure recovery.\n\nBuilding a completely automatic process of collection scaling would require control over the cluster machines themself.\n\nCheck out our [cloud solution](https://qdrant.to/cloud), where we made exactly that.\n\n\n\n\n\n**Recover from snapshot**\n\n\n\nIf there are no copies of data in the cluster, it is still possible to recover from a snapshot.\n\n\n\nFollow the same steps to detach failed node and create a new one in the cluster:"", 'For example, if you have 3 nodes, 6 shards could be a good option.\n\n\n\nShards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.\n\n\n\n### Moving shards\n\n\n\n*Available as of v0.9.0*\n\n\n\nQdrant allows moving shards between nodes in the cluster and removing nodes from the cluster. This functionality unlocks the ability to dynamically scale the cluster size without downtime. It also allows you to upgrade or migrate nodes without downtime.\n\n\n\nQdrant provides the information regarding the current shard distribution in the cluster with the [Collection Cluster info API](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/collection_cluster_info).', 'Practically, it means that if the cluster is in a transition state - either electing a new leader after a failure or starting up, the collection update operations will be denied.\n\n\n\nYou may use the cluster [REST API](https://qdrant.github.io/qdrant/redoc/index.html?v=master#tag/cluster) to check the state of the consensus.\n\n\n\n## Sharding\n\n\n\nA Collection in Qdrant is made of one or more shards.\n\nA shard is an independent store of points which is able to perform all operations provided by collections.\n\nThere are two methods of distributing points across shards:\n\n\n\n- **Automatic sharding**: Points are distributed among shards by using a [consistent hashing](https://en.wikipedia.org/wiki/Consistent_hashing) algorithm, so that shards are managing non-intersecting subsets of points. This is the default behavior.']","Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.

Use the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.



It's worth mentioning that Qdrant only provides the necessary building blocks to create an automated failure recovery.

Building a completely automatic process of collection scaling would require control over the cluster machines themself.

Check out our [cloud solution](https://qdrant.to/cloud), where we made exactly that.





**Recover from snapshot**



If there are no copies of data in the cluster, it is still possible to recover from a snapshot.



Follow the same steps to detach failed node and create a new one in the cluster:
For example, if you have 3 nodes, 6 shards could be a good option.



Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.



### Moving shards



*Available as of v0.9.0*



Qdrant allows moving shards between nodes in the cluster and removing nodes from the cluster. This functionality unlocks the ability to dynamically scale the cluster size without downtime. It also allows you to upgrade or migrate nodes without downtime.



Qdrant provides the information regarding the current shard distribution in the cluster with the [Collection Cluster info API](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/collection_cluster_info).
Practically, it means that if the cluster is in a transition state - either electing a new leader after a failure or starting up, the collection update operations will be denied.



You may use the cluster [REST API](https://qdrant.github.io/qdrant/redoc/index.html?v=master#tag/cluster) to check the state of the consensus.



## Sharding



A Collection in Qdrant is made of one or more shards.

A shard is an independent store of points which is able to perform all operations provided by collections.

There are two methods of distributing points across shards:



- **Automatic sharding**: Points are distributed among shards by using a [consistent hashing](https://en.wikipedia.org/wiki/Consistent_hashing) algorithm, so that shards are managing non-intersecting subsets of points. This is the default behavior."
Why would someone use a vector database?,"A vector database is used for various reasons, primarily for its efficiency in handling complex data structures and enabling advanced search capabilities. In the context of search and discovery, a vector database allows for state-of-the-art vector-search functionalities, making it ideal for applications requiring similarity search, recommendation systems, and content-based search. By leveraging vectors to represent data points, a vector database can efficiently compute similarities between vectors, enabling quick and accurate retrieval of relevant information","[""And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query. So that is also embedded and it's used to look up relevant sources from the data that was previously uploaded efficiently directly on the database, and then we can fit the resulting text, the language model, to synthesize an answer. And this is how the RHe works in very basic form. Now you can see that if you have only a single document that you work with, it's nice if the problem set that you want to solve is very constrained, but the more data you can bring to your system, the more workflows you can build on that data. So if you have, for example, access to a complete book or many books, it's easy to see you can also generate higher quality content from that data"", ""of our tutorials. If you've never used a vector database, go ahead and jump straight into \n\nthe **Getting Started** section. Conversely, if you are a seasoned developer in these \n\ntechnology, jump to the section most relevant to your use case.\n\n\n\nAs you go through the tutorials, please let us know if any questions come up in our \n\n[Discord channel here](https://qdrant.to/discord). 😎"", 'There is no one-size-fits-all approach that would not compromise on performance or flexibility.\n\nSo if your use case utilizes vector search in any significant way, it is worth investing in a dedicated vector search engine, aka vector database.']","And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query. So that is also embedded and it's used to look up relevant sources from the data that was previously uploaded efficiently directly on the database, and then we can fit the resulting text, the language model, to synthesize an answer. And this is how the RHe works in very basic form. Now you can see that if you have only a single document that you work with, it's nice if the problem set that you want to solve is very constrained, but the more data you can bring to your system, the more workflows you can build on that data. So if you have, for example, access to a complete book or many books, it's easy to see you can also generate higher quality content from that data
of our tutorials. If you've never used a vector database, go ahead and jump straight into 

the **Getting Started** section. Conversely, if you are a seasoned developer in these 

technology, jump to the section most relevant to your use case.



As you go through the tutorials, please let us know if any questions come up in our 

[Discord channel here](https://qdrant.to/discord). 😎
There is no one-size-fits-all approach that would not compromise on performance or flexibility.

So if your use case utilizes vector search in any significant way, it is worth investing in a dedicated vector search engine, aka vector database."
What benefits does Qdrant Cloud on Microsoft Azure offer for rapid application development?,"Qdrant Cloud on Microsoft Azure offers the benefit of rapid application development by allowing users to deploy their own cluster through the Qdrant Cloud Console within seconds. This means that users can set up their environment on Azure quickly, reducing deployment time and enabling them to scale their resources as needed. This rapid deployment capability enables users to hit the ground running with their development projects, facilitating faster development cycles and improved scalability.","['---\n\ndraft: false\n\ntitle: Introducing Qdrant Cloud on Microsoft Azure\n\nslug: qdrant-cloud-on-microsoft-azure\n\nshort_description: Qdrant Cloud is now available on Microsoft Azure\n\ndescription: ""Learn the benefits of Qdrant Cloud on Azure.""\n\npreview_image: /blog/from_cms/qdrant-azure-2-1.png\n\ndate: 2024-01-17T08:40:42Z\n\nauthor: Manuel Meyer\n\nfeatured: false\n\ntags:\n\n  - Data Science\n\n  - Vector Database\n\n  - Machine Learning\n\n  - Information Retrieval\n\n  - Cloud\n\n  - Azure\n\n---\n\nGreat news! We\'ve expanded Qdrant\'s managed vector database offering — [Qdrant Cloud](https://cloud.qdrant.io/) — to be available on Microsoft Azure. \n\nYou can now effortlessly set up your environment on Azure, which reduces deployment time, so you can hit the ground running.\n\n\n\n[Get started](https://cloud.qdrant.io/)\n\n\n\nWhat this means for you:\n\n\n\n- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.', '### Cluster configuration\n\n\n\nEach instance comes pre-configured with the following tools, features and support services:\n\n\n\n- Automatically created with the latest available version of Qdrant.\n\n- Upgradeable to later versions of Qdrant as they are released.\n\n- Equipped with monitoring and logging to observe the health of each cluster. \n\n- Accessible through the Qdrant Cloud Console.\n\n- Vertically scalable.\n\n- Offered on AWS and GCP, with Azure currently in development. \n\n\n\n### Getting started with Qdrant Cloud\n\n\n\nTo use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:\n\n1. [**Create a Free Tier cluster**]({{< ref ""/documentation/cloud/quickstart-cloud"" >}}) with 1 node and a default configuration (1GB RAM, 0.5 CPU and 4GB Disk). This option is perfect for prototyping and you don\'t need a credit card to join.', '| **Deployment Modes**                | SaaS-only                     | Local, on-premise, Cloud                     | Qdrant offers more flexibility in deployment modes       |\n\n| **Supported Technologies**          | Python, JavaScript/TypeScript | Python, JavaScript/TypeScript, Rust, Go      | Qdrant supports a broader range of programming languages |\n\n| **Performance** (e.g., query speed) | TnC Prohibit Benchmarking     | [Benchmark result](/benchmarks/)             | Compare performance metrics                              |\n\n| **Pricing**                         | Starts at $70/mo              | Free and Open Source, Cloud starts at $25/mo | Pricing as of May 2023                                   |\n\n\n\n## Prototyping options\n\n\n\nQdrant offers multiple ways of deployment, including local mode, on-premise, and [Qdrant Cloud](https://cloud.qdrant.io/).']","---

draft: false

title: Introducing Qdrant Cloud on Microsoft Azure

slug: qdrant-cloud-on-microsoft-azure

short_description: Qdrant Cloud is now available on Microsoft Azure

description: ""Learn the benefits of Qdrant Cloud on Azure.""

preview_image: /blog/from_cms/qdrant-azure-2-1.png

date: 2024-01-17T08:40:42Z

author: Manuel Meyer

featured: false

tags:

  - Data Science

  - Vector Database

  - Machine Learning

  - Information Retrieval

  - Cloud

  - Azure

---

Great news! We've expanded Qdrant's managed vector database offering — [Qdrant Cloud](https://cloud.qdrant.io/) — to be available on Microsoft Azure. 

You can now effortlessly set up your environment on Azure, which reduces deployment time, so you can hit the ground running.



[Get started](https://cloud.qdrant.io/)



What this means for you:



- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.
### Cluster configuration



Each instance comes pre-configured with the following tools, features and support services:



- Automatically created with the latest available version of Qdrant.

- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:

1. [**Create a Free Tier cluster**]({{< ref ""/documentation/cloud/quickstart-cloud"" >}}) with 1 node and a default configuration (1GB RAM, 0.5 CPU and 4GB Disk). This option is perfect for prototyping and you don't need a credit card to join.
| **Deployment Modes**                | SaaS-only                     | Local, on-premise, Cloud                     | Qdrant offers more flexibility in deployment modes       |

| **Supported Technologies**          | Python, JavaScript/TypeScript | Python, JavaScript/TypeScript, Rust, Go      | Qdrant supports a broader range of programming languages |

| **Performance** (e.g., query speed) | TnC Prohibit Benchmarking     | [Benchmark result](/benchmarks/)             | Compare performance metrics                              |

| **Pricing**                         | Starts at $70/mo              | Free and Open Source, Cloud starts at $25/mo | Pricing as of May 2023                                   |



## Prototyping options



Qdrant offers multiple ways of deployment, including local mode, on-premise, and [Qdrant Cloud](https://cloud.qdrant.io/)."
What is the purpose of the `groupBy` parameter?,"The `groupBy` parameter is used to specify how the results should be grouped based on a specific field, like ""document_id"". By setting the `groupBy` parameter to ""document_id"", the results are organized into groups where each group contains all the hits associated with a particular document_id.","['```http\n\nPOST /collections/chunks/points/search/groups\n\n{\n\n    // Same as in the regular search API\n\n    ""vector"": [1.1],\n\n    ...,\n\n\n\n    // Grouping parameters\n\n    ""group_by"": ""document_id"",  \n\n    ""limit"": 2,                 \n\n    ""group_size"": 2,            \n\n\n\n    // Lookup parameters\n\n    ""with_lookup"": {\n\n        // Name of the collection to look up points in\n\n        ""collection_name"": ""documents"",\n\n\n\n        // Options for specifying what to bring from the payload \n\n        // of the looked up point, true by default\n\n        ""with_payload"": [""title"", ""text""],\n\n\n\n        // Options for specifying what to bring from the vector(s) \n\n        // of the looked up point, true by default\n\n        ""with_vectors: false,\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.search_groups(\n\n    collection_name=""chunks"",\n\n\n\n    # Same as in the regular search() API\n\n    query_vector=[1.1],\n\n    ...,\n\n    \n\n    # Grouping parameters\n\n    group_by=""document_id"", # Path of the field to group by', '```json\n\n{\n\n    ""result"": {\n\n        ""groups"": [\n\n            {\n\n                ""id"": ""a"",\n\n                ""hits"": [\n\n                    { ""id"": 0, ""score"": 0.91 },\n\n                    { ""id"": 1, ""score"": 0.85 }\n\n                ]\n\n            },\n\n            {\n\n                ""id"": ""b"",\n\n                ""hits"": [\n\n                    { ""id"": 1, ""score"": 0.85 }\n\n                ]\n\n            },\n\n            {\n\n                ""id"": 123,\n\n                ""hits"": [\n\n                    { ""id"": 3, ""score"": 0.79 },\n\n                    { ""id"": 4, ""score"": 0.75 }\n\n                ]\n\n            },\n\n            {\n\n                ""id"": -10,\n\n                ""hits"": [\n\n                    { ""id"": 5, ""score"": 0.6 }\n\n                ]\n\n            }\n\n        ]\n\n    },\n\n    ""status"": ""ok"",\n\n    ""time"": 0.001\n\n}\n\n```\n\n\n\nThe groups are ordered by the score of the top point in the group. Inside each group the points are sorted too.', '""{collection_name}"".to_string(),\n\n        None,\n\n        vec![\n\n            PointStruct::new(\n\n                1,\n\n                vec![0.9, 0.1, 0.1],\n\n                json!(\n\n                    {""group_id"": ""user_1""}\n\n                )\n\n                .try_into()\n\n                .unwrap(),\n\n            ),\n\n            PointStruct::new(\n\n                2,\n\n                vec![0.1, 0.9, 0.1],\n\n                json!(\n\n                    {""group_id"": ""user_1""}\n\n                )\n\n                .try_into()\n\n                .unwrap(),\n\n            ),\n\n            PointStruct::new(\n\n                3,\n\n                vec![0.1, 0.1, 0.9],\n\n                json!(\n\n                    {""group_id"": ""user_2""}\n\n                )\n\n                .try_into()\n\n                .unwrap(),\n\n            ),\n\n        ],\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport java.util.List;\n\nimport java.util.Map;\n\n\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;']","```http

POST /collections/chunks/points/search/groups

{

    // Same as in the regular search API

    ""vector"": [1.1],

    ...,



    // Grouping parameters

    ""group_by"": ""document_id"",  

    ""limit"": 2,                 

    ""group_size"": 2,            



    // Lookup parameters

    ""with_lookup"": {

        // Name of the collection to look up points in

        ""collection_name"": ""documents"",



        // Options for specifying what to bring from the payload 

        // of the looked up point, true by default

        ""with_payload"": [""title"", ""text""],



        // Options for specifying what to bring from the vector(s) 

        // of the looked up point, true by default

        ""with_vectors: false,

    }

}

```



```python

client.search_groups(

    collection_name=""chunks"",



    # Same as in the regular search() API

    query_vector=[1.1],

    ...,

    

    # Grouping parameters

    group_by=""document_id"", # Path of the field to group by
```json

{

    ""result"": {

        ""groups"": [

            {

                ""id"": ""a"",

                ""hits"": [

                    { ""id"": 0, ""score"": 0.91 },

                    { ""id"": 1, ""score"": 0.85 }

                ]

            },

            {

                ""id"": ""b"",

                ""hits"": [

                    { ""id"": 1, ""score"": 0.85 }

                ]

            },

            {

                ""id"": 123,

                ""hits"": [

                    { ""id"": 3, ""score"": 0.79 },

                    { ""id"": 4, ""score"": 0.75 }

                ]

            },

            {

                ""id"": -10,

                ""hits"": [

                    { ""id"": 5, ""score"": 0.6 }

                ]

            }

        ]

    },

    ""status"": ""ok"",

    ""time"": 0.001

}

```



The groups are ordered by the score of the top point in the group. Inside each group the points are sorted too.
""{collection_name}"".to_string(),

        None,

        vec![

            PointStruct::new(

                1,

                vec![0.9, 0.1, 0.1],

                json!(

                    {""group_id"": ""user_1""}

                )

                .try_into()

                .unwrap(),

            ),

            PointStruct::new(

                2,

                vec![0.1, 0.9, 0.1],

                json!(

                    {""group_id"": ""user_1""}

                )

                .try_into()

                .unwrap(),

            ),

            PointStruct::new(

                3,

                vec![0.1, 0.1, 0.9],

                json!(

                    {""group_id"": ""user_2""}

                )

                .try_into()

                .unwrap(),

            ),

        ],

        None,

    )

    .await?;

```



```java

import java.util.List;

import java.util.Map;



import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;"
How can one change or correct Qdrant's behavior and default collection settings using configuration files?,"To change or correct Qdrant's behavior and default collection settings, one can utilize configuration files. The default configuration file for Qdrant is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml). If you wish to modify the default configuration, you can add a new configuration file and specify the path using `--config-path path/to/custom_config.yaml","['qdrant::{\n\n        vectors_config::Config, CreateCollection, Distance, VectorParams, VectorParamsMap,\n\n        VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".to_string(),\n\n        vectors_config: Some(VectorsConfig {\n\n            config: Some(Config::ParamsMap(VectorParamsMap {\n\n                map: [\n\n                    (\n\n                        ""image"".to_string(),\n\n                        VectorParams {\n\n                            size: 4,\n\n                            distance: Distance::Dot.into(),\n\n                            ..Default::default()\n\n                        },\n\n                    ),\n\n                    (\n\n                        ""text"".to_string(),\n\n                        VectorParams {\n\n                            size: 8,\n\n                            distance: Distance::Cosine.into(),', ""---\n\ntitle: Configuration\n\nweight: 160\n\naliases:\n\n  - ../configuration\n\n---\n\n\n\n# Configuration\n\n\n\nTo change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.\n\n\n\nThe default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\n\n\n\nTo change the default configuration, add a new configuration file and specify\n\nthe path with `--config-path path/to/custom_config.yaml`. If running in\n\nproduction mode, you could also choose to overwrite `config/production.yaml`.\n\nSee [ordering](#order-and-priority) for details on how configurations are\n\nloaded.\n\n\n\nThe [Installation](../installation) guide contains examples of how to set up Qdrant with a custom configuration for the different deployment methods.\n\n\n\n## Order and priority\n\n\n\n*Effective as of v1.2.1*\n\n\n\nMultiple configurations may be loaded on startup. All of them are merged into a"", 'qdrant::{vectors_config::Config, CreateCollection, Distance, VectorParams, VectorsConfig},\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".into(),\n\n        vectors_config: Some(VectorsConfig {\n\n            config: Some(Config::Params(VectorParams {\n\n                size: 300,\n\n                distance: Distance::Cosine.into(),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        shard_number: Some(6),\n\n        replication_factor: Some(2),\n\n        write_consistency_factor: Some(2),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;']","qdrant::{

        vectors_config::Config, CreateCollection, Distance, VectorParams, VectorParamsMap,

        VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".to_string(),

        vectors_config: Some(VectorsConfig {

            config: Some(Config::ParamsMap(VectorParamsMap {

                map: [

                    (

                        ""image"".to_string(),

                        VectorParams {

                            size: 4,

                            distance: Distance::Dot.into(),

                            ..Default::default()

                        },

                    ),

                    (

                        ""text"".to_string(),

                        VectorParams {

                            size: 8,

                            distance: Distance::Cosine.into(),
---

title: Configuration

weight: 160

aliases:

  - ../configuration

---



# Configuration



To change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.



The default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).



To change the default configuration, add a new configuration file and specify

the path with `--config-path path/to/custom_config.yaml`. If running in

production mode, you could also choose to overwrite `config/production.yaml`.

See [ordering](#order-and-priority) for details on how configurations are

loaded.



The [Installation](../installation) guide contains examples of how to set up Qdrant with a custom configuration for the different deployment methods.



## Order and priority



*Effective as of v1.2.1*



Multiple configurations may be loaded on startup. All of them are merged into a
qdrant::{vectors_config::Config, CreateCollection, Distance, VectorParams, VectorsConfig},

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".into(),

        vectors_config: Some(VectorsConfig {

            config: Some(Config::Params(VectorParams {

                size: 300,

                distance: Distance::Cosine.into(),

                ..Default::default()

            })),

        }),

        shard_number: Some(6),

        replication_factor: Some(2),

        write_consistency_factor: Some(2),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;"
What are the two ways of creating batches supported by the Qdrant API and how do they differ internally?,"The Qdrant API supports two ways of creating batches - record-oriented and column-oriented. Internally, these options do not differ and are made only for the convenience of interaction. This means that both record-oriented and column-oriented batch creation methods achieve the same result in terms of processing and storing data within the Qdrant system.","['using Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.UpsertAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tpoints: new List<PointStruct>\n\n\t{\n\n\t\tnew()\n\n\t\t{\n\n\t\t\tId = 1,\n\n\t\t\tVectors = new[] { 0.05f, 0.61f, 0.76f, 0.74f },\n\n\t\t\tPayload = { [""city""] = ""red"" }\n\n\t\t}\n\n\t}\n\n);\n\n\n\n```\n\n\n\nare both possible.\n\n\n\n## Upload points\n\n\n\nTo optimize performance, Qdrant supports batch loading of points. I.e., you can load several points into the service in one API call.\n\nBatching allows you to minimize the overhead of creating a network connection.\n\n\n\nThe Qdrant API supports two ways of creating batches - record-oriented and column-oriented.\n\nInternally, these options do not differ and are made only for the convenience of interaction.\n\n\n\nCreate points with batch:\n\n\n\n```http\n\nPUT /collections/{collection_name}/points\n\n{\n\n    ""batch"": {\n\n        ""ids"": [1, 2, 3],\n\n        ""payloads"": [\n\n            {""color"": ""red""},\n\n            {""color"": ""green""},\n\n            {""color"": ""blue""}', 'location=""http://localhost:6333"",\n\n            collection_name=""test"",\n\n        ),\n\n        write_config=QdrantWriteConfig(batch_size=80),\n\n    )\n\n\n\nif __name__ == ""__main__"":\n\n    writer = get_writer()\n\n    runner = LocalRunner(\n\n        processor_config=ProcessorConfig(\n\n            verbose=True,\n\n            output_dir=""local-output-to-qdrant"",\n\n            num_processes=2,\n\n        ),\n\n        connector_config=SimpleLocalConfig(\n\n            input_path=""example-docs/book-war-and-peace-1225p.txt"",\n\n        ),\n\n        read_config=ReadConfig(),\n\n        partition_config=PartitionConfig(),\n\n        chunking_config=ChunkingConfig(chunk_elements=True),\n\n        embedding_config=EmbeddingConfig(provider=""langchain-huggingface""),\n\n        writer=writer,\n\n        writer_kwargs={},\n\n    )\n\n    runner.run()\n\n```\n\n\n\n## Next steps\n\n\n\n- Unstructured API [reference](https://unstructured-io.github.io/unstructured/api.html).', '## Summary\n\n\n\nBatch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!']","using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.UpsertAsync(

	collectionName: ""{collection_name}"",

	points: new List<PointStruct>

	{

		new()

		{

			Id = 1,

			Vectors = new[] { 0.05f, 0.61f, 0.76f, 0.74f },

			Payload = { [""city""] = ""red"" }

		}

	}

);



```



are both possible.



## Upload points



To optimize performance, Qdrant supports batch loading of points. I.e., you can load several points into the service in one API call.

Batching allows you to minimize the overhead of creating a network connection.



The Qdrant API supports two ways of creating batches - record-oriented and column-oriented.

Internally, these options do not differ and are made only for the convenience of interaction.



Create points with batch:



```http

PUT /collections/{collection_name}/points

{

    ""batch"": {

        ""ids"": [1, 2, 3],

        ""payloads"": [

            {""color"": ""red""},

            {""color"": ""green""},

            {""color"": ""blue""}
location=""http://localhost:6333"",

            collection_name=""test"",

        ),

        write_config=QdrantWriteConfig(batch_size=80),

    )



if __name__ == ""__main__"":

    writer = get_writer()

    runner = LocalRunner(

        processor_config=ProcessorConfig(

            verbose=True,

            output_dir=""local-output-to-qdrant"",

            num_processes=2,

        ),

        connector_config=SimpleLocalConfig(

            input_path=""example-docs/book-war-and-peace-1225p.txt"",

        ),

        read_config=ReadConfig(),

        partition_config=PartitionConfig(),

        chunking_config=ChunkingConfig(chunk_elements=True),

        embedding_config=EmbeddingConfig(provider=""langchain-huggingface""),

        writer=writer,

        writer_kwargs={},

    )

    runner.run()

```



## Next steps



- Unstructured API [reference](https://unstructured-io.github.io/unstructured/api.html).
## Summary



Batch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!"
How can you create an index for a specific field in a payload using the Qdrant library?,"To create an index for a specific field in a payload using the Qdrant library, you can utilize the `CreatePayloadIndexAsync` method provided by the client. This method takes in the parameters `collectionName` and `fieldName`. ","['field_name=""name_of_the_field_to_index"",\n\n    field_schema=models.TextIndexParams(\n\n        type=""text"",\n\n        tokenizer=models.TokenizerType.WORD,\n\n        min_token_len=2,\n\n        max_token_len=15,\n\n        lowercase=True,\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient, Schemas } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createPayloadIndex(""{collection_name}"", {\n\n  field_name: ""name_of_the_field_to_index"",\n\n  field_schema: {\n\n    type: ""text"",\n\n    tokenizer: ""word"",\n\n    min_token_len: 2,\n\n    max_token_len: 15,\n\n    lowercase: true,\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\n\n        TokenizerType,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_field_index(\n\n        ""{collection_name}"",', 'PUT /collections/{collection_name}/index\n\n{\n\n    ""field_name"": ""group_id"",\n\n    ""field_schema"": ""keyword""\n\n}\n\n```\n\n\n\n```python\n\nclient.create_payload_index(\n\n    collection_name=""{collection_name}"",\n\n    field_name=""group_id"",\n\n    field_schema=models.PayloadSchemaType.KEYWORD,\n\n)\n\n```\n\n\n\n```typescript\n\nclient.createPayloadIndex(""{collection_name}"", {\n\n  field_name: ""group_id"",\n\n  field_schema: ""keyword"",\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{client::QdrantClient, qdrant::FieldType};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_field_index(\n\n        ""{collection_name}"",\n\n        ""group_id"",\n\n        FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient', 'REST API ([Schema](https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_field_index))\n\n\n\n```http\n\nPUT /collections/{collection_name}/index\n\n{\n\n    ""field_name"": ""name_of_the_field_to_index"",\n\n    ""field_schema"": ""keyword""\n\n}\n\n```\n\n\n\n```python\n\nclient.create_payload_index(\n\n    collection_name=""{collection_name}"",\n\n    field_name=""name_of_the_field_to_index"",\n\n    field_schema=""keyword"",\n\n)\n\n```\n\n\n\n```typescript\n\nclient.createPayloadIndex(""{collection_name}"", {\n\n  field_name: ""name_of_the_field_to_index"",\n\n  field_schema: ""keyword"",\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::qdrant::FieldType;\n\n\n\nclient\n\n    .create_field_index(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nclient.createPayloadIndexAsync(\n\n    ""{collection_name}"",\n\n    ""name_of_the_field_to_index"",\n\n    PayloadSchemaType.Keyword,']","field_name=""name_of_the_field_to_index"",

    field_schema=models.TextIndexParams(

        type=""text"",

        tokenizer=models.TokenizerType.WORD,

        min_token_len=2,

        max_token_len=15,

        lowercase=True,

    ),

)

```



```typescript

import { QdrantClient, Schemas } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createPayloadIndex(""{collection_name}"", {

  field_name: ""name_of_the_field_to_index"",

  field_schema: {

    type: ""text"",

    tokenizer: ""word"",

    min_token_len: 2,

    max_token_len: 15,

    lowercase: true,

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,

        TokenizerType,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_field_index(

        ""{collection_name}"",
PUT /collections/{collection_name}/index

{

    ""field_name"": ""group_id"",

    ""field_schema"": ""keyword""

}

```



```python

client.create_payload_index(

    collection_name=""{collection_name}"",

    field_name=""group_id"",

    field_schema=models.PayloadSchemaType.KEYWORD,

)

```



```typescript

client.createPayloadIndex(""{collection_name}"", {

  field_name: ""group_id"",

  field_schema: ""keyword"",

});

```



```rust

use qdrant_client::{client::QdrantClient, qdrant::FieldType};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_field_index(

        ""{collection_name}"",

        ""group_id"",

        FieldType::Keyword,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client
REST API ([Schema](https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_field_index))



```http

PUT /collections/{collection_name}/index

{

    ""field_name"": ""name_of_the_field_to_index"",

    ""field_schema"": ""keyword""

}

```



```python

client.create_payload_index(

    collection_name=""{collection_name}"",

    field_name=""name_of_the_field_to_index"",

    field_schema=""keyword"",

)

```



```typescript

client.createPayloadIndex(""{collection_name}"", {

  field_name: ""name_of_the_field_to_index"",

  field_schema: ""keyword"",

});

```



```rust

use qdrant_client::qdrant::FieldType;



client

    .create_field_index(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        FieldType::Keyword,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



client.createPayloadIndexAsync(

    ""{collection_name}"",

    ""name_of_the_field_to_index"",

    PayloadSchemaType.Keyword,"
What is the purpose of the Quantization?,"Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces. In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.","[""Andrey Vasnetsov:\n\nRight, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear. Would it be efficient to convert into four bits and then apply multiplication of four bits? So this would require additional investigation, and I cannot say that we have immediate plans to do so because still the binary quincellation field is not yet explored on 100% and we think it's a lot more potential with this than currently unlocked.\n\n\n\nDemetrios:"", ""Demetrios:\n\nYeah, there's some low hanging fruits still on the binary quantization field, so tackle those first and then move your way over to four bit and all that fun stuff. Last question that I've got for you is can we remove original vectors and only keep quantized ones in order to save disk space?\n\n\n\nAndrey Vasnetsov:"", '## Quantum Quantization and Entanglement\n\n\n\nQuantum quantization is a novel approach that leverages the power of quantum computing to speed up the search process in ANNs. By converting traditional float32 vectors into qbit vectors, we can create quantum entanglement between the qbits. Quantum entanglement is a unique phenomenon in which the states of two or more particles become interdependent, regardless of the distance between them. This property of quantum systems can be harnessed to create highly efficient vector search algorithms.\n\n\n\n\n\nThe conversion of float32 vectors to qbit vectors can be represented by the following formula:\n\n\n\n```text\n\nqbit_vector = Q( float32_vector )\n\n```\n\n\n\nwhere Q is the quantum quantization function that transforms the float32_vector into a quantum entangled qbit_vector.\n\n\n\n\n\n## Vector Search in Constant Time\n\n\n\nThe primary advantage of using quantum quantization for ANN vector search is the ability to search through an arbitrary-sized database in constant time.']","Andrey Vasnetsov:

Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear. Would it be efficient to convert into four bits and then apply multiplication of four bits? So this would require additional investigation, and I cannot say that we have immediate plans to do so because still the binary quincellation field is not yet explored on 100% and we think it's a lot more potential with this than currently unlocked.



Demetrios:
Demetrios:

Yeah, there's some low hanging fruits still on the binary quantization field, so tackle those first and then move your way over to four bit and all that fun stuff. Last question that I've got for you is can we remove original vectors and only keep quantized ones in order to save disk space?



Andrey Vasnetsov:
## Quantum Quantization and Entanglement



Quantum quantization is a novel approach that leverages the power of quantum computing to speed up the search process in ANNs. By converting traditional float32 vectors into qbit vectors, we can create quantum entanglement between the qbits. Quantum entanglement is a unique phenomenon in which the states of two or more particles become interdependent, regardless of the distance between them. This property of quantum systems can be harnessed to create highly efficient vector search algorithms.





The conversion of float32 vectors to qbit vectors can be represented by the following formula:



```text

qbit_vector = Q( float32_vector )

```



where Q is the quantum quantization function that transforms the float32_vector into a quantum entangled qbit_vector.





## Vector Search in Constant Time



The primary advantage of using quantum quantization for ANN vector search is the ability to search through an arbitrary-sized database in constant time."
How can the retrieval quality of an approximation be measured in the context of semantic search?,"The retrieval quality of an approximation in semantic search can be measured using various quality metrics. Some of these metrics include Precision@k, Mean Reciprocal Rank (MRR), and DCG and NDCG.","['---\n\ntitle: Measure retrieval quality\n\nweight: 21\n\n---\n\n\n\n# Measure retrieval quality\n\n\n\n| Time: 30 min | Level: Intermediate |  |    |\n\n|--------------|---------------------|--|----|\n\n\n\nSemantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might\n\nbe far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another\n\ncomponent of the process which can also degrade the quality of the search results. It is the ANN algorithm itself. \n\n\n\nIn this tutorial, we will show how to measure the quality of the semantic retrieval and how to tune the parameters of the HNSW, the ANN \n\nalgorithm used in Qdrant, to obtain the best results.\n\n\n\n## Embeddings quality\n\n\n\nThe quality of the embeddings is a topic for a separate tutorial. In a nutshell, it is usually measured and compared by benchmarks, such as', 'but can return suboptimal results. We can also **measure the retrieval quality of that approximation** which also contributes to the overall\n\nsearch quality.\n\n\n\n### Quality metrics\n\n\n\nThere are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), \n\nare based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank), \n\ntake into account the position of the first relevant document in the search results. [DCG and NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) \n\nmetrics are, in turn, based on the relevance score of the documents.\n\n\n\nIf we treat the search pipeline as a whole, we could use them all. The same is true for the embeddings quality evaluation. However, for the', 'to do it. \n\n\n\n## Wrapping up\n\n\n\nAssessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.\n\nyour search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, \n\neven in an automated way, as part of your CI/CD pipeline.\n\n\n\nAgain, **the quality of the embeddings is the most important factor**. HNSW does a pretty good job in terms of precision, and it is\n\nparameterizable and tunable, when required. There are some other ANN algorithms available out there, such as [IVF*](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#cell-probe-methods-indexivf-indexes), \n\nbut they usually [perform worse than HNSW in terms of quality and performance](https://nirantk.com/writing/pgvector-vs-qdrant/#correctness).']","---

title: Measure retrieval quality

weight: 21

---



# Measure retrieval quality



| Time: 30 min | Level: Intermediate |  |    |

|--------------|---------------------|--|----|



Semantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might

be far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another

component of the process which can also degrade the quality of the search results. It is the ANN algorithm itself. 



In this tutorial, we will show how to measure the quality of the semantic retrieval and how to tune the parameters of the HNSW, the ANN 

algorithm used in Qdrant, to obtain the best results.



## Embeddings quality



The quality of the embeddings is a topic for a separate tutorial. In a nutshell, it is usually measured and compared by benchmarks, such as
but can return suboptimal results. We can also **measure the retrieval quality of that approximation** which also contributes to the overall

search quality.



### Quality metrics



There are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), 

are based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank), 

take into account the position of the first relevant document in the search results. [DCG and NDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) 

metrics are, in turn, based on the relevance score of the documents.



If we treat the search pipeline as a whole, we could use them all. The same is true for the embeddings quality evaluation. However, for the
to do it. 



## Wrapping up



Assessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.

your search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, 

even in an automated way, as part of your CI/CD pipeline.



Again, **the quality of the embeddings is the most important factor**. HNSW does a pretty good job in terms of precision, and it is

parameterizable and tunable, when required. There are some other ANN algorithms available out there, such as [IVF*](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#cell-probe-methods-indexivf-indexes), 

but they usually [perform worse than HNSW in terms of quality and performance](https://nirantk.com/writing/pgvector-vs-qdrant/#correctness)."
Why does Qdrant deliberately exclude libraries or algorithm implementations in their benchmark comparisons?,"Qdrant excludes libraries or algorithm implementations in their benchmark comparisons because their primary focus is on vector databases. By limiting their comparisons to open-source solutions and avoiding external cloud components, Qdrant ensures hardware parity and minimizes biases. This allows them to provide accurate and unbiased benchmarks specifically tailored to the performance of vector databases, enabling users to make informed decisions based on the data provided.","[""![mean-time vs precision benchmark - down and to the right is better](/blog/qdrant-updated-benchmarks-2024/latency-bench.png)\n\n### What Hasn't Changed?\n\n\n\n#### Our Principles of Benchmarking\n\n\n\nAt Qdrant all code stays open-source. We ensure our benchmarks are accessible for everyone, allowing you to run them on your own hardware. Your input matters to us, and contributions and sharing of best practices are welcome!\n\n\n\n\n\nOur benchmarks are strictly limited to open-source solutions, ensuring hardware parity and avoiding biases from external cloud components. \n\n\n\n\n\nWe deliberately don't include libraries or algorithm implementations in our comparisons because our focus is squarely on vector databases. \n\n\n\nWhy?"", ""Andrey Vasnetsov:\n\nRight? So unfortunately Qdrant architecture is not designed and not expecting this type of behavior for several reasons. First of all, removing of the original vectors will compromise some features like oversampling, like segment building. And actually removing of those original vectors will only be compatible with some types of quantization for example, it won't be compatible with scalar quantization because in this case we won't be able to rebuild index to do maintenance of the system. And in order to maintain, how would you say, consistency of the API, consistency of the engine, we decided to enforce always enforced storing of the original vectors. But the good news is that you can always keep original vectors on just disk storage. It's very cheap. Usually it's ten times or even more times cheaper than RAM, and it already gives you great advantage in terms of price. That's answer excellent.\n\n\n\nDemetrios:"", '{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),\n\n    quantization_config=models.ScalarQuantization(\n\n        scalar=models.ScalarQuantizationConfig(\n\n            type=models.ScalarType.INT8,\n\n            always_ram=True,\n\n        ),\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });']","![mean-time vs precision benchmark - down and to the right is better](/blog/qdrant-updated-benchmarks-2024/latency-bench.png)

### What Hasn't Changed?



#### Our Principles of Benchmarking



At Qdrant all code stays open-source. We ensure our benchmarks are accessible for everyone, allowing you to run them on your own hardware. Your input matters to us, and contributions and sharing of best practices are welcome!





Our benchmarks are strictly limited to open-source solutions, ensuring hardware parity and avoiding biases from external cloud components. 





We deliberately don't include libraries or algorithm implementations in our comparisons because our focus is squarely on vector databases. 



Why?
Andrey Vasnetsov:

Right? So unfortunately Qdrant architecture is not designed and not expecting this type of behavior for several reasons. First of all, removing of the original vectors will compromise some features like oversampling, like segment building. And actually removing of those original vectors will only be compatible with some types of quantization for example, it won't be compatible with scalar quantization because in this case we won't be able to rebuild index to do maintenance of the system. And in order to maintain, how would you say, consistency of the API, consistency of the engine, we decided to enforce always enforced storing of the original vectors. But the good news is that you can always keep original vectors on just disk storage. It's very cheap. Usually it's ten times or even more times cheaper than RAM, and it already gives you great advantage in terms of price. That's answer excellent.



Demetrios:
{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": true

        }

    }

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),

    quantization_config=models.ScalarQuantization(

        scalar=models.ScalarQuantizationConfig(

            type=models.ScalarType.INT8,

            always_ram=True,

        ),

    ),

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });"
What is the primary purpose of a Vector Database and why would someone choose to use it over traditional databases?,"A Vector Database is a specialized database system that is specifically designed for efficiently indexing, querying, and retrieving high-dimensional vector data. The primary purpose of a Vector Database is to enable advanced data analysis and similarity-search operations that go beyond the capabilities of traditional, structured query approaches used in conventional databases.","[""or, in other words, to understand how far apart they are.\n\n\n\nNow that we know what vector databases are and how they are structurally different than other \n\ndatabases, let's go over why they are important.\n\n\n\n## Why do we need Vector Databases?\n\n\n\nVector databases play a crucial role in various applications that require similarity search, such \n\nas recommendation systems, content-based image retrieval, and personalized search. By taking \n\nadvantage of their efficient indexing and searching techniques, vector databases enable faster \n\nand more accurate retrieval of unstructured data already represented as vectors, which can \n\nhelp put in front of users the most relevant results to their queries.\n\n\n\nIn addition, other benefits of using vector databases include:\n\n1. Efficient storage and indexing of high-dimensional data.\n\n3. Ability to handle large-scale datasets with billions of data points.\n\n4. Support for real-time analytics and queries."", ""tags: \n\n  - vector-search\n\n  - vector-database\n\n  - embeddings\n\n\n\naliases: [ /blog/what-is-a-vector-database/ ]\n\n---\n\n\n\n> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.\n\n\n\n\n\n## Why use a Vector Database?\n\n\n\nThe data flood is real. \n\n\n\nIn 2024, we're drowning in unstructured data like images, text, and audio, that don’t fit into neatly organized tables. Still, we need a way to easily tap into the value within this chaos of almost 330 million terabytes of data being created each day.\n\n\n\nTraditional databases, even with extensions that provide some vector handling capabilities, struggle with the complexities and demands of high-dimensional vector data."", ""And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query. So that is also embedded and it's used to look up relevant sources from the data that was previously uploaded efficiently directly on the database, and then we can fit the resulting text, the language model, to synthesize an answer. And this is how the RHe works in very basic form. Now you can see that if you have only a single document that you work with, it's nice if the problem set that you want to solve is very constrained, but the more data you can bring to your system, the more workflows you can build on that data. So if you have, for example, access to a complete book or many books, it's easy to see you can also generate higher quality content from that data""]","or, in other words, to understand how far apart they are.



Now that we know what vector databases are and how they are structurally different than other 

databases, let's go over why they are important.



## Why do we need Vector Databases?



Vector databases play a crucial role in various applications that require similarity search, such 

as recommendation systems, content-based image retrieval, and personalized search. By taking 

advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
tags: 

  - vector-search

  - vector-database

  - embeddings



aliases: [ /blog/what-is-a-vector-database/ ]

---



> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.





## Why use a Vector Database?



The data flood is real. 



In 2024, we're drowning in unstructured data like images, text, and audio, that don’t fit into neatly organized tables. Still, we need a way to easily tap into the value within this chaos of almost 330 million terabytes of data being created each day.



Traditional databases, even with extensions that provide some vector handling capabilities, struggle with the complexities and demands of high-dimensional vector data.
And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query. So that is also embedded and it's used to look up relevant sources from the data that was previously uploaded efficiently directly on the database, and then we can fit the resulting text, the language model, to synthesize an answer. And this is how the RHe works in very basic form. Now you can see that if you have only a single document that you work with, it's nice if the problem set that you want to solve is very constrained, but the more data you can bring to your system, the more workflows you can build on that data. So if you have, for example, access to a complete book or many books, it's easy to see you can also generate higher quality content from that data"
"How does oversampling impact the performance of machine learning models, especially in the context of imbalanced datasets?","Oversampling is a technique used in machine learning to address imbalances in datasets, where one class significantly outnumbers others. This imbalance can lead to skewed model performance, as the model may favor the majority class at the expense of minority classes. By generating additional samples from the minority classes, oversampling helps to equalize the representation of different classes in the training dataset.","['index=""limit"", columns=[""oversampling"", ""rescore""], values=""accuracy""\n\n    )\n\n    print(acc)\n\n```\n\n\n\n#### Impact of Oversampling\n\n\n\nYou can use oversampling in machine learning to counteract imbalances in datasets.\n\nIt works well when one class significantly outnumbers others. This imbalance\n\ncan skew the performance of models, which favors the majority class at the\n\nexpense of others. By creating additional samples from the minority classes,\n\noversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.\n\n\n\nThe screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren\'t shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one.', 'Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects.\n\n\n\n![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png)\n\n\n\n### Leveraging Binary Quantization: Best Practices\n\n\n\nWe recommend the following best practices for leveraging Binary Quantization to enhance OpenAI embeddings:\n\n\n\n1. Embedding Model: Use the text-embedding-3-large from MTEB. It is most accurate among those tested.\n\n2. Dimensions: Use the highest dimension available for the model, to maximize accuracy. The results are true for English and other languages.\n\n3. Oversampling: Use an oversampling factor of 3 for the best balance between accuracy and efficiency. This factor is suitable for a wide range of applications.\n\n4. Rescoring: Enable rescoring to improve the accuracy of search results.', '**Oversampling:**\n\nIn the figure below, we illustrate the relationship between recall and number of candidates:\n\n\n\n![Correct vs candidates](/articles_data/binary-quantization/bq-5.png)\n\n\n\nWe see that ""correct"" results i.e. recall increases as the number of potential ""candidates"" increase (limit x oversampling). To highlight the impact of changing the `limit`, different limit values are broken apart into different curves. For example, we see that the lowest recall for limit 50 is around 94 correct, with 100 candidates. This also implies we used an oversampling of 2.0\n\n\n\nAs oversampling increases, we see a general improvement in results – but that does not hold in every case. \n\n\n\n**Rescore:**\n\nAs expected, rescoring increases the time it takes to return a query. \n\nWe also repeated the experiment with oversampling except this time we looked at how rescore impacted result accuracy. \n\n\n\n![Relationship between limit and rescore on correct](/articles_data/binary-quantization/bq-7.png)\n\n\n\n**Limit:**']","index=""limit"", columns=[""oversampling"", ""rescore""], values=""accuracy""

    )

    print(acc)

```



#### Impact of Oversampling



You can use oversampling in machine learning to counteract imbalances in datasets.

It works well when one class significantly outnumbers others. This imbalance

can skew the performance of models, which favors the majority class at the

expense of others. By creating additional samples from the minority classes,

oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.



The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one.
Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects.



![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png)



### Leveraging Binary Quantization: Best Practices



We recommend the following best practices for leveraging Binary Quantization to enhance OpenAI embeddings:



1. Embedding Model: Use the text-embedding-3-large from MTEB. It is most accurate among those tested.

2. Dimensions: Use the highest dimension available for the model, to maximize accuracy. The results are true for English and other languages.

3. Oversampling: Use an oversampling factor of 3 for the best balance between accuracy and efficiency. This factor is suitable for a wide range of applications.

4. Rescoring: Enable rescoring to improve the accuracy of search results.
**Oversampling:**

In the figure below, we illustrate the relationship between recall and number of candidates:



![Correct vs candidates](/articles_data/binary-quantization/bq-5.png)



We see that ""correct"" results i.e. recall increases as the number of potential ""candidates"" increase (limit x oversampling). To highlight the impact of changing the `limit`, different limit values are broken apart into different curves. For example, we see that the lowest recall for limit 50 is around 94 correct, with 100 candidates. This also implies we used an oversampling of 2.0



As oversampling increases, we see a general improvement in results – but that does not hold in every case. 



**Rescore:**

As expected, rescoring increases the time it takes to return a query. 

We also repeated the experiment with oversampling except this time we looked at how rescore impacted result accuracy. 



![Relationship between limit and rescore on correct](/articles_data/binary-quantization/bq-7.png)



**Limit:**"
How does binary quantization work in the context of vector comparisons?,"Binary quantization is a method used in indexing and data compression, particularly by Qdrant, that involves splitting a data point's vector in half at a certain point. This process essentially divides the vector into two parts, marking everything above the split point as ""1"" and everything below as ""0"". The result is a string of bits that represents the original vector in a compressed form. This quantized code is much smaller and easier to compare. Especially for OpenAI embeddings, this type of quantization has proven to achieve a massive performance improvement at a lower cost of accuracy.","[""Demetrios:\n\nYeah, there's some low hanging fruits still on the binary quantization field, so tackle those first and then move your way over to four bit and all that fun stuff. Last question that I've got for you is can we remove original vectors and only keep quantized ones in order to save disk space?\n\n\n\nAndrey Vasnetsov:"", 'However, this value depends on the data and the quantization parameters.\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n\n\n## Binary Quantization\n\n\n\n*Available as of v1.5.0*\n\n\n\nBinary quantization is an extreme case of scalar quantization.\n\nThis feature lets you represent each vector component as a single bit, effectively reducing the memory footprint by a **factor of 32**.\n\n\n\nThis is the fastest quantization method, since it lets you perform a vector comparison with a few CPU instructions.\n\n\n\nBinary quantization can achieve up to a **40x** speedup compared to the original vectors.\n\n\n\nHowever, binary quantization is only efficient for high-dimensional vectors and require a centered distribution of vector components. \n\n\n\nAt the moment, binary quantization shows good accuracy results with the following models:', '[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).\n\n\n\nThink of each data point as a ruler. Binary quantization splits this ruler in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This [binarization](https://deepai.org/machine-learning-glossary-and-terms/binarization) process results in a string of bits, representing the original vector.\n\n\n\n\n\n\n\n![](/articles_data/what-is-a-vector-database/Binary-Quant.png)\n\n\n\n\n\nThis ""quantized"" code is much smaller and easier to compare. Especially for OpenAI embeddings, this type of quantization has proven to achieve a massive performance improvement at a lower cost of accuracy.\n\n\n\n\n\n### What is Similarity Search?']","Demetrios:

Yeah, there's some low hanging fruits still on the binary quantization field, so tackle those first and then move your way over to four bit and all that fun stuff. Last question that I've got for you is can we remove original vectors and only keep quantized ones in order to save disk space?



Andrey Vasnetsov:
However, this value depends on the data and the quantization parameters.

Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.





## Binary Quantization



*Available as of v1.5.0*



Binary quantization is an extreme case of scalar quantization.

This feature lets you represent each vector component as a single bit, effectively reducing the memory footprint by a **factor of 32**.



This is the fastest quantization method, since it lets you perform a vector comparison with a few CPU instructions.



Binary quantization can achieve up to a **40x** speedup compared to the original vectors.



However, binary quantization is only efficient for high-dimensional vectors and require a centered distribution of vector components. 



At the moment, binary quantization shows good accuracy results with the following models:
[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).



Think of each data point as a ruler. Binary quantization splits this ruler in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This [binarization](https://deepai.org/machine-learning-glossary-and-terms/binarization) process results in a string of bits, representing the original vector.







![](/articles_data/what-is-a-vector-database/Binary-Quant.png)





This ""quantized"" code is much smaller and easier to compare. Especially for OpenAI embeddings, this type of quantization has proven to achieve a massive performance improvement at a lower cost of accuracy.





### What is Similarity Search?"
What is the significance of the 'always_ram' parameter in the context of vector quantization in Qdrant?,"In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times.","['You will still get faster boolean operations and reduced RAM usage, but the accuracy degradation might be too high. \n\n\n\n## Sample Implementation\n\n\n\nNow that we have introduced you to binary quantization, let’s try our a basic implementation. In this example, we will be using OpenAI and Cohere with Qdrant.\n\n\n\n#### Create a collection with Binary Quantization enabled\n\n\n\nHere is what you should do at indexing time when you create the collection: \n\n\n\n1. We store all the ""full"" vectors on disk.\n\n2. Then we set the binary embeddings to be in RAM.\n\n\n\nBy default, both the full vectors and BQ get stored in RAM. We move the full vectors to disk because this saves us memory and allows us to store more vectors in RAM. By doing this, we explicitly move the binary vectors to memory by setting `always_ram=True`. \n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\n#collect to our Qdrant Server\n\nclient = QdrantClient(\n\n    url=""http://localhost:6333"",\n\n    prefer_grpc=True,\n\n)', ""Andrey Vasnetsov:\n\nRight? So unfortunately Qdrant architecture is not designed and not expecting this type of behavior for several reasons. First of all, removing of the original vectors will compromise some features like oversampling, like segment building. And actually removing of those original vectors will only be compatible with some types of quantization for example, it won't be compatible with scalar quantization because in this case we won't be able to rebuild index to do maintenance of the system. And in order to maintain, how would you say, consistency of the API, consistency of the engine, we decided to enforce always enforced storing of the original vectors. But the good news is that you can always keep original vectors on just disk storage. It's very cheap. Usually it's ten times or even more times cheaper than RAM, and it already gives you great advantage in terms of price. That's answer excellent.\n\n\n\nDemetrios:"", 'Using quantiles lower than `1.0` might be useful if there are outliers in your vector components.\n\nThis parameter only affects the resulting precision and not the memory footprint.\n\nIt might be worth tuning this parameter if you experience a significant decrease in search quality.\n\n\n\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\n\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\n\n\n\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\n\n\n\n### Setting up Binary Quantization\n\n\n\nTo enable binary quantization, you need to specify the quantization parameters in the `quantization_config` section of the collection configuration.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 1536,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""quantization_config"": {\n\n        ""binary"": {']","You will still get faster boolean operations and reduced RAM usage, but the accuracy degradation might be too high. 



## Sample Implementation



Now that we have introduced you to binary quantization, let’s try our a basic implementation. In this example, we will be using OpenAI and Cohere with Qdrant.



#### Create a collection with Binary Quantization enabled



Here is what you should do at indexing time when you create the collection: 



1. We store all the ""full"" vectors on disk.

2. Then we set the binary embeddings to be in RAM.



By default, both the full vectors and BQ get stored in RAM. We move the full vectors to disk because this saves us memory and allows us to store more vectors in RAM. By doing this, we explicitly move the binary vectors to memory by setting `always_ram=True`. 



```python

from qdrant_client import QdrantClient



#collect to our Qdrant Server

client = QdrantClient(

    url=""http://localhost:6333"",

    prefer_grpc=True,

)
Andrey Vasnetsov:

Right? So unfortunately Qdrant architecture is not designed and not expecting this type of behavior for several reasons. First of all, removing of the original vectors will compromise some features like oversampling, like segment building. And actually removing of those original vectors will only be compatible with some types of quantization for example, it won't be compatible with scalar quantization because in this case we won't be able to rebuild index to do maintenance of the system. And in order to maintain, how would you say, consistency of the API, consistency of the engine, we decided to enforce always enforced storing of the original vectors. But the good news is that you can always keep original vectors on just disk storage. It's very cheap. Usually it's ten times or even more times cheaper than RAM, and it already gives you great advantage in terms of price. That's answer excellent.



Demetrios:
Using quantiles lower than `1.0` might be useful if there are outliers in your vector components.

This parameter only affects the resulting precision and not the memory footprint.

It might be worth tuning this parameter if you experience a significant decrease in search quality.



`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.

However, in some setups you might want to keep quantized vectors in RAM to speed up the search process.



In this case, you can set `always_ram` to `true` to store quantized vectors in RAM.



### Setting up Binary Quantization



To enable binary quantization, you need to specify the quantization parameters in the `quantization_config` section of the collection configuration.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 1536,

      ""distance"": ""Cosine""

    },

    ""quantization_config"": {

        ""binary"": {"
How can automatic backups be set up for clusters using the Cloud UI?,"Automatic backups for clusters can be set up using the Cloud UI by following the procedures listed on the page. These procedures allow you to configure snapshots on a daily, weekly, or monthly basis. You have the flexibility to keep as many snapshots as needed and can restore a cluster from the snapshot of your choice. It is important to note that during the restoration of a snapshot, the affected cluster will not be available.","['- [Create a cluster](/documentation/cloud/create-cluster/)\n\n- Set up [Authentication](/documentation/cloud/authentication/)\n\n- Configure one or more [Collections](/documentation/concepts/collections/)\n\n\n\n## Automatic backups\n\n\n\nYou can set up automatic backups of your clusters with our Cloud UI. With the\n\nprocedures listed in this page, you can set up\n\nsnapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you\n\nneed. You can restore a cluster from the snapshot of your choice.\n\n\n\n> Note: When you restore a snapshot, consider the following:\n\n> - The affected cluster is not available while a snapshot is being restored.\n\n> - If you changed the cluster setup after the copy was created, the cluster \n\n    resets to the previous configuration.\n\n> - The previous configuration includes:\n\n>   - CPU\n\n>   - Memory\n\n>   - Node count\n\n>   - Qdrant version\n\n\n\n### Configure a backup\n\n\n\nAfter you have taken the prerequisite steps, you can configure a backup with the', ""### Configure a backup\n\n\n\nAfter you have taken the prerequisite steps, you can configure a backup with the\n\n[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:\n\n\n\n1. Sign in to the dashboard\n\n1. Select Clusters.\n\n1. Select the cluster that you want to back up.\n\n   ![Select a cluster](/documentation/cloud/select-cluster.png)\n\n1. Find and select the **Backups** tab.\n\n1. Now you can set up a backup schedule.\n\n   The **Days of Retention** is the number of days after a backup snapshot is\n\n   deleted.\n\n1. Alternatively, you can select **Backup now** to take an immediate snapshot.\n\n\n\n![Configure a cluster backup](/documentation/cloud/backup-schedule.png)\n\n\n\n### Restore a backup\n\n\n\nIf you have a backup, it appears in the list of **Available Backups**. You can\n\nchoose to restore or delete the backups of your choice.\n\n\n\n![Restore or delete a cluster backup](/documentation/cloud/restore-delete.png)\n\n\n\n<!-- I think we should move this to the Snapshot page, but I'll do it later -->"", ""---\n\ntitle: Backups\n\nweight: 70\n\n---\n\n\n\n# Cloud Backups\n\n\n\nQdrant organizes cloud instances as clusters. On occasion, you may need to\n\nrestore your cluster because of application or system failure.\n\n\n\nYou may already have a source of truth for your data in a regular database. If you\n\nhave a problem, you could reindex the data into your Qdrant vector search cluster.\n\nHowever, this process can take time. For high availability critical projects we\n\nrecommend replication. It guarantees the proper cluster functionality as long as\n\nat least one replica is running.\n\n\n\nFor other use-cases such as disaster recovery, you can set up automatic or\n\nself-service backups.\n\n\n\n## Prerequisites\n\n\n\nYou can back up your Qdrant clusters though the Qdrant Cloud\n\nDashboard at https://cloud.qdrant.io. This section assumes that you've already\n\nset up your cluster, as described in the following sections:\n\n\n\n- [Create a cluster](/documentation/cloud/create-cluster/)\n\n- Set up [Authentication](/documentation/cloud/authentication/)""]","- [Create a cluster](/documentation/cloud/create-cluster/)

- Set up [Authentication](/documentation/cloud/authentication/)

- Configure one or more [Collections](/documentation/concepts/collections/)



## Automatic backups



You can set up automatic backups of your clusters with our Cloud UI. With the

procedures listed in this page, you can set up

snapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you

need. You can restore a cluster from the snapshot of your choice.



> Note: When you restore a snapshot, consider the following:

> - The affected cluster is not available while a snapshot is being restored.

> - If you changed the cluster setup after the copy was created, the cluster 

    resets to the previous configuration.

> - The previous configuration includes:

>   - CPU

>   - Memory

>   - Node count

>   - Qdrant version



### Configure a backup



After you have taken the prerequisite steps, you can configure a backup with the
### Configure a backup



After you have taken the prerequisite steps, you can configure a backup with the

[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:



1. Sign in to the dashboard

1. Select Clusters.

1. Select the cluster that you want to back up.

   ![Select a cluster](/documentation/cloud/select-cluster.png)

1. Find and select the **Backups** tab.

1. Now you can set up a backup schedule.

   The **Days of Retention** is the number of days after a backup snapshot is

   deleted.

1. Alternatively, you can select **Backup now** to take an immediate snapshot.



![Configure a cluster backup](/documentation/cloud/backup-schedule.png)



### Restore a backup



If you have a backup, it appears in the list of **Available Backups**. You can

choose to restore or delete the backups of your choice.



![Restore or delete a cluster backup](/documentation/cloud/restore-delete.png)



<!-- I think we should move this to the Snapshot page, but I'll do it later -->
---

title: Backups

weight: 70

---



# Cloud Backups



Qdrant organizes cloud instances as clusters. On occasion, you may need to

restore your cluster because of application or system failure.



You may already have a source of truth for your data in a regular database. If you

have a problem, you could reindex the data into your Qdrant vector search cluster.

However, this process can take time. For high availability critical projects we

recommend replication. It guarantees the proper cluster functionality as long as

at least one replica is running.



For other use-cases such as disaster recovery, you can set up automatic or

self-service backups.



## Prerequisites



You can back up your Qdrant clusters though the Qdrant Cloud

Dashboard at https://cloud.qdrant.io. This section assumes that you've already

set up your cluster, as described in the following sections:



- [Create a cluster](/documentation/cloud/create-cluster/)

- Set up [Authentication](/documentation/cloud/authentication/)"
What are snapshots in the context of Qdrant Cloud and how are they used in a distributed setup?,"Snapshots in Qdrant Cloud are `tar` archive files that contain data and configuration of a specific collection on a specific node at a specific time. In a distributed setup with multiple nodes in a cluster, snapshots must be created for each node separately when dealing with a single collection. These snapshots can be used to archive data or easily replicate an existing deployment.","['```bash\n\ncurl -X POST \'http://{qdrant-url}:6333/collections/{collection_name}/snapshots/upload?priority=snapshot\' \\\n\n    -H \'api-key: ********\' \\\n\n    -H \'Content-Type:multipart/form-data\' \\\n\n    -F \'snapshot=@/path/to/snapshot-2022-10-10.shapshot\'\n\n```\n\n\n\nThis method is typically used to migrate data from one cluster to another, so we recommend setting the [priority](#snapshot-priority) to ""snapshot"" for that use-case.\n\n\n\n### Recover during start-up\n\n\n\n<aside role=""alert"">This method cannot be used in a multi-node deployment and cannot be used in Qdrant Cloud.</aside>\n\n\n\nIf you have a single-node deployment, you can recover any collection at start-up and it will be immediately available.\n\nRestoring snapshots is done through the Qdrant CLI at start-up time via the `--snapshot` argument which accepts a list of pairs such as `<snapshot_file_path>:<target_collection_name>`\n\n\n\nFor example:\n\n\n\n```bash', '```\n\n\n\n## Snapshots for the whole storage\n\n\n\n*Available as of v0.8.5*\n\n\n\nSometimes it might be handy to create snapshot not just for a single collection, but for the whole storage, including collection aliases.\n\nQdrant provides a dedicated API for that as well. It is similar to collection-level snapshots, but does not require `collection_name`.\n\n\n\n<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>\n\n\n\n### Create full storage snapshot\n\n\n\n```http\n\nPOST /snapshots\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_full_snapshot()\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });', 'Try it out now! If you have Docker running, you can [quickstart Qdrant](https://qdrant.tech/documentation/quick-start/) and access the Dashboard locally from [http://localhost:6333/dashboard](http://localhost:6333/dashboard). You should see this simple access point to Qdrant:\n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Temporary directory for Snapshots\n\n\n\nCurrently, temporary snapshot files are created inside the `/storage` directory. Oftentimes `/storage` is a network-mounted disk. Therefore, we found this method suboptimal because `/storage` is limited in disk size and also because writing data to it may affect disk performance as it consumes bandwidth. This new feature allows you to specify a different directory on another disk that is faster. We expect this feature to significantly optimize cloud performance.\n\n\n\nTo change it, access `config.yaml` and set `storage.temp_path` to another directory location.\n\n\n\n## Important changes']","```bash

curl -X POST 'http://{qdrant-url}:6333/collections/{collection_name}/snapshots/upload?priority=snapshot' \

    -H 'api-key: ********' \

    -H 'Content-Type:multipart/form-data' \

    -F 'snapshot=@/path/to/snapshot-2022-10-10.shapshot'

```



This method is typically used to migrate data from one cluster to another, so we recommend setting the [priority](#snapshot-priority) to ""snapshot"" for that use-case.



### Recover during start-up



<aside role=""alert"">This method cannot be used in a multi-node deployment and cannot be used in Qdrant Cloud.</aside>



If you have a single-node deployment, you can recover any collection at start-up and it will be immediately available.

Restoring snapshots is done through the Qdrant CLI at start-up time via the `--snapshot` argument which accepts a list of pairs such as `<snapshot_file_path>:<target_collection_name>`



For example:



```bash
```



## Snapshots for the whole storage



*Available as of v0.8.5*



Sometimes it might be handy to create snapshot not just for a single collection, but for the whole storage, including collection aliases.

Qdrant provides a dedicated API for that as well. It is similar to collection-level snapshots, but does not require `collection_name`.



<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>



### Create full storage snapshot



```http

POST /snapshots

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.create_full_snapshot()

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });
Try it out now! If you have Docker running, you can [quickstart Qdrant](https://qdrant.tech/documentation/quick-start/) and access the Dashboard locally from [http://localhost:6333/dashboard](http://localhost:6333/dashboard). You should see this simple access point to Qdrant:



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Temporary directory for Snapshots



Currently, temporary snapshot files are created inside the `/storage` directory. Oftentimes `/storage` is a network-mounted disk. Therefore, we found this method suboptimal because `/storage` is limited in disk size and also because writing data to it may affect disk performance as it consumes bandwidth. This new feature allows you to specify a different directory on another disk that is faster. We expect this feature to significantly optimize cloud performance.



To change it, access `config.yaml` and set `storage.temp_path` to another directory location.



## Important changes"
What is the significance of the lowercase parameter in the context of text filters in Qdrant?,"The lowercase parameter in Qdrant is used to specify whether the index should be case-insensitive or not. When set to true, Qdrant will convert all the texts to lowercase before indexing them. This means that during searches, the case of the letters in the query will not affect the results.","['There are also some additional parameters you can provide, such as\n\n\n\n* **min_token_len**\xa0— minimal length of the token\n\n* **max_token_len**\xa0— maximal length of the token\n\n* **lowercase**\xa0— if set to\xa0*true*, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase\n\n\n\n## Using text filters in practice\n\n\n\n![](/blog/from_cms/1_pbtd2tzqtjqqlbi61r8czg.webp ""There are also some additional parameters you can provide, such as  min_token_len — minimal length of the token max_token_len — maximal length of the token lowercase — if set to true, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase Using text filters in practice"")', 'List.of(matchKeyword(""city"", ""London""), matchKeyword(""color"", ""red"")))\n\n                    .build())\n\n            .build())\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing static Qdrant.Client.Grpc.Conditions;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\n// & operator combines two conditions in an AND conjunction(must)\n\nawait client.ScrollAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tfilter: MatchKeyword(""city"", ""London"") & MatchKeyword(""color"", ""red"")\n\n);\n\n```\n\n\n\nFiltered points would be:\n\n\n\n```json\n\n[{ ""id"": 2, ""city"": ""London"", ""color"": ""red"" }]\n\n```\n\n\n\nWhen using `must`, the clause becomes `true` only if every condition listed inside `must` is satisfied.\n\nIn this sense, `must` is equivalent to the operator `AND`.\n\n\n\n### Should\n\n\n\nExample:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""should"": [\n\n            { ""key"": ""city"", ""match"": { ""value"": ""London"" } },', '.build())\n\n            .build())\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing static Qdrant.Client.Grpc.Conditions;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\n// | operator combines two conditions in an OR disjunction(should)\n\nawait client.ScrollAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tfilter: MatchKeyword(""city"", ""London"") | MatchKeyword(""color"", ""red"")\n\n);\n\n```\n\n\n\nFiltered points would be:\n\n\n\n```json\n\n[\n\n  { ""id"": 1, ""city"": ""London"", ""color"": ""green"" },\n\n  { ""id"": 2, ""city"": ""London"", ""color"": ""red"" },\n\n  { ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },\n\n  { ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" }\n\n]\n\n```\n\n\n\nWhen using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied.\n\nIn this sense, `should` is equivalent to the operator `OR`.\n\n\n\n### Must Not\n\n\n\nExample:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""must_not"": [']","There are also some additional parameters you can provide, such as



* **min_token_len** — minimal length of the token

* **max_token_len** — maximal length of the token

* **lowercase** — if set to *true*, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase



## Using text filters in practice



![](/blog/from_cms/1_pbtd2tzqtjqqlbi61r8czg.webp ""There are also some additional parameters you can provide, such as  min_token_len — minimal length of the token max_token_len — maximal length of the token lowercase — if set to true, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase Using text filters in practice"")
List.of(matchKeyword(""city"", ""London""), matchKeyword(""color"", ""red"")))

                    .build())

            .build())

    .get();

```



```csharp

using Qdrant.Client;

using static Qdrant.Client.Grpc.Conditions;



var client = new QdrantClient(""localhost"", 6334);



// & operator combines two conditions in an AND conjunction(must)

await client.ScrollAsync(

	collectionName: ""{collection_name}"",

	filter: MatchKeyword(""city"", ""London"") & MatchKeyword(""color"", ""red"")

);

```



Filtered points would be:



```json

[{ ""id"": 2, ""city"": ""London"", ""color"": ""red"" }]

```



When using `must`, the clause becomes `true` only if every condition listed inside `must` is satisfied.

In this sense, `must` is equivalent to the operator `AND`.



### Should



Example:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""should"": [

            { ""key"": ""city"", ""match"": { ""value"": ""London"" } },
.build())

            .build())

    .get();

```



```csharp

using Qdrant.Client;

using static Qdrant.Client.Grpc.Conditions;



var client = new QdrantClient(""localhost"", 6334);



// | operator combines two conditions in an OR disjunction(should)

await client.ScrollAsync(

	collectionName: ""{collection_name}"",

	filter: MatchKeyword(""city"", ""London"") | MatchKeyword(""color"", ""red"")

);

```



Filtered points would be:



```json

[

  { ""id"": 1, ""city"": ""London"", ""color"": ""green"" },

  { ""id"": 2, ""city"": ""London"", ""color"": ""red"" },

  { ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },

  { ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" }

]

```



When using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied.

In this sense, `should` is equivalent to the operator `OR`.



### Must Not



Example:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""must_not"": ["
How does adjusting the `write_consistency_factor` parameter impact write operations in a distributed deployment using Qdrant?,"The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully.","['Qdrant provides a few options to control consistency guarantees:\n\n\n\n- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.\n\n- Read `consistency` param, can be used with search and retrieve operations to ensure that the results obtained from all replicas are the same. If this option is used, Qdrant will perform the read operation on multiple replicas and resolve the result according to the selected strategy. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if the update operations are frequent and the number of replicas is low.', 'using Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\n\tshardNumber: 6,\n\n\treplicationFactor: 2,\n\n\twriteConsistencyFactor: 2\n\n);\n\n```\n\n\n\nWrite operations will fail if the number of active replicas is less than the `write_consistency_factor`.\n\n\n\n### Read consistency\n\n\n\nRead `consistency` can be specified for most read requests and will ensure that the returned result\n\nis consistent across cluster nodes.\n\n\n\n- `all` will query all nodes and return points, which present on all of them\n\n- `majority` will query all nodes and return points, which present on the majority of them\n\n- `quorum` will query randomly selected majority of nodes and return points, which present on all of them\n\n- `1`/`2`/`3`/etc - will query specified number of randomly selected nodes and return points which present on all of them\n\n- default `consistency` is `1`', '- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if read operations are more frequent than update and if search performance is critical.\n\n\n\n\n\n### Write consistency factor\n\n\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\n\nIt can be configured at the collection\'s creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python']","Qdrant provides a few options to control consistency guarantees:



- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.

- Read `consistency` param, can be used with search and retrieve operations to ensure that the results obtained from all replicas are the same. If this option is used, Qdrant will perform the read operation on multiple replicas and resolve the result according to the selected strategy. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if the update operations are frequent and the number of replicas is low.
using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

	shardNumber: 6,

	replicationFactor: 2,

	writeConsistencyFactor: 2

);

```



Write operations will fail if the number of active replicas is less than the `write_consistency_factor`.



### Read consistency



Read `consistency` can be specified for most read requests and will ensure that the returned result

is consistent across cluster nodes.



- `all` will query all nodes and return points, which present on all of them

- `majority` will query all nodes and return points, which present on the majority of them

- `quorum` will query randomly selected majority of nodes and return points, which present on all of them

- `1`/`2`/`3`/etc - will query specified number of randomly selected nodes and return points which present on all of them

- default `consistency` is `1`
- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if read operations are more frequent than update and if search performance is critical.





### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

    ""write_consistency_factor"": 2,

}

```



```python"
What are some of the features and support services that come pre-configured with each instance in Qdrant Cloud?,"Each instance in Qdrant Cloud comes pre-configured with the following tools, features, and support services:1. Automatically created with the latest available version of Qdrant.
2. Upgradeable to later versions of Qdrant as they are released.
3. Equipped with monitoring and logging to observe the health of each cluster.
4. Accessible through the Qdrant Cloud Console.
5. Vertically scalable.
6. Offered on AWS and GCP, with Azure currently in development.
","['### Cluster configuration\n\n\n\nEach instance comes pre-configured with the following tools, features and support services:\n\n\n\n- Automatically created with the latest available version of Qdrant.\n\n- Upgradeable to later versions of Qdrant as they are released.\n\n- Equipped with monitoring and logging to observe the health of each cluster. \n\n- Accessible through the Qdrant Cloud Console.\n\n- Vertically scalable.\n\n- Offered on AWS and GCP, with Azure currently in development. \n\n\n\n### Getting started with Qdrant Cloud\n\n\n\nTo use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:\n\n1. [**Create a Free Tier cluster**]({{< ref ""/documentation/cloud/quickstart-cloud"" >}}) with 1 node and a default configuration (1GB RAM, 0.5 CPU and 4GB Disk). This option is perfect for prototyping and you don\'t need a credit card to join.', 'Qdrant offers multiple ways of deployment, including local mode, on-premise, and [Qdrant Cloud](https://cloud.qdrant.io/). \n\nYou can [get started with local mode quickly](/documentation/quick-start/) and without signing up for SaaS. With Pinecone you will have to connect your development environment to the cloud service just to test the product.', '```python\n\nQDRANT_MAIN_URL = ""https://my-cluster.com:6333""\n\nQDRANT_NODES = (\n\n    ""https://node-0.my-cluster.com:6333"",\n\n    ""https://node-1.my-cluster.com:6333"",\n\n    ""https://node-2.my-cluster.com:6333"",\n\n)\n\nQDRANT_API_KEY = ""my-api-key""\n\n```\n\n\n\n<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>\n\n\n\nWe can now create a client instance:\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(QDRANT_MAIN_URL, api_key=QDRANT_API_KEY)\n\n```\n\n\n\nFirst of all, we are going to create a collection from a precomputed dataset. If you already have a collection, you can skip this step and start by [creating a snapshot](#create-and-download-snapshots).\n\n\n\n<details>\n\n    <summary>(Optional) Create collection and import data</summary>\n\n\n\n### Load the dataset']","### Cluster configuration



Each instance comes pre-configured with the following tools, features and support services:



- Automatically created with the latest available version of Qdrant.

- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:

1. [**Create a Free Tier cluster**]({{< ref ""/documentation/cloud/quickstart-cloud"" >}}) with 1 node and a default configuration (1GB RAM, 0.5 CPU and 4GB Disk). This option is perfect for prototyping and you don't need a credit card to join.
Qdrant offers multiple ways of deployment, including local mode, on-premise, and [Qdrant Cloud](https://cloud.qdrant.io/). 

You can [get started with local mode quickly](/documentation/quick-start/) and without signing up for SaaS. With Pinecone you will have to connect your development environment to the cloud service just to test the product.
```python

QDRANT_MAIN_URL = ""https://my-cluster.com:6333""

QDRANT_NODES = (

    ""https://node-0.my-cluster.com:6333"",

    ""https://node-1.my-cluster.com:6333"",

    ""https://node-2.my-cluster.com:6333"",

)

QDRANT_API_KEY = ""my-api-key""

```



<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>



We can now create a client instance:



```python

from qdrant_client import QdrantClient



client = QdrantClient(QDRANT_MAIN_URL, api_key=QDRANT_API_KEY)

```



First of all, we are going to create a collection from a precomputed dataset. If you already have a collection, you can skip this step and start by [creating a snapshot](#create-and-download-snapshots).



<details>

    <summary>(Optional) Create collection and import data</summary>



### Load the dataset"
How is the `must_not` clause used in conjunction with other clauses in filtering data?,"The `must_not` clause in filtering data is used to exclude documents that meet specific criteria. The `must_not` clause can be used in combination with the `must` clause to create a more refined filter. Specifically, the `must_not` clause is used to specify conditions that must not be met for a document to be included in the results.","['.build())\n\n            .build())\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing static Qdrant.Client.Grpc.Conditions;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\n// The ! operator negates the condition(must not)\n\nawait client.ScrollAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tfilter: !(MatchKeyword(""city"", ""London"") & MatchKeyword(""color"", ""red""))\n\n);\n\n```\n\n\n\nFiltered points would be:\n\n\n\n```json\n\n[\n\n  { ""id"": 5, ""city"": ""Moscow"", ""color"": ""green"" },\n\n  { ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }\n\n]\n\n```\n\n\n\nWhen using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.\n\nIn this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.\n\n\n\n### Clauses combination\n\n\n\nIt is also possible to use several clauses simultaneously:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""must"": [\n\n            { ""key"": ""city"", ""match"": { ""value"": ""London"" } }', 'List.of(matchKeyword(""city"", ""London""), matchKeyword(""color"", ""red"")))\n\n                    .build())\n\n            .build())\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing static Qdrant.Client.Grpc.Conditions;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\n// & operator combines two conditions in an AND conjunction(must)\n\nawait client.ScrollAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tfilter: MatchKeyword(""city"", ""London"") & MatchKeyword(""color"", ""red"")\n\n);\n\n```\n\n\n\nFiltered points would be:\n\n\n\n```json\n\n[{ ""id"": 2, ""city"": ""London"", ""color"": ""red"" }]\n\n```\n\n\n\nWhen using `must`, the clause becomes `true` only if every condition listed inside `must` is satisfied.\n\nIn this sense, `must` is equivalent to the operator `AND`.\n\n\n\n### Should\n\n\n\nExample:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""should"": [\n\n            { ""key"": ""city"", ""match"": { ""value"": ""London"" } },', '""match"": {\n\n                                ""value"": ""London""\n\n                            }\n\n                        }\n\n                    ]\n\n            },\n\n            ""negative"": [718],\n\n            ""positive"": [100, 231],\n\n            ""limit"": 10\n\n        },\n\n        {\n\n            ""filter"": {\n\n                ""must"": [\n\n                    {\n\n                        ""key"": ""city"",\n\n                        ""match"": {\n\n                            ""value"": ""London""\n\n                        }\n\n                    }\n\n                    ]\n\n            },\n\n            ""negative"": [300],\n\n            ""positive"": [200, 67],\n\n            ""limit"": 10\n\n        }\n\n    ]\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nfilter = models.Filter(\n\n    must=[\n\n        models.FieldCondition(\n\n            key=""city"",\n\n            match=models.MatchValue(\n\n                value=""London"",']",".build())

            .build())

    .get();

```



```csharp

using Qdrant.Client;

using static Qdrant.Client.Grpc.Conditions;



var client = new QdrantClient(""localhost"", 6334);



// The ! operator negates the condition(must not)

await client.ScrollAsync(

	collectionName: ""{collection_name}"",

	filter: !(MatchKeyword(""city"", ""London"") & MatchKeyword(""color"", ""red""))

);

```



Filtered points would be:



```json

[

  { ""id"": 5, ""city"": ""Moscow"", ""color"": ""green"" },

  { ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }

]

```



When using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.

In this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.



### Clauses combination



It is also possible to use several clauses simultaneously:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""must"": [

            { ""key"": ""city"", ""match"": { ""value"": ""London"" } }
List.of(matchKeyword(""city"", ""London""), matchKeyword(""color"", ""red"")))

                    .build())

            .build())

    .get();

```



```csharp

using Qdrant.Client;

using static Qdrant.Client.Grpc.Conditions;



var client = new QdrantClient(""localhost"", 6334);



// & operator combines two conditions in an AND conjunction(must)

await client.ScrollAsync(

	collectionName: ""{collection_name}"",

	filter: MatchKeyword(""city"", ""London"") & MatchKeyword(""color"", ""red"")

);

```



Filtered points would be:



```json

[{ ""id"": 2, ""city"": ""London"", ""color"": ""red"" }]

```



When using `must`, the clause becomes `true` only if every condition listed inside `must` is satisfied.

In this sense, `must` is equivalent to the operator `AND`.



### Should



Example:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""should"": [

            { ""key"": ""city"", ""match"": { ""value"": ""London"" } },
""match"": {

                                ""value"": ""London""

                            }

                        }

                    ]

            },

            ""negative"": [718],

            ""positive"": [100, 231],

            ""limit"": 10

        },

        {

            ""filter"": {

                ""must"": [

                    {

                        ""key"": ""city"",

                        ""match"": {

                            ""value"": ""London""

                        }

                    }

                    ]

            },

            ""negative"": [300],

            ""positive"": [200, 67],

            ""limit"": 10

        }

    ]

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



filter = models.Filter(

    must=[

        models.FieldCondition(

            key=""city"",

            match=models.MatchValue(

                value=""London"","
What are some advantages of vector search over keyword-based search?,"Vector search has clear advantages over keyword-based search in various scenarios as outlined in the document chunk. Some of these advantages include:

1. Multi-lingual & multi-modal search: Vector search is effective in handling searches across multiple languages and modalities.
2. For short texts with typos and ambiguous content-dependent meanings: Vector search excels in situations where the search queries are short, contain typos, or have ambiguous meanings.
3. Specialized domains with tuned encoder models: Vector search is beneficial in specialized domains where encoder models can be fine-tuned for better search results.
4. Document-as-a-Query similarity search: Vector search allows for similarity searches where the entire document can be used as a query to find similar documents.

While vector search offers these advantages, it is important to note that keyword-based search still has its relevance in certain cases. For example, in out-of-domain search.","['There is no one-size-fits-all approach that would not compromise on performance or flexibility.\n\nSo if your use case utilizes vector search in any significant way, it is worth investing in a dedicated vector search engine, aka vector database.', ""When you introduce vector similarity capabilities into your text search engine, you extend its functionality.\n\nHowever, it doesn't work the other way around, as the vector similarity as a concept is much broader than some task-specific implementations of full-text search.\n\n\n\nVector Databases, which introduce built-in full-text functionality, must make several compromises:\n\n\n\n- Choose a specific full-text search variant.\n\n- Either sacrifice API consistency or limit vector similarity functionality to only basic kNN search.\n\n- Introduce additional complexity to the system.\n\n\n\n\n\nQdrant, on the contrary, puts vector similarity in the center of it's API and architecture, such that it allows us to move towards a new stack of vector-native operations.\n\nWe believe that this is the future of vector databases, and we are excited to see what new use-cases will be unlocked by these techniques.\n\n\n\n\n\n## Wrapping up"", 'For example, the exact phrase matching and counting of results are native to full-text search, while vector similarity support for this type of operation is limited.\n\nOn the other hand, vector similarity easily allows cross-modal retrieval of images by text or vice-versa, which is impossible with full-text search.\n\n\n\nThis mismatch in expectations might sometimes lead to confusion.\n\nAttempting to use a vector similarity as a full-text search can result in a range of frustrations, from slow response times to poor search results, to limited functionality.\n\nAs an outcome, they are getting only a fraction of the benefits of vector similarity.\n\n\n\nBelow we will explore why vector similarity stack deserves new interfaces and design patterns that will unlock the full potential of this technology, which can still be used in conjunction with full-text search.\n\n\n\n\n\n## New Ways to Interact with Similarities\n\n\n\nHaving a vector representation of unstructured data unlocks new ways of interacting with it.']","There is no one-size-fits-all approach that would not compromise on performance or flexibility.

So if your use case utilizes vector search in any significant way, it is worth investing in a dedicated vector search engine, aka vector database.
When you introduce vector similarity capabilities into your text search engine, you extend its functionality.

However, it doesn't work the other way around, as the vector similarity as a concept is much broader than some task-specific implementations of full-text search.



Vector Databases, which introduce built-in full-text functionality, must make several compromises:



- Choose a specific full-text search variant.

- Either sacrifice API consistency or limit vector similarity functionality to only basic kNN search.

- Introduce additional complexity to the system.





Qdrant, on the contrary, puts vector similarity in the center of it's API and architecture, such that it allows us to move towards a new stack of vector-native operations.

We believe that this is the future of vector databases, and we are excited to see what new use-cases will be unlocked by these techniques.





## Wrapping up
For example, the exact phrase matching and counting of results are native to full-text search, while vector similarity support for this type of operation is limited.

On the other hand, vector similarity easily allows cross-modal retrieval of images by text or vice-versa, which is impossible with full-text search.



This mismatch in expectations might sometimes lead to confusion.

Attempting to use a vector similarity as a full-text search can result in a range of frustrations, from slow response times to poor search results, to limited functionality.

As an outcome, they are getting only a fraction of the benefits of vector similarity.



Below we will explore why vector similarity stack deserves new interfaces and design patterns that will unlock the full potential of this technology, which can still be used in conjunction with full-text search.





## New Ways to Interact with Similarities



Having a vector representation of unstructured data unlocks new ways of interacting with it."
What is the purpose of the DocArray library and how does it relate to the Qdrant engine?,"The DocArray library is designed to define a structure for unstructured data, making it easier to process a collection of documents that may include various data types such as audio, video, and text. This library simplifies the handling of these diverse types of data within a single framework. On the other hand, the Qdrant engine focuses on enabling the scaling of vector search and storage capabilities. By integrating the DocArray library with the Qdrant engine, users can benefit from efficient processing of unstructured data and enhanced scalability for vector search operations. This integration streamlines the handling of different data types while also optimizing the performance of vector search and storage functions.","['DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,\n\nincluding audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.\n\n\n\nRead more about the integration by this [link](/documentation/install/#docarray)', '---\n\ntitle: DocArray\n\nweight: 300\n\naliases: [ ../integrations/docarray/ ]\n\n---\n\n\n\n# DocArray\n\nYou can use Qdrant natively in DocArray, where Qdrant serves as a high-performance document store to enable scalable vector search.\n\n\n\nDocArray is a library from Jina AI for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc.\n\nIt allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API.\n\n\n\n\n\nTo install DocArray with Qdrant support, please do\n\n\n\n```bash\n\npip install ""docarray[qdrant]""\n\n```\n\n\n\nMore information can be found in [DocArray\'s documentations](https://docarray.jina.ai/advanced/document-store/qdrant/).', '---\n\ndraft: false\n\npreview_image: /blog/from_cms/docarray.png\n\nsitemapExclude: true\n\ntitle: ""Qdrant and Jina integration: storage backend support for DocArray""\n\nslug: qdrant-and-jina-integration\n\nshort_description: ""One more way to use Qdrant: Jina\'s DocArray is now\n\n  supporting Qdrant as a storage backend.""\n\ndescription: We are happy to announce that Jina.AI integrates Qdrant engine as a\n\n  storage backend to their DocArray solution.\n\ndate: 2022-03-15T15:00:00+03:00\n\nauthor: Alyona Kavyerina\n\nfeatured: false\n\nauthor_link: https://medium.com/@alyona.kavyerina\n\ntags:\n\n  - jina integration\n\n  - docarray\n\ncategories:\n\n  - News\n\n---\n\nWe are happy to announce that [Jina.AI](https://jina.ai/) integrates Qdrant engine as a storage backend to their [DocArray](https://docarray.jina.ai/) solution.\n\n\n\nNow you can experience the convenience of Pythonic API and Rust performance in a single workflow.\n\n\n\nDocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,']","DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,

including audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.



Read more about the integration by this [link](/documentation/install/#docarray)
---

title: DocArray

weight: 300

aliases: [ ../integrations/docarray/ ]

---



# DocArray

You can use Qdrant natively in DocArray, where Qdrant serves as a high-performance document store to enable scalable vector search.



DocArray is a library from Jina AI for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc.

It allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API.





To install DocArray with Qdrant support, please do



```bash

pip install ""docarray[qdrant]""

```



More information can be found in [DocArray's documentations](https://docarray.jina.ai/advanced/document-store/qdrant/).
---

draft: false

preview_image: /blog/from_cms/docarray.png

sitemapExclude: true

title: ""Qdrant and Jina integration: storage backend support for DocArray""

slug: qdrant-and-jina-integration

short_description: ""One more way to use Qdrant: Jina's DocArray is now

  supporting Qdrant as a storage backend.""

description: We are happy to announce that Jina.AI integrates Qdrant engine as a

  storage backend to their DocArray solution.

date: 2022-03-15T15:00:00+03:00

author: Alyona Kavyerina

featured: false

author_link: https://medium.com/@alyona.kavyerina

tags:

  - jina integration

  - docarray

categories:

  - News

---

We are happy to announce that [Jina.AI](https://jina.ai/) integrates Qdrant engine as a storage backend to their [DocArray](https://docarray.jina.ai/) solution.



Now you can experience the convenience of Pythonic API and Rust performance in a single workflow.



DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,"
What are the search-time parameters that can be used to tune the search accuracy and speed?,"The search-time parameters that can be used to tune the search accuracy and speed are ""hnsw_ef"" and ""exact"". The ""hnsw_ef"" parameter is set to 128, which controls the search accuracy by specifying the number of neighbors to inspect during the search process. The ""exact"" parameter is set to false, indicating that an approximate search method is used for faster retrieval of results. These parameters are essential for optimizing the search process in the context of the QdrantClient for efficient retrieval of relevant data points.","['<td><span style=""color: red;"">-0.1%</span></td>\n\n         <td><span style=""color: green;"">-35.62%</span></td>\n\n         <td>0%</td>\n\n         <td><span style=""color: green;"">-28.57%</span></td>\n\n      </tr>\n\n   </tbody>\n\n</table>\n\n\n\nA slight decrease in search precision results in a considerable improvement in the \n\nlatency. Unless you aim for the highest precision possible, you should not notice the \n\ndifference in your search quality.\n\n\n\n#### Gist-960\n\n\n\n<table>\n\n   <thead>\n\n      <tr>\n\n         <th colspan=""2""></th>\n\n         <th colspan=""2"">ef = 128</th>\n\n         <th colspan=""2"">ef = 256</th>\n\n         <th colspan=""2"">ef = 512</th>\n\n      </tr>\n\n      <tr>\n\n         <th></th>\n\n         <th><small>Upload and indexing time</small></th>\n\n         <th><small>Mean search precision</small></th>\n\n         <th><small>Mean search time</small></th>\n\n         <th><small>Mean search precision</small></th>\n\n         <th><small>Mean search time</small></th>', '<th><small>Mean search precision</small></th>\n\n         <th><small>Mean search time</small></th>\n\n         <th><small>Mean search precision</small></th>\n\n         <th><small>Mean search time</small></th>\n\n      </tr>\n\n   </thead>\n\n   <tbody>\n\n      <tr>\n\n         <th>Non-quantized vectors</th>\n\n         <td>452</td>\n\n         <td>0.802</td>\n\n         <td>0.077</td>\n\n         <td>0.887</td>\n\n         <td>0.135</td>\n\n         <td>0.941</td>\n\n         <td>0.231</td>\n\n      </tr>\n\n      <tr>\n\n         <th>Scalar Quantization</th>\n\n         <td>312</td>\n\n         <td>0.802</td>\n\n         <td>0.043</td>\n\n         <td>0.888</td>\n\n         <td>0.077</td>\n\n         <td>0.941</td>\n\n         <td>0.135</td>\n\n      </tr>\n\n      <tr>\n\n         <td>Difference</td>\n\n         <td><span style=""color: green;"">-30.79%</span></td>\n\n         <td>0%</td>\n\n         <td><span style=""color: green;"">-44,16%</span></td>\n\n         <td><span style=""color: green;"">+0.11%</span></td>', 'searchParams: new SearchParams { HnswEf = 128, Exact = false },\n\n\tlimit: 3\n\n);\n\n```\n\n\n\n- `hnsw_ef` - controls the number of neighbors to visit during search. The higher the value, the more accurate and slower the search will be. Recommended range is 32-512.\n\n- `exact` - if set to `true`, will perform exact search, which will be slower, but more accurate. You can use it to compare results of the search with different `hnsw_ef` values versus the ground truth.\n\n\n\n## Latency vs Throughput\n\n\n\n- There are two main approaches to measure the speed of search:\n\n  - latency of the request - the time from the moment request is submitted to the moment a response is received\n\n  - throughput - the number of requests per second the system can handle\n\n\n\nThose approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.\n\n\n\nTo prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.']","<td><span style=""color: red;"">-0.1%</span></td>

         <td><span style=""color: green;"">-35.62%</span></td>

         <td>0%</td>

         <td><span style=""color: green;"">-28.57%</span></td>

      </tr>

   </tbody>

</table>



A slight decrease in search precision results in a considerable improvement in the 

latency. Unless you aim for the highest precision possible, you should not notice the 

difference in your search quality.



#### Gist-960



<table>

   <thead>

      <tr>

         <th colspan=""2""></th>

         <th colspan=""2"">ef = 128</th>

         <th colspan=""2"">ef = 256</th>

         <th colspan=""2"">ef = 512</th>

      </tr>

      <tr>

         <th></th>

         <th><small>Upload and indexing time</small></th>

         <th><small>Mean search precision</small></th>

         <th><small>Mean search time</small></th>

         <th><small>Mean search precision</small></th>

         <th><small>Mean search time</small></th>
<th><small>Mean search precision</small></th>

         <th><small>Mean search time</small></th>

         <th><small>Mean search precision</small></th>

         <th><small>Mean search time</small></th>

      </tr>

   </thead>

   <tbody>

      <tr>

         <th>Non-quantized vectors</th>

         <td>452</td>

         <td>0.802</td>

         <td>0.077</td>

         <td>0.887</td>

         <td>0.135</td>

         <td>0.941</td>

         <td>0.231</td>

      </tr>

      <tr>

         <th>Scalar Quantization</th>

         <td>312</td>

         <td>0.802</td>

         <td>0.043</td>

         <td>0.888</td>

         <td>0.077</td>

         <td>0.941</td>

         <td>0.135</td>

      </tr>

      <tr>

         <td>Difference</td>

         <td><span style=""color: green;"">-30.79%</span></td>

         <td>0%</td>

         <td><span style=""color: green;"">-44,16%</span></td>

         <td><span style=""color: green;"">+0.11%</span></td>
searchParams: new SearchParams { HnswEf = 128, Exact = false },

	limit: 3

);

```



- `hnsw_ef` - controls the number of neighbors to visit during search. The higher the value, the more accurate and slower the search will be. Recommended range is 32-512.

- `exact` - if set to `true`, will perform exact search, which will be slower, but more accurate. You can use it to compare results of the search with different `hnsw_ef` values versus the ground truth.



## Latency vs Throughput



- There are two main approaches to measure the speed of search:

  - latency of the request - the time from the moment request is submitted to the moment a response is received

  - throughput - the number of requests per second the system can handle



Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\."
What is the significance of using named vectors in a collection within the Qdrant system?,"Named vectors in a collection within the Qdrant system allow for the inclusion of multiple vectors in a single point, with each vector having its own dimensionality and metric requirements. This feature enables more flexibility in organizing and structuring data within a collection, as different vectors can represent distinct aspects or features of the data points. By utilizing named vectors, users can better tailor the representation of their data to suit specific analysis or search requirements, enhancing the overall efficiency and effectiveness of the system.","['""image"": VectorParams(\n\n           size=786,\n\n           distance=Distance.COSINE,\n\n       ),\n\n   }\n\n)\n\n```\n\n\n\nIn case you want to keep a single vector per collection, you can still do it without putting a name though.\n\n\n\n```python\n\nclient.recreate_collection(\n\n   collection_name=""single_vector"",\n\n   vectors_config=VectorParams(\n\n       size=100,\n\n       distance=Distance.COSINE,\n\n   )\n\n)\n\n```\n\n\n\nAll the search-related operations have slightly changed their interfaces as well, so you can choose which vector to use in a specific request. However, it might be easier to see all the changes by following an end-to-end Qdrant usage on a real-world example.\n\n\n\n## Building service with multiple embeddings', '.get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParamsMap\n\n\t{\n\n\t\tMap =\n\n\t\t{\n\n\t\t\t[""image""] = new VectorParams { Size = 4, Distance = Distance.Dot },\n\n\t\t\t[""text""] = new VectorParams { Size = 8, Distance = Distance.Cosine },\n\n\t\t}\n\n\t}\n\n);\n\n```\n\n\n\nFor rare use cases, it is possible to create a collection without any vector storage.\n\n\n\n*Available as of v1.1.1*\n\n\n\nFor each named vector you can optionally specify\n\n[`hnsw_config`](../indexing/#vector-index) or\n\n[`quantization_config`](../../guides/quantization/#setting-up-quantization-in-qdrant) to\n\ndeviate from the collection configuration. This can be useful to fine-tune\n\nsearch performance on a vector level.\n\n\n\n*Available as of v1.2.0*\n\n\n\nVectors all live in RAM for very quick access. On a per-vector basis you can set', '""{collection_name}"",\n\n        None,\n\n        &[\n\n            PointVectors {\n\n                id: Some(1.into()),\n\n                vectors: Some(\n\n                    HashMap::from([(""image"".to_string(), vec![0.1, 0.2, 0.3, 0.4])]).into(),\n\n                ),\n\n            },\n\n            PointVectors {\n\n                id: Some(2.into()),\n\n                vectors: Some(\n\n                    HashMap::from([(\n\n                        ""text"".to_string(),\n\n                        vec![0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2],\n\n                    )])\n\n                    .into(),\n\n                ),\n\n            },\n\n        ],\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport java.util.List;\n\nimport java.util.Map;\n\n\n\nimport static io.qdrant.client.PointIdFactory.id;\n\nimport static io.qdrant.client.VectorFactory.vector;\n\nimport static io.qdrant.client.VectorsFactory.namedVectors;\n\n\n\nclient\n\n    .updateVectorsAsync(\n\n        ""{collection_name}"",\n\n        List.of(']","""image"": VectorParams(

           size=786,

           distance=Distance.COSINE,

       ),

   }

)

```



In case you want to keep a single vector per collection, you can still do it without putting a name though.



```python

client.recreate_collection(

   collection_name=""single_vector"",

   vectors_config=VectorParams(

       size=100,

       distance=Distance.COSINE,

   )

)

```



All the search-related operations have slightly changed their interfaces as well, so you can choose which vector to use in a specific request. However, it might be easier to see all the changes by following an end-to-end Qdrant usage on a real-world example.



## Building service with multiple embeddings
.get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParamsMap

	{

		Map =

		{

			[""image""] = new VectorParams { Size = 4, Distance = Distance.Dot },

			[""text""] = new VectorParams { Size = 8, Distance = Distance.Cosine },

		}

	}

);

```



For rare use cases, it is possible to create a collection without any vector storage.



*Available as of v1.1.1*



For each named vector you can optionally specify

[`hnsw_config`](../indexing/#vector-index) or

[`quantization_config`](../../guides/quantization/#setting-up-quantization-in-qdrant) to

deviate from the collection configuration. This can be useful to fine-tune

search performance on a vector level.



*Available as of v1.2.0*



Vectors all live in RAM for very quick access. On a per-vector basis you can set
""{collection_name}"",

        None,

        &[

            PointVectors {

                id: Some(1.into()),

                vectors: Some(

                    HashMap::from([(""image"".to_string(), vec![0.1, 0.2, 0.3, 0.4])]).into(),

                ),

            },

            PointVectors {

                id: Some(2.into()),

                vectors: Some(

                    HashMap::from([(

                        ""text"".to_string(),

                        vec![0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2],

                    )])

                    .into(),

                ),

            },

        ],

        None,

    )

    .await?;

```



```java

import java.util.List;

import java.util.Map;



import static io.qdrant.client.PointIdFactory.id;

import static io.qdrant.client.VectorFactory.vector;

import static io.qdrant.client.VectorsFactory.namedVectors;



client

    .updateVectorsAsync(

        ""{collection_name}"",

        List.of("
What parameters can be configured in the configuration file to improve performance in HNSW indexing?,"In the context of HNSW indexing, the parameters that can be configured in the configuration file to enhance performance are `m`, `ef_construct`, and `ef`. 

- `m`: This parameter represents the number of edges per node in the index graph. A higher value for `m` leads to more accurate search results but also requires more space. By adjusting `m`, you can control the trade-off between search accuracy and space efficiency.

- `ef_construct`: When building the index, `ef_construct` specifies the number of neighbors to consider. Increasing the value of `ef_construct` improves the accuracy of the search but also increases the time required to build the index. 

- `ef`: This parameter is used when searching for targets. It determines the search range for finding nearest neighbors. By setting an appropriate value for `ef`, you can optimize the search process to balance efficiency and accuracy in finding the nearest neighbors within the specified search range.","['## Tweaking the HNSW parameters\n\n\n\nHNSW is a hierarchical graph, where each node has a set of links to other nodes. The number of edges per node is called the `m` parameter. \n\nThe larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \n\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\n\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let\'s try to increase them to `m=32` and `ef_construct=200` and\n\nsee how it affects the precision. Of course, we need to wait until the indexing is finished before we can perform the search.\n\n\n\n```python\n\nclient.update_collection(\n\n    collection_name=""arxiv-titles-instructorxl-embeddings"",\n\n    hnsw_config=models.HnswConfigDiff(\n\n        m=32,  # Increase the number of edges per node from the default 16 to 32', '""indexed_vectors_count"": 1024232,\n\n        ""points_count"": 1068786,\n\n        ""segments_count"": 31,\n\n        ""config"": {\n\n            ""params"": {\n\n                ""vectors"": {\n\n                    ""size"": 384,\n\n                    ""distance"": ""Cosine""\n\n                },\n\n                ""shard_number"": 1,\n\n                ""replication_factor"": 1,\n\n                ""write_consistency_factor"": 1,\n\n                ""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,\n\n                ""memmap_threshold"": null,\n\n                ""indexing_threshold"": 20000,\n\n                ""flush_interval_sec"": 5,', 'hnswConfig: new HnswConfigDiff { OnDisk = true }\n\n);\n\n```\n\n\n\nIn this scenario you can increase the precision of the search by increasing the `ef` and `m` parameters of the HNSW index, even with limited RAM.\n\n\n\n```json\n\n...\n\n""hnsw_config"": {\n\n    ""m"": 64,\n\n    ""ef_construct"": 512,\n\n    ""on_disk"": true\n\n}\n\n...\n\n```\n\n\n\nThe disk IOPS is a critical factor in this scenario, it will determine how fast you can perform search.\n\nYou can use [fio](https://gist.github.com/superboum/aaa45d305700a7873a8ebbab1abddf2b) to measure disk IOPS.\n\n\n\n## Prefer high precision with high speed search\n\n\n\nFor high speed and high precision search it is critical to keep as much data in RAM as possible.\n\nBy default, Qdrant follows this approach, but you can tune it to your needs.\n\n\n\nIt is possible to achieve high search speed and tunable accuracy by applying quantization with re-scoring.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {']","## Tweaking the HNSW parameters



HNSW is a hierarchical graph, where each node has a set of links to other nodes. The number of edges per node is called the `m` parameter. 

The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of 

neighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.

The default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and

see how it affects the precision. Of course, we need to wait until the indexing is finished before we can perform the search.



```python

client.update_collection(

    collection_name=""arxiv-titles-instructorxl-embeddings"",

    hnsw_config=models.HnswConfigDiff(

        m=32,  # Increase the number of edges per node from the default 16 to 32
""indexed_vectors_count"": 1024232,

        ""points_count"": 1068786,

        ""segments_count"": 31,

        ""config"": {

            ""params"": {

                ""vectors"": {

                    ""size"": 384,

                    ""distance"": ""Cosine""

                },

                ""shard_number"": 1,

                ""replication_factor"": 1,

                ""write_consistency_factor"": 1,

                ""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,

                ""memmap_threshold"": null,

                ""indexing_threshold"": 20000,

                ""flush_interval_sec"": 5,
hnswConfig: new HnswConfigDiff { OnDisk = true }

);

```



In this scenario you can increase the precision of the search by increasing the `ef` and `m` parameters of the HNSW index, even with limited RAM.



```json

...

""hnsw_config"": {

    ""m"": 64,

    ""ef_construct"": 512,

    ""on_disk"": true

}

...

```



The disk IOPS is a critical factor in this scenario, it will determine how fast you can perform search.

You can use [fio](https://gist.github.com/superboum/aaa45d305700a7873a8ebbab1abddf2b) to measure disk IOPS.



## Prefer high precision with high speed search



For high speed and high precision search it is critical to keep as much data in RAM as possible.

By default, Qdrant follows this approach, but you can tune it to your needs.



It is possible to achieve high search speed and tunable accuracy by applying quantization with re-scoring.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {"
How does product quantization differ from scalar quantization in terms of compression and computational efficiency?,"Product quantization involves dividing data into chunks and quantizing each segment individually, approximating each chunk with a centroid index that represents the original vector component. The positions of these centroids are determined through clustering algorithms like k-means, with Qdrant currently utilizing 256 centroids, allowing each centroid index to be represented by a single byte. Product quantization can achieve higher compression factors compared to scalar quantization. However, there are tradeoffs to consider. Product quantization distance calculations are not SIMD-friendly, resulting in slower computational speed compared to scalar quantization. Additionally, product quantization introduces a loss of accuracy, making it more suitable for high-dimensional vectors. To optimize quantization parameters for specific use cases, it is recommended to refer to the ""Quantization Tips"" section for more information.","['. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.', 'but also the search time.\n\n\n\n## Good practices\n\n\n\nCompared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\n\n\n\nProduct Quantization tends to be favored in certain specific scenarios:\n\n\n\n- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself\n\n- Situations where the dimensionality of the original vectors is sufficiently high\n\n- Cases where indexing speed is not a critical factor\n\n\n\nIn circumstances that do not align with the above, Scalar Quantization should be the preferred choice.\n\n\n\nQdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization) \n\nwill help you to set and configure the new quantization for your data and achieve even \n\nup to 64x memory reduction.', 'Product quantization can compress by a more prominent factor than a scalar one.\n\nBut there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\n\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\n\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n## How to choose the right quantization method\n\n\n\nHere is a brief table of the pros and cons of each quantization method:\n\n\n\n| Quantization method | Accuracy | Speed        | Compression |\n\n|---------------------|----------|--------------|-------------|\n\n| Scalar              | 0.99     | up to x2     | 4           |\n\n| Product             | 0.7      | 0.5          | up to 64    |\n\n| Binary              | 0.95*    | up to x40    | 32          |\n\n\n\n`*` - for compatible models']",". For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
but also the search time.



## Good practices



Compared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.



Product Quantization tends to be favored in certain specific scenarios:



- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself

- Situations where the dimensionality of the original vectors is sufficiently high

- Cases where indexing speed is not a critical factor



In circumstances that do not align with the above, Scalar Quantization should be the preferred choice.



Qdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization) 

will help you to set and configure the new quantization for your data and achieve even 

up to 64x memory reduction.
Product quantization can compress by a more prominent factor than a scalar one.

But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method



Here is a brief table of the pros and cons of each quantization method:



| Quantization method | Accuracy | Speed        | Compression |

|---------------------|----------|--------------|-------------|

| Scalar              | 0.99     | up to x2     | 4           |

| Product             | 0.7      | 0.5          | up to 64    |

| Binary              | 0.95*    | up to x40    | 32          |



`*` - for compatible models"
What is the significance of Qdrant's approach to storing multiple vectors per object in data indexing?,"Qdrant's approach to storing multiple vectors per object in data indexing is significant as it opens up new possibilities in data representation and retrieval. By allowing multiple vectors to be associated with a single object, Qdrant enables more nuanced and detailed analysis of the data. This approach can be particularly beneficial in applications such as data science, neural networks, database management, and similarity search. It provides a more comprehensive understanding of the relationships between objects and enhances the accuracy and efficiency of search and retrieval processes. Overall, Qdrant's innovative vector storage strategy contributes to pushing the boundaries of data indexing and offers practical applications and benefits in various fields.","[""---\n\ndraft: false\n\ntitle: Storing multiple vectors per object in Qdrant\n\nslug: storing-multiple-vectors-per-object-in-qdrant\n\nshort_description: Qdrant's approach to storing multiple vectors per object,\n\n  unraveling new possibilities in data representation and retrieval.\n\ndescription: Discover how Qdrant continues to push the boundaries of data\n\n  indexing, providing insights into the practical applications and benefits of\n\n  this novel vector storage strategy.\n\npreview_image: /blog/from_cms/andrey.vasnetsov_a_space_station_with_multiple_attached_modules_853a27c7-05c4-45d2-aebc-700a6d1e79d0.png\n\ndate: 2022-10-05T10:05:43.329Z\n\nauthor: Kacper Łukawski\n\nfeatured: false\n\ntags:\n\n  - Data Science\n\n  - Neural Networks\n\n  - Database\n\n  - Search\n\n  - Similarity Search\n\n---"", 'In a real case scenario, a single object might be described in several different ways. If you run an e-commerce business, then your items will typically have a name, longer textual description and also a bunch of photos. While cooking, you may care about the list of ingredients, and description of the taste but also the recipe and the way your meal is going to look. Up till now, if you wanted to enable semantic search with multiple vectors per object, Qdrant would require you to create separate collections for each vector type, even though they could share some other attributes in a payload. However, since Qdrant 0.10 you are able to store all those vectors together in the same collection and share a single copy of the payload!\n\n\n\nRunning the new version of Qdrant is as simple as it always was. By running the following command, you are able to set up a single instance that will also expose the HTTP API:\n\n\n\n\n\n```\n\ndocker run -p 6333:6333 qdrant/qdrant:v0.10.1\n\n```\n\n\n\n## Creating a collection', '""text"": models.SparseVector(\n\n                    indices=indices.tolist(), values=values.tolist()\n\n                )\n\n            },\n\n        )\n\n    ],\n\n)\n\n```\n\nBy upserting points with sparse vectors, we prepare our dataset for rapid first-stage retrieval, laying the groundwork for subsequent detailed analysis using dense vectors. Notice that we use ""text"" to denote the name of the sparse vector.\n\n\n\nThose familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you\'re able to **apply payload filters**, shard keys, and other advanced features you\'ve come to expect from Qdrant. To make things easier for you, the indices and values don\'t have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.\n\n\n\n### 4. Querying with Sparse Vectors']","---

draft: false

title: Storing multiple vectors per object in Qdrant

slug: storing-multiple-vectors-per-object-in-qdrant

short_description: Qdrant's approach to storing multiple vectors per object,

  unraveling new possibilities in data representation and retrieval.

description: Discover how Qdrant continues to push the boundaries of data

  indexing, providing insights into the practical applications and benefits of

  this novel vector storage strategy.

preview_image: /blog/from_cms/andrey.vasnetsov_a_space_station_with_multiple_attached_modules_853a27c7-05c4-45d2-aebc-700a6d1e79d0.png

date: 2022-10-05T10:05:43.329Z

author: Kacper Łukawski

featured: false

tags:

  - Data Science

  - Neural Networks

  - Database

  - Search

  - Similarity Search

---
In a real case scenario, a single object might be described in several different ways. If you run an e-commerce business, then your items will typically have a name, longer textual description and also a bunch of photos. While cooking, you may care about the list of ingredients, and description of the taste but also the recipe and the way your meal is going to look. Up till now, if you wanted to enable semantic search with multiple vectors per object, Qdrant would require you to create separate collections for each vector type, even though they could share some other attributes in a payload. However, since Qdrant 0.10 you are able to store all those vectors together in the same collection and share a single copy of the payload!



Running the new version of Qdrant is as simple as it always was. By running the following command, you are able to set up a single instance that will also expose the HTTP API:





```

docker run -p 6333:6333 qdrant/qdrant:v0.10.1

```



## Creating a collection
""text"": models.SparseVector(

                    indices=indices.tolist(), values=values.tolist()

                )

            },

        )

    ],

)

```

By upserting points with sparse vectors, we prepare our dataset for rapid first-stage retrieval, laying the groundwork for subsequent detailed analysis using dense vectors. Notice that we use ""text"" to denote the name of the sparse vector.



Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.



### 4. Querying with Sparse Vectors"
What configuration is required to enable in-memory quantization with on-disk original vectors in Qdrant?,"To configure in-memory quantization with on-disk original vectors in Qdrant, you need to create a collection with the following configuration:

```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 768,

        ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": true

        }

    }

}
```

Additionally, in the Python code snippet provided, you can use the QdrantClient library to create the collection with the specified configuration parameters. This includes defining vector size, distance metric (such as Cosine similarity), memmap threshold for optimization, and scalar quantization configuration with type set to INT8","[""```text\n\nmemory_size = 1.5 * number_of_vectors * vector_dimension * 4 bytes\n\n```\n\n\n\nWhile Qdrant offers various options to store some parts of the data on disk, starting \n\nfrom version 1.1.0, you can also optimize your memory by compressing the embeddings. \n\nWe've implemented the mechanism of **Scalar Quantization**! It turns out to have not \n\nonly a positive impact on memory but also on the performance. \n\n\n\n## Scalar Quantization\n\n\n\nScalar quantization is a data compression technique that converts floating point values \n\ninto integers. In case of Qdrant `float32` gets converted into `int8`, so a single number \n\nneeds 75% less memory. It's not a simple rounding though! It's a process that makes that\n\ntransformation partially reversible, so we can also revert integers back to floats with \n\na small loss of precision. \n\n\n\n### Theoretical background\n\n\n\nAssume we have a collection of `float32` vectors and denote a single value as `f32`."", 'You will still get faster boolean operations and reduced RAM usage, but the accuracy degradation might be too high. \n\n\n\n## Sample Implementation\n\n\n\nNow that we have introduced you to binary quantization, let’s try our a basic implementation. In this example, we will be using OpenAI and Cohere with Qdrant.\n\n\n\n#### Create a collection with Binary Quantization enabled\n\n\n\nHere is what you should do at indexing time when you create the collection: \n\n\n\n1. We store all the ""full"" vectors on disk.\n\n2. Then we set the binary embeddings to be in RAM.\n\n\n\nBy default, both the full vectors and BQ get stored in RAM. We move the full vectors to disk because this saves us memory and allows us to store more vectors in RAM. By doing this, we explicitly move the binary vectors to memory by setting `always_ram=True`. \n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\n#collect to our Qdrant Server\n\nclient = QdrantClient(\n\n    url=""http://localhost:6333"",\n\n    prefer_grpc=True,\n\n)', 'optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 },\n\n\tquantizationConfig: new QuantizationConfig\n\n\t{\n\n\t\tScalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }\n\n\t}\n\n);\n\n```\n\n\n\n`mmmap_threshold` will ensure that vectors will be stored on disk, while `always_ram` will ensure that quantized vectors will be stored in RAM.\n\n\n\nOptionally, you can disable rescoring with search `params`, which will reduce the number of disk reads even further, but potentially slightly decrease the precision.\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{\n\n    ""params"": {\n\n        ""quantization"": {\n\n            ""rescore"": false\n\n        }\n\n    },\n\n    ""vector"": [0.2, 0.1, 0.9, 0.7],\n\n    ""limit"": 10\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.search(\n\n    collection_name=""{collection_name}"",\n\n    query_vector=[0.2, 0.1, 0.9, 0.7],']","```text

memory_size = 1.5 * number_of_vectors * vector_dimension * 4 bytes

```



While Qdrant offers various options to store some parts of the data on disk, starting 

from version 1.1.0, you can also optimize your memory by compressing the embeddings. 

We've implemented the mechanism of **Scalar Quantization**! It turns out to have not 

only a positive impact on memory but also on the performance. 



## Scalar Quantization



Scalar quantization is a data compression technique that converts floating point values 

into integers. In case of Qdrant `float32` gets converted into `int8`, so a single number 

needs 75% less memory. It's not a simple rounding though! It's a process that makes that

transformation partially reversible, so we can also revert integers back to floats with 

a small loss of precision. 



### Theoretical background



Assume we have a collection of `float32` vectors and denote a single value as `f32`.
You will still get faster boolean operations and reduced RAM usage, but the accuracy degradation might be too high. 



## Sample Implementation



Now that we have introduced you to binary quantization, let’s try our a basic implementation. In this example, we will be using OpenAI and Cohere with Qdrant.



#### Create a collection with Binary Quantization enabled



Here is what you should do at indexing time when you create the collection: 



1. We store all the ""full"" vectors on disk.

2. Then we set the binary embeddings to be in RAM.



By default, both the full vectors and BQ get stored in RAM. We move the full vectors to disk because this saves us memory and allows us to store more vectors in RAM. By doing this, we explicitly move the binary vectors to memory by setting `always_ram=True`. 



```python

from qdrant_client import QdrantClient



#collect to our Qdrant Server

client = QdrantClient(

    url=""http://localhost:6333"",

    prefer_grpc=True,

)
optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 },

	quantizationConfig: new QuantizationConfig

	{

		Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }

	}

);

```



`mmmap_threshold` will ensure that vectors will be stored on disk, while `always_ram` will ensure that quantized vectors will be stored in RAM.



Optionally, you can disable rescoring with search `params`, which will reduce the number of disk reads even further, but potentially slightly decrease the precision.



```http

POST /collections/{collection_name}/points/search

{

    ""params"": {

        ""quantization"": {

            ""rescore"": false

        }

    },

    ""vector"": [0.2, 0.1, 0.9, 0.7],

    ""limit"": 10

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.search(

    collection_name=""{collection_name}"",

    query_vector=[0.2, 0.1, 0.9, 0.7],"
How can dissimilarity search be used for outlier detection in a dataset?,"Dissimilarity search can be utilized for outlier detection in a dataset by first establishing a set of reference points that are considered ""normal"". These reference points serve as a benchmark for comparison. Subsequently, the dissimilarity search algorithm is applied to identify the data points that are the most dissimilar to the reference set. These identified points are then flagged as potential outliers or anomalies in the dataset. This approach allows for the detection of abnormal data points even in cases where labels are not available, enabling the identification of outliers based on their deviation from the established ""normal"" reference points. This method can be a valuable tool in data analysis and anomaly detection tasks.","['dataset_combinations = [\n\n    {\n\n        ""model_name"": ""text-embedding-3-large"",\n\n        ""dimensions"": 3072,\n\n    },\n\n    {\n\n        ""model_name"": ""text-embedding-3-large"",\n\n        ""dimensions"": 1024,\n\n    },\n\n    {\n\n        ""model_name"": ""text-embedding-3-large"",\n\n        ""dimensions"": 1536,\n\n    },\n\n    {\n\n        ""model_name"": ""text-embedding-3-small"",\n\n        ""dimensions"": 512,\n\n    },\n\n    {\n\n        ""model_name"": ""text-embedding-3-small"",\n\n        ""dimensions"": 1024,\n\n    },\n\n    {\n\n        ""model_name"": ""text-embedding-3-small"",\n\n        ""dimensions"": 1536,\n\n    },\n\n]\n\n```\n\n#### Exploring Dataset Combinations and Their Impacts on Model Performance', 'Diversity search utilizes the very same embeddings, and you can reuse them.\n\nIf your data is huge and does not fit into memory, vector search engines like [Qdrant](https://qdrant.tech/) might be helpful.\n\n\n\nAlthough the described methods can be used independently. But they are simple to combine and improve detection capabilities.\n\nIf the quality remains insufficient, you can fine-tune the models using a similarity learning approach (e.g. with [Quaterion](https://quaterion.qdrant.tech) both to provide a better representation of your data and pull apart dissimilar objects in space.\n\n\n\n## Conclusion\n\n\n\nIn this article, we enlightened distance-based methods to find errors in categorized datasets.\n\nShowed how to find incorrectly placed items in the furniture web store.\n\nI hope these methods will help you catch sneaky samples leaked into the wrong categories in your data, and make your users` experience more enjoyable.\n\n\n\nPoke the [demo](https://dataset-quality.qdrant.tech).\n\n\n\nStay tuned :)', 'On the contrary, if the proportion of out-of-place elements is high enough, outlier search methods are likely to be useless.\n\n\n\n### Similarity search\n\n\n\nThe idea behind similarity search is to measure semantic similarity between related parts of the data.\n\nE.g. between category title and item images.\n\nThe hypothesis is, that unsuitable items will be less similar.\n\n\n\nWe can\'t directly compare text and image data.\n\nFor this we need an intermediate representation - embeddings.\n\nEmbeddings are just numeric vectors containing semantic information.\n\nWe can apply a pre-trained model to our data to produce these vectors.\n\nAfter embeddings are created, we can measure the distances between them.\n\n\n\nAssume we want to search for something other than a single bed in «Single beds» category.\n\n\n\n{{< figure src=https://storage.googleapis.com/demo-dataset-quality-public/article/similarity_search.png caption=""Similarity search"" >}}\n\n\n\nOne of the possible pipelines would look like this:']","dataset_combinations = [

    {

        ""model_name"": ""text-embedding-3-large"",

        ""dimensions"": 3072,

    },

    {

        ""model_name"": ""text-embedding-3-large"",

        ""dimensions"": 1024,

    },

    {

        ""model_name"": ""text-embedding-3-large"",

        ""dimensions"": 1536,

    },

    {

        ""model_name"": ""text-embedding-3-small"",

        ""dimensions"": 512,

    },

    {

        ""model_name"": ""text-embedding-3-small"",

        ""dimensions"": 1024,

    },

    {

        ""model_name"": ""text-embedding-3-small"",

        ""dimensions"": 1536,

    },

]

```

#### Exploring Dataset Combinations and Their Impacts on Model Performance
Diversity search utilizes the very same embeddings, and you can reuse them.

If your data is huge and does not fit into memory, vector search engines like [Qdrant](https://qdrant.tech/) might be helpful.



Although the described methods can be used independently. But they are simple to combine and improve detection capabilities.

If the quality remains insufficient, you can fine-tune the models using a similarity learning approach (e.g. with [Quaterion](https://quaterion.qdrant.tech) both to provide a better representation of your data and pull apart dissimilar objects in space.



## Conclusion



In this article, we enlightened distance-based methods to find errors in categorized datasets.

Showed how to find incorrectly placed items in the furniture web store.

I hope these methods will help you catch sneaky samples leaked into the wrong categories in your data, and make your users` experience more enjoyable.



Poke the [demo](https://dataset-quality.qdrant.tech).



Stay tuned :)
On the contrary, if the proportion of out-of-place elements is high enough, outlier search methods are likely to be useless.



### Similarity search



The idea behind similarity search is to measure semantic similarity between related parts of the data.

E.g. between category title and item images.

The hypothesis is, that unsuitable items will be less similar.



We can't directly compare text and image data.

For this we need an intermediate representation - embeddings.

Embeddings are just numeric vectors containing semantic information.

We can apply a pre-trained model to our data to produce these vectors.

After embeddings are created, we can measure the distances between them.



Assume we want to search for something other than a single bed in «Single beds» category.



{{< figure src=https://storage.googleapis.com/demo-dataset-quality-public/article/similarity_search.png caption=""Similarity search"" >}}



One of the possible pipelines would look like this:"
How does binary quantization work and what benefits does it offer in terms of query processing times and data compression?,"Binary quantization is a method used by Qdrant for fast indexing and data compression. In this process, each data point is represented as a vector. The quantization splits the vector in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This results in a string of bits that represents the original vector, making it much smaller and easier to compare. By supporting vector comparisons, binary quantization can significantly speed up query processing times, up to 40 times faster. This method is especially effective for OpenAI embeddings, where it has been shown to achieve a massive performance improvement at a lower cost of accuracy.","['The accompanying graph illustrates the promising accuracy levels achievable with binary quantization across different model sizes, showcasing its practicality without severely compromising on performance. This dual advantage of storage reduction and accelerated search capabilities underscores the transformative potential of Binary Quantization in deploying OpenAI embeddings more effectively across various real-world applications.\n\n\n\n![](/blog/openai/Accuracy_Models.png)\n\n\n\nThe efficiency gains from Binary Quantization are as follows: \n\n\n\n- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. \n\n- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. \n\n- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.\n\n\n\n### Experiment Setup: OpenAI Embeddings in Focus', '</tr>\n\n      <tr>\n\n         <th>Compression</th>\n\n         <td>x1</td>\n\n         <td>x4</td>\n\n         <td>x8</td>\n\n         <td>x12</td>\n\n      </tr>\n\n      <tr>\n\n         <th>Upload & indexing time</th>\n\n         <td>147 s</td>\n\n         <td>339 s</td>\n\n         <td>217 s</td>\n\n         <td>178 s</td>\n\n      </tr>\n\n   </tbody>\n\n</table>\n\n\n\nProduct Quantization increases both indexing and searching time. The higher the compression ratio, \n\nthe lower the search precision. The main benefit is undoubtedly the reduced usage of memory.\n\n\n\n#### Arxiv-titles-384-angular-no-filters\n\n\n\n<table>\n\n   <thead>\n\n      <tr>\n\n         <th></th>\n\n         <th>Original</th>\n\n         <th>1D clusters</th>\n\n         <th>2D clusters</th>\n\n         <th>4D clusters</th>\n\n         <th>8D clusters</th>\n\n      </tr>\n\n   </thead>\n\n   <tbody>\n\n      <tr>\n\n         <th>Mean precision</th>\n\n         <td>0.9837</td>\n\n         <td>0.9677</td>\n\n         <td>0.9143</td>\n\n         <td>0.8068</td>\n\n         <td>0.6618</td>\n\n      </tr>', 'On the one hand, quantization allows for significant reductions in storage requirements and faster search times.\n\nThis can be particularly beneficial in large-scale applications where minimizing the use of resources is a top priority.\n\nOn the other hand, quantization introduces an approximation error, which can lead to a slight decrease in search quality.\n\nThe level of this tradeoff depends on the quantization method and its parameters, as well as the characteristics of the data.\n\n\n\n\n\n## Scalar Quantization\n\n\n\n*Available as of v1.1.0*\n\n\n\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\n\n\n\n\n\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\n\nIn other words, Qdrant performs `float32 -> uint8` conversion for each vector component.']","The accompanying graph illustrates the promising accuracy levels achievable with binary quantization across different model sizes, showcasing its practicality without severely compromising on performance. This dual advantage of storage reduction and accelerated search capabilities underscores the transformative potential of Binary Quantization in deploying OpenAI embeddings more effectively across various real-world applications.



![](/blog/openai/Accuracy_Models.png)



The efficiency gains from Binary Quantization are as follows: 



- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. 

- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. 

- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.



### Experiment Setup: OpenAI Embeddings in Focus
</tr>

      <tr>

         <th>Compression</th>

         <td>x1</td>

         <td>x4</td>

         <td>x8</td>

         <td>x12</td>

      </tr>

      <tr>

         <th>Upload & indexing time</th>

         <td>147 s</td>

         <td>339 s</td>

         <td>217 s</td>

         <td>178 s</td>

      </tr>

   </tbody>

</table>



Product Quantization increases both indexing and searching time. The higher the compression ratio, 

the lower the search precision. The main benefit is undoubtedly the reduced usage of memory.



#### Arxiv-titles-384-angular-no-filters



<table>

   <thead>

      <tr>

         <th></th>

         <th>Original</th>

         <th>1D clusters</th>

         <th>2D clusters</th>

         <th>4D clusters</th>

         <th>8D clusters</th>

      </tr>

   </thead>

   <tbody>

      <tr>

         <th>Mean precision</th>

         <td>0.9837</td>

         <td>0.9677</td>

         <td>0.9143</td>

         <td>0.8068</td>

         <td>0.6618</td>

      </tr>
On the one hand, quantization allows for significant reductions in storage requirements and faster search times.

This can be particularly beneficial in large-scale applications where minimizing the use of resources is a top priority.

On the other hand, quantization introduces an approximation error, which can lead to a slight decrease in search quality.

The level of this tradeoff depends on the quantization method and its parameters, as well as the characteristics of the data.





## Scalar Quantization



*Available as of v1.1.0*



Scalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.





For instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.

In other words, Qdrant performs `float32 -> uint8` conversion for each vector component."
What is the primary function of vector embeddings in the context of machine learning and artificial intelligence?,"Vector embeddings play a crucial role in machine learning and artificial intelligence by converting complex data into a format that machines can easily understand and process. These embeddings represent data points in a multi-dimensional space, where similar data points are closer together and dissimilar data points are farther apart. By capturing the underlying relationships and similarities between data points, vector embeddings enable algorithms to perform tasks such as similarity search, recommendation systems, and natural language processing more effectively. In essence, vector embeddings help in transforming raw data into a structured and meaningful representation that can be utilized by machine learning models to make accurate predictions and decisions.","[""tags: \n\n  - vector-search\n\n  - vector-database\n\n  - embeddings\n\n  - machine-learning\n\n  - artificial intelligence\n\n\n\n---\n\n\n\n> **Embeddings** are numerical machine learning representations of the semantic of the input data. They capture the meaning of complex, high-dimensional data, like text, images, or audio, into vectors. Enabling algorithms to process and analyze the data more efficiently.\n\n\n\nYou know when you’re scrolling through your social media feeds and the content just feels incredibly tailored to you? There's the news you care about, followed by a perfect tutorial with your favorite tech stack, and then a meme that makes you laugh so hard you snort.\n\n\n\nOr what about how YouTube recommends videos you ended up loving. It’s by creators you've never even heard of and you didn’t even send YouTube a note about your ideal content lineup.\n\n\n\nThis is the magic of embeddings."", 'The meaning of a data point is implicitly defined by its **position** on the vector space. After the vectors are stored, we can use their spatial properties to perform [nearest neighbor searches](https://en.wikipedia.org/wiki/Nearest_neighbor_search#:~:text=Nearest%20neighbor%20search%20(NNS)%2C,the%20larger%20the%20function%20values.). These searches retrieve semantically similar items based on how close they are in this space.  \n\n\n\n> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.\n\n\n\n\n\n### Creating Vector Embeddings\n\n\n\nEmbeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.\n\n\n\n\n\n![The process of using Neural Networks to create vector embeddings](/articles_data/what-are-embeddings/How-Do-Embeddings-Work_.jpg)', 'This capability is crucial for creating search systems, recommendation engines, retrieval augmented generation (RAG) and any application that benefits from a deep understanding of content.\n\n\n\n## How do embeddings work?\n\n\n\nEmbeddings are created through neural networks. They capture complex relationships and semantics into [dense vectors](https://www1.se.cuhk.edu.hk/~seem5680/lecture/semantics-with-dense-vectors-2018.pdf) which are more suitable for machine learning and data processing applications. They can then project these vectors into a proper **high-dimensional** space, specifically, a [Vector Database](https://qdrant.tech/articles/what-is-a-vector-database/). \n\n\n\n\n\n\n\n![The process for turning raw data into embeddings and placing them into the vector space](/articles_data/what-are-embeddings/How-Embeddings-Work.jpg)']","tags: 

  - vector-search

  - vector-database

  - embeddings

  - machine-learning

  - artificial intelligence



---



> **Embeddings** are numerical machine learning representations of the semantic of the input data. They capture the meaning of complex, high-dimensional data, like text, images, or audio, into vectors. Enabling algorithms to process and analyze the data more efficiently.



You know when you’re scrolling through your social media feeds and the content just feels incredibly tailored to you? There's the news you care about, followed by a perfect tutorial with your favorite tech stack, and then a meme that makes you laugh so hard you snort.



Or what about how YouTube recommends videos you ended up loving. It’s by creators you've never even heard of and you didn’t even send YouTube a note about your ideal content lineup.



This is the magic of embeddings.
The meaning of a data point is implicitly defined by its **position** on the vector space. After the vectors are stored, we can use their spatial properties to perform [nearest neighbor searches](https://en.wikipedia.org/wiki/Nearest_neighbor_search#:~:text=Nearest%20neighbor%20search%20(NNS)%2C,the%20larger%20the%20function%20values.). These searches retrieve semantically similar items based on how close they are in this space.  



> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.





### Creating Vector Embeddings



Embeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.





![The process of using Neural Networks to create vector embeddings](/articles_data/what-are-embeddings/How-Do-Embeddings-Work_.jpg)
This capability is crucial for creating search systems, recommendation engines, retrieval augmented generation (RAG) and any application that benefits from a deep understanding of content.



## How do embeddings work?



Embeddings are created through neural networks. They capture complex relationships and semantics into [dense vectors](https://www1.se.cuhk.edu.hk/~seem5680/lecture/semantics-with-dense-vectors-2018.pdf) which are more suitable for machine learning and data processing applications. They can then project these vectors into a proper **high-dimensional** space, specifically, a [Vector Database](https://qdrant.tech/articles/what-is-a-vector-database/). 







![The process for turning raw data into embeddings and placing them into the vector space](/articles_data/what-are-embeddings/How-Embeddings-Work.jpg)"
What is the concept of the `best_score` strategy introduced in version 1.6.0 of the recommendation system?,"The `best_score` strategy, introduced in version 1.6.0 of the recommendation system, is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided.","['{ ""id"": 14, ""score"": 0.75 },\n\n    { ""id"": 11, ""score"": 0.73 }\n\n  ],\n\n  ""status"": ""ok"",\n\n  ""time"": 0.001\n\n}\n\n```\n\n\n\nThe algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.\n\n\n\n### Average vector strategy\n\n\n\nThe default and first strategy added to Qdrant is called `average_vector`. It preprocesses the input examples to create a single vector that is used for the search. Since the preprocessing step happens very fast, the performance of this strategy is on-par with regular search. The intuition behind this kind of recommendation is that each vector component represents an independent feature of the data, so, by averaging the examples, we should get a good recommendation.', '</aside>\n\n\n\nTo use this algorithm, you need to set `""strategy"": ""best_score""` in the recommendation request.\n\n\n\n#### Using only negative examples\n\n\n\nA beneficial side-effect of `best_score` strategy is that you can use it with only negative examples. This will allow you to find the most dissimilar vectors to the ones you provide. This can be useful for finding outliers in your data, or for finding the most dissimilar vectors to a given one.\n\n\n\nCombining negative-only examples with filtering can be a powerful tool for data exploration and cleaning.\n\n\n\n### Multiple vectors\n\n\n\n*Available as of v0.10.0*\n\n\n\nIf the collection was created with multiple vectors, the name of the vector should be specified in the recommendation request:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/recommend\n\n{\n\n  ""positive"": [100, 231],\n\n  ""negative"": [718],\n\n  ""using"": ""image"",\n\n  ""limit"": 10\n\n }\n\n```\n\n\n\n```python\n\nclient.recommend(\n\n    collection_name=""{collection_name}"",\n\n    positive=[100, 231],\n\n    negative=[718],', '```rust\n\nlet score = if best_positive_score > best_negative_score {\n\n    best_positive_score;\n\n} else {\n\n    -(best_negative_score * best_negative_score);\n\n};\n\n```\n\n\n\n<aside role=""alert"">\n\nThe performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.\n\n</aside>\n\n\n\nSince we are computing similarities to every example at each step of the search, the performance of this strategy will be linearly impacted by the amount of examples. This means that the more examples you provide, the slower the search will be. However, this strategy can be very powerful and should be more embedding-agnostic.\n\n\n\n<aside role=""status"">\n\nAccuracy may be impacted with this strategy. To improve it, increasing the <code>ef</code> search parameter to something above 32 will already be much better than the default 16, e.g: <code>""params"": { ""ef"": 64 }</code>\n\n</aside>\n\n\n\nTo use this algorithm, you need to set `""strategy"": ""best_score""` in the recommendation request.']","{ ""id"": 14, ""score"": 0.75 },

    { ""id"": 11, ""score"": 0.73 }

  ],

  ""status"": ""ok"",

  ""time"": 0.001

}

```



The algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.



### Average vector strategy



The default and first strategy added to Qdrant is called `average_vector`. It preprocesses the input examples to create a single vector that is used for the search. Since the preprocessing step happens very fast, the performance of this strategy is on-par with regular search. The intuition behind this kind of recommendation is that each vector component represents an independent feature of the data, so, by averaging the examples, we should get a good recommendation.
</aside>



To use this algorithm, you need to set `""strategy"": ""best_score""` in the recommendation request.



#### Using only negative examples



A beneficial side-effect of `best_score` strategy is that you can use it with only negative examples. This will allow you to find the most dissimilar vectors to the ones you provide. This can be useful for finding outliers in your data, or for finding the most dissimilar vectors to a given one.



Combining negative-only examples with filtering can be a powerful tool for data exploration and cleaning.



### Multiple vectors



*Available as of v0.10.0*



If the collection was created with multiple vectors, the name of the vector should be specified in the recommendation request:



```http

POST /collections/{collection_name}/points/recommend

{

  ""positive"": [100, 231],

  ""negative"": [718],

  ""using"": ""image"",

  ""limit"": 10

 }

```



```python

client.recommend(

    collection_name=""{collection_name}"",

    positive=[100, 231],

    negative=[718],
```rust

let score = if best_positive_score > best_negative_score {

    best_positive_score;

} else {

    -(best_negative_score * best_negative_score);

};

```



<aside role=""alert"">

The performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.

</aside>



Since we are computing similarities to every example at each step of the search, the performance of this strategy will be linearly impacted by the amount of examples. This means that the more examples you provide, the slower the search will be. However, this strategy can be very powerful and should be more embedding-agnostic.



<aside role=""status"">

Accuracy may be impacted with this strategy. To improve it, increasing the <code>ef</code> search parameter to something above 32 will already be much better than the default 16, e.g: <code>""params"": { ""ef"": 64 }</code>

</aside>



To use this algorithm, you need to set `""strategy"": ""best_score""` in the recommendation request."
How does Qdrant handle sparse vectors and what changes have been made to accommodate them?,"Qdrant, until recently, was not able to handle sparse vectors natively. The majority of dimensions in sparse vectors are usually zeros, making them theoretically high dimensional. However, Qdrant stores sparse vectors differently by only keeping track of the non-zero dimensions. Previously, some users attempted to convert sparse vectors to dense vectors as a workaround, but this was not considered the best solution. In response to user demand for a single tool that can handle both sparse and dense vectors, Qdrant has now introduced support for sparse vectors. This change allows users to work with both sparse and dense vectors seamlessly within the Qdrant tool, eliminating the need for alternative solutions or tools for keyword lookup.","['""text"": models.SparseVector(\n\n                    indices=indices.tolist(), values=values.tolist()\n\n                )\n\n            },\n\n        )\n\n    ],\n\n)\n\n```\n\nBy upserting points with sparse vectors, we prepare our dataset for rapid first-stage retrieval, laying the groundwork for subsequent detailed analysis using dense vectors. Notice that we use ""text"" to denote the name of the sparse vector.\n\n\n\nThose familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you\'re able to **apply payload filters**, shard keys, and other advanced features you\'ve come to expect from Qdrant. To make things easier for you, the indices and values don\'t have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.\n\n\n\n### 4. Querying with Sparse Vectors', '*Available as of v1.2.0*\n\n\n\nVectors all live in RAM for very quick access. On a per-vector basis you can set\n\n`on_disk` to true to store all vectors on disk at all times. This will enable\n\nthe use of\n\n[memmaps](../../concepts/storage/#configuring-memmap-storage),\n\nwhich is suitable for ingesting a large amount of data.\n\n\n\n\n\n### Collection with sparse vectors\n\n\n\n*Available as of v1.7.0*\n\n\n\nQdrant supports sparse vectors as a first-class citizen.\n\n\n\nSparse vectors are useful for text search, where each word is represented as a separate dimension.\n\n\n\nCollections can contain sparse vectors as additional [named vectors](#collection-with-multiple-vectors) along side regular dense vectors in a single point.\n\n\n\nUnlike dense vectors, sparse vectors must be named.\n\nAnd additionally, sparse vectors and dense vectors must have different names within a collection.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""sparse_vectors"": {\n\n        ""text"": { },\n\n    }\n\n}\n\n```\n\n\n\n```bash', 'performance.\n\n\n\n## Sparse Vector Index\n\n\n\n*Available as of v1.7.0*\n\n\n\n### Key Features of Sparse Vector Index\n\n- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.\n\n- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.\n\n\n\n### Search Mechanism\n\n- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.\n\n- **Scoring Method:** Vectors are scored using the dot product.\n\n\n\n### Optimizations\n\n- **Reducing Vectors to Score:** Implementations are in place to minimize the number of vectors scored, especially for dimensions with numerous vectors.\n\n\n\n### Filtering and Configuration\n\n- **Filtering Support:** Similar to dense vectors, supports filtering by payload fields.\n\n- **`full_scan_threshold` Configuration:** Allows control over when to switch search from the payload index to minimize scoring vectors.']","""text"": models.SparseVector(

                    indices=indices.tolist(), values=values.tolist()

                )

            },

        )

    ],

)

```

By upserting points with sparse vectors, we prepare our dataset for rapid first-stage retrieval, laying the groundwork for subsequent detailed analysis using dense vectors. Notice that we use ""text"" to denote the name of the sparse vector.



Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.



### 4. Querying with Sparse Vectors
*Available as of v1.2.0*



Vectors all live in RAM for very quick access. On a per-vector basis you can set

`on_disk` to true to store all vectors on disk at all times. This will enable

the use of

[memmaps](../../concepts/storage/#configuring-memmap-storage),

which is suitable for ingesting a large amount of data.





### Collection with sparse vectors



*Available as of v1.7.0*



Qdrant supports sparse vectors as a first-class citizen.



Sparse vectors are useful for text search, where each word is represented as a separate dimension.



Collections can contain sparse vectors as additional [named vectors](#collection-with-multiple-vectors) along side regular dense vectors in a single point.



Unlike dense vectors, sparse vectors must be named.

And additionally, sparse vectors and dense vectors must have different names within a collection.



```http

PUT /collections/{collection_name}

{

    ""sparse_vectors"": {

        ""text"": { },

    }

}

```



```bash
performance.



## Sparse Vector Index



*Available as of v1.7.0*



### Key Features of Sparse Vector Index

- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.

- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.



### Search Mechanism

- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.

- **Scoring Method:** Vectors are scored using the dot product.



### Optimizations

- **Reducing Vectors to Score:** Implementations are in place to minimize the number of vectors scored, especially for dimensions with numerous vectors.



### Filtering and Configuration

- **Filtering Support:** Similar to dense vectors, supports filtering by payload fields.

- **`full_scan_threshold` Configuration:** Allows control over when to switch search from the payload index to minimize scoring vectors."
What is the purpose of the `prefix` parameter in the configuration options for the Qdrant instance?,"The `prefix` parameter in the configuration options for the Qdrant instance serves the purpose of adding a specific prefix to the REST URL path. For example, setting the `prefix` parameter to `service/v1` would result in the REST API endpoint being accessed at `http://localhost:6333/service/v1/{qdrant-endpoint}`. This allows for customization and organization of the API endpoints under a specific path, making it easier to manage and access different functionalities within the Qdrant instance.","['likely that they are absent on your local machine if you run the `qdrant` binary\n\nmanually.\n\n\n\nIf file 2 or 3 are not found, a warning is shown on startup.\n\nIf file 5 is provided but not found, an error is shown on startup.\n\n\n\nOther supported configuration file formats and extensions include: `.toml`, `.json`, `.ini`.\n\n\n\n## Environment variables\n\n\n\nIt is possible to set configuration properties using environment variables.\n\nEnvironment variables are always the most significant and cannot be overwritten\n\n(see [ordering](#order-and-priority)).\n\n\n\nAll environment variables are prefixed with `QDRANT__` and are separated with\n\n`__`.\n\n\n\nThese variables:\n\n\n\n```bash\n\nQDRANT__LOG_LEVEL=INFO\n\nQDRANT__SERVICE__HTTP_PORT=6333\n\nQDRANT__SERVICE__ENABLE_TLS=1\n\nQDRANT__TLS__CERT=./tls/cert.pem\n\nQDRANT__TLS__CERT_TTL=3600\n\n```\n\n\n\nresult in this configuration:\n\n\n\n```yaml\n\nlog_level: INFO\n\nservice:\n\n  http_port: 6333\n\n  enable_tls: true\n\ntls:\n\n  cert: ./tls/cert.pem\n\n  cert_ttl: 3600\n\n```', 'qdrant::{\n\n        vectors_config::Config, CreateCollection, Distance, VectorParams, VectorParamsMap,\n\n        VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".to_string(),\n\n        vectors_config: Some(VectorsConfig {\n\n            config: Some(Config::ParamsMap(VectorParamsMap {\n\n                map: [\n\n                    (\n\n                        ""image"".to_string(),\n\n                        VectorParams {\n\n                            size: 4,\n\n                            distance: Distance::Dot.into(),\n\n                            ..Default::default()\n\n                        },\n\n                    ),\n\n                    (\n\n                        ""text"".to_string(),\n\n                        VectorParams {\n\n                            size: 8,\n\n                            distance: Distance::Cosine.into(),', 'qdrant::{vectors_config::Config, CreateCollection, Distance, VectorParams, VectorsConfig},\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".into(),\n\n        vectors_config: Some(VectorsConfig {\n\n            config: Some(Config::Params(VectorParams {\n\n                size: 300,\n\n                distance: Distance::Cosine.into(),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        shard_number: Some(6),\n\n        replication_factor: Some(2),\n\n        write_consistency_factor: Some(2),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;']","likely that they are absent on your local machine if you run the `qdrant` binary

manually.



If file 2 or 3 are not found, a warning is shown on startup.

If file 5 is provided but not found, an error is shown on startup.



Other supported configuration file formats and extensions include: `.toml`, `.json`, `.ini`.



## Environment variables



It is possible to set configuration properties using environment variables.

Environment variables are always the most significant and cannot be overwritten

(see [ordering](#order-and-priority)).



All environment variables are prefixed with `QDRANT__` and are separated with

`__`.



These variables:



```bash

QDRANT__LOG_LEVEL=INFO

QDRANT__SERVICE__HTTP_PORT=6333

QDRANT__SERVICE__ENABLE_TLS=1

QDRANT__TLS__CERT=./tls/cert.pem

QDRANT__TLS__CERT_TTL=3600

```



result in this configuration:



```yaml

log_level: INFO

service:

  http_port: 6333

  enable_tls: true

tls:

  cert: ./tls/cert.pem

  cert_ttl: 3600

```
qdrant::{

        vectors_config::Config, CreateCollection, Distance, VectorParams, VectorParamsMap,

        VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".to_string(),

        vectors_config: Some(VectorsConfig {

            config: Some(Config::ParamsMap(VectorParamsMap {

                map: [

                    (

                        ""image"".to_string(),

                        VectorParams {

                            size: 4,

                            distance: Distance::Dot.into(),

                            ..Default::default()

                        },

                    ),

                    (

                        ""text"".to_string(),

                        VectorParams {

                            size: 8,

                            distance: Distance::Cosine.into(),
qdrant::{vectors_config::Config, CreateCollection, Distance, VectorParams, VectorsConfig},

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".into(),

        vectors_config: Some(VectorsConfig {

            config: Some(Config::Params(VectorParams {

                size: 300,

                distance: Distance::Cosine.into(),

                ..Default::default()

            })),

        }),

        shard_number: Some(6),

        replication_factor: Some(2),

        write_consistency_factor: Some(2),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;"
What is the default scoring metric used in Qdrant for search operations?,"The default scoring metric used in Qdrant for search operations is `Dot product`. It is not necessary to specify this metric explicitly as it is the default setting. However, Qdrant also supports other distance metrics such as Cosine for scoring. When performing a search operation in Qdrant, if no specific metric is mentioned, the system will automatically use the Dot product as the scoring metric.","['#                payload=None, vector=None)],\n\n#   [ScoredPoint(id=3, version=0, score=0.09, \n\n#                payload=None, vector=[0.0, 0.0, 0.0, 0.1]),\n\n#    ScoredPoint(id=1, version=0, score=0.10049876, \n\n#                payload=None, vector=[0.0, 0.1, 0.0, 0.0])]\n\n# ]\n\n\n\n```\n\n\n\nEach instance of the SearchRequest class may provide its own search parameters, including vector query but also some additional filters. The response will be a list of individual results for each request. In case any of the requests is malformed, there will be an exception thrown, so either all of them pass or none of them.\n\n\n\nAnd that’s it! You no longer have to handle the multiple requests on your own. Qdrant will do it under the hood.\n\n\n\n## Benchmark\n\n\n\nThe batch search is fairly easy to be integrated into your application, but if you prefer to see some numbers before deciding to switch, then it’s worth comparing four different options:\n\n\n\n1. Querying the database sequentially.', '{ ""id"": 11, ""score"": 0.73 }\n\n    ],\n\n    [\n\n        { ""id"": 1, ""score"": 0.92 },\n\n        { ""id"": 3, ""score"": 0.89 },\n\n        { ""id"": 9, ""score"": 0.75 }\n\n    ]\n\n  ],\n\n  ""status"": ""ok"",\n\n  ""time"": 0.001\n\n}\n\n```\n\n\n\n## Pagination\n\n\n\n*Available as of v0.8.3*\n\n\n\nSearch and [recommendation](../explore/#recommendation-api) APIs allow to skip first results of the search and return only the result starting from some specified offset:\n\n\n\nExample:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{\n\n    ""vector"": [0.2, 0.1, 0.9, 0.7],\n\n    ""with_vectors"": true,\n\n    ""with_payload"": true,\n\n    ""limit"": 10,\n\n    ""offset"": 100\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.search(\n\n    collection_name=""{collection_name}"",\n\n    query_vector=[0.2, 0.1, 0.9, 0.7],\n\n    with_vectors=True,\n\n    with_payload=True,\n\n    limit=10,\n\n    offset=100,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', 'from qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.search(\n\n    collection_name=""{collection_name}"",\n\n    query_vector=[0.2, 0.1, 0.9, 0.7],\n\n    search_params=models.SearchParams(\n\n        quantization=models.QuantizationSearchParams(\n\n            ignore=False,\n\n            rescore=True,\n\n            oversampling=2.4\n\n        )\n\n    )\n\n)\n\n```\n\n\n\nIn this case, if `oversampling` is 2.4 and `limit` is 100, then 240 vectors will be pre-selected using quantized index, and then the top 100 points will be returned after re-scoring with the unquantized vectors.\n\n\n\nAs you can see from the example above, this parameter is set during the query. This is a flexible method that will let you tune query accuracy. While the index is not changed, you can decide how many points you want to retrieve using quantized vectors.\n\n\n\n### Grouping API lookup']","#                payload=None, vector=None)],

#   [ScoredPoint(id=3, version=0, score=0.09, 

#                payload=None, vector=[0.0, 0.0, 0.0, 0.1]),

#    ScoredPoint(id=1, version=0, score=0.10049876, 

#                payload=None, vector=[0.0, 0.1, 0.0, 0.0])]

# ]



```



Each instance of the SearchRequest class may provide its own search parameters, including vector query but also some additional filters. The response will be a list of individual results for each request. In case any of the requests is malformed, there will be an exception thrown, so either all of them pass or none of them.



And that’s it! You no longer have to handle the multiple requests on your own. Qdrant will do it under the hood.



## Benchmark



The batch search is fairly easy to be integrated into your application, but if you prefer to see some numbers before deciding to switch, then it’s worth comparing four different options:



1. Querying the database sequentially.
{ ""id"": 11, ""score"": 0.73 }

    ],

    [

        { ""id"": 1, ""score"": 0.92 },

        { ""id"": 3, ""score"": 0.89 },

        { ""id"": 9, ""score"": 0.75 }

    ]

  ],

  ""status"": ""ok"",

  ""time"": 0.001

}

```



## Pagination



*Available as of v0.8.3*



Search and [recommendation](../explore/#recommendation-api) APIs allow to skip first results of the search and return only the result starting from some specified offset:



Example:



```http

POST /collections/{collection_name}/points/search

{

    ""vector"": [0.2, 0.1, 0.9, 0.7],

    ""with_vectors"": true,

    ""with_payload"": true,

    ""limit"": 10,

    ""offset"": 100

}

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.search(

    collection_name=""{collection_name}"",

    query_vector=[0.2, 0.1, 0.9, 0.7],

    with_vectors=True,

    with_payload=True,

    limit=10,

    offset=100,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.search(

    collection_name=""{collection_name}"",

    query_vector=[0.2, 0.1, 0.9, 0.7],

    search_params=models.SearchParams(

        quantization=models.QuantizationSearchParams(

            ignore=False,

            rescore=True,

            oversampling=2.4

        )

    )

)

```



In this case, if `oversampling` is 2.4 and `limit` is 100, then 240 vectors will be pre-selected using quantized index, and then the top 100 points will be returned after re-scoring with the unquantized vectors.



As you can see from the example above, this parameter is set during the query. This is a flexible method that will let you tune query accuracy. While the index is not changed, you can decide how many points you want to retrieve using quantized vectors.



### Grouping API lookup"
What options are available for using Qdrant if the official clients do not support the language you are using?,"If the official clients for Qdrant do not support the language you are using, you have the option to either use the REST API directly or generate a client for your language using OpenAPI definitions or protobuf definitions. By utilizing the OpenAPI definitions available at [this link](https://github.com/qdrant/qdrant/blob/master/docs/redoc/master/openapi.json) or the protobuf definitions at [this link](https://github.com/qdrant/qdrant/tree/master/lib/api/src/grpc/proto), you can interact with Qdrant's functionalities even if there is no official client available for your specific programming language. This flexibility allows developers to integrate Qdrant into a wide range of applications and environments.","['[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information\n\nmight also be found in the [LangChain documentation](https://python.langchain.com/docs/integrations/vectorstores/qdrant).', 'This right of use is subject to your payment of the total amount of the usage fees due under the Licence.\n\n\n\nThis License does not confer any exclusivity of any kind. Qdrant remains free to grant Licenses to third parties of its choice.\n\n\n\nYou acknowledge having been informed by Qdrant of all the technical requirements necessary to access and use the Solution. You are also informed that these requirements may change, particularly for technical reasons. In case of any change, you will be informed in advance.\n\n\n\nYou accept these conditions and agree not to use the Solution or its content for purposes other than its original function, particularly for comparative analysis or development of competing software.\n\n\n\n\n\n### 5. Financial terms\n\n\n\nThe prices applicable at the date of subscription to the Solution are accessible through the following [link](https://qdrant.com/pricing).\n\n\n\nUnless otherwise stated, prices are in dollars and exclusive of any applicable taxes.', '```python\n\nqdrant = Qdrant.from_documents(\n\n    docs, embeddings, \n\n    path=""/tmp/local_qdrant"",\n\n    collection_name=""my_documents"",\n\n)\n\n```\n\n\n\n### On-premise server deployment\n\n\n\nNo matter if you choose to launch Qdrant locally with [a Docker container](/documentation/guides/installation/), or \n\nselect a Kubernetes deployment with [the official Helm chart](https://github.com/qdrant/qdrant-helm), the way you\'re \n\ngoing to connect to such an instance will be identical. You\'ll need to provide a URL pointing to the service.\n\n\n\n```python\n\nurl = ""<---qdrant url here --->""\n\nqdrant = Qdrant.from_documents(\n\n    docs, \n\n    embeddings, \n\n    url, \n\n    prefer_grpc=True, \n\n    collection_name=""my_documents"",\n\n)\n\n```\n\n\n\n## Next steps\n\n\n\nIf you\'d like to know more about running Qdrant in a LangChain-based application, please read our article \n\n[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information']","[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information

might also be found in the [LangChain documentation](https://python.langchain.com/docs/integrations/vectorstores/qdrant).
This right of use is subject to your payment of the total amount of the usage fees due under the Licence.



This License does not confer any exclusivity of any kind. Qdrant remains free to grant Licenses to third parties of its choice.



You acknowledge having been informed by Qdrant of all the technical requirements necessary to access and use the Solution. You are also informed that these requirements may change, particularly for technical reasons. In case of any change, you will be informed in advance.



You accept these conditions and agree not to use the Solution or its content for purposes other than its original function, particularly for comparative analysis or development of competing software.





### 5. Financial terms



The prices applicable at the date of subscription to the Solution are accessible through the following [link](https://qdrant.com/pricing).



Unless otherwise stated, prices are in dollars and exclusive of any applicable taxes.
```python

qdrant = Qdrant.from_documents(

    docs, embeddings, 

    path=""/tmp/local_qdrant"",

    collection_name=""my_documents"",

)

```



### On-premise server deployment



No matter if you choose to launch Qdrant locally with [a Docker container](/documentation/guides/installation/), or 

select a Kubernetes deployment with [the official Helm chart](https://github.com/qdrant/qdrant-helm), the way you're 

going to connect to such an instance will be identical. You'll need to provide a URL pointing to the service.



```python

url = ""<---qdrant url here --->""

qdrant = Qdrant.from_documents(

    docs, 

    embeddings, 

    url, 

    prefer_grpc=True, 

    collection_name=""my_documents"",

)

```



## Next steps



If you'd like to know more about running Qdrant in a LangChain-based application, please read our article 

[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information"
What are the two main interfaces available for interacting with Qdrant and where can the documentation for each be found?,"The two main interfaces available for interacting with Qdrant are REST API and gRPC API. The documentation for REST API can be found in the [OpenAPI Specification](https://qdrant.github.io/qdrant/redoc/index.html), while the documentation for gRPC API can be found [here](https://github.com/qdrant/qdrant/blob/master/docs/grpc/docs.md). The gRPC methods in Qdrant follow the same principles as REST, with each REST endpoint having a corresponding gRPC method. The gRPC interface in Qdrant is available on a specified port as outlined in the configuration file, with the default port being 6334. If choosing to use gRPC, it is necessary to expose the port when starting Qdrant, such as when running the service inside a Docker container.","['---\n\ntitle: Interfaces\n\nweight: 14\n\n---\n\n\n\n# Interfaces\n\n\n\nQdrant supports these ""official"" clients. \n\n\n\n> **Note:** If you are using a language that is not listed here, you can use the REST API directly or generate a client for your language \n\nusing [OpenAPI](https://github.com/qdrant/qdrant/blob/master/docs/redoc/master/openapi.json)\n\nor [protobuf](https://github.com/qdrant/qdrant/tree/master/lib/api/src/grpc/proto) definitions. \n\n\n\n## Client Libraries\n\n||Client Repository|Installation|Version|\n\n|-|-|-|-|\n\n|[![python](/docs/misc/python.webp)](https://python-client.qdrant.tech/)|**[Python](https://github.com/qdrant/qdrant-client)** + **[(Client Docs)](https://python-client.qdrant.tech/)**|`pip install qdrant-client[fastembed]`|[Latest Release](https://github.com/qdrant/qdrant-client/releases)|\n\n|![typescript](/docs/misc/ts.webp)|**[JavaScript / Typescript](https://github.com/qdrant/qdrant-js)**|`npm install @qdrant/js-client-rest`|[Latest Release](https://github.com/qdrant/qdrant-js/releases)|', '```python\n\nqdrant = Qdrant.from_documents(\n\n    docs, embeddings, \n\n    path=""/tmp/local_qdrant"",\n\n    collection_name=""my_documents"",\n\n)\n\n```\n\n\n\n### On-premise server deployment\n\n\n\nNo matter if you choose to launch Qdrant locally with [a Docker container](/documentation/guides/installation/), or \n\nselect a Kubernetes deployment with [the official Helm chart](https://github.com/qdrant/qdrant-helm), the way you\'re \n\ngoing to connect to such an instance will be identical. You\'ll need to provide a URL pointing to the service.\n\n\n\n```python\n\nurl = ""<---qdrant url here --->""\n\nqdrant = Qdrant.from_documents(\n\n    docs, \n\n    embeddings, \n\n    url, \n\n    prefer_grpc=True, \n\n    collection_name=""my_documents"",\n\n)\n\n```\n\n\n\n## Next steps\n\n\n\nIf you\'d like to know more about running Qdrant in a LangChain-based application, please read our article \n\n[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information', 'location=""http://localhost:6333"",\n\n            collection_name=""test"",\n\n        ),\n\n        write_config=QdrantWriteConfig(batch_size=80),\n\n    )\n\n\n\nif __name__ == ""__main__"":\n\n    writer = get_writer()\n\n    runner = LocalRunner(\n\n        processor_config=ProcessorConfig(\n\n            verbose=True,\n\n            output_dir=""local-output-to-qdrant"",\n\n            num_processes=2,\n\n        ),\n\n        connector_config=SimpleLocalConfig(\n\n            input_path=""example-docs/book-war-and-peace-1225p.txt"",\n\n        ),\n\n        read_config=ReadConfig(),\n\n        partition_config=PartitionConfig(),\n\n        chunking_config=ChunkingConfig(chunk_elements=True),\n\n        embedding_config=EmbeddingConfig(provider=""langchain-huggingface""),\n\n        writer=writer,\n\n        writer_kwargs={},\n\n    )\n\n    runner.run()\n\n```\n\n\n\n## Next steps\n\n\n\n- Unstructured API [reference](https://unstructured-io.github.io/unstructured/api.html).']","---

title: Interfaces

weight: 14

---



# Interfaces



Qdrant supports these ""official"" clients. 



> **Note:** If you are using a language that is not listed here, you can use the REST API directly or generate a client for your language 

using [OpenAPI](https://github.com/qdrant/qdrant/blob/master/docs/redoc/master/openapi.json)

or [protobuf](https://github.com/qdrant/qdrant/tree/master/lib/api/src/grpc/proto) definitions. 



## Client Libraries

||Client Repository|Installation|Version|

|-|-|-|-|

|[![python](/docs/misc/python.webp)](https://python-client.qdrant.tech/)|**[Python](https://github.com/qdrant/qdrant-client)** + **[(Client Docs)](https://python-client.qdrant.tech/)**|`pip install qdrant-client[fastembed]`|[Latest Release](https://github.com/qdrant/qdrant-client/releases)|

|![typescript](/docs/misc/ts.webp)|**[JavaScript / Typescript](https://github.com/qdrant/qdrant-js)**|`npm install @qdrant/js-client-rest`|[Latest Release](https://github.com/qdrant/qdrant-js/releases)|
```python

qdrant = Qdrant.from_documents(

    docs, embeddings, 

    path=""/tmp/local_qdrant"",

    collection_name=""my_documents"",

)

```



### On-premise server deployment



No matter if you choose to launch Qdrant locally with [a Docker container](/documentation/guides/installation/), or 

select a Kubernetes deployment with [the official Helm chart](https://github.com/qdrant/qdrant-helm), the way you're 

going to connect to such an instance will be identical. You'll need to provide a URL pointing to the service.



```python

url = ""<---qdrant url here --->""

qdrant = Qdrant.from_documents(

    docs, 

    embeddings, 

    url, 

    prefer_grpc=True, 

    collection_name=""my_documents"",

)

```



## Next steps



If you'd like to know more about running Qdrant in a LangChain-based application, please read our article 

[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information
location=""http://localhost:6333"",

            collection_name=""test"",

        ),

        write_config=QdrantWriteConfig(batch_size=80),

    )



if __name__ == ""__main__"":

    writer = get_writer()

    runner = LocalRunner(

        processor_config=ProcessorConfig(

            verbose=True,

            output_dir=""local-output-to-qdrant"",

            num_processes=2,

        ),

        connector_config=SimpleLocalConfig(

            input_path=""example-docs/book-war-and-peace-1225p.txt"",

        ),

        read_config=ReadConfig(),

        partition_config=PartitionConfig(),

        chunking_config=ChunkingConfig(chunk_elements=True),

        embedding_config=EmbeddingConfig(provider=""langchain-huggingface""),

        writer=writer,

        writer_kwargs={},

    )

    runner.run()

```



## Next steps



- Unstructured API [reference](https://unstructured-io.github.io/unstructured/api.html)."
When should one use gRPC with Qdrant and what factors should be considered in making this choice?,"gRPC is recommended for use with Qdrant when optimizing the performance of an application is a priority and when the user is already familiar with Qdrant. The decision to use gRPC over the REST API is a trade-off between convenience and speed. gRPC is a binary protocol that offers higher performance compared to REST API, but it can be more challenging to debug. Therefore, if the goal is to maximize performance and the user is comfortable with the potential debugging complexities associated with gRPC, it is advisable to opt for gRPC when interacting with Qdrant.","[""```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```\n\n\n\n**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.\n\n\n\n## Qdrant Web UI\n\n\n\nQdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points.\n\n\n\nIn the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. \n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Accessing the Web UI\n\n\n\nFirst, run the Docker container:\n\n\n\n```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```"", '![REST API vs gRPC upload time, sec](/blog/qdrant-v-0-6-0-engine-with-grpc-released/upload_time.png)\n\n\n\nRead more about the gRPC interface and whether you should use it by this [link](/documentation/quick_start/#grpc).\n\n\n\nThe release v0.6.0 includes several bug fixes. More information is available in a [changelog](https://github.com/qdrant/qdrant/releases/tag/v0.6.0).\n\n\n\nNew version was provided in addition to the REST API that the company keeps supporting due to its easy debugging.', ""| location     | If `:memory:` - use in-memory Qdrant instance.<br>If `str` - use it as a `url` parameter.|\n\n| url          | Either host or str of `Optional[scheme], host, Optional[port], Optional[prefix]`.<br> Eg. `http://localhost:6333` |\n\n| port         | Port of the REST API interface. Default: `6333` |\n\n| grpc_port    | Port of the gRPC interface. Default: `6334` |\n\n| prefer_grpc  | If `true` - use gRPC interface whenever possible in custom methods. |\n\n| https        | If `true` - use HTTPS(SSL) protocol.|\n\n| api_key      | API key for authentication in Qdrant Cloud.|\n\n| prefix       | If set, add `prefix` to the REST URL path.<br>Example: `service/v1` will result in `http://localhost:6333/service/v1/{qdrant-endpoint}` for REST API.|\n\n| timeout      | Timeout for REST and gRPC API requests.<br>Default: 5.0 seconds for REST and unlimited for gRPC |\n\n| host         | Host name of Qdrant service. If url and host are not set, defaults to 'localhost'.|""]","```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```



**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.



## Qdrant Web UI



Qdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points.



In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. 



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Accessing the Web UI



First, run the Docker container:



```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```
![REST API vs gRPC upload time, sec](/blog/qdrant-v-0-6-0-engine-with-grpc-released/upload_time.png)



Read more about the gRPC interface and whether you should use it by this [link](/documentation/quick_start/#grpc).



The release v0.6.0 includes several bug fixes. More information is available in a [changelog](https://github.com/qdrant/qdrant/releases/tag/v0.6.0).



New version was provided in addition to the REST API that the company keeps supporting due to its easy debugging.
| location     | If `:memory:` - use in-memory Qdrant instance.<br>If `str` - use it as a `url` parameter.|

| url          | Either host or str of `Optional[scheme], host, Optional[port], Optional[prefix]`.<br> Eg. `http://localhost:6333` |

| port         | Port of the REST API interface. Default: `6333` |

| grpc_port    | Port of the gRPC interface. Default: `6334` |

| prefer_grpc  | If `true` - use gRPC interface whenever possible in custom methods. |

| https        | If `true` - use HTTPS(SSL) protocol.|

| api_key      | API key for authentication in Qdrant Cloud.|

| prefix       | If set, add `prefix` to the REST URL path.<br>Example: `service/v1` will result in `http://localhost:6333/service/v1/{qdrant-endpoint}` for REST API.|

| timeout      | Timeout for REST and gRPC API requests.<br>Default: 5.0 seconds for REST and unlimited for gRPC |

| host         | Host name of Qdrant service. If url and host are not set, defaults to 'localhost'.|"
Where can the GUI for Qdrant be accessed?,"The GUI for Qdrant can be accessed at `http://localhost:6333/dashboard`. This web address will lead you to the graphical user interface (GUI) for Qdrant, providing a visual interface for interacting with the Qdrant system. The GUI allows users to perform various operations and tasks related to Qdrant, such as managing indexes, querying data, and configuring settings. By accessing the GUI at the specified URL, users can easily navigate and interact with the Qdrant system in a user-friendly and intuitive manner.","['```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```\n\n\n\nThe GUI is available at `http://localhost:6333/dashboard`', 'location=""http://localhost:6333"",\n\n            collection_name=""test"",\n\n        ),\n\n        write_config=QdrantWriteConfig(batch_size=80),\n\n    )\n\n\n\nif __name__ == ""__main__"":\n\n    writer = get_writer()\n\n    runner = LocalRunner(\n\n        processor_config=ProcessorConfig(\n\n            verbose=True,\n\n            output_dir=""local-output-to-qdrant"",\n\n            num_processes=2,\n\n        ),\n\n        connector_config=SimpleLocalConfig(\n\n            input_path=""example-docs/book-war-and-peace-1225p.txt"",\n\n        ),\n\n        read_config=ReadConfig(),\n\n        partition_config=PartitionConfig(),\n\n        chunking_config=ChunkingConfig(chunk_elements=True),\n\n        embedding_config=EmbeddingConfig(provider=""langchain-huggingface""),\n\n        writer=writer,\n\n        writer_kwargs={},\n\n    )\n\n    runner.run()\n\n```\n\n\n\n## Next steps\n\n\n\n- Unstructured API [reference](https://unstructured-io.github.io/unstructured/api.html).', '```python\n\nqdrant = Qdrant.from_documents(\n\n    docs, embeddings, \n\n    path=""/tmp/local_qdrant"",\n\n    collection_name=""my_documents"",\n\n)\n\n```\n\n\n\n### On-premise server deployment\n\n\n\nNo matter if you choose to launch Qdrant locally with [a Docker container](/documentation/guides/installation/), or \n\nselect a Kubernetes deployment with [the official Helm chart](https://github.com/qdrant/qdrant-helm), the way you\'re \n\ngoing to connect to such an instance will be identical. You\'ll need to provide a URL pointing to the service.\n\n\n\n```python\n\nurl = ""<---qdrant url here --->""\n\nqdrant = Qdrant.from_documents(\n\n    docs, \n\n    embeddings, \n\n    url, \n\n    prefer_grpc=True, \n\n    collection_name=""my_documents"",\n\n)\n\n```\n\n\n\n## Next steps\n\n\n\nIf you\'d like to know more about running Qdrant in a LangChain-based application, please read our article \n\n[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information']","```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```



The GUI is available at `http://localhost:6333/dashboard`
location=""http://localhost:6333"",

            collection_name=""test"",

        ),

        write_config=QdrantWriteConfig(batch_size=80),

    )



if __name__ == ""__main__"":

    writer = get_writer()

    runner = LocalRunner(

        processor_config=ProcessorConfig(

            verbose=True,

            output_dir=""local-output-to-qdrant"",

            num_processes=2,

        ),

        connector_config=SimpleLocalConfig(

            input_path=""example-docs/book-war-and-peace-1225p.txt"",

        ),

        read_config=ReadConfig(),

        partition_config=PartitionConfig(),

        chunking_config=ChunkingConfig(chunk_elements=True),

        embedding_config=EmbeddingConfig(provider=""langchain-huggingface""),

        writer=writer,

        writer_kwargs={},

    )

    runner.run()

```



## Next steps



- Unstructured API [reference](https://unstructured-io.github.io/unstructured/api.html).
```python

qdrant = Qdrant.from_documents(

    docs, embeddings, 

    path=""/tmp/local_qdrant"",

    collection_name=""my_documents"",

)

```



### On-premise server deployment



No matter if you choose to launch Qdrant locally with [a Docker container](/documentation/guides/installation/), or 

select a Kubernetes deployment with [the official Helm chart](https://github.com/qdrant/qdrant-helm), the way you're 

going to connect to such an instance will be identical. You'll need to provide a URL pointing to the service.



```python

url = ""<---qdrant url here --->""

qdrant = Qdrant.from_documents(

    docs, 

    embeddings, 

    url, 

    prefer_grpc=True, 

    collection_name=""my_documents"",

)

```



## Next steps



If you'd like to know more about running Qdrant in a LangChain-based application, please read our article 

[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information"
What are the three ways to use Qdrant?,"According to the documentation, there are three ways to use Qdrant:

1. Run a Docker image: This method is recommended for users who do not have a Python development environment. By running a Docker image, users can quickly set up a local Qdrant server and storage.

2. Get the Python client: Users familiar with Python can install the Qdrant client by using `pip install qdrant-client`. The Python client also supports an in-memory database.

3. Spin up a Qdrant Cloud cluster: This method is recommended for running Qdrant in a production environment. Users can set up their first instance by following the Quickstart guide provided in the documentation.","[""|----------------------------------------------------|----------------------------------------------|---------|------------------|\n\n| [Installation](guides/installation/) | Different ways to install Qdrant. | [Collections](concepts/collections/) | Learn about the central concept behind Qdrant. |                  \n\n| [Configuration](guides/configuration/)   | Update the default configuration.    | [Bulk Upload](tutorials/bulk-upload/) |   Efficiently upload a large number of vectors. |                  \n\n| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  \n\n\n\n## Common Use Cases:"", 'The easiest way to use Qdrant is to run a pre-built image.\n\nSo make sure you have Docker installed on your system.\n\n\n\nTo start Qdrant, use the instructions on its [homepage](https://github.com/qdrant/qdrant).\n\n\n\nDownload image from [DockerHub](https://hub.docker.com/r/qdrant/qdrant):\n\n\n\n```bash\n\ndocker pull qdrant/qdrant\n\n```\n\n\n\nAnd run the service inside the docker:\n\n\n\n```bash\n\ndocker run -p 6333:6333 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage \\\n\n    qdrant/qdrant\n\n```\n\nYou should see output like this\n\n\n\n```text\n\n...\n\n[2021-02-05T00:08:51Z INFO  actix_server::builder] Starting 12 workers\n\n[2021-02-05T00:08:51Z INFO  actix_server::builder] Starting ""actix-web-service-0.0.0.0:6333"" service on 0.0.0.0:6333\n\n```\n\n\n\nThis means that the service is successfully launched and listening port 6333.\n\nTo make sure you can test [http://localhost:6333/](http://localhost:6333/) in your browser and get qdrant version info.', 'hook = QdrantHook(conn_id=""qdrant_connection"")\n\n\n\nhook.verify_connection()\n\n```\n\n\n\nA [`qdrant_client#QdrantClient`](https://pypi.org/project/qdrant-client/) instance is available via `@property conn` of the `QdrantHook` instance for use within your Airflow workflows.\n\n\n\n```python\n\nfrom qdrant_client import models\n\n\n\nhook.conn.count(""<COLLECTION_NAME>"")\n\n\n\nhook.conn.upsert(\n\n    ""<COLLECTION_NAME>"",\n\n    points=[\n\n        models.PointStruct(id=32, vector=[0.32, 0.12, 0.123], payload={""color"": ""red""})\n\n    ],\n\n)\n\n\n\n```\n\n\n\n## Qdrant Ingest Operator\n\n\n\nThe Qdrant provider also provides a convenience operator for uploading data to a Qdrant collection that internally uses the Qdrant hook.\n\n\n\n```python\n\nfrom airflow.providers.qdrant.operators.qdrant import QdrantIngestOperator\n\n\n\nvectors = [\n\n    [0.11, 0.22, 0.33, 0.44],\n\n    [0.55, 0.66, 0.77, 0.88],\n\n    [0.88, 0.11, 0.12, 0.13],\n\n]\n\nids = [32, 21, ""b626f6a9-b14d-4af9-b7c3-43d8deb719a6""]']","|----------------------------------------------------|----------------------------------------------|---------|------------------|

| [Installation](guides/installation/) | Different ways to install Qdrant. | [Collections](concepts/collections/) | Learn about the central concept behind Qdrant. |                  

| [Configuration](guides/configuration/)   | Update the default configuration.    | [Bulk Upload](tutorials/bulk-upload/) |   Efficiently upload a large number of vectors. |                  

| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  



## Common Use Cases:
The easiest way to use Qdrant is to run a pre-built image.

So make sure you have Docker installed on your system.



To start Qdrant, use the instructions on its [homepage](https://github.com/qdrant/qdrant).



Download image from [DockerHub](https://hub.docker.com/r/qdrant/qdrant):



```bash

docker pull qdrant/qdrant

```



And run the service inside the docker:



```bash

docker run -p 6333:6333 \

    -v $(pwd)/qdrant_storage:/qdrant/storage \

    qdrant/qdrant

```

You should see output like this



```text

...

[2021-02-05T00:08:51Z INFO  actix_server::builder] Starting 12 workers

[2021-02-05T00:08:51Z INFO  actix_server::builder] Starting ""actix-web-service-0.0.0.0:6333"" service on 0.0.0.0:6333

```



This means that the service is successfully launched and listening port 6333.

To make sure you can test [http://localhost:6333/](http://localhost:6333/) in your browser and get qdrant version info.
hook = QdrantHook(conn_id=""qdrant_connection"")



hook.verify_connection()

```



A [`qdrant_client#QdrantClient`](https://pypi.org/project/qdrant-client/) instance is available via `@property conn` of the `QdrantHook` instance for use within your Airflow workflows.



```python

from qdrant_client import models



hook.conn.count(""<COLLECTION_NAME>"")



hook.conn.upsert(

    ""<COLLECTION_NAME>"",

    points=[

        models.PointStruct(id=32, vector=[0.32, 0.12, 0.123], payload={""color"": ""red""})

    ],

)



```



## Qdrant Ingest Operator



The Qdrant provider also provides a convenience operator for uploading data to a Qdrant collection that internally uses the Qdrant hook.



```python

from airflow.providers.qdrant.operators.qdrant import QdrantIngestOperator



vectors = [

    [0.11, 0.22, 0.33, 0.44],

    [0.55, 0.66, 0.77, 0.88],

    [0.88, 0.11, 0.12, 0.13],

]

ids = [32, 21, ""b626f6a9-b14d-4af9-b7c3-43d8deb719a6""]"
What are the recommended ways for Qdrant users to provide feedback or contribute to the project?,"Qdrant encourages feedback and contributions from its users, specifically Data Scientists, ML Engineers, and MLOps professionals. The best way for Qdrant users to contribute is by providing feedback on their experience with Qdrant. This can include reporting problems, unexpected behaviors, or suggesting improvements in documentation. Users can share their feedback through various channels such as creating an issue on the Qdrant GitHub repository, starting a discussion on GitHub, or sending a message on Discord. Additionally, users who use Qdrant or Metric Learning in their projects are encouraged to share their stories, articles, and demos with the Qdrant community. For those familiar with Rust, there is a specific contribution guide available on the Qdrant GitHub repository. If users encounter problems with understanding the code or architecture, they can reach out to the Qdrant team at any time for assistance.","['name=""assistant"",\n\n    system_message=""You are a helpful assistant."",\n\n    llm_config={\n\n        ""request_timeout"": 600,\n\n        ""seed"": 42,\n\n        ""config_list"": config_list,\n\n    },\n\n)\n\n\n\n# 2. create a QdrantRetrieveUserProxyAgent instance named ""qdrantagent""\n\n# By default, the human_input_mode is ""ALWAYS"", i.e. the agent will ask for human input at every step.\n\n# `docs_path` is the path to the docs directory.\n\n# `task` indicates the kind of task we\'re working on.\n\n# `chunk_token_size` is the chunk token size for the retrieve chat.\n\n# We use an in-memory QdrantClient instance here. Not recommended for production.\n\n\n\nragproxyagent = QdrantRetrieveUserProxyAgent(\n\n    name=""qdrantagent"",\n\n    human_input_mode=""NEVER"",\n\n    max_consecutive_auto_reply=10,\n\n    retrieve_config={\n\n        ""task"": ""code"",\n\n        ""docs_path"": ""./path/to/docs"",\n\n        ""chunk_token_size"": 2000,\n\n        ""model"": config_list[0][""model""],\n\n        ""client"": QdrantClient("":memory:""),', ""---\n\ntitle: Contribution Guidelines\n\nweight: 35\n\ndraft: true\n\n---\n\n\n\n# How to contribute\n\n\n\nIf you are a Qdrant user - Data Scientist, ML Engineer, or MLOps, the best contribution would be the feedback on your experience with Qdrant.\n\nLet us know whenever you have a problem, face an unexpected behavior, or see a lack of documentation.\n\nYou can do it in any convenient way - create an [issue](https://github.com/qdrant/qdrant/issues), start a [discussion](https://github.com/qdrant/qdrant/discussions), or drop up a [message](https://discord.gg/tdtYvXjC4h).\n\nIf you use Qdrant or Metric Learning in your projects, we'd love to hear your story! Feel free to share articles and demos in our community.\n\n\n\nFor those familiar with Rust - check out our [contribution guide](https://github.com/qdrant/qdrant/blob/master/CONTRIBUTING.md).\n\nIf you have problems with code or architecture understanding - reach us at any time.\n\nFeeling confident and want to contribute more? - Come to [work with us](https://qdrant.join.com/)!"", '### 1. Prerequisites\n\n\n\nYou certify that you hold all the rights and authority necessary to agree to the T&Cs in the name of the legal person you represent, if applicable.\n\n\n\n\n\n### 2. Description of the Solution\n\n\n\nQdrant is a vector database. It deploys as an API service providing a search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\n\n\n\nQdrant’s guidelines and description of the Solution are detailed in its documentation (the “Documentation”) made available to you and updated regularly.\n\n\n\nYou may subscribe for specific maintenance and support services. The description and prices are disclosed on demand.\n\n\n\nYou can contact us for any questions or inquiries you may have at the following address: contact@qdrant.com.\n\n\n\n\n\n### 3. Set up and installation\n\n\n\nTo install the Solution, you first need to create an account on the Website.']","name=""assistant"",

    system_message=""You are a helpful assistant."",

    llm_config={

        ""request_timeout"": 600,

        ""seed"": 42,

        ""config_list"": config_list,

    },

)



# 2. create a QdrantRetrieveUserProxyAgent instance named ""qdrantagent""

# By default, the human_input_mode is ""ALWAYS"", i.e. the agent will ask for human input at every step.

# `docs_path` is the path to the docs directory.

# `task` indicates the kind of task we're working on.

# `chunk_token_size` is the chunk token size for the retrieve chat.

# We use an in-memory QdrantClient instance here. Not recommended for production.



ragproxyagent = QdrantRetrieveUserProxyAgent(

    name=""qdrantagent"",

    human_input_mode=""NEVER"",

    max_consecutive_auto_reply=10,

    retrieve_config={

        ""task"": ""code"",

        ""docs_path"": ""./path/to/docs"",

        ""chunk_token_size"": 2000,

        ""model"": config_list[0][""model""],

        ""client"": QdrantClient("":memory:""),
---

title: Contribution Guidelines

weight: 35

draft: true

---



# How to contribute



If you are a Qdrant user - Data Scientist, ML Engineer, or MLOps, the best contribution would be the feedback on your experience with Qdrant.

Let us know whenever you have a problem, face an unexpected behavior, or see a lack of documentation.

You can do it in any convenient way - create an [issue](https://github.com/qdrant/qdrant/issues), start a [discussion](https://github.com/qdrant/qdrant/discussions), or drop up a [message](https://discord.gg/tdtYvXjC4h).

If you use Qdrant or Metric Learning in your projects, we'd love to hear your story! Feel free to share articles and demos in our community.



For those familiar with Rust - check out our [contribution guide](https://github.com/qdrant/qdrant/blob/master/CONTRIBUTING.md).

If you have problems with code or architecture understanding - reach us at any time.

Feeling confident and want to contribute more? - Come to [work with us](https://qdrant.join.com/)!
### 1. Prerequisites



You certify that you hold all the rights and authority necessary to agree to the T&Cs in the name of the legal person you represent, if applicable.





### 2. Description of the Solution



Qdrant is a vector database. It deploys as an API service providing a search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!



Qdrant’s guidelines and description of the Solution are detailed in its documentation (the “Documentation”) made available to you and updated regularly.



You may subscribe for specific maintenance and support services. The description and prices are disclosed on demand.



You can contact us for any questions or inquiries you may have at the following address: contact@qdrant.com.





### 3. Set up and installation



To install the Solution, you first need to create an account on the Website."
What is Aleph Alpha and what capabilities does their API offer?,"Aleph Alpha is a multimodal and multilingual embeddings provider. Their API allows for the creation of embeddings for both text and images, placing them in the same latent space. They offer an official Python client that can be installed using pip. The API provides both synchronous and asynchronous clients. Users can obtain embeddings for images and store them into Qdrant by utilizing the provided Python client and following the specified steps in the documentation.","['2. You need to have a registered [Aleph Alpha account](https://app.aleph-alpha.com/). \n\n3. Upon registration, create an API key (see: [API Tokens](https://app.aleph-alpha.com/profile)).\n\n\n\nNow you can store the Aleph Alpha API key in a variable and choose the model your are going to use.\n\n\n\n```python\n\naa_token = ""<< your_token >>""\n\nmodel = ""luminous-base""\n\n```\n\n\n\n## Vectorize the dataset\n\n\n\nIn this example, images have been extracted and are stored in the `val2017` directory:\n\n\n\n```python\n\nfrom aleph_alpha_client import (\n\n    Prompt,\n\n    AsyncClient,\n\n    SemanticEmbeddingRequest,\n\n    SemanticRepresentation,\n\n    Image,\n\n)\n\n\n\nfrom glob import glob\n\n\n\nids, vectors, payloads = [], [], []\n\nasync with AsyncClient(token=aa_token) as client:\n\n    for i, image_path in enumerate(glob(""./val2017/*.jpg"")):\n\n        # Convert the JPEG file into the embedding by calling\n\n        # Aleph Alpha API\n\n        prompt = Image.from_file(image_path)\n\n        prompt = Prompt.from_image(prompt)\n\n        query_params = {', '---\n\ntitle: Aleph Alpha\n\nweight: 900\n\naliases: [ ../integrations/aleph-alpha/ ]\n\n---\n\n\n\nAleph Alpha is a multimodal and multilingual embeddings\' provider. Their API allows creating the embeddings for text and images, both \n\nin the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be \n\ninstalled with pip:\n\n\n\n```bash\n\npip install aleph-alpha-client\n\n```\n\n\n\nThere is both synchronous and asynchronous client available. Obtaining the embeddings for an image and storing it into Qdrant might \n\nbe done in the following way:\n\n\n\n```python\n\nimport qdrant_client\n\n\n\nfrom aleph_alpha_client import (\n\n    Prompt,\n\n    AsyncClient,\n\n    SemanticEmbeddingRequest,\n\n    SemanticRepresentation,\n\n    ImagePrompt\n\n)\n\nfrom qdrant_client.http.models import Batch\n\n\n\naa_token = ""<< your_token >>""\n\nmodel = ""luminous-base""\n\n\n\nqdrant_client = qdrant_client.QdrantClient()\n\nasync with AsyncClient(token=aa_token) as client:', '{\n\n                        ""id"": 1,\n\n                        ""vector"": [1.0, 2.0, 3.0, 4.0],\n\n                        ""payload"": {}\n\n                    }\n\n                ]\n\n            }\n\n        },\n\n        {\n\n            ""update_vectors"": {\n\n                ""points"": [\n\n                    {\n\n                        ""id"": 1,\n\n                        ""vector"": [1.0, 2.0, 3.0, 4.0]\n\n                    }\n\n                ]\n\n            }\n\n        },\n\n        {\n\n            ""delete_vectors"": {\n\n                ""points"": [1],\n\n                ""vector"": [""""]\n\n            }\n\n        },\n\n        {\n\n            ""overwrite_payload"": {\n\n                ""payload"": {\n\n                    ""test_payload"": ""1""\n\n                },\n\n                ""points"": [1]\n\n            }\n\n        },\n\n        {\n\n            ""set_payload"": {\n\n                ""payload"": {\n\n                    ""test_payload_2"": ""2"",\n\n                    ""test_payload_3"": ""3""\n\n                },\n\n                ""points"": [1]']","2. You need to have a registered [Aleph Alpha account](https://app.aleph-alpha.com/). 

3. Upon registration, create an API key (see: [API Tokens](https://app.aleph-alpha.com/profile)).



Now you can store the Aleph Alpha API key in a variable and choose the model your are going to use.



```python

aa_token = ""<< your_token >>""

model = ""luminous-base""

```



## Vectorize the dataset



In this example, images have been extracted and are stored in the `val2017` directory:



```python

from aleph_alpha_client import (

    Prompt,

    AsyncClient,

    SemanticEmbeddingRequest,

    SemanticRepresentation,

    Image,

)



from glob import glob



ids, vectors, payloads = [], [], []

async with AsyncClient(token=aa_token) as client:

    for i, image_path in enumerate(glob(""./val2017/*.jpg"")):

        # Convert the JPEG file into the embedding by calling

        # Aleph Alpha API

        prompt = Image.from_file(image_path)

        prompt = Prompt.from_image(prompt)

        query_params = {
---

title: Aleph Alpha

weight: 900

aliases: [ ../integrations/aleph-alpha/ ]

---



Aleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both 

in the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be 

installed with pip:



```bash

pip install aleph-alpha-client

```



There is both synchronous and asynchronous client available. Obtaining the embeddings for an image and storing it into Qdrant might 

be done in the following way:



```python

import qdrant_client



from aleph_alpha_client import (

    Prompt,

    AsyncClient,

    SemanticEmbeddingRequest,

    SemanticRepresentation,

    ImagePrompt

)

from qdrant_client.http.models import Batch



aa_token = ""<< your_token >>""

model = ""luminous-base""



qdrant_client = qdrant_client.QdrantClient()

async with AsyncClient(token=aa_token) as client:
{

                        ""id"": 1,

                        ""vector"": [1.0, 2.0, 3.0, 4.0],

                        ""payload"": {}

                    }

                ]

            }

        },

        {

            ""update_vectors"": {

                ""points"": [

                    {

                        ""id"": 1,

                        ""vector"": [1.0, 2.0, 3.0, 4.0]

                    }

                ]

            }

        },

        {

            ""delete_vectors"": {

                ""points"": [1],

                ""vector"": [""""]

            }

        },

        {

            ""overwrite_payload"": {

                ""payload"": {

                    ""test_payload"": ""1""

                },

                ""points"": [1]

            }

        },

        {

            ""set_payload"": {

                ""payload"": {

                    ""test_payload_2"": ""2"",

                    ""test_payload_3"": ""3""

                },

                ""points"": [1]"
What is the purpose of the `task_type` parameter when obtaining Nomic embeddings?,"The `task_type` parameter in the `nomic-embed-text-v1` model is used to define the type of embeddings that are obtained for documents. When obtaining Nomic embeddings for documents, you need to set the `task_type` parameter to `search_document`. This ensures that the embeddings generated are specifically tailored for document search purposes. By specifying the `task_type` as `search_document`, you are instructing the model to encode the text in a way that is optimized for searching and retrieving relevant documents based on the input query. This parameter helps customize the embeddings to suit the specific requirements of document retrieval tasks within the Nomic framework.","['---\n\ntitle: Embeddings\n\nweight: 33\n\n# If the index.md file is empty, the link to the section will be hidden from the sidebar\n\nis_empty: true\n\n---\n\n\n\n| Embedding |\n\n|---|\n\n| [Gemini](./gemini/) | \n\n| [Aleph Alpha](./aleph-alpha/) | \n\n| [Cohere](./cohere/) | \n\n| [Jina](./jina-emebddngs/) |\n\n| [OpenAI](./openai/) |', '{\n\n                        ""id"": 1,\n\n                        ""vector"": [1.0, 2.0, 3.0, 4.0],\n\n                        ""payload"": {}\n\n                    }\n\n                ]\n\n            }\n\n        },\n\n        {\n\n            ""update_vectors"": {\n\n                ""points"": [\n\n                    {\n\n                        ""id"": 1,\n\n                        ""vector"": [1.0, 2.0, 3.0, 4.0]\n\n                    }\n\n                ]\n\n            }\n\n        },\n\n        {\n\n            ""delete_vectors"": {\n\n                ""points"": [1],\n\n                ""vector"": [""""]\n\n            }\n\n        },\n\n        {\n\n            ""overwrite_payload"": {\n\n                ""payload"": {\n\n                    ""test_payload"": ""1""\n\n                },\n\n                ""points"": [1]\n\n            }\n\n        },\n\n        {\n\n            ""set_payload"": {\n\n                ""payload"": {\n\n                    ""test_payload_2"": ""2"",\n\n                    ""test_payload_3"": ""3""\n\n                },\n\n                ""points"": [1]', '---\n\ntitle: ""Nomic""\n\nweight: 1100\n\n---\n\n\n\n# Nomic\n\n\n\nThe `nomic-embed-text-v1` model is an open source [8192 context length](https://github.com/nomic-ai/contrastors) text encoder.\n\nWhile you can find it on the [Hugging Face Hub](https://huggingface.co/nomic-ai/nomic-embed-text-v1), \n\nyou may find it easier to obtain them through the [Nomic Text Embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).\n\nOnce installed, you can configure it with the official Python client or through direct HTTP requests.\n\n\n\n<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>\n\n\n\nYou can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings\n\nare obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.\n\nFor documents, set the `task_type` to `search_document`:\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\nfrom nomic import embed\n\n\n\noutput = embed.text(']","---

title: Embeddings

weight: 33

# If the index.md file is empty, the link to the section will be hidden from the sidebar

is_empty: true

---



| Embedding |

|---|

| [Gemini](./gemini/) | 

| [Aleph Alpha](./aleph-alpha/) | 

| [Cohere](./cohere/) | 

| [Jina](./jina-emebddngs/) |

| [OpenAI](./openai/) |
{

                        ""id"": 1,

                        ""vector"": [1.0, 2.0, 3.0, 4.0],

                        ""payload"": {}

                    }

                ]

            }

        },

        {

            ""update_vectors"": {

                ""points"": [

                    {

                        ""id"": 1,

                        ""vector"": [1.0, 2.0, 3.0, 4.0]

                    }

                ]

            }

        },

        {

            ""delete_vectors"": {

                ""points"": [1],

                ""vector"": [""""]

            }

        },

        {

            ""overwrite_payload"": {

                ""payload"": {

                    ""test_payload"": ""1""

                },

                ""points"": [1]

            }

        },

        {

            ""set_payload"": {

                ""payload"": {

                    ""test_payload_2"": ""2"",

                    ""test_payload_3"": ""3""

                },

                ""points"": [1]
---

title: ""Nomic""

weight: 1100

---



# Nomic



The `nomic-embed-text-v1` model is an open source [8192 context length](https://github.com/nomic-ai/contrastors) text encoder.

While you can find it on the [Hugging Face Hub](https://huggingface.co/nomic-ai/nomic-embed-text-v1), 

you may find it easier to obtain them through the [Nomic Text Embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).

Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text("
How can you use the Nomic library to upsert data into a collection and query it using text embeddings?,"To upsert data into a collection and query it using text embeddings with the Nomic library, you first need to use the `embed.text` method with the `task_type` set to `search_document` to generate embeddings for the text data you want to upsert. After obtaining the embeddings, you can use the `QdrantClient` to upsert the data into a specific collection by providing the collection name and the batch of points containing the IDs and corresponding vectors.

To query the collection, you need to set the `task_type` to `search_query` in the `embed.text` method with the query text. Once you have the query embeddings, you can use the `QdrantClient` to search the collection by providing the collection name and the query embedding to retrieve relevant results based on similarity.

For more detailed information and examples on using text , refer to : https://qdrant.tech/documentation/embeddings/nomic/","['```python\n\nfrom qdrant_client import QdrantClient, models\n\nfrom nomic import embed\n\n\n\noutput = embed.text(\n\n    texts=[""Qdrant is the best vector database!""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_document"",\n\n)\n\n\n\nqdrant_client = QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""my-collection"",\n\n    points=models.Batch(\n\n        ids=[1],\n\n        vectors=output[""embeddings""],\n\n    ),\n\n)\n\n```\n\n\n\nTo query the collection, set the `task_type` to `search_query`:\n\n\n\n```python\n\noutput = embed.text(\n\n    texts=[""What is the best vector database?""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_query"",\n\n)\n\n\n\nqdrant_client.search(\n\n    collection_name=""my-collection"",\n\n    query=output[""embeddings""][0],\n\n)\n\n```\n\n\n\nFor more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).', 'If the collection was created with multiple vectors, each vector data can be provided using the vector\'s name:\n\n\n\n```http\n\nPUT /collections/{collection_name}/points\n\n{\n\n    ""points"": [\n\n        {\n\n            ""id"": 1,\n\n            ""vector"": {\n\n                ""image"": [0.9, 0.1, 0.1, 0.2],\n\n                ""text"": [0.4, 0.7, 0.1, 0.8, 0.1, 0.1, 0.9, 0.2]\n\n            }\n\n        },\n\n        {\n\n            ""id"": 2,\n\n            ""vector"": {\n\n                ""image"": [0.2, 0.1, 0.3, 0.9],\n\n                ""text"": [0.5, 0.2, 0.7, 0.4, 0.7, 0.2, 0.3, 0.9]\n\n            }\n\n        }\n\n    ]\n\n}\n\n```\n\n\n\n```python\n\nclient.upsert(\n\n    collection_name=""{collection_name}"",\n\n    points=[\n\n        models.PointStruct(\n\n            id=1,\n\n            vector={\n\n                ""image"": [0.9, 0.1, 0.1, 0.2],\n\n                ""text"": [0.4, 0.7, 0.1, 0.8, 0.1, 0.1, 0.9, 0.2],\n\n            },\n\n        ),\n\n        models.PointStruct(\n\n            id=2,\n\n            vector={\n\n                ""image"": [0.2, 0.1, 0.3, 0.9],', '{\n\n    ""points"": [\n\n        {\n\n            ""id"": 1,\n\n            ""vector"": {\n\n                ""text"": {\n\n                    ""indices"": [6, 7],\n\n                    ""values"": [1.0, 2.0]\n\n                }\n\n            }\n\n        },\n\n        {\n\n            ""id"": 2,\n\n            ""vector"": {\n\n                ""text"": {\n\n                    ""indices"": [1, 1, 2, 3, 4, 5],\n\n                    ""values"": [0.1, 0.2, 0.3, 0.4, 0.5]\n\n                }\n\n            }\n\n        }\n\n    ]\n\n}\n\n```\n\n\n\n```python\n\nclient.upsert(\n\n    collection_name=""{collection_name}"",\n\n    points=[\n\n        models.PointStruct(\n\n            id=1,\n\n            vector={\n\n                ""text"": models.SparseVector(\n\n                    indices=[6, 7],\n\n                    values=[1.0, 2.0],\n\n                )\n\n            },\n\n        ),\n\n        models.PointStruct(\n\n            id=2,\n\n            vector={\n\n                ""text"": models.SparseVector(\n\n                    indices=[1, 2, 3, 4, 5],']","```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

    texts=[""Qdrant is the best vector database!""],

    model=""nomic-embed-text-v1"",

    task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

    collection_name=""my-collection"",

    points=models.Batch(

        ids=[1],

        vectors=output[""embeddings""],

    ),

)

```



To query the collection, set the `task_type` to `search_query`:



```python

output = embed.text(

    texts=[""What is the best vector database?""],

    model=""nomic-embed-text-v1"",

    task_type=""search_query"",

)



qdrant_client.search(

    collection_name=""my-collection"",

    query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
If the collection was created with multiple vectors, each vector data can be provided using the vector's name:



```http

PUT /collections/{collection_name}/points

{

    ""points"": [

        {

            ""id"": 1,

            ""vector"": {

                ""image"": [0.9, 0.1, 0.1, 0.2],

                ""text"": [0.4, 0.7, 0.1, 0.8, 0.1, 0.1, 0.9, 0.2]

            }

        },

        {

            ""id"": 2,

            ""vector"": {

                ""image"": [0.2, 0.1, 0.3, 0.9],

                ""text"": [0.5, 0.2, 0.7, 0.4, 0.7, 0.2, 0.3, 0.9]

            }

        }

    ]

}

```



```python

client.upsert(

    collection_name=""{collection_name}"",

    points=[

        models.PointStruct(

            id=1,

            vector={

                ""image"": [0.9, 0.1, 0.1, 0.2],

                ""text"": [0.4, 0.7, 0.1, 0.8, 0.1, 0.1, 0.9, 0.2],

            },

        ),

        models.PointStruct(

            id=2,

            vector={

                ""image"": [0.2, 0.1, 0.3, 0.9],
{

    ""points"": [

        {

            ""id"": 1,

            ""vector"": {

                ""text"": {

                    ""indices"": [6, 7],

                    ""values"": [1.0, 2.0]

                }

            }

        },

        {

            ""id"": 2,

            ""vector"": {

                ""text"": {

                    ""indices"": [1, 1, 2, 3, 4, 5],

                    ""values"": [0.1, 0.2, 0.3, 0.4, 0.5]

                }

            }

        }

    ]

}

```



```python

client.upsert(

    collection_name=""{collection_name}"",

    points=[

        models.PointStruct(

            id=1,

            vector={

                ""text"": models.SparseVector(

                    indices=[6, 7],

                    values=[1.0, 2.0],

                )

            },

        ),

        models.PointStruct(

            id=2,

            vector={

                ""text"": models.SparseVector(

                    indices=[1, 2, 3, 4, 5],"
What is the purpose of the `task_type` parameter in the Gemini Embedding Model API?,"The `task_type` parameter in the Gemini Embedding Model API serves to designate the intended purpose for the embeddings utilized. It allows users to specify the type of task they want to perform with the given text. The supported task types include `retrieval_query` for search/retrieval queries, `retrieval_document` for documents from the corpus being searched, `semantic_similarity` for Semantic Text Similarity, and `classification` for text classification. This parameter helps in customizing the functionality of the Gemini Embedding Model API based on the specific task requirements.","['---\n\ntitle: Gemini\n\nweight: 700\n\n---\n\n\n\n# Gemini\n\n\n\nQdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:\n\n\n\nGemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. \n\n\n\nIn the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.\n\n\n\nThe Embedding Model API supports various task types, outlined as follows:\n\n\n\n1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.\n\n2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.\n\n3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.\n\n4. `classification`: Specifies that the given text will be classified.\n\n5. `clustering`: Specifies that the embeddings will be used for clustering.', '---\n\ntitle: Embeddings\n\nweight: 33\n\n# If the index.md file is empty, the link to the section will be hidden from the sidebar\n\nis_empty: true\n\n---\n\n\n\n| Embedding |\n\n|---|\n\n| [Gemini](./gemini/) | \n\n| [Aleph Alpha](./aleph-alpha/) | \n\n| [Cohere](./cohere/) | \n\n| [Jina](./jina-emebddngs/) |\n\n| [OpenAI](./openai/) |', 'genai.configure(api_key=GEMINI_API_KEY)\n\n\n\nresult = genai.embed_content(\n\n    model=""models/embedding-001"",\n\n    content=""Qdrant is the best vector search engine to use with Gemini"",\n\n    task_type=""retrieval_document"",\n\n    title=""Qdrant x Gemini"",\n\n)\n\n```\n\n\n\nThe returned result is a dictionary with a key: `embedding`. The value of this key is a list of floats representing the embedding of the document.\n\n\n\n## Indexing documents with Qdrant\n\n\n\n```python\n\nfrom qdrant_client.http.models import Batch\n\n\n\nqdrant_client = qdrant_client.QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""GeminiCollection"",\n\n    points=Batch(\n\n        ids=[1],\n\n        vectors=genai.embed_content(\n\n            model=""models/embedding-001"",\n\n            content=""Qdrant is the best vector search engine to use with Gemini"",\n\n            task_type=""retrieval_document"",\n\n            title=""Qdrant x Gemini"",\n\n        )[""embedding""],\n\n    ),\n\n)\n\n```\n\n\n\n## Searching for documents with Qdrant']","---

title: Gemini

weight: 700

---



# Gemini



Qdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:



Gemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. 



In the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.



The Embedding Model API supports various task types, outlined as follows:



1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.

2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.

3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.

4. `classification`: Specifies that the given text will be classified.

5. `clustering`: Specifies that the embeddings will be used for clustering.
---

title: Embeddings

weight: 33

# If the index.md file is empty, the link to the section will be hidden from the sidebar

is_empty: true

---



| Embedding |

|---|

| [Gemini](./gemini/) | 

| [Aleph Alpha](./aleph-alpha/) | 

| [Cohere](./cohere/) | 

| [Jina](./jina-emebddngs/) |

| [OpenAI](./openai/) |
genai.configure(api_key=GEMINI_API_KEY)



result = genai.embed_content(

    model=""models/embedding-001"",

    content=""Qdrant is the best vector search engine to use with Gemini"",

    task_type=""retrieval_document"",

    title=""Qdrant x Gemini"",

)

```



The returned result is a dictionary with a key: `embedding`. The value of this key is a list of floats representing the embedding of the document.



## Indexing documents with Qdrant



```python

from qdrant_client.http.models import Batch



qdrant_client = qdrant_client.QdrantClient()

qdrant_client.upsert(

    collection_name=""GeminiCollection"",

    points=Batch(

        ids=[1],

        vectors=genai.embed_content(

            model=""models/embedding-001"",

            content=""Qdrant is the best vector search engine to use with Gemini"",

            task_type=""retrieval_document"",

            title=""Qdrant x Gemini"",

        )[""embedding""],

    ),

)

```



## Searching for documents with Qdrant"
What is the maximum number of tokens that Jina embeddings allow for model input lengths?,"Jina embeddings allow for model input lengths of up to 8192 tokens. This means that the models utilizing Jina embeddings can process sequences with a maximum length of 8192 tokens, providing flexibility and capability for handling large amounts of text data efficiently.","['---\n\ntitle: Jina Embeddings\n\nweight: 800\n\naliases: [ ../integrations/jina-embeddings/ ]\n\n---\n\n\n\n# Jina Embeddings\n\n\n\nQdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.\n\n\n\nTo call their endpoint, all you need is an API key obtainable [here](https://jina.ai/embeddings/). By the way, our friends from **Jina AI** provided us with a code (**QDRANT**) that will grant you a **10% discount** if you plan to use Jina Embeddings in production.\n\n\n\n```python\n\nimport qdrant_client\n\nimport requests\n\n\n\nfrom qdrant_client.http.models import Distance, VectorParams\n\nfrom qdrant_client.http.models import Batch\n\n\n\n# Provide Jina API key and choose one of the available models.\n\n# You can get a free trial key here: https://jina.ai/embeddings/\n\nJINA_API_KEY = ""jina_xxxxxxxxxxx""\n\nMODEL = ""jina-embeddings-v2-base-en""  # or ""jina-embeddings-v2-base-en""\n\nEMBEDDING_SIZE = 768  # 512 for small variant\n\n\n\n# Get embeddings from the API', 'EMBEDDING_SIZE = 768  # 512 for small variant\n\n\n\n# Get embeddings from the API\n\nurl = ""https://api.jina.ai/v1/embeddings""\n\n\n\nheaders = {\n\n    ""Content-Type"": ""application/json"",\n\n    ""Authorization"": f""Bearer {JINA_API_KEY}"",\n\n}\n\n\n\ndata = {\n\n    ""input"": [""Your text string goes here"", ""You can send multiple texts""],\n\n    ""model"": MODEL,\n\n}\n\n\n\nresponse = requests.post(url, headers=headers, json=data)\n\nembeddings = [d[""embedding""] for d in response.json()[""data""]]\n\n\n\n\n\n# Index the embeddings into Qdrant\n\nqdrant_client = qdrant_client.QdrantClient("":memory:"")\n\nqdrant_client.create_collection(\n\n    collection_name=""MyCollection"",\n\n    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),\n\n)\n\n\n\n\n\nqdrant_client.upsert(\n\n    collection_name=""MyCollection"",\n\n    points=Batch(\n\n        ids=list(range(len(embeddings))),\n\n        vectors=embeddings,\n\n    ),\n\n)\n\n\n\n```', ""```python\n\nembedding_model =\xa0DefaultEmbedding()\n\n```\n\n\n\nThe default model and several other models have a context window of maximum 512 tokens. This maximum limit comes from the embedding model training and design itself.If you'd like to embed sequences larger than that, we'd recommend using some pooling strategy to get a single vector out of the sequence. For example, you can use the mean of the embeddings of different chunks of a document. This is also what the [SBERT Paper recommends](https://lilianweng.github.io/posts/2021-05-31-contrastive/#sentence-bert)\n\n\n\nThis model strikes a balance between speed and accuracy, ideal for real-world applications.\n\n\n\n```python\n\nembeddings: List[np.ndarray] =\xa0list(embedding_model.embed(documents))\n\n```""]","---

title: Jina Embeddings

weight: 800

aliases: [ ../integrations/jina-embeddings/ ]

---



# Jina Embeddings



Qdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.



To call their endpoint, all you need is an API key obtainable [here](https://jina.ai/embeddings/). By the way, our friends from **Jina AI** provided us with a code (**QDRANT**) that will grant you a **10% discount** if you plan to use Jina Embeddings in production.



```python

import qdrant_client

import requests



from qdrant_client.http.models import Distance, VectorParams

from qdrant_client.http.models import Batch



# Provide Jina API key and choose one of the available models.

# You can get a free trial key here: https://jina.ai/embeddings/

JINA_API_KEY = ""jina_xxxxxxxxxxx""

MODEL = ""jina-embeddings-v2-base-en""  # or ""jina-embeddings-v2-base-en""

EMBEDDING_SIZE = 768  # 512 for small variant



# Get embeddings from the API
EMBEDDING_SIZE = 768  # 512 for small variant



# Get embeddings from the API

url = ""https://api.jina.ai/v1/embeddings""



headers = {

    ""Content-Type"": ""application/json"",

    ""Authorization"": f""Bearer {JINA_API_KEY}"",

}



data = {

    ""input"": [""Your text string goes here"", ""You can send multiple texts""],

    ""model"": MODEL,

}



response = requests.post(url, headers=headers, json=data)

embeddings = [d[""embedding""] for d in response.json()[""data""]]





# Index the embeddings into Qdrant

qdrant_client = qdrant_client.QdrantClient("":memory:"")

qdrant_client.create_collection(

    collection_name=""MyCollection"",

    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),

)





qdrant_client.upsert(

    collection_name=""MyCollection"",

    points=Batch(

        ids=list(range(len(embeddings))),

        vectors=embeddings,

    ),

)



```
```python

embedding_model = DefaultEmbedding()

```



The default model and several other models have a context window of maximum 512 tokens. This maximum limit comes from the embedding model training and design itself.If you'd like to embed sequences larger than that, we'd recommend using some pooling strategy to get a single vector out of the sequence. For example, you can use the mean of the embeddings of different chunks of a document. This is also what the [SBERT Paper recommends](https://lilianweng.github.io/posts/2021-05-31-contrastive/#sentence-bert)



This model strikes a balance between speed and accuracy, ideal for real-world applications.



```python

embeddings: List[np.ndarray] = list(embedding_model.embed(documents))

```"
Why does Qdrant by default not return vectors in search results?,"By default, Qdrant tries to minimize network traffic and doesn't return vectors in search results. This default behavior is in place to optimize performance and reduce unnecessary data transfer. However, if you specifically require the vectors to be included in the search results, you can override this default setting by setting the `with_vector` parameter of the Search/Scroll to `true`. This allows you to retrieve the vectors along with other search results, providing you with the option to access the vector data when needed.","['{ ""id"": 11, ""score"": 0.73 }\n\n    ],\n\n    [\n\n        { ""id"": 1, ""score"": 0.92 },\n\n        { ""id"": 3, ""score"": 0.89 },\n\n        { ""id"": 9, ""score"": 0.75 }\n\n    ]\n\n  ],\n\n  ""status"": ""ok"",\n\n  ""time"": 0.001\n\n}\n\n```\n\n\n\n## Pagination\n\n\n\n*Available as of v0.8.3*\n\n\n\nSearch and [recommendation](../explore/#recommendation-api) APIs allow to skip first results of the search and return only the result starting from some specified offset:\n\n\n\nExample:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{\n\n    ""vector"": [0.2, 0.1, 0.9, 0.7],\n\n    ""with_vectors"": true,\n\n    ""with_payload"": true,\n\n    ""limit"": 10,\n\n    ""offset"": 100\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.search(\n\n    collection_name=""{collection_name}"",\n\n    query_vector=[0.2, 0.1, 0.9, 0.7],\n\n    with_vectors=True,\n\n    with_payload=True,\n\n    limit=10,\n\n    offset=100,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', 'let client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .search_points(&SearchPoints {\n\n        collection_name: ""{collection_name}"".to_string(),\n\n        vector: vec![0.2, 0.1, 0.9, 0.7],\n\n        with_payload: Some(vec![""city"", ""village"", ""town""].into()),\n\n        limit: 3,\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport java.util.List;\n\n\n\nimport static io.qdrant.client.WithPayloadSelectorFactory.include;\n\n\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Points.SearchPoints;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .searchAsync(\n\n        SearchPoints.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .addAllVector(List.of(0.2f, 0.1f, 0.9f, 0.7f))\n\n            .setWithPayload(include(List.of(""city"", ""village"", ""town"")))\n\n            .setLimit(3)\n\n            .build())', 'Example result of this API would be\n\n\n\n```json\n\n{\n\n  ""result"": [\n\n    { ""id"": 10, ""score"": 0.81 },\n\n    { ""id"": 14, ""score"": 0.75 },\n\n    { ""id"": 11, ""score"": 0.73 }\n\n  ],\n\n  ""status"": ""ok"",\n\n  ""time"": 0.001\n\n}\n\n```\n\n\n\nThe `result` contains ordered by `score` list of found point ids.\n\n\n\nNote that payload and vector data is missing in these results by default.\n\nSee [payload and vector in the result](#payload-and-vector-in-the-result) on how\n\nto include it.\n\n\n\n*Available as of v0.10.0*\n\n\n\nIf the collection was created with multiple vectors, the name of the vector to use for searching should be provided:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{\n\n    ""vector"": {\n\n        ""name"": ""image"",\n\n        ""vector"": [0.2, 0.1, 0.9, 0.7]\n\n    },\n\n    ""limit"": 3\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.search(\n\n    collection_name=""{collection_name}"",']","{ ""id"": 11, ""score"": 0.73 }

    ],

    [

        { ""id"": 1, ""score"": 0.92 },

        { ""id"": 3, ""score"": 0.89 },

        { ""id"": 9, ""score"": 0.75 }

    ]

  ],

  ""status"": ""ok"",

  ""time"": 0.001

}

```



## Pagination



*Available as of v0.8.3*



Search and [recommendation](../explore/#recommendation-api) APIs allow to skip first results of the search and return only the result starting from some specified offset:



Example:



```http

POST /collections/{collection_name}/points/search

{

    ""vector"": [0.2, 0.1, 0.9, 0.7],

    ""with_vectors"": true,

    ""with_payload"": true,

    ""limit"": 10,

    ""offset"": 100

}

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.search(

    collection_name=""{collection_name}"",

    query_vector=[0.2, 0.1, 0.9, 0.7],

    with_vectors=True,

    with_payload=True,

    limit=10,

    offset=100,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .search_points(&SearchPoints {

        collection_name: ""{collection_name}"".to_string(),

        vector: vec![0.2, 0.1, 0.9, 0.7],

        with_payload: Some(vec![""city"", ""village"", ""town""].into()),

        limit: 3,

        ..Default::default()

    })

    .await?;

```



```java

import java.util.List;



import static io.qdrant.client.WithPayloadSelectorFactory.include;



import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Points.SearchPoints;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .searchAsync(

        SearchPoints.newBuilder()

            .setCollectionName(""{collection_name}"")

            .addAllVector(List.of(0.2f, 0.1f, 0.9f, 0.7f))

            .setWithPayload(include(List.of(""city"", ""village"", ""town"")))

            .setLimit(3)

            .build())
Example result of this API would be



```json

{

  ""result"": [

    { ""id"": 10, ""score"": 0.81 },

    { ""id"": 14, ""score"": 0.75 },

    { ""id"": 11, ""score"": 0.73 }

  ],

  ""status"": ""ok"",

  ""time"": 0.001

}

```



The `result` contains ordered by `score` list of found point ids.



Note that payload and vector data is missing in these results by default.

See [payload and vector in the result](#payload-and-vector-in-the-result) on how

to include it.



*Available as of v0.10.0*



If the collection was created with multiple vectors, the name of the vector to use for searching should be provided:



```http

POST /collections/{collection_name}/points/search

{

    ""vector"": {

        ""name"": ""image"",

        ""vector"": [0.2, 0.1, 0.9, 0.7]

    },

    ""limit"": 3

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.search(

    collection_name=""{collection_name}"","
What does Qdrant not plan to support in the future?,"In the future, Qdrant does not plan to support BM25 or other non-vector-based retrieval or ranking functions, built-in ontologies or knowledge graphs, and query analyzers and other NLP tools. Qdrant aims to introduce support for sparse vectors, as used in SPLADE or similar models, but does not intend to incorporate the aforementioned features into its functionality. Users can combine Qdrant with specialized tools, including full-text search engines, to complement its capabilities.","[""You acknowledge and accept that Qdrant cannot be held responsible for the Content.\n\n\n\n### 9. Accessibility of the Solution\n\n\n\nQdrant undertakes to supply the Solution with diligence, and according to best practice, it is specified that it has an obligation of means to the exclusion of any obligation of result, which you expressly acknowledge and accept.\n\n\n\nQdrant will do its best to ensure that the Solution is accessible at all times, with the exception of cases of unavailability or maintenance.\n\n\n\nYou acknowledge that you are informed that the unavailability of the Solution may be the result of (a) a maintenance operation, (b) an urgent operation relating in particular to security, (c) a case of “force majeure” or (d) the malfunctioning of computer applications of Qdrant's third-party partners. Qdrant undertakes to restore the availability of the Solution as soon as possible once the problem causing the unavailability has been resolved."", '* your use of the Solution in a manner that does not comply with its purpose or its Documentation;\n\n* unauthorized access to the Solution by a third-party caused by you, including through your negligence;\n\n* your failure to fulfill your obligations under the T&Cs;\n\n* implementation of any software package, software or operating system not compatible with the Solution;\n\n* failure of the electronic communication networks which is not the fault of Qdrant;\n\n* your refusal to collaborate with Qdrant in the resolution of the anomalies and in particular to answer questions and requests for information;\n\n* voluntary act of degradation, malice, sabotage;\n\n* deterioration due to a case of “force majeure”.\n\n\n\nYou will benefit from the updates, and functional evolutions of the Solution decided by Qdrant and accept them from now on.\n\n\n\nYou cannot claim any indemnity or hold Qdrant responsible for any of the reasons mentioned above.\n\n\n\n\n\n### 10. Violations – Sanctions', ""But obviously some stuff that are on our roadmap.\n\nAnd another thing that we don't support, which is one type of action would.\n\nBe the first we will be working on is obviously code interpretation, which is I think is one of the things that all users ask because they use.\n\nIt on Chat GPT.\n\nAnd so we'll be looking into that as well.\n\n\n\n\n\nDemetrios:\n\nWhat made you choose Qdrant?\n\n\n\n\n\nStanislas Polu:\n\nSo the decision was made, if I.\n\nRemember correctly, something like February or March last year. And so the alternatives I looked into.\n\nWere pine cone wavy eight, some click owls because Chroma was using click owls at the time. But Chroma was.\n\n2000 lines of code.\n\nAt the time as well.\n\nAnd so I was like, oh, Chroma, we're part of AI grant. And Chroma is as an example also part of AI grant. So I was like, oh well, let's look at Chroma.\n\nAnd however, what I'm describing is last.\n\nYear, but they were very early. And so it was definitely not something.\n\nThat seemed like to make sense for us.""]","You acknowledge and accept that Qdrant cannot be held responsible for the Content.



### 9. Accessibility of the Solution



Qdrant undertakes to supply the Solution with diligence, and according to best practice, it is specified that it has an obligation of means to the exclusion of any obligation of result, which you expressly acknowledge and accept.



Qdrant will do its best to ensure that the Solution is accessible at all times, with the exception of cases of unavailability or maintenance.



You acknowledge that you are informed that the unavailability of the Solution may be the result of (a) a maintenance operation, (b) an urgent operation relating in particular to security, (c) a case of “force majeure” or (d) the malfunctioning of computer applications of Qdrant's third-party partners. Qdrant undertakes to restore the availability of the Solution as soon as possible once the problem causing the unavailability has been resolved.
* your use of the Solution in a manner that does not comply with its purpose or its Documentation;

* unauthorized access to the Solution by a third-party caused by you, including through your negligence;

* your failure to fulfill your obligations under the T&Cs;

* implementation of any software package, software or operating system not compatible with the Solution;

* failure of the electronic communication networks which is not the fault of Qdrant;

* your refusal to collaborate with Qdrant in the resolution of the anomalies and in particular to answer questions and requests for information;

* voluntary act of degradation, malice, sabotage;

* deterioration due to a case of “force majeure”.



You will benefit from the updates, and functional evolutions of the Solution decided by Qdrant and accept them from now on.



You cannot claim any indemnity or hold Qdrant responsible for any of the reasons mentioned above.





### 10. Violations – Sanctions
But obviously some stuff that are on our roadmap.

And another thing that we don't support, which is one type of action would.

Be the first we will be working on is obviously code interpretation, which is I think is one of the things that all users ask because they use.

It on Chat GPT.

And so we'll be looking into that as well.





Demetrios:

What made you choose Qdrant?





Stanislas Polu:

So the decision was made, if I.

Remember correctly, something like February or March last year. And so the alternatives I looked into.

Were pine cone wavy eight, some click owls because Chroma was using click owls at the time. But Chroma was.

2000 lines of code.

At the time as well.

And so I was like, oh, Chroma, we're part of AI grant. And Chroma is as an example also part of AI grant. So I was like, oh well, let's look at Chroma.

And however, what I'm describing is last.

Year, but they were very early. And so it was definitely not something.

That seemed like to make sense for us."
 What is the recommended approach for configuring multitenancy in terms of the number of collections to create?,"In most cases, the recommended approach for configuring multitenancy is to create only a single collection with payload-based partitioning. This approach, known as multitenancy, is efficient for the majority of users. By using a single collection with payload-based partitioning, each user can access their own vectors while ensuring that they cannot see the vectors of other users. However, it is important to note that this approach requires additional configuration to set up properly.","[""---\n\ntitle: Multitenancy\n\nweight: 12\n\naliases:\n\n  - ../tutorials/multiple-partitions\n\n---\n\n# Configure Multitenancy\n\n\n\n**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.\n\n\n\n**When should you create multiple collections?** When you have a limited number of users and you need isolation. This approach is flexible, but it may be more costly, since creating numerous collections may result in resource overhead. Also, you need to ensure that they do not affect each other in any way, including performance-wise. \n\n\n\n## Partition by payload\n\n\n\nWhen an instance is shared between multiple users, you may need to partition vectors by user. This is done so that each user can only access their own vectors and can't see the vectors of other users."", '![Qdrant Multitenancy](/articles_data/multitenancy/multitenancy.png)\n\n\n\n## Create custom shards for a single collection\n\n\n\nWhen creating a collection, you will need to configure user-defined sharding. This lets you control the shard placement of your data, so that operations can hit only the subset of shards they actually need. In big clusters, this can significantly improve the performance of operations, since you won\'t need to go through the entire collection to retrieve data.\n\n\n\n```python\n\nclient.create_collection(\n\n    collection_name=""{tenant_data}"",\n\n    shard_number=2,\n\n    sharding_method=models.ShardingMethod.CUSTOM,\n\n    # ... other collection parameters\n\n)\n\nclient.create_shard_key(""{tenant_data}"", ""canada"")\n\nclient.create_shard_key(""{tenant_data}"", ""germany"")\n\n```', ""---\n\ntitle: Multitenancy with LlamaIndex\n\nweight: 18\n\n---\n\n\n\n# Multitenancy with LlamaIndex\n\n\n\nIf you are building a service that serves vectors for many independent users, and you want to isolate their\n\ndata, the best practice is to use a single collection with payload-based partitioning. This approach is \n\ncalled **multitenancy**. Our guide on the [Separate Partitions](/documentation/guides/multiple-partitions/) describes \n\nhow to set it up in general, but if you use [LlamaIndex](/documentation/integrations/llama-index/) as a \n\nbackend, you may prefer reading a more specific instruction. So here it is!\n\n\n\n## Prerequisites\n\n\n\nThis tutorial assumes that you have already installed Qdrant and LlamaIndex. If you haven't, please run the \n\nfollowing commands:\n\n\n\n```bash\n\npip install qdrant-client llama-index\n\n```\n\n\n\nWe are going to use a local Docker-based instance of Qdrant. If you want to use a remote instance, please\n\nadjust the code accordingly. Here is how we can start a local instance:\n\n\n\n```bash""]","---

title: Multitenancy

weight: 12

aliases:

  - ../tutorials/multiple-partitions

---

# Configure Multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.



**When should you create multiple collections?** When you have a limited number of users and you need isolation. This approach is flexible, but it may be more costly, since creating numerous collections may result in resource overhead. Also, you need to ensure that they do not affect each other in any way, including performance-wise. 



## Partition by payload



When an instance is shared between multiple users, you may need to partition vectors by user. This is done so that each user can only access their own vectors and can't see the vectors of other users.
![Qdrant Multitenancy](/articles_data/multitenancy/multitenancy.png)



## Create custom shards for a single collection



When creating a collection, you will need to configure user-defined sharding. This lets you control the shard placement of your data, so that operations can hit only the subset of shards they actually need. In big clusters, this can significantly improve the performance of operations, since you won't need to go through the entire collection to retrieve data.



```python

client.create_collection(

    collection_name=""{tenant_data}"",

    shard_number=2,

    sharding_method=models.ShardingMethod.CUSTOM,

    # ... other collection parameters

)

client.create_shard_key(""{tenant_data}"", ""canada"")

client.create_shard_key(""{tenant_data}"", ""germany"")

```
---

title: Multitenancy with LlamaIndex

weight: 18

---



# Multitenancy with LlamaIndex



If you are building a service that serves vectors for many independent users, and you want to isolate their

data, the best practice is to use a single collection with payload-based partitioning. This approach is 

called **multitenancy**. Our guide on the [Separate Partitions](/documentation/guides/multiple-partitions/) describes 

how to set it up in general, but if you use [LlamaIndex](/documentation/integrations/llama-index/) as a 

backend, you may prefer reading a more specific instruction. So here it is!



## Prerequisites



This tutorial assumes that you have already installed Qdrant and LlamaIndex. If you haven't, please run the 

following commands:



```bash

pip install qdrant-client llama-index

```



We are going to use a local Docker-based instance of Qdrant. If you want to use a remote instance, please

adjust the code accordingly. Here is how we can start a local instance:



```bash"
What is the purpose of the locking feature in Qdrant administration tools?,"The locking feature in Qdrant administration tools serves the purpose of restricting the possible operations on a Qdrant process. It allows users to control the behavior of a Qdrant instance at runtime without manually changing its configuration. When a lock is applied, specific operations such as creating new collections or adding new data to the existing storage can be disabled. It is important to note that the locking configuration is not persistent, so it needs to be reapplied after a restart. Additionally, the locking feature applies to a single node only, requiring the lock to be set on all desired nodes in a distributed deployment setup. The locking API in Qdrant enables users to manage and control the access and operations on the Qdrant instance effectively.","['---\n\ntitle: Administration\n\nweight: 10\n\naliases:\n\n  - ../administration\n\n---\n\n\n\n# Administration\n\n\n\nQdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.\n\n\n\n## Locking\n\n\n\nA locking API enables users to restrict the possible operations on a qdrant process.\n\nIt is important to mention that:\n\n- The configuration is not persistent therefore it is necessary to lock again following a restart.\n\n- Locking applies to a single node only. It is necessary to call lock on all the desired nodes in a distributed deployment setup.\n\n\n\nLock request sample:\n\n\n\n```http\n\nPOST /locks\n\n{\n\n    ""error_message"": ""write is forbidden"",\n\n    ""write"": true\n\n}\n\n```\n\n\n\nWrite flags enables/disables write lock.\n\nIf the write lock is set to true, qdrant doesn\'t allow creating new collections or adding new data to the existing storage.\n\nHowever, deletion operations or updates are not forbidden under the write lock.', 'However, deletion operations or updates are not forbidden under the write lock.\n\nThis feature enables administrators to prevent a qdrant process from using more disk space while permitting users to search and delete unnecessary data.\n\n\n\nYou can optionally provide the error message that should be used for error responses to users.\n\n\n\n## Recovery mode\n\n\n\n*Available as of v1.2.0*\n\n\n\nRecovery mode can help in situations where Qdrant fails to start repeatedly.\n\nWhen starting in recovery mode, Qdrant only loads collection metadata to prevent\n\ngoing out of memory. This allows you to resolve out of memory situations, for\n\nexample, by deleting a collection. After resolving Qdrant can be restarted\n\nnormally to continue operation.\n\n\n\nIn recovery mode, collection operations are limited to\n\n[deleting](../../concepts/collections/#delete-collection) a\n\ncollection. That is because only collection metadata is loaded during recovery.\n\n\n\nTo enable recovery mode with the Qdrant Docker image you must set the', ""|----------------------------------------------------|----------------------------------------------|---------|------------------|\n\n| [Installation](guides/installation/) | Different ways to install Qdrant. | [Collections](concepts/collections/) | Learn about the central concept behind Qdrant. |                  \n\n| [Configuration](guides/configuration/)   | Update the default configuration.    | [Bulk Upload](tutorials/bulk-upload/) |   Efficiently upload a large number of vectors. |                  \n\n| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  \n\n\n\n## Common Use Cases:""]","---

title: Administration

weight: 10

aliases:

  - ../administration

---



# Administration



Qdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.



## Locking



A locking API enables users to restrict the possible operations on a qdrant process.

It is important to mention that:

- The configuration is not persistent therefore it is necessary to lock again following a restart.

- Locking applies to a single node only. It is necessary to call lock on all the desired nodes in a distributed deployment setup.



Lock request sample:



```http

POST /locks

{

    ""error_message"": ""write is forbidden"",

    ""write"": true

}

```



Write flags enables/disables write lock.

If the write lock is set to true, qdrant doesn't allow creating new collections or adding new data to the existing storage.

However, deletion operations or updates are not forbidden under the write lock.
However, deletion operations or updates are not forbidden under the write lock.

This feature enables administrators to prevent a qdrant process from using more disk space while permitting users to search and delete unnecessary data.



You can optionally provide the error message that should be used for error responses to users.



## Recovery mode



*Available as of v1.2.0*



Recovery mode can help in situations where Qdrant fails to start repeatedly.

When starting in recovery mode, Qdrant only loads collection metadata to prevent

going out of memory. This allows you to resolve out of memory situations, for

example, by deleting a collection. After resolving Qdrant can be restarted

normally to continue operation.



In recovery mode, collection operations are limited to

[deleting](../../concepts/collections/#delete-collection) a

collection. That is because only collection metadata is loaded during recovery.



To enable recovery mode with the Qdrant Docker image you must set the
|----------------------------------------------------|----------------------------------------------|---------|------------------|

| [Installation](guides/installation/) | Different ways to install Qdrant. | [Collections](concepts/collections/) | Learn about the central concept behind Qdrant. |                  

| [Configuration](guides/configuration/)   | Update the default configuration.    | [Bulk Upload](tutorials/bulk-upload/) |   Efficiently upload a large number of vectors. |                  

| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  



## Common Use Cases:"
What is the significance of the `on_disk_payload` setting?,"The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes.","['{\n\n                        ""id"": 1,\n\n                        ""vector"": [1.0, 2.0, 3.0, 4.0],\n\n                        ""payload"": {}\n\n                    }\n\n                ]\n\n            }\n\n        },\n\n        {\n\n            ""update_vectors"": {\n\n                ""points"": [\n\n                    {\n\n                        ""id"": 1,\n\n                        ""vector"": [1.0, 2.0, 3.0, 4.0]\n\n                    }\n\n                ]\n\n            }\n\n        },\n\n        {\n\n            ""delete_vectors"": {\n\n                ""points"": [1],\n\n                ""vector"": [""""]\n\n            }\n\n        },\n\n        {\n\n            ""overwrite_payload"": {\n\n                ""payload"": {\n\n                    ""test_payload"": ""1""\n\n                },\n\n                ""points"": [1]\n\n            }\n\n        },\n\n        {\n\n            ""set_payload"": {\n\n                ""payload"": {\n\n                    ""test_payload_2"": ""2"",\n\n                    ""test_payload_3"": ""3""\n\n                },\n\n                ""points"": [1]', '""indexed_vectors_count"": 1024232,\n\n        ""points_count"": 1068786,\n\n        ""segments_count"": 31,\n\n        ""config"": {\n\n            ""params"": {\n\n                ""vectors"": {\n\n                    ""size"": 384,\n\n                    ""distance"": ""Cosine""\n\n                },\n\n                ""shard_number"": 1,\n\n                ""replication_factor"": 1,\n\n                ""write_consistency_factor"": 1,\n\n                ""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,\n\n                ""memmap_threshold"": null,\n\n                ""indexing_threshold"": 20000,\n\n                ""flush_interval_sec"": 5,', 'You can specify the desired type of payload storage with [configuration file](../../guides/configuration/) or with collection parameter `on_disk_payload` during [creation](../collections/#create-collection) of the collection.\n\n\n\n## Versioning\n\n\n\nTo ensure data integrity, Qdrant performs all data changes in 2 stages.\n\nIn the first step, the data is written to the Write-ahead-log(WAL), which orders all operations and assigns them a sequential number.\n\n\n\nOnce a change has been added to the WAL, it will not be lost even if a power loss occurs.\n\nThen the changes go into the segments.\n\nEach segment stores the last version of the change applied to it as well as the version of each individual point.\n\nIf the new change has a sequential number less than the current version of the point, the updater will ignore the change.\n\nThis mechanism allows Qdrant to safely and efficiently restore the storage from the WAL in case of an abnormal shutdown.']","{

                        ""id"": 1,

                        ""vector"": [1.0, 2.0, 3.0, 4.0],

                        ""payload"": {}

                    }

                ]

            }

        },

        {

            ""update_vectors"": {

                ""points"": [

                    {

                        ""id"": 1,

                        ""vector"": [1.0, 2.0, 3.0, 4.0]

                    }

                ]

            }

        },

        {

            ""delete_vectors"": {

                ""points"": [1],

                ""vector"": [""""]

            }

        },

        {

            ""overwrite_payload"": {

                ""payload"": {

                    ""test_payload"": ""1""

                },

                ""points"": [1]

            }

        },

        {

            ""set_payload"": {

                ""payload"": {

                    ""test_payload_2"": ""2"",

                    ""test_payload_3"": ""3""

                },

                ""points"": [1]
""indexed_vectors_count"": 1024232,

        ""points_count"": 1068786,

        ""segments_count"": 31,

        ""config"": {

            ""params"": {

                ""vectors"": {

                    ""size"": 384,

                    ""distance"": ""Cosine""

                },

                ""shard_number"": 1,

                ""replication_factor"": 1,

                ""write_consistency_factor"": 1,

                ""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,

                ""memmap_threshold"": null,

                ""indexing_threshold"": 20000,

                ""flush_interval_sec"": 5,
You can specify the desired type of payload storage with [configuration file](../../guides/configuration/) or with collection parameter `on_disk_payload` during [creation](../collections/#create-collection) of the collection.



## Versioning



To ensure data integrity, Qdrant performs all data changes in 2 stages.

In the first step, the data is written to the Write-ahead-log(WAL), which orders all operations and assigns them a sequential number.



Once a change has been added to the WAL, it will not be lost even if a power loss occurs.

Then the changes go into the segments.

Each segment stores the last version of the change applied to it as well as the version of each individual point.

If the new change has a sequential number less than the current version of the point, the updater will ignore the change.

This mechanism allows Qdrant to safely and efficiently restore the storage from the WAL in case of an abnormal shutdown."
What is the significance of the parameter `max_segment_size_kb` in the context of vector indexation?,"The `max_segment_size_kb` parameter in the context of vector indexation determines the maximum size, in kilobytes, that a segment can have. Segments larger than this specified size might lead to disproportionately long indexation times. Therefore, it is essential to limit the size of segments to optimize the indexation process. The choice of this parameter should be based on the priority between indexation speed and search speed. If indexation speed is more critical, the parameter should be set lower. Conversely, if search speed is more important, the parameter should be set higher. It is important to note that 1Kb is equivalent to 1 vector of size 256. If the `max_segment_size_kb` parameter is not explicitly set, it will be automatically selected based on the number of available CPUs.","['""indexed_vectors_count"": 1024232,\n\n        ""points_count"": 1068786,\n\n        ""segments_count"": 31,\n\n        ""config"": {\n\n            ""params"": {\n\n                ""vectors"": {\n\n                    ""size"": 384,\n\n                    ""distance"": ""Cosine""\n\n                },\n\n                ""shard_number"": 1,\n\n                ""replication_factor"": 1,\n\n                ""write_consistency_factor"": 1,\n\n                ""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,\n\n                ""memmap_threshold"": null,\n\n                ""indexing_threshold"": 20000,\n\n                ""flush_interval_sec"": 5,', 'max_segment_size_kb: null\n\n\n\n    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.\n\n    # Segments larger than this threshold will be stored as read-only memmaped file.\n\n    # To enable memmap storage, lower the threshold\n\n    # Note: 1Kb = 1 vector of size 256\n\n    # To explicitly disable mmap optimization, set to `0`.\n\n    # If not set, will be disabled by default.\n\n    memmap_threshold_kb: null\n\n\n\n    # Maximum size (in KiloBytes) of vectors allowed for plain index.\n\n    # Default value based on https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md\n\n    # Note: 1Kb = 1 vector of size 256\n\n    # To explicitly disable vector indexing, set to `0`.\n\n    # If not set, the default value will be used.\n\n    indexing_threshold_kb: 20000\n\n\n\n    # Interval between forced flushes.\n\n    flush_interval_sec: 5\n\n\n\n    # Max number of threads, which can be used for optimization per collection.', '#  - Amount of stored points\n\n    #  - Current write RPS\n\n    #\n\n    # It is recommended to select default number of segments as a factor of the number of search threads,\n\n    # so that each segment would be handled evenly by one of the threads.\n\n    # If `default_segment_number = 0`, will be automatically selected by the number of available CPUs\n\n    default_segment_number: 0\n\n\n\n    # Do not create segments larger this size (in KiloBytes).\n\n    # Large segments might require disproportionately long indexation times,\n\n    # therefore it makes sense to limit the size of segments.\n\n    #\n\n    # If indexation speed have more priority for your - make this parameter lower.\n\n    # If search speed is more important - make this parameter higher.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    # If not set, will be automatically selected considering the number of available CPUs.\n\n    max_segment_size_kb: null\n\n\n\n    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.']","""indexed_vectors_count"": 1024232,

        ""points_count"": 1068786,

        ""segments_count"": 31,

        ""config"": {

            ""params"": {

                ""vectors"": {

                    ""size"": 384,

                    ""distance"": ""Cosine""

                },

                ""shard_number"": 1,

                ""replication_factor"": 1,

                ""write_consistency_factor"": 1,

                ""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,

                ""memmap_threshold"": null,

                ""indexing_threshold"": 20000,

                ""flush_interval_sec"": 5,
max_segment_size_kb: null



    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.

    # Segments larger than this threshold will be stored as read-only memmaped file.

    # To enable memmap storage, lower the threshold

    # Note: 1Kb = 1 vector of size 256

    # To explicitly disable mmap optimization, set to `0`.

    # If not set, will be disabled by default.

    memmap_threshold_kb: null



    # Maximum size (in KiloBytes) of vectors allowed for plain index.

    # Default value based on https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md

    # Note: 1Kb = 1 vector of size 256

    # To explicitly disable vector indexing, set to `0`.

    # If not set, the default value will be used.

    indexing_threshold_kb: 20000



    # Interval between forced flushes.

    flush_interval_sec: 5



    # Max number of threads, which can be used for optimization per collection.
#  - Amount of stored points

    #  - Current write RPS

    #

    # It is recommended to select default number of segments as a factor of the number of search threads,

    # so that each segment would be handled evenly by one of the threads.

    # If `default_segment_number = 0`, will be automatically selected by the number of available CPUs

    default_segment_number: 0



    # Do not create segments larger this size (in KiloBytes).

    # Large segments might require disproportionately long indexation times,

    # therefore it makes sense to limit the size of segments.

    #

    # If indexation speed have more priority for your - make this parameter lower.

    # If search speed is more important - make this parameter higher.

    # Note: 1Kb = 1 vector of size 256

    # If not set, will be automatically selected considering the number of available CPUs.

    max_segment_size_kb: null



    # Maximum size (in KiloBytes) of vectors to store in-memory per segment."
What is the significance of the `indexing_threshold_kb` parameter?,"The `indexing_threshold_kb` parameter specifies the maximum size, in Kilobytes, allowed for vectors for plain index. The default value for this parameter is set to 20000 KB. It is important to note that 1 KB is equivalent to 1 vector of size 256. This parameter plays a crucial role in determining the maximum size of vectors that can be used for indexing. If the `indexing_threshold_kb` parameter is not explicitly set, the default value will be used. Additionally, setting this parameter to `0` will explicitly disable vector indexing. This parameter directly impacts the efficiency and performance of the indexing process for the given vectors.","['""indexed_vectors_count"": 1024232,\n\n        ""points_count"": 1068786,\n\n        ""segments_count"": 31,\n\n        ""config"": {\n\n            ""params"": {\n\n                ""vectors"": {\n\n                    ""size"": 384,\n\n                    ""distance"": ""Cosine""\n\n                },\n\n                ""shard_number"": 1,\n\n                ""replication_factor"": 1,\n\n                ""write_consistency_factor"": 1,\n\n                ""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,\n\n                ""memmap_threshold"": null,\n\n                ""indexing_threshold"": 20000,\n\n                ""flush_interval_sec"": 5,', '""indexing_threshold"": 20000,\n\n                ""flush_interval_sec"": 5,\n\n                ""max_optimization_threads"": 1\n\n            },\n\n            ""wal_config"": {\n\n                ""wal_capacity_mb"": 32,\n\n                ""wal_segments_ahead"": 0\n\n            }\n\n        },\n\n        ""payload_schema"": {}\n\n    },\n\n    ""status"": ""ok"",\n\n    ""time"": 0.00010143\n\n}\n\n```\n\n\n\n</details>\n\n<br/>\n\n\n\n\n\n\n\n```csharp\n\nawait client.GetCollectionInfoAsync(""{collection_name}"");\n\n```\n\n\n\nIf you insert the vectors into the collection, the `status` field may become\n\n`yellow` whilst it is optimizing. It will become `green` once all the points are\n\nsuccessfully processed.\n\n\n\nThe following color statuses are possible:\n\n\n\n- 🟢 `green`: collection is ready\n\n- 🟡 `yellow`: collection is optimizing\n\n- 🔴 `red`: an error occurred which the engine could not recover from\n\n\n\n### Approximate point and vector counts\n\n\n\nYou may be interested in the count attributes:', '--data-raw \'{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 10000\n\n    }\n\n  }\'\n\n```\n\n\n\n```python\n\nclient.update_collection(\n\n    collection_name=""{collection_name}"",\n\n    optimizer_config=models.OptimizersConfigDiff(indexing_threshold=10000),\n\n)\n\n```\n\n\n\n```typescript\n\nclient.updateCollection(""{collection_name}"", {\n\n  optimizers_config: {\n\n    indexing_threshold: 10000,\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::qdrant::OptimizersConfigDiff;\n\n\n\nclient\n\n    .update_collection(\n\n        ""{collection_name}"",\n\n        &OptimizersConfigDiff {\n\n            indexing_threshold: Some(10000),\n\n            ..Default::default()\n\n        },\n\n        None,\n\n        None,\n\n        None,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.grpc.Collections.OptimizersConfigDiff;\n\nimport io.qdrant.client.grpc.Collections.UpdateCollection;\n\n\n\nclient.updateCollectionAsync(\n\n    UpdateCollection.newBuilder()\n\n        .setCollectionName(""{collection_name}"")']","""indexed_vectors_count"": 1024232,

        ""points_count"": 1068786,

        ""segments_count"": 31,

        ""config"": {

            ""params"": {

                ""vectors"": {

                    ""size"": 384,

                    ""distance"": ""Cosine""

                },

                ""shard_number"": 1,

                ""replication_factor"": 1,

                ""write_consistency_factor"": 1,

                ""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,

                ""memmap_threshold"": null,

                ""indexing_threshold"": 20000,

                ""flush_interval_sec"": 5,
""indexing_threshold"": 20000,

                ""flush_interval_sec"": 5,

                ""max_optimization_threads"": 1

            },

            ""wal_config"": {

                ""wal_capacity_mb"": 32,

                ""wal_segments_ahead"": 0

            }

        },

        ""payload_schema"": {}

    },

    ""status"": ""ok"",

    ""time"": 0.00010143

}

```



</details>

<br/>







```csharp

await client.GetCollectionInfoAsync(""{collection_name}"");

```



If you insert the vectors into the collection, the `status` field may become

`yellow` whilst it is optimizing. It will become `green` once all the points are

successfully processed.



The following color statuses are possible:



- 🟢 `green`: collection is ready

- 🟡 `yellow`: collection is optimizing

- 🔴 `red`: an error occurred which the engine could not recover from



### Approximate point and vector counts



You may be interested in the count attributes:
--data-raw '{

    ""optimizers_config"": {

        ""indexing_threshold"": 10000

    }

  }'

```



```python

client.update_collection(

    collection_name=""{collection_name}"",

    optimizer_config=models.OptimizersConfigDiff(indexing_threshold=10000),

)

```



```typescript

client.updateCollection(""{collection_name}"", {

  optimizers_config: {

    indexing_threshold: 10000,

  },

});

```



```rust

use qdrant_client::qdrant::OptimizersConfigDiff;



client

    .update_collection(

        ""{collection_name}"",

        &OptimizersConfigDiff {

            indexing_threshold: Some(10000),

            ..Default::default()

        },

        None,

        None,

        None,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.grpc.Collections.OptimizersConfigDiff;

import io.qdrant.client.grpc.Collections.UpdateCollection;



client.updateCollectionAsync(

    UpdateCollection.newBuilder()

        .setCollectionName(""{collection_name}"")"
How can you optimize Qdrant for minimizing latency in search requests?,"To optimize Qdrant for minimizing latency in search requests, you can set up the system to use as many cores as possible for a single request. This can be achieved by setting the number of segments in the collection to be equal to the number of cores in the system. By doing this, each segment will be processed in parallel, leading to a faster final result. This approach allows for the efficient utilization of system resources and can significantly reduce the time taken from the moment a request is submitted to the moment a response is received. By optimizing for latency in this manner, you can enhance the overall speed and responsiveness of the search functionality in Qdrant.","['![sequential vs. concurrent flow](/articles_data/search-as-you-type/sayt_concurrency.png)\n\n\n\nWhile this means more load for the Qdrant vector search engine, this is not the limiting factor. The relevant data is already in cache in many cases, so the overhead stays within acceptable bounds, and the maximum latency in case of prefix cache misses is measurably reduced.\n\n\n\nThe code is available on the [Qdrant github](https://github.com/qdrant/page-search)\n\n\n\nTo sum up: Rust is fast, recommend lets us use precomputed embeddings, batch requests are awesome and one can do a semantic search in mere milliseconds.', '{ ""id"": 11, ""score"": 0.73 }\n\n    ],\n\n    [\n\n        { ""id"": 1, ""score"": 0.92 },\n\n        { ""id"": 3, ""score"": 0.89 },\n\n        { ""id"": 9, ""score"": 0.75 }\n\n    ]\n\n  ],\n\n  ""status"": ""ok"",\n\n  ""time"": 0.001\n\n}\n\n```\n\n\n\n## Pagination\n\n\n\n*Available as of v0.8.3*\n\n\n\nSearch and [recommendation](../explore/#recommendation-api) APIs allow to skip first results of the search and return only the result starting from some specified offset:\n\n\n\nExample:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{\n\n    ""vector"": [0.2, 0.1, 0.9, 0.7],\n\n    ""with_vectors"": true,\n\n    ""with_payload"": true,\n\n    ""limit"": 10,\n\n    ""offset"": 100\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.search(\n\n    collection_name=""{collection_name}"",\n\n    query_vector=[0.2, 0.1, 0.9, 0.7],\n\n    with_vectors=True,\n\n    with_payload=True,\n\n    limit=10,\n\n    offset=100,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', 'To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""default_segment_number"": 16\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    optimizers_config=models.OptimizersConfigDiff(default_segment_number=16),\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";']","![sequential vs. concurrent flow](/articles_data/search-as-you-type/sayt_concurrency.png)



While this means more load for the Qdrant vector search engine, this is not the limiting factor. The relevant data is already in cache in many cases, so the overhead stays within acceptable bounds, and the maximum latency in case of prefix cache misses is measurably reduced.



The code is available on the [Qdrant github](https://github.com/qdrant/page-search)



To sum up: Rust is fast, recommend lets us use precomputed embeddings, batch requests are awesome and one can do a semantic search in mere milliseconds.
{ ""id"": 11, ""score"": 0.73 }

    ],

    [

        { ""id"": 1, ""score"": 0.92 },

        { ""id"": 3, ""score"": 0.89 },

        { ""id"": 9, ""score"": 0.75 }

    ]

  ],

  ""status"": ""ok"",

  ""time"": 0.001

}

```



## Pagination



*Available as of v0.8.3*



Search and [recommendation](../explore/#recommendation-api) APIs allow to skip first results of the search and return only the result starting from some specified offset:



Example:



```http

POST /collections/{collection_name}/points/search

{

    ""vector"": [0.2, 0.1, 0.9, 0.7],

    ""with_vectors"": true,

    ""with_payload"": true,

    ""limit"": 10,

    ""offset"": 100

}

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.search(

    collection_name=""{collection_name}"",

    query_vector=[0.2, 0.1, 0.9, 0.7],

    with_vectors=True,

    with_payload=True,

    limit=10,

    offset=100,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""default_segment_number"": 16

    }

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    optimizers_config=models.OptimizersConfigDiff(default_segment_number=16),

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";"
How can Qdrant be configured to optimize throughput by utilizing multiple cores for parallel processing?,"To optimize throughput in Qdrant and utilize multiple cores for parallel processing, you can configure the system to use minimal number of segments, typically set to 2. By setting the default_segment_number to 2 in the optimizers_config section of the PUT request for creating a collection, you can instruct Qdrant to use a smaller number of segments. This approach benefits from the size of the index and reduces the overall number of vector comparisons needed to find the nearest neighbors. However, it is important to note that larger segments require more time to build the index. By adjusting this configuration parameter, you can strike a balance between maximizing throughput and optimizing the efficiency of vector search operations in Qdrant.","['To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""default_segment_number"": 16\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    optimizers_config=models.OptimizersConfigDiff(default_segment_number=16),\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', ""|----------------------------------------------------|----------------------------------------------|---------|------------------|\n\n| [Installation](guides/installation/) | Different ways to install Qdrant. | [Collections](concepts/collections/) | Learn about the central concept behind Qdrant. |                  \n\n| [Configuration](guides/configuration/)   | Update the default configuration.    | [Bulk Upload](tutorials/bulk-upload/) |   Efficiently upload a large number of vectors. |                  \n\n| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  \n\n\n\n## Common Use Cases:"", 'optimizersConfig: new OptimizersConfigDiff { DefaultSegmentNumber = 16 }\n\n);\n\n```\n\n\n\nTo prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.\n\nTo do that, you can configure qdrant to use minimal number of segments, which is usually 2.\n\nLarge segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""default_segment_number"": 2\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),']","To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""default_segment_number"": 16

    }

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    optimizers_config=models.OptimizersConfigDiff(default_segment_number=16),

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
|----------------------------------------------------|----------------------------------------------|---------|------------------|

| [Installation](guides/installation/) | Different ways to install Qdrant. | [Collections](concepts/collections/) | Learn about the central concept behind Qdrant. |                  

| [Configuration](guides/configuration/)   | Update the default configuration.    | [Bulk Upload](tutorials/bulk-upload/) |   Efficiently upload a large number of vectors. |                  

| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  



## Common Use Cases:
optimizersConfig: new OptimizersConfigDiff { DefaultSegmentNumber = 16 }

);

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""default_segment_number"": 2

    }

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),"
How can one enable distributed deployment mode in Qdrant?,"To enable distributed deployment mode in Qdrant, one needs to enable the cluster mode in the configuration or set the ENV variable `QDRANT__CLUSTER__ENABLED=true`. This allows multiple Qdrant services to communicate with each other, distributing data across peers to extend storage capabilities and increase stability. In the configuration file, under the `cluster` section, set `enabled: true` to run Qdrant in distributed deployment mode. Additionally, configure the internal communication port between peers using the `p2p` section with the specified port number. Furthermore, adjust the parameters related to the distributed consensus algorithm as needed for optimal performance.","['---\n\ntitle: Distributed Deployment\n\nweight: 100\n\naliases:\n\n  - ../distributed_deployment\n\n---\n\n\n\n# Distributed deployment\n\n\n\nSince version v0.8.0 Qdrant supports a distributed deployment mode.\n\nIn this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.\n\n\n\nTo enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.\n\n\n\n```yaml\n\ncluster:\n\n  # Use `enabled: true` to run Qdrant in distributed deployment mode\n\n  enabled: true\n\n  # Configuration of the inter-cluster communication\n\n  p2p:\n\n    # Port for internal communication between peers\n\n    port: 6335\n\n\n\n  # Configuration related to distributed consensus algorithm\n\n  consensus:\n\n    # How frequently peers should ping each other.\n\n    # Setting this parameter to lower value will allow consensus\n\n    # to detect disconnected node earlier, but too frequent', '```python\n\nqdrant = Qdrant.from_documents(\n\n    docs, embeddings, \n\n    path=""/tmp/local_qdrant"",\n\n    collection_name=""my_documents"",\n\n)\n\n```\n\n\n\n### On-premise server deployment\n\n\n\nNo matter if you choose to launch Qdrant locally with [a Docker container](/documentation/guides/installation/), or \n\nselect a Kubernetes deployment with [the official Helm chart](https://github.com/qdrant/qdrant-helm), the way you\'re \n\ngoing to connect to such an instance will be identical. You\'ll need to provide a URL pointing to the service.\n\n\n\n```python\n\nurl = ""<---qdrant url here --->""\n\nqdrant = Qdrant.from_documents(\n\n    docs, \n\n    embeddings, \n\n    url, \n\n    prefer_grpc=True, \n\n    collection_name=""my_documents"",\n\n)\n\n```\n\n\n\n## Next steps\n\n\n\nIf you\'d like to know more about running Qdrant in a LangChain-based application, please read our article \n\n[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information', '### Recommended Workflow:\n\n\n\n![Local mode workflow](https://raw.githubusercontent.com/qdrant/qdrant-client/master/docs/images/try-develop-deploy.png)\n\n\n\nFirst, try Qdrant locally using the [Qdrant Client](https://github.com/qdrant/qdrant-client) and with the help of our [Tutorials](tutorials/) and Guides. Develop a sample app from our [Examples](examples/) list and try it using a [Qdrant Docker](guides/installation/) container. Then, when you are ready for production, deploy to a Free Tier [Qdrant Cloud](cloud/) cluster.\n\n\n\n### Try Qdrant with Practice Data:\n\n\n\nYou may always use our [Practice Datasets](datasets/) to build with Qdrant. This page will be regularly updated with dataset snapshots you can use to bootstrap complete projects.\n\n\n\n## Popular Topics:\n\n\n\n| Tutorial                                           | Description                                  | Tutorial| Description      |']","---

title: Distributed Deployment

weight: 100

aliases:

  - ../distributed_deployment

---



# Distributed deployment



Since version v0.8.0 Qdrant supports a distributed deployment mode.

In this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.



To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.



```yaml

cluster:

  # Use `enabled: true` to run Qdrant in distributed deployment mode

  enabled: true

  # Configuration of the inter-cluster communication

  p2p:

    # Port for internal communication between peers

    port: 6335



  # Configuration related to distributed consensus algorithm

  consensus:

    # How frequently peers should ping each other.

    # Setting this parameter to lower value will allow consensus

    # to detect disconnected node earlier, but too frequent
```python

qdrant = Qdrant.from_documents(

    docs, embeddings, 

    path=""/tmp/local_qdrant"",

    collection_name=""my_documents"",

)

```



### On-premise server deployment



No matter if you choose to launch Qdrant locally with [a Docker container](/documentation/guides/installation/), or 

select a Kubernetes deployment with [the official Helm chart](https://github.com/qdrant/qdrant-helm), the way you're 

going to connect to such an instance will be identical. You'll need to provide a URL pointing to the service.



```python

url = ""<---qdrant url here --->""

qdrant = Qdrant.from_documents(

    docs, 

    embeddings, 

    url, 

    prefer_grpc=True, 

    collection_name=""my_documents"",

)

```



## Next steps



If you'd like to know more about running Qdrant in a LangChain-based application, please read our article 

[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information
### Recommended Workflow:



![Local mode workflow](https://raw.githubusercontent.com/qdrant/qdrant-client/master/docs/images/try-develop-deploy.png)



First, try Qdrant locally using the [Qdrant Client](https://github.com/qdrant/qdrant-client) and with the help of our [Tutorials](tutorials/) and Guides. Develop a sample app from our [Examples](examples/) list and try it using a [Qdrant Docker](guides/installation/) container. Then, when you are ready for production, deploy to a Free Tier [Qdrant Cloud](cloud/) cluster.



### Try Qdrant with Practice Data:



You may always use our [Practice Datasets](datasets/) to build with Qdrant. This page will be regularly updated with dataset snapshots you can use to bootstrap complete projects.



## Popular Topics:



| Tutorial                                           | Description                                  | Tutorial| Description      |"
What is the recommended approach for determining the number of shards when creating a collection using Qdrant?,"When creating a collection using Qdrant, it is recommended to set the number of shards to be a multiple of the number of nodes currently running in your cluster. For example, if you have 3 nodes in your cluster, setting the number of shards to 6 could be a good option. Shards are evenly distributed across all existing nodes when a collection is first created. It is important to note that Qdrant does not automatically rebalance shards if your cluster size or replication factor changes, as this operation can be expensive on large clusters. If scaling operations require moving shards, there are specific procedures outlined for this purpose, available as of version 0.9.0 of Qdrant.","['.setSize(300)\n\n                            .setDistance(Distance.Cosine)\n\n                            .build())\n\n                    .build())\n\n            .setShardNumber(6)\n\n            .setReplicationFactor(2)\n\n            .build())\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\n\tshardNumber: 6,\n\n\treplicationFactor: 2\n\n);\n\n```\n\n\n\nThis code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.\n\n\n\nSince a replication factor of ""2"" would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.\n\n\n\n### Creating new shard replicas', '.setCollectionName(""{collection_name}"")\n\n            // ... other collection parameters\n\n            .setShardNumber(1)\n\n            .setShardingMethod(ShardingMethod.Custom)\n\n            .build())\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\t// ... other collection parameters\n\n\tshardNumber: 1,\n\n\tshardingMethod: ShardingMethod.Custom\n\n);\n\n```\n\n\n\nIn this mode, the `shard_number` means the number of shards per shard key, where points will be distributed evenly. For example, if you have 10 shard keys and a collection config with these settings:\n\n\n\n```json\n\n{\n\n    ""shard_number"": 1,\n\n    ""sharding_method"": ""custom"",\n\n    ""replication_factor"": 2\n\n}\n\n```\n\n\n\nThen you will have `1 * 10 * 2 = 20` total physical shards in the collection.\n\n\n\nTo specify the shard for each point, you need to provide the `shard_key` field in the upsert request:', '![Qdrant Multitenancy](/articles_data/multitenancy/multitenancy.png)\n\n\n\n## Create custom shards for a single collection\n\n\n\nWhen creating a collection, you will need to configure user-defined sharding. This lets you control the shard placement of your data, so that operations can hit only the subset of shards they actually need. In big clusters, this can significantly improve the performance of operations, since you won\'t need to go through the entire collection to retrieve data.\n\n\n\n```python\n\nclient.create_collection(\n\n    collection_name=""{tenant_data}"",\n\n    shard_number=2,\n\n    sharding_method=models.ShardingMethod.CUSTOM,\n\n    # ... other collection parameters\n\n)\n\nclient.create_shard_key(""{tenant_data}"", ""canada"")\n\nclient.create_shard_key(""{tenant_data}"", ""germany"")\n\n```']",".setSize(300)

                            .setDistance(Distance.Cosine)

                            .build())

                    .build())

            .setShardNumber(6)

            .setReplicationFactor(2)

            .build())

    .get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

	shardNumber: 6,

	replicationFactor: 2

);

```



This code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.



Since a replication factor of ""2"" would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.



### Creating new shard replicas
.setCollectionName(""{collection_name}"")

            // ... other collection parameters

            .setShardNumber(1)

            .setShardingMethod(ShardingMethod.Custom)

            .build())

    .get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",

	// ... other collection parameters

	shardNumber: 1,

	shardingMethod: ShardingMethod.Custom

);

```



In this mode, the `shard_number` means the number of shards per shard key, where points will be distributed evenly. For example, if you have 10 shard keys and a collection config with these settings:



```json

{

    ""shard_number"": 1,

    ""sharding_method"": ""custom"",

    ""replication_factor"": 2

}

```



Then you will have `1 * 10 * 2 = 20` total physical shards in the collection.



To specify the shard for each point, you need to provide the `shard_key` field in the upsert request:
![Qdrant Multitenancy](/articles_data/multitenancy/multitenancy.png)



## Create custom shards for a single collection



When creating a collection, you will need to configure user-defined sharding. This lets you control the shard placement of your data, so that operations can hit only the subset of shards they actually need. In big clusters, this can significantly improve the performance of operations, since you won't need to go through the entire collection to retrieve data.



```python

client.create_collection(

    collection_name=""{tenant_data}"",

    shard_number=2,

    sharding_method=models.ShardingMethod.CUSTOM,

    # ... other collection parameters

)

client.create_shard_key(""{tenant_data}"", ""canada"")

client.create_shard_key(""{tenant_data}"", ""germany"")

```"
How is the discovery score computed in the context provided?,"In the context provided, the discovery score is computed using the formula: 

Discovery Score = sigmoid(s(v_t)) + Σ rank(v_i^+, v_i^-)

Here, s(v_t) represents the similarity function of the target vector, and v_i^+ and v_i^- represent the positive and negative examples, respectively. The sigmoid function is utilized to normalize the score within the range of 0 to 1. The sum of ranks is used to penalize vectors that are closer to negative examples than to positive ones. This means that the sum of individual ranks determines how many positive zones a point is in, while the closeness hierarchy serves as a secondary factor in the computation of the discovery score.","['Negative = new VectorExample { Id = 718 }\n\n\t\t},\n\n\t\tnew()\n\n\t\t{\n\n\t\t\tPositive = new VectorExample { Id = 200 },\n\n\t\t\tNegative = new VectorExample { Id = 300 }\n\n\t\t}\n\n\t],\n\n\tlimit: 10\n\n);\n\n```\n\n\n\n<aside role=""status"">\n\nNotes about discovery search:\n\n\n\n* When providing ids as examples, they will be excluded from the results.\n\n* Score is always in descending order (larger is better), regardless of the metric used.\n\n* Since the space is hard-constrained by the context, accuracy is normal to drop when using default settings. To mitigate this, increasing the `ef` search parameter to something above 64 will already be much better than the default 16, e.g: `""params"": { ""ef"": 128 }`\n\n\n\n</aside>\n\n\n\n### Context search', '$$\n\n\\text{rank}(v^+, v^-) = \\begin{cases}\n\n    1, &\\quad s(v^+) \\geq s(v^-) \\\\\\\\\n\n    -1, &\\quad s(v^+) < s(v^-)\n\n\\end{cases}\n\n$$\n\nwhere $v^+$ represents a positive example, $v^-$ represents a negative example, and $s(v)$ is the similarity score of a vector $v$ to the target vector. The discovery score is then computed as:\n\n$$\n\n \\text{discovery score} = \\text{sigmoid}(s(v_t))+ \\sum \\text{rank}(v_i^+, v_i^-),\n\n$$\n\nwhere $s(v)$ is the similarity function, $v_t$ is the target vector, and again $v_i^+$ and $v_i^-$ are the positive and negative examples, respectively. The sigmoid function is used to normalize the score between 0 and 1 and the sum of ranks is used to penalize vectors that are closer to the negative examples than to the positive ones. In other words, the sum of individual ranks determines how many positive zones a point is in, while the closeness hierarchy comes second.\n\n\n\nExample:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/discover\n\n\n\n{\n\n  ""target"": [0.2, 0.1, 0.9, 0.7],', 'In this API, Qdrant introduces the concept of `context`, which is used for splitting the space. Context is a set of positive-negative pairs, and each pair divides the space into positive and negative zones. In that mode, the search operation prefers points based on how many positive zones they belong to (or how much they avoid negative zones).\n\n\n\nThe interface for providing context is similar to the recommendation API (ids or raw vectors). Still, in this case, they need to be provided in the form of positive-negative pairs.\n\n\n\nDiscovery API lets you do two new types of search:\n\n- **Discovery search**: Uses the context (the pairs of positive-negative vectors) and a target to return the points more similar to the target, but constrained by the context.\n\n- **Context search**: Using only the context pairs, get the points that live in the best zone, where loss is minimized']","Negative = new VectorExample { Id = 718 }

		},

		new()

		{

			Positive = new VectorExample { Id = 200 },

			Negative = new VectorExample { Id = 300 }

		}

	],

	limit: 10

);

```



<aside role=""status"">

Notes about discovery search:



* When providing ids as examples, they will be excluded from the results.

* Score is always in descending order (larger is better), regardless of the metric used.

* Since the space is hard-constrained by the context, accuracy is normal to drop when using default settings. To mitigate this, increasing the `ef` search parameter to something above 64 will already be much better than the default 16, e.g: `""params"": { ""ef"": 128 }`



</aside>



### Context search
$$

\text{rank}(v^+, v^-) = \begin{cases}

    1, &\quad s(v^+) \geq s(v^-) \\\\

    -1, &\quad s(v^+) < s(v^-)

\end{cases}

$$

where $v^+$ represents a positive example, $v^-$ represents a negative example, and $s(v)$ is the similarity score of a vector $v$ to the target vector. The discovery score is then computed as:

$$

 \text{discovery score} = \text{sigmoid}(s(v_t))+ \sum \text{rank}(v_i^+, v_i^-),

$$

where $s(v)$ is the similarity function, $v_t$ is the target vector, and again $v_i^+$ and $v_i^-$ are the positive and negative examples, respectively. The sigmoid function is used to normalize the score between 0 and 1 and the sum of ranks is used to penalize vectors that are closer to the negative examples than to the positive ones. In other words, the sum of individual ranks determines how many positive zones a point is in, while the closeness hierarchy comes second.



Example:



```http

POST /collections/{collection_name}/points/discover



{

  ""target"": [0.2, 0.1, 0.9, 0.7],
In this API, Qdrant introduces the concept of `context`, which is used for splitting the space. Context is a set of positive-negative pairs, and each pair divides the space into positive and negative zones. In that mode, the search operation prefers points based on how many positive zones they belong to (or how much they avoid negative zones).



The interface for providing context is similar to the recommendation API (ids or raw vectors). Still, in this case, they need to be provided in the form of positive-negative pairs.



Discovery API lets you do two new types of search:

- **Discovery search**: Uses the context (the pairs of positive-negative vectors) and a target to return the points more similar to the target, but constrained by the context.

- **Context search**: Using only the context pairs, get the points that live in the best zone, where loss is minimized"
How does Qdrant optimize storage at the segment level?,"Qdrant optimizes storage at the segment level by applying changes in batches rather than individually. When optimization is needed, the segment to be optimized remains readable during the rebuild process. This is achieved by wrapping the segment into a proxy that handles data changes transparently. Changed data is placed in a copy-on-write segment, prioritizing retrieval and subsequent updates. This approach ensures efficiency in storage optimization within Qdrant.","[""|----------------------------------------------------|----------------------------------------------|---------|------------------|\n\n| [Installation](guides/installation/) | Different ways to install Qdrant. | [Collections](concepts/collections/) | Learn about the central concept behind Qdrant. |                  \n\n| [Configuration](guides/configuration/)   | Update the default configuration.    | [Bulk Upload](tutorials/bulk-upload/) |   Efficiently upload a large number of vectors. |                  \n\n| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  \n\n\n\n## Common Use Cases:"", '---\n\ntitle: Optimizer\n\nweight: 70\n\naliases:\n\n  - ../optimizer\n\n---\n\n\n\n# Optimizer\n\n\n\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\n\n\n\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).\n\nIn this case, the segment to be optimized remains readable for the time of the rebuild.\n\n\n\n![Segment optimization](/docs/optimization.svg)\n\n\n\nThe availability is achieved by wrapping the segment into a proxy that transparently handles data changes.\n\nChanged data is placed in the copy-on-write segment, which has priority for retrieval and subsequent updates.\n\n\n\n## Vacuum Optimizer\n\n\n\nThe simplest example of a case where you need to rebuild a segment repository is to remove points.', ""---\n\ntitle: Optimize Resources\n\nweight: 11\n\naliases:\n\n  - ../tutorials/optimize\n\n---\n\n\n\n# Optimize Qdrant\n\n\n\nDifferent use cases have different requirements for balancing between memory, speed, and precision.\n\nQdrant is designed to be flexible and customizable so you can tune it to your needs.\n\n\n\n![Trafeoff](/docs/tradeoff.png)\n\n\n\nLet's look deeper into each of those possible optimization scenarios.\n\n\n\n## Prefer low memory footprint with high speed search\n\n\n\nThe main way to achieve high speed search with low memory footprint is to keep vectors on disk while at the same time minimizing the number of disk reads.\n\n\n\nVector quantization is one way to achieve this. Quantization converts vectors into a more compact representation, which can be stored in memory and used for search. With smaller vectors you can cache more in RAM and reduce the number of disk reads.\n\n\n\nTo configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration:\n\n\n\n```http""]","|----------------------------------------------------|----------------------------------------------|---------|------------------|

| [Installation](guides/installation/) | Different ways to install Qdrant. | [Collections](concepts/collections/) | Learn about the central concept behind Qdrant. |                  

| [Configuration](guides/configuration/)   | Update the default configuration.    | [Bulk Upload](tutorials/bulk-upload/) |   Efficiently upload a large number of vectors. |                  

| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  



## Common Use Cases:
---

title: Optimizer

weight: 70

aliases:

  - ../optimizer

---



# Optimizer



It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.



Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).

In this case, the segment to be optimized remains readable for the time of the rebuild.



![Segment optimization](/docs/optimization.svg)



The availability is achieved by wrapping the segment into a proxy that transparently handles data changes.

Changed data is placed in the copy-on-write segment, which has priority for retrieval and subsequent updates.



## Vacuum Optimizer



The simplest example of a case where you need to rebuild a segment repository is to remove points.
---

title: Optimize Resources

weight: 11

aliases:

  - ../tutorials/optimize

---



# Optimize Qdrant



Different use cases have different requirements for balancing between memory, speed, and precision.

Qdrant is designed to be flexible and customizable so you can tune it to your needs.



![Trafeoff](/docs/tradeoff.png)



Let's look deeper into each of those possible optimization scenarios.



## Prefer low memory footprint with high speed search



The main way to achieve high speed search with low memory footprint is to keep vectors on disk while at the same time minimizing the number of disk reads.



Vector quantization is one way to achieve this. Quantization converts vectors into a more compact representation, which can be stored in memory and used for search. With smaller vectors you can cache more in RAM and reduce the number of disk reads.



To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration:



```http"
What is the purpose of the Vacuum Optimizer in the Qdrant database system?,"The Vacuum Optimizer in the Qdrant database system is utilized to address the issue of accumulated deleted records within segments. When records are marked as deleted instead of being immediately removed, they can accumulate over time, occupying memory and slowing down the system. The Vacuum Optimizer is triggered when a segment has accumulated a significant number of deleted records, as defined by the criteria set in the configuration file. This optimizer helps in optimizing segments by removing these accumulated deleted records, thereby improving system performance and efficiency. The configuration file specifies parameters such as the minimal fraction of deleted vectors and the minimal number of vectors in a segment required to trigger the Vacuum Optimizer. By running the Vacuum Optimizer, the system can effectively manage and optimize segments to prevent performance degradation due to accumulated deleted records.","['---\n\ntitle: Optimizer\n\nweight: 70\n\naliases:\n\n  - ../optimizer\n\n---\n\n\n\n# Optimizer\n\n\n\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\n\n\n\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).\n\nIn this case, the segment to be optimized remains readable for the time of the rebuild.\n\n\n\n![Segment optimization](/docs/optimization.svg)\n\n\n\nThe availability is achieved by wrapping the segment into a proxy that transparently handles data changes.\n\nChanged data is placed in the copy-on-write segment, which has priority for retrieval and subsequent updates.\n\n\n\n## Vacuum Optimizer\n\n\n\nThe simplest example of a case where you need to rebuild a segment repository is to remove points.', '## Vacuum Optimizer\n\n\n\nThe simplest example of a case where you need to rebuild a segment repository is to remove points.\n\nLike many other databases, Qdrant does not delete entries immediately after a query.\n\nInstead, it marks records as deleted and ignores them for future queries.\n\n\n\nThis strategy allows us to minimize disk access - one of the slowest operations.\n\nHowever, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.\n\n\n\nTo avoid these adverse effects, Vacuum Optimizer is used.\n\nIt is used if the segment has accumulated too many deleted records.\n\n\n\nThe criteria for starting the optimizer are defined in the configuration file.\n\n\n\nHere is an example of parameter values:\n\n\n\n```yaml\n\nstorage:\n\n  optimizers:\n\n    # The minimal fraction of deleted vectors in a segment, required to perform segment optimization\n\n    deleted_threshold: 0.2\n\n    # The minimal number of vectors in a segment, required to perform segment optimization', 'acknowledges: “Qdrant’s ability to handle large-scale models and the flexibility\n\nit offers in terms of data management has been crucial for us. The observability\n\nfeatures, such as historical graphs of RAM, Disk, and CPU, provided by Qdrant are\n\nalso particularly useful, allowing us to plan our scaling strategy effectively.”\n\n\n\n![“We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as\n\nwe don’t have to run lots of nodes in parallel. While being memory-bound, we were\n\nable to push the same instances further with the help of quantization. While you\n\nget pressure on MMAP in this case you maintain very good performance even if the\n\nRAM is fully used. With this we were able to reduce our cost by 2x.” - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)\n\n\n\nDust was able to scale its application with Qdrant while maintaining low latency\n\nacross hundreds of thousands of collections with retrieval only taking']","---

title: Optimizer

weight: 70

aliases:

  - ../optimizer

---



# Optimizer



It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.



Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).

In this case, the segment to be optimized remains readable for the time of the rebuild.



![Segment optimization](/docs/optimization.svg)



The availability is achieved by wrapping the segment into a proxy that transparently handles data changes.

Changed data is placed in the copy-on-write segment, which has priority for retrieval and subsequent updates.



## Vacuum Optimizer



The simplest example of a case where you need to rebuild a segment repository is to remove points.
## Vacuum Optimizer



The simplest example of a case where you need to rebuild a segment repository is to remove points.

Like many other databases, Qdrant does not delete entries immediately after a query.

Instead, it marks records as deleted and ignores them for future queries.



This strategy allows us to minimize disk access - one of the slowest operations.

However, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.



To avoid these adverse effects, Vacuum Optimizer is used.

It is used if the segment has accumulated too many deleted records.



The criteria for starting the optimizer are defined in the configuration file.



Here is an example of parameter values:



```yaml

storage:

  optimizers:

    # The minimal fraction of deleted vectors in a segment, required to perform segment optimization

    deleted_threshold: 0.2

    # The minimal number of vectors in a segment, required to perform segment optimization
acknowledges: “Qdrant’s ability to handle large-scale models and the flexibility

it offers in terms of data management has been crucial for us. The observability

features, such as historical graphs of RAM, Disk, and CPU, provided by Qdrant are

also particularly useful, allowing us to plan our scaling strategy effectively.”



![“We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as

we don’t have to run lots of nodes in parallel. While being memory-bound, we were

able to push the same instances further with the help of quantization. While you

get pressure on MMAP in this case you maintain very good performance even if the

RAM is fully used. With this we were able to reduce our cost by 2x.” - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)



Dust was able to scale its application with Qdrant while maintaining low latency

across hundreds of thousands of collections with retrieval only taking"
What is the purpose of the `payload` field in the data points?,"The `payload` field in the data points being upserted using the QdrantClient serves as a container for additional metadata or information related to the vector data being stored. It allows users to associate supplementary details with each data point, such as the city name, price, or any other custom attributes that provide context or additional insights into the vector data. This metadata can be used for filtering, searching, or categorizing the vectors during retrieval or query operations, enabling more efficient and targeted data analysis and retrieval processes. In the examples given, the `payload` field includes information like city names, prices, and other relevant details specific to each data point, enhancing the overall utility and relevance of the stored vector data.","['{\n\n                        ""id"": 1,\n\n                        ""vector"": [1.0, 2.0, 3.0, 4.0],\n\n                        ""payload"": {}\n\n                    }\n\n                ]\n\n            }\n\n        },\n\n        {\n\n            ""update_vectors"": {\n\n                ""points"": [\n\n                    {\n\n                        ""id"": 1,\n\n                        ""vector"": [1.0, 2.0, 3.0, 4.0]\n\n                    }\n\n                ]\n\n            }\n\n        },\n\n        {\n\n            ""delete_vectors"": {\n\n                ""points"": [1],\n\n                ""vector"": [""""]\n\n            }\n\n        },\n\n        {\n\n            ""overwrite_payload"": {\n\n                ""payload"": {\n\n                    ""test_payload"": ""1""\n\n                },\n\n                ""points"": [1]\n\n            }\n\n        },\n\n        {\n\n            ""set_payload"": {\n\n                ""payload"": {\n\n                    ""test_payload_2"": ""2"",\n\n                    ""test_payload_3"": ""3""\n\n                },\n\n                ""points"": [1]', '""payloads"": [\n\n            {""color"": ""red""},\n\n            {""color"": ""green""},\n\n            {""color"": ""blue""}\n\n        ],\n\n        ""vectors"": [\n\n            [0.9, 0.1, 0.1],\n\n            [0.1, 0.9, 0.1],\n\n            [0.1, 0.1, 0.9]\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.upsert(\n\n    collection_name=""{collection_name}"",\n\n    points=models.Batch(\n\n        ids=[1, 2, 3],\n\n        payloads=[\n\n            {""color"": ""red""},\n\n            {""color"": ""green""},\n\n            {""color"": ""blue""},\n\n        ],\n\n        vectors=[\n\n            [0.9, 0.1, 0.1],\n\n            [0.1, 0.9, 0.1],\n\n            [0.1, 0.1, 0.9],\n\n        ],\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nclient.upsert(""{collection_name}"", {\n\n  batch: {\n\n    ids: [1, 2, 3],\n\n    payloads: [{ color: ""red"" }, { color: ""green"" }, { color: ""blue"" }],\n\n    vectors: [\n\n      [0.9, 0.1, 0.1],\n\n      [0.1, 0.9, 0.1],\n\n      [0.1, 0.1, 0.9],\n\n    ],\n\n  },\n\n});\n\n```\n\n\n\n\n\nor record-oriented equivalent:\n\n\n\n```http\n\nPUT /collections/{collection_name}/points\n\n{', '{\n\n    ""payload"": {\n\n        ""property1"": ""string"",\n\n        ""property2"": ""string""\n\n    },\n\n    ""points"": [\n\n        0, 3, 100\n\n    ]\n\n}\n\n```\n\n\n\n```python\n\nclient.set_payload(\n\n    collection_name=""{collection_name}"",\n\n    payload={\n\n        ""property1"": ""string"",\n\n        ""property2"": ""string"",\n\n    },\n\n    points=[0, 3, 10],\n\n)\n\n```\n\n\n\n```typescript\n\nclient.setPayload(""{collection_name}"", {\n\n  payload: {\n\n    property1: ""string"",\n\n    property2: ""string"",\n\n  },\n\n  points: [0, 3, 10],\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::qdrant::{\n\n    points_selector::PointsSelectorOneOf, PointsIdsList, PointsSelector,\n\n};\n\nuse serde_json::json;\n\n\n\nclient\n\n    .set_payload_blocking(\n\n        ""{collection_name}"",\n\n        None,\n\n        &PointsSelector {\n\n            points_selector_one_of: Some(PointsSelectorOneOf::Points(PointsIdsList {\n\n                ids: vec![0.into(), 3.into(), 10.into()],\n\n            })),\n\n        },\n\n        json!({\n\n            ""property1"": ""string"",\n\n            ""property2"": ""string"",']","{

                        ""id"": 1,

                        ""vector"": [1.0, 2.0, 3.0, 4.0],

                        ""payload"": {}

                    }

                ]

            }

        },

        {

            ""update_vectors"": {

                ""points"": [

                    {

                        ""id"": 1,

                        ""vector"": [1.0, 2.0, 3.0, 4.0]

                    }

                ]

            }

        },

        {

            ""delete_vectors"": {

                ""points"": [1],

                ""vector"": [""""]

            }

        },

        {

            ""overwrite_payload"": {

                ""payload"": {

                    ""test_payload"": ""1""

                },

                ""points"": [1]

            }

        },

        {

            ""set_payload"": {

                ""payload"": {

                    ""test_payload_2"": ""2"",

                    ""test_payload_3"": ""3""

                },

                ""points"": [1]
""payloads"": [

            {""color"": ""red""},

            {""color"": ""green""},

            {""color"": ""blue""}

        ],

        ""vectors"": [

            [0.9, 0.1, 0.1],

            [0.1, 0.9, 0.1],

            [0.1, 0.1, 0.9]

        ]

    }

}

```



```python

client.upsert(

    collection_name=""{collection_name}"",

    points=models.Batch(

        ids=[1, 2, 3],

        payloads=[

            {""color"": ""red""},

            {""color"": ""green""},

            {""color"": ""blue""},

        ],

        vectors=[

            [0.9, 0.1, 0.1],

            [0.1, 0.9, 0.1],

            [0.1, 0.1, 0.9],

        ],

    ),

)

```



```typescript

client.upsert(""{collection_name}"", {

  batch: {

    ids: [1, 2, 3],

    payloads: [{ color: ""red"" }, { color: ""green"" }, { color: ""blue"" }],

    vectors: [

      [0.9, 0.1, 0.1],

      [0.1, 0.9, 0.1],

      [0.1, 0.1, 0.9],

    ],

  },

});

```





or record-oriented equivalent:



```http

PUT /collections/{collection_name}/points

{
{

    ""payload"": {

        ""property1"": ""string"",

        ""property2"": ""string""

    },

    ""points"": [

        0, 3, 100

    ]

}

```



```python

client.set_payload(

    collection_name=""{collection_name}"",

    payload={

        ""property1"": ""string"",

        ""property2"": ""string"",

    },

    points=[0, 3, 10],

)

```



```typescript

client.setPayload(""{collection_name}"", {

  payload: {

    property1: ""string"",

    property2: ""string"",

  },

  points: [0, 3, 10],

});

```



```rust

use qdrant_client::qdrant::{

    points_selector::PointsSelectorOneOf, PointsIdsList, PointsSelector,

};

use serde_json::json;



client

    .set_payload_blocking(

        ""{collection_name}"",

        None,

        &PointsSelector {

            points_selector_one_of: Some(PointsSelectorOneOf::Points(PointsIdsList {

                ids: vec![0.into(), 3.into(), 10.into()],

            })),

        },

        json!({

            ""property1"": ""string"",

            ""property2"": ""string"","
What is the purpose of shard replication in Qdrant?,"Shard replication in Qdrant increases the reliability of the cluster by keeping several copies of a shard spread across the cluster. This ensures the availability of the data in case of node failures, except if all replicas are lost.","['immediately available. This way, Qdrant ensures that there will be no\n\ndegradation in performance at the end of the transfer. Especially on large\n\nshards, this can give a huge performance improvement. 2. The consistency and\n\nordering guarantees can be `strong`[^ordered], required for some applications.\n\n\n\nThe `stream_records` method is currently used as default. This may change in the\n\nfuture.\n\n\n\n## Replication\n\n\n\n*Available as of v0.11.0*\n\n\n\nQdrant allows you to replicate shards between nodes in the cluster.\n\n\n\nShard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.\n\nThis ensures the availability of the data in case of node failures, except if all replicas are lost.\n\n\n\n### Replication factor', '},\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n\n    shard_number=6,\n\n    replication_factor=2,\n\n    write_consistency_factor=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createCollection(""{collection_name}"", {\n\n  vectors: {\n\n    size: 300,\n\n    distance: ""Cosine"",\n\n  },\n\n  shard_number: 6,\n\n  replication_factor: 2,\n\n  write_consistency_factor: 2,\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{vectors_config::Config, CreateCollection, Distance, VectorParams, VectorsConfig},\n\n};', '.setSize(300)\n\n                            .setDistance(Distance.Cosine)\n\n                            .build())\n\n                    .build())\n\n            .setShardNumber(6)\n\n            .setReplicationFactor(2)\n\n            .build())\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\n\tshardNumber: 6,\n\n\treplicationFactor: 2\n\n);\n\n```\n\n\n\nThis code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.\n\n\n\nSince a replication factor of ""2"" would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.\n\n\n\n### Creating new shard replicas']","immediately available. This way, Qdrant ensures that there will be no

degradation in performance at the end of the transfer. Especially on large

shards, this can give a huge performance improvement. 2. The consistency and

ordering guarantees can be `strong`[^ordered], required for some applications.



The `stream_records` method is currently used as default. This may change in the

future.



## Replication



*Available as of v0.11.0*



Qdrant allows you to replicate shards between nodes in the cluster.



Shard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.

This ensures the availability of the data in case of node failures, except if all replicas are lost.



### Replication factor
},

    ""shard_number"": 6,

    ""replication_factor"": 2,

    ""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

    shard_number=6,

    replication_factor=2,

    write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createCollection(""{collection_name}"", {

  vectors: {

    size: 300,

    distance: ""Cosine"",

  },

  shard_number: 6,

  replication_factor: 2,

  write_consistency_factor: 2,

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{vectors_config::Config, CreateCollection, Distance, VectorParams, VectorsConfig},

};
.setSize(300)

                            .setDistance(Distance.Cosine)

                            .build())

                    .build())

            .setShardNumber(6)

            .setReplicationFactor(2)

            .build())

    .get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

	shardNumber: 6,

	replicationFactor: 2

);

```



This code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.



Since a replication factor of ""2"" would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.



### Creating new shard replicas"
What are the steps to set up product quantization in QdrantClient?,"To set up product quantization in QdrantClient, you need to specify the quantization parameters in the quantization_config section of the collection configuration. The compression ratio can be set to x16 and the always_ram parameter can be set to true to store quantized vectors in RAM. The vectors_config section should also be specified with the desired vector size and distance metric. Finally, you can use the create_collection method of the QdrantClient to create the collection with the specified configurations.","['{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),\n\n    quantization_config=models.ScalarQuantization(\n\n        scalar=models.ScalarQuantizationConfig(\n\n            type=models.ScalarType.INT8,\n\n            always_ram=True,\n\n        ),\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });', 'quantization_config=models.ScalarQuantization(\n\n        scalar=models.ScalarQuantizationConfig(\n\n            type=models.ScalarType.INT8,\n\n            always_ram=False,\n\n        ),\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createCollection(""{collection_name}"", {\n\n  vectors: {\n\n    size: 768,\n\n    distance: ""Cosine"",\n\n  },\n\n  optimizers_config: {\n\n    memmap_threshold: 20000,\n\n  },\n\n  quantization_config: {\n\n    scalar: {\n\n      type: ""int8"",\n\n      always_ram: false,\n\n    },\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,\n\n        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,\n\n        VectorParams, VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient', 'using Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },\n\n\tquantizationConfig: new QuantizationConfig\n\n\t{\n\n\t\tScalar = new ScalarQuantization\n\n\t\t{\n\n\t\t\tType = QuantizationType.Int8,\n\n\t\t\tQuantile = 0.99f,\n\n\t\t\tAlwaysRam = true\n\n\t\t}\n\n\t}\n\n);\n\n```\n\n\n\nThere are 3 parameters that you can specify in the `quantization_config` section:\n\n\n\n`type` - the type of the quantized vector components. Currently, Qdrant supports only `int8`.\n\n\n\n`quantile` - the quantile of the quantized vector components.\n\nThe quantile is used to calculate the quantization bounds.\n\nFor instance, if you specify `0.99` as the quantile, 1% of extreme values will be excluded from the quantization bounds.\n\n\n\nUsing quantiles lower than `1.0` might be useful if there are outliers in your vector components.']","{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": true

        }

    }

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),

    quantization_config=models.ScalarQuantization(

        scalar=models.ScalarQuantizationConfig(

            type=models.ScalarType.INT8,

            always_ram=True,

        ),

    ),

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });
quantization_config=models.ScalarQuantization(

        scalar=models.ScalarQuantizationConfig(

            type=models.ScalarType.INT8,

            always_ram=False,

        ),

    ),

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createCollection(""{collection_name}"", {

  vectors: {

    size: 768,

    distance: ""Cosine"",

  },

  optimizers_config: {

    memmap_threshold: 20000,

  },

  quantization_config: {

    scalar: {

      type: ""int8"",

      always_ram: false,

    },

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,

        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,

        VectorParams, VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client
using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },

	quantizationConfig: new QuantizationConfig

	{

		Scalar = new ScalarQuantization

		{

			Type = QuantizationType.Int8,

			Quantile = 0.99f,

			AlwaysRam = true

		}

	}

);

```



There are 3 parameters that you can specify in the `quantization_config` section:



`type` - the type of the quantized vector components. Currently, Qdrant supports only `int8`.



`quantile` - the quantile of the quantized vector components.

The quantile is used to calculate the quantization bounds.

For instance, if you specify `0.99` as the quantile, 1% of extreme values will be excluded from the quantization bounds.



Using quantiles lower than `1.0` might be useful if there are outliers in your vector components."
What are the different write ordering options available in Qdrant?,"Qdrant provides three different write ordering options: weak, medium, and strong. Weak ordering does not provide any additional guarantees and allows write operations to be freely reordered. Medium ordering serializes write operations through a dynamically elected leader, which may cause minor inconsistencies in case of leader change. Strong ordering serializes write operations through the permanent leader, providing strong consistency but may result in write operations being unavailable if the leader is down.","['location=""http://localhost:6333"",\n\n            collection_name=""test"",\n\n        ),\n\n        write_config=QdrantWriteConfig(batch_size=80),\n\n    )\n\n\n\nif __name__ == ""__main__"":\n\n    writer = get_writer()\n\n    runner = LocalRunner(\n\n        processor_config=ProcessorConfig(\n\n            verbose=True,\n\n            output_dir=""local-output-to-qdrant"",\n\n            num_processes=2,\n\n        ),\n\n        connector_config=SimpleLocalConfig(\n\n            input_path=""example-docs/book-war-and-peace-1225p.txt"",\n\n        ),\n\n        read_config=ReadConfig(),\n\n        partition_config=PartitionConfig(),\n\n        chunking_config=ChunkingConfig(chunk_elements=True),\n\n        embedding_config=EmbeddingConfig(provider=""langchain-huggingface""),\n\n        writer=writer,\n\n        writer_kwargs={},\n\n    )\n\n    runner.run()\n\n```\n\n\n\n## Next steps\n\n\n\n- Unstructured API [reference](https://unstructured-io.github.io/unstructured/api.html).', '- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if read operations are more frequent than update and if search performance is critical.\n\n\n\n\n\n### Write consistency factor\n\n\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\n\nIt can be configured at the collection\'s creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python', 'Qdrant provides a few options to control consistency guarantees:\n\n\n\n- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.\n\n- Read `consistency` param, can be used with search and retrieve operations to ensure that the results obtained from all replicas are the same. If this option is used, Qdrant will perform the read operation on multiple replicas and resolve the result according to the selected strategy. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if the update operations are frequent and the number of replicas is low.']","location=""http://localhost:6333"",

            collection_name=""test"",

        ),

        write_config=QdrantWriteConfig(batch_size=80),

    )



if __name__ == ""__main__"":

    writer = get_writer()

    runner = LocalRunner(

        processor_config=ProcessorConfig(

            verbose=True,

            output_dir=""local-output-to-qdrant"",

            num_processes=2,

        ),

        connector_config=SimpleLocalConfig(

            input_path=""example-docs/book-war-and-peace-1225p.txt"",

        ),

        read_config=ReadConfig(),

        partition_config=PartitionConfig(),

        chunking_config=ChunkingConfig(chunk_elements=True),

        embedding_config=EmbeddingConfig(provider=""langchain-huggingface""),

        writer=writer,

        writer_kwargs={},

    )

    runner.run()

```



## Next steps



- Unstructured API [reference](https://unstructured-io.github.io/unstructured/api.html).
- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if read operations are more frequent than update and if search performance is critical.





### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

    ""write_consistency_factor"": 2,

}

```



```python
Qdrant provides a few options to control consistency guarantees:



- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.

- Read `consistency` param, can be used with search and retrieve operations to ensure that the results obtained from all replicas are the same. If this option is used, Qdrant will perform the read operation on multiple replicas and resolve the result according to the selected strategy. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents. This options is preferred if the update operations are frequent and the number of replicas is low."
How can Qdrant be optimized for low memory footprint and high speed search?,"To optimize Qdrant for low memory footprint and high speed search, you can use vector quantization. This involves converting vectors into a more compact representation that can be stored in memory and used for search. By using smaller vectors, you can cache more in RAM and reduce the number of disk reads. To configure in-memory quantization with on-disk original vectors, you need to create a collection with the specified configuration.","['It provides configurable trade-offs between RAM usage and search speed.\n\n\n\nWe are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.\n\nPlease feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!', ""---\n\ntitle: Optimize Resources\n\nweight: 11\n\naliases:\n\n  - ../tutorials/optimize\n\n---\n\n\n\n# Optimize Qdrant\n\n\n\nDifferent use cases have different requirements for balancing between memory, speed, and precision.\n\nQdrant is designed to be flexible and customizable so you can tune it to your needs.\n\n\n\n![Trafeoff](/docs/tradeoff.png)\n\n\n\nLet's look deeper into each of those possible optimization scenarios.\n\n\n\n## Prefer low memory footprint with high speed search\n\n\n\nThe main way to achieve high speed search with low memory footprint is to keep vectors on disk while at the same time minimizing the number of disk reads.\n\n\n\nVector quantization is one way to achieve this. Quantization converts vectors into a more compact representation, which can be stored in memory and used for search. With smaller vectors you can cache more in RAM and reduce the number of disk reads.\n\n\n\nTo configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration:\n\n\n\n```http"", ""```\n\n\n\nLet's see how these results translate into search speed:\n\n\n\n| Memory | RPS with IOPS=63.2k | RPS with IOPS=183k |\n\n|--------|---------------------|--------------------|\n\n| 600mb  | 5                   | 50                 |\n\n| 300mb  | 0.9                 | 13                 |\n\n| 200mb  | 0.5                 | 8                  |\n\n| 150mb  | 0.4                 | 7                  |\n\n\n\n\n\nAs you can see, the speed of the disk has a significant impact on the search speed.\n\nWith a local SSD, we were able to increase the search speed by 10x!\n\n\n\nWith the production-grade disk, the search speed could be even higher. \n\nSome configurations of the SSDs can reach 1M IOPS and more.\n\n\n\nWhich might be an interesting option to serve large datasets with low search latency in Qdrant.\n\n\n\n\n\n## Conclusion\n\n\n\nIn this article, we showed that Qdrant have flexibility in terms of RAM usage and can be used to serve large datasets.\n\nIt provides configurable trade-offs between RAM usage and search speed.""]","It provides configurable trade-offs between RAM usage and search speed.



We are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.

Please feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!
---

title: Optimize Resources

weight: 11

aliases:

  - ../tutorials/optimize

---



# Optimize Qdrant



Different use cases have different requirements for balancing between memory, speed, and precision.

Qdrant is designed to be flexible and customizable so you can tune it to your needs.



![Trafeoff](/docs/tradeoff.png)



Let's look deeper into each of those possible optimization scenarios.



## Prefer low memory footprint with high speed search



The main way to achieve high speed search with low memory footprint is to keep vectors on disk while at the same time minimizing the number of disk reads.



Vector quantization is one way to achieve this. Quantization converts vectors into a more compact representation, which can be stored in memory and used for search. With smaller vectors you can cache more in RAM and reduce the number of disk reads.



To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration:



```http
```



Let's see how these results translate into search speed:



| Memory | RPS with IOPS=63.2k | RPS with IOPS=183k |

|--------|---------------------|--------------------|

| 600mb  | 5                   | 50                 |

| 300mb  | 0.9                 | 13                 |

| 200mb  | 0.5                 | 8                  |

| 150mb  | 0.4                 | 7                  |





As you can see, the speed of the disk has a significant impact on the search speed.

With a local SSD, we were able to increase the search speed by 10x!



With the production-grade disk, the search speed could be even higher. 

Some configurations of the SSDs can reach 1M IOPS and more.



Which might be an interesting option to serve large datasets with low search latency in Qdrant.





## Conclusion



In this article, we showed that Qdrant have flexibility in terms of RAM usage and can be used to serve large datasets.

It provides configurable trade-offs between RAM usage and search speed."
How does Qdrant optimize memory and search speed for sparse vectors?,"Qdrant optimizes memory and search speed for sparse vectors by utilizing an inverted index structure to store vectors for each non-zero dimension. This approach allows Qdrant to efficiently represent sparse vectors, which are characterized by a high proportion of zeroes. By only storing information about non-zero dimensions, Qdrant reduces the memory footprint required to store sparse vectors and also speeds up search operations by focusing only on relevant dimensions during indexing and querying processes. This optimization ensures that Qdrant can handle sparse vectors effectively while maintaining efficient memory usage and search performance.","['In other words, Qdrant performs `float32 -> uint8` conversion for each vector component.\n\nEffectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.\n\n\n\nIn addition to reducing the memory footprint, scalar quantization also speeds up the search process.\n\nQdrant uses a special SIMD CPU instruction to perform fast vector comparison.\n\nThis instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.\n\n\n\nThe main drawback of scalar quantization is the loss of accuracy. The `float32 -> uint8` conversion introduces an error that can lead to a slight decrease in search quality.\n\nHowever, this error is usually negligible, and tends to be less significant for high-dimensional vectors.\n\nIn our experiments, we found that the error introduced by scalar quantization is usually less than 1%. \n\n\n\nHowever, this value depends on the data and the quantization parameters.', 'performance.\n\n\n\n## Sparse Vector Index\n\n\n\n*Available as of v1.7.0*\n\n\n\n### Key Features of Sparse Vector Index\n\n- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.\n\n- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.\n\n\n\n### Search Mechanism\n\n- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.\n\n- **Scoring Method:** Vectors are scored using the dot product.\n\n\n\n### Optimizations\n\n- **Reducing Vectors to Score:** Implementations are in place to minimize the number of vectors scored, especially for dimensions with numerous vectors.\n\n\n\n### Filtering and Configuration\n\n- **Filtering Support:** Similar to dense vectors, supports filtering by payload fields.\n\n- **`full_scan_threshold` Configuration:** Allows control over when to switch search from the payload index to minimize scoring vectors.', '""text"": models.SparseVector(\n\n                    indices=indices.tolist(), values=values.tolist()\n\n                )\n\n            },\n\n        )\n\n    ],\n\n)\n\n```\n\nBy upserting points with sparse vectors, we prepare our dataset for rapid first-stage retrieval, laying the groundwork for subsequent detailed analysis using dense vectors. Notice that we use ""text"" to denote the name of the sparse vector.\n\n\n\nThose familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you\'re able to **apply payload filters**, shard keys, and other advanced features you\'ve come to expect from Qdrant. To make things easier for you, the indices and values don\'t have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.\n\n\n\n### 4. Querying with Sparse Vectors']","In other words, Qdrant performs `float32 -> uint8` conversion for each vector component.

Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.



The main drawback of scalar quantization is the loss of accuracy. The `float32 -> uint8` conversion introduces an error that can lead to a slight decrease in search quality.

However, this error is usually negligible, and tends to be less significant for high-dimensional vectors.

In our experiments, we found that the error introduced by scalar quantization is usually less than 1%. 



However, this value depends on the data and the quantization parameters.
performance.



## Sparse Vector Index



*Available as of v1.7.0*



### Key Features of Sparse Vector Index

- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.

- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.



### Search Mechanism

- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.

- **Scoring Method:** Vectors are scored using the dot product.



### Optimizations

- **Reducing Vectors to Score:** Implementations are in place to minimize the number of vectors scored, especially for dimensions with numerous vectors.



### Filtering and Configuration

- **Filtering Support:** Similar to dense vectors, supports filtering by payload fields.

- **`full_scan_threshold` Configuration:** Allows control over when to switch search from the payload index to minimize scoring vectors.
""text"": models.SparseVector(

                    indices=indices.tolist(), values=values.tolist()

                )

            },

        )

    ],

)

```

By upserting points with sparse vectors, we prepare our dataset for rapid first-stage retrieval, laying the groundwork for subsequent detailed analysis using dense vectors. Notice that we use ""text"" to denote the name of the sparse vector.



Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.



### 4. Querying with Sparse Vectors"
How does Qdrant address the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters?,"Qdrant addresses the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters by extending the HNSW graph with additional edges based on the stored payload values. This extension allows for more efficient searching of nearby vectors using the HNSW index while applying filters during the search in the graph. By adding extra edges, Qdrant minimizes the overhead on condition checks since the conditions only need to be calculated for a small fraction of the points involved in the search. This approach improves the performance of the HNSW index in cases where the traditional methods of weak filters or complete rescore are not optimal, providing a more balanced solution for scenarios that fall in between.","[""Demetrios:\n\nAll right, cool. Well, that sets the scene for us. Now, I feel like you brought some slides along. Feel free to share those whenever you want. I'm going to fire away the first question and ask about this. I'm going to go straight into Qdrant questions and ask you to elaborate on how the unique modification of Qdrant of the HNSW algorithm benefits your solution. So what are you doing there? How are you leveraging that? And how also to add another layer to this question, this ridiculously long question that I'm starting to get myself into, how do you handle geo filtering based on longitude and latitude? So, to summarize my lengthy question, let's just start with the HNSW algorithm. How does that benefit your solution?\n\n\n\nRishabh Bhardwaj:"", 'Rishabh Bhardwaj:\n\nNo, not at all.\n\n\n\nDemetrios:\n\nAll right, keep going. I like it.\n\n\n\nRishabh Bhardwaj:\n\nYeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.\n\n\n\nDemetrios:\n\nRight.\n\n\n\nRishabh Bhardwaj:\n\nSo also the other thing is, Qdrant also provides the functionality to specify those parameters while making the search as well. So it does not mean if we build the index initially, we only have to use those specifications. We can again specify them during the search as well.\n\n\n\nDemetrios:\n\nOkay.\n\n\n\nRishabh Bhardwaj:', 'Separately, payload index and vector index cannot solve the problem of search using the filter completely.\n\n\n\nIn the case of weak filters, you can use the HNSW index as it is. In the case of stringent filters, you can use the payload index and complete rescore.\n\nHowever, for cases in the middle, this approach does not work well.\n\n\n\nOn the one hand, we cannot apply a full scan on too many vectors. On the other hand, the HNSW graph starts to fall apart when using too strict filters.\n\n\n\n![HNSW fail](/docs/precision_by_m.png)\n\n\n\n![hnsw graph](/docs/graph.gif)\n\n\n\nYou can find more information on why this happens in our [blog post](https://blog.vasnetsov.com/posts/categorical-hnsw/).\n\nQdrant solves this problem by extending the HNSW graph with additional edges based on the stored payload values.\n\n\n\nExtra edges allow you to efficiently search for nearby vectors using the HNSW index and apply filters as you search in the graph.']","Demetrios:

All right, cool. Well, that sets the scene for us. Now, I feel like you brought some slides along. Feel free to share those whenever you want. I'm going to fire away the first question and ask about this. I'm going to go straight into Qdrant questions and ask you to elaborate on how the unique modification of Qdrant of the HNSW algorithm benefits your solution. So what are you doing there? How are you leveraging that? And how also to add another layer to this question, this ridiculously long question that I'm starting to get myself into, how do you handle geo filtering based on longitude and latitude? So, to summarize my lengthy question, let's just start with the HNSW algorithm. How does that benefit your solution?



Rishabh Bhardwaj:
Rishabh Bhardwaj:

No, not at all.



Demetrios:

All right, keep going. I like it.



Rishabh Bhardwaj:

Yeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.



Demetrios:

Right.



Rishabh Bhardwaj:

So also the other thing is, Qdrant also provides the functionality to specify those parameters while making the search as well. So it does not mean if we build the index initially, we only have to use those specifications. We can again specify them during the search as well.



Demetrios:

Okay.



Rishabh Bhardwaj:
Separately, payload index and vector index cannot solve the problem of search using the filter completely.



In the case of weak filters, you can use the HNSW index as it is. In the case of stringent filters, you can use the payload index and complete rescore.

However, for cases in the middle, this approach does not work well.



On the one hand, we cannot apply a full scan on too many vectors. On the other hand, the HNSW graph starts to fall apart when using too strict filters.



![HNSW fail](/docs/precision_by_m.png)



![hnsw graph](/docs/graph.gif)



You can find more information on why this happens in our [blog post](https://blog.vasnetsov.com/posts/categorical-hnsw/).

Qdrant solves this problem by extending the HNSW graph with additional edges based on the stored payload values.



Extra edges allow you to efficiently search for nearby vectors using the HNSW index and apply filters as you search in the graph."
How is metric learning utilized in addressing the challenge of detecting anomalies in coffee beans?,"Metric learning is utilized in addressing the challenge of detecting anomalies in coffee beans by encoding images in an n-dimensional vector space and using learned similarities to label images during the inference process. This approach involves representing the images in a vector space where similar images are closer together in the space. By doing so, the model can effectively classify and detect anomalies in coffee beans based on the similarities learned during training. The KNN (K-Nearest Neighbors) classification method is commonly used in this approach as it simplifies the process of determining the class label of an image based on the labels of its nearest neighbors in the vector space. This method allows for the adaptation to new types of defects and changing shooting conditions, making the model more robust and accurate in detecting anomalies in coffee beans.","['In this post, we will detail the lessons learned from such a use case.\n\n\n\n## Coffee Beans\n\n\n\n[Agrivero.ai](https://agrivero.ai/) - is a company making AI-enabled solution for quality control & traceability of green coffee for producers, traders, and roasters.\n\nThey have collected and labeled more than **30 thousand** images of coffee beans with various defects - wet, broken, chipped, or bug-infested samples.\n\nThis data is used to train a classifier that evaluates crop quality and highlights possible problems.\n\n\n\n{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}\n\n\n\nWe should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it\'s own.\n\nIn the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.\n\n\n\nLet\'s find out how metric learning might help to address this challenge.\n\n\n\n## Metric Learning Approach', '---\n\ntitle: Metric Learning for Anomaly Detection\n\nshort_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""\n\ndescription: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.\n\nsocial_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg\n\npreview_dir: /articles_data/detecting-coffee-anomalies/preview\n\nsmall_preview_image: /articles_data/detecting-coffee-anomalies/anomalies_icon.svg\n\nweight: 30\n\nauthor: Yusuf Sarıgöz\n\nauthor_link: https://medium.com/@yusufsarigoz\n\ndate: 2022-05-04T13:00:00+03:00\n\ndraft: false\n\n# aliases: [ /articles/detecting-coffee-anomalies/ ]\n\n---\n\n\n\nAnomaly detection is a thirsting yet challenging task that has numerous use cases across various industries.\n\nThe complexity results mainly from the fact that the task is data-scarce by definition.', '{{< figure src=/articles_data/detecting-coffee-anomalies/ft_report_knn.png caption=""Metrics for the finetuned model with KNN classifier"" >}}\n\n\n\nWe repeated this experiment with 500 and 2000 samples, but it showed only a slight improvement.\n\nThus we decided to stick to 200 samples - see below for why.\n\n\n\n## Supervised Classification Approach\n\nWe also wanted to compare our results with the metrics of a traditional supervised classification model.\n\nFor this purpose, a Resnet50 model was finetuned with ~30k labeled images, made available for training.\n\nSurprisingly, the F1 score was around ~0.86.\n\n\n\nPlease note that we used only 200 labeled samples in the metric learning approach instead of ~30k in the supervised classification approach.\n\nThese numbers indicate a huge saving with no considerable compromise in the performance.\n\n\n\n## Conclusion\n\nWe obtained results comparable to those of the supervised classification method by using **only 0.66%** of the labeled data with metric learning.']","In this post, we will detail the lessons learned from such a use case.



## Coffee Beans



[Agrivero.ai](https://agrivero.ai/) - is a company making AI-enabled solution for quality control & traceability of green coffee for producers, traders, and roasters.

They have collected and labeled more than **30 thousand** images of coffee beans with various defects - wet, broken, chipped, or bug-infested samples.

This data is used to train a classifier that evaluates crop quality and highlights possible problems.



{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}



We should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it's own.

In the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.



Let's find out how metric learning might help to address this challenge.



## Metric Learning Approach
---

title: Metric Learning for Anomaly Detection

short_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""

description: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.

social_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg

preview_dir: /articles_data/detecting-coffee-anomalies/preview

small_preview_image: /articles_data/detecting-coffee-anomalies/anomalies_icon.svg

weight: 30

author: Yusuf Sarıgöz

author_link: https://medium.com/@yusufsarigoz

date: 2022-05-04T13:00:00+03:00

draft: false

# aliases: [ /articles/detecting-coffee-anomalies/ ]

---



Anomaly detection is a thirsting yet challenging task that has numerous use cases across various industries.

The complexity results mainly from the fact that the task is data-scarce by definition.
{{< figure src=/articles_data/detecting-coffee-anomalies/ft_report_knn.png caption=""Metrics for the finetuned model with KNN classifier"" >}}



We repeated this experiment with 500 and 2000 samples, but it showed only a slight improvement.

Thus we decided to stick to 200 samples - see below for why.



## Supervised Classification Approach

We also wanted to compare our results with the metrics of a traditional supervised classification model.

For this purpose, a Resnet50 model was finetuned with ~30k labeled images, made available for training.

Surprisingly, the F1 score was around ~0.86.



Please note that we used only 200 labeled samples in the metric learning approach instead of ~30k in the supervised classification approach.

These numbers indicate a huge saving with no considerable compromise in the performance.



## Conclusion

We obtained results comparable to those of the supervised classification method by using **only 0.66%** of the labeled data with metric learning."
How can one reproduce the benchmark for Open Source vector databases?,"To reproduce the benchmark for Open Source vector databases, one can access the source code available on Github at https://github.com/qdrant/vector-db-benchmark. The repository contains a `README.md` file that provides detailed instructions on how to run the benchmark for a specific engine. By following the guidelines outlined in the README file, individuals can easily replicate the benchmarks conducted on the Open Source vector databases. Additionally, the document emphasizes the importance of transparency in the benchmarking process and invites contributions from individuals who may be able to provide insights, identify misconfigurations, or suggest improvements. Interested parties can contribute to the benchmarking efforts by accessing the benchmark repository at https://github.com/qdrant/vector-db-benchmark.","['## What about closed-source SaaS platforms?\n\n\n\nThere are some vector databases available as SaaS only so that we couldn’t test them on the same machine as the rest of the systems.\n\nThat makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.\n\n\n\nThis is not the final list, and we’ll continue benchmarking as many different engines as possible.\n\n\n\n## How to reproduce the benchmark?\n\n\n\nThe source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.\n\n\n\n## How to contribute?\n\n\n\nWe made the benchmark Open Source because we believe that it has to be transparent. We could have misconfigured one of the engines or just done it inefficiently. If you feel like you could help us out, check out our [benchmark repository](https://github.com/qdrant/vector-db-benchmark).', ""If you're interested in testing the benchmark yourself or want to contribute to its development, head over to our [benchmark repository](https://github.com/qdrant/vector-db-benchmark). We appreciate your support and involvement in improving the performance of vector databases."", '---\n\ntitle: Vector Database Benchmarks\n\ndescription: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.\n\nkeywords:\n\n  - vector databases comparative benchmark\n\n  - ANN Benchmark\n\n  - Qdrant vs Milvus\n\n  - Qdrant vs Weaviate\n\n  - Qdrant vs Redis\n\n  - Qdrant vs ElasticSearch\n\n  - benchmark\n\n  - performance\n\n  - latency\n\n  - RPS\n\n  - comparison\n\n  - vector search\n\n  - embedding\n\npreview_image: /benchmarks/benchmark-1.png\n\nseo_schema: { ""@context"": ""https://schema.org"", ""@type"": ""Article"", ""headline"": ""Vector Search Comparative Benchmarks"", ""image"": [ ""https://qdrant.tech/benchmarks/benchmark-1.png"" ], ""abstract"": ""The first comparative benchmark and benchmarking framework for vector search engines"", ""datePublished"": ""2022-08-23"", ""dateModified"": ""2022-08-23"", ""author"": [{ ""@type"": ""Organization"", ""name"": ""Qdrant"", ""url"": ""https://qdrant.tech"" }] }\n\n \n\n---']","## What about closed-source SaaS platforms?



There are some vector databases available as SaaS only so that we couldn’t test them on the same machine as the rest of the systems.

That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.



This is not the final list, and we’ll continue benchmarking as many different engines as possible.



## How to reproduce the benchmark?



The source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.



## How to contribute?



We made the benchmark Open Source because we believe that it has to be transparent. We could have misconfigured one of the engines or just done it inefficiently. If you feel like you could help us out, check out our [benchmark repository](https://github.com/qdrant/vector-db-benchmark).
If you're interested in testing the benchmark yourself or want to contribute to its development, head over to our [benchmark repository](https://github.com/qdrant/vector-db-benchmark). We appreciate your support and involvement in improving the performance of vector databases.
---

title: Vector Database Benchmarks

description: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.

keywords:

  - vector databases comparative benchmark

  - ANN Benchmark

  - Qdrant vs Milvus

  - Qdrant vs Weaviate

  - Qdrant vs Redis

  - Qdrant vs ElasticSearch

  - benchmark

  - performance

  - latency

  - RPS

  - comparison

  - vector search

  - embedding

preview_image: /benchmarks/benchmark-1.png

seo_schema: { ""@context"": ""https://schema.org"", ""@type"": ""Article"", ""headline"": ""Vector Search Comparative Benchmarks"", ""image"": [ ""https://qdrant.tech/benchmarks/benchmark-1.png"" ], ""abstract"": ""The first comparative benchmark and benchmarking framework for vector search engines"", ""datePublished"": ""2022-08-23"", ""dateModified"": ""2022-08-23"", ""author"": [{ ""@type"": ""Organization"", ""name"": ""Qdrant"", ""url"": ""https://qdrant.tech"" }] }

 

---"
What is the purpose of the `ordering` parameter in Qdrant and when should it be used?,"The `ordering` parameter in Qdrant is used with update and delete operations to ensure that the operations are executed in the same order on all replicas. When this option is enabled, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This is beneficial in preventing data inconsistency that may arise from concurrent updates of the same documents. The `ordering` parameter is particularly recommended when read operations are more frequent than updates and when search performance is critical. By enforcing a specific order of operations across replicas, the `ordering` parameter helps maintain data consistency and integrity in distributed environments.","['.build())\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.UpsertAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tpoints: new List<PointStruct>\n\n\t{\n\n\t\tnew()\n\n\t\t{\n\n\t\t\tId = 1,\n\n\t\t\tVectors = new[] { 0.9f, 0.1f, 0.1f },\n\n\t\t\tPayload = { [""city""] = ""red"" }\n\n\t\t},\n\n\t\tnew()\n\n\t\t{\n\n\t\t\tId = 2,\n\n\t\t\tVectors = new[] { 0.1f, 0.9f, 0.1f },\n\n\t\t\tPayload = { [""city""] = ""green"" }\n\n\t\t},\n\n\t\tnew()\n\n\t\t{\n\n\t\t\tId = 3,\n\n\t\t\tVectors = new[] { 0.1f, 0.1f, 0.9f },\n\n\t\t\tPayload = { [""city""] = ""blue"" }\n\n\t\t}\n\n\t},\n\n\tordering: WriteOrderingType.Strong\n\n);\n\n```\n\n\n\n## Listener mode\n\n\n\n<aside role=""alert"">This is an experimental feature, its behavior may change in the future.</aside>\n\n\n\nIn some cases it might be useful to have a Qdrant node that only accumulates data and does not participate in search operations.\n\nThere are several scenarios where this can be useful:', '```http\n\nPOST /collections/{collection_name}/points/recommend\n\n{\n\n  ""positive"": [100, 231],\n\n  ""negative"": [718, [0.2, 0.3, 0.4, 0.5]],\n\n  ""filter"": {\n\n        ""must"": [\n\n            {\n\n                ""key"": ""city"",\n\n                ""match"": {\n\n                    ""value"": ""London""\n\n                }\n\n            }\n\n        ]\n\n  },\n\n  ""strategy"": ""average_vector"",\n\n  ""limit"": 3\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.recommend(\n\n    collection_name=""{collection_name}"",\n\n    positive=[100, 231],\n\n    negative=[718, [0.2, 0.3, 0.4, 0.5]],\n\n    strategy=models.RecommendStrategy.AVERAGE_VECTOR,\n\n    query_filter=models.Filter(\n\n        must=[\n\n            models.FieldCondition(\n\n                key=""city"",\n\n                match=models.MatchValue(\n\n                    value=""London"",\n\n                ),\n\n            )\n\n        ]\n\n    ),\n\n    limit=3,\n\n)\n\n```\n\n\n\n```typescript', '{ ""id"": 11, ""score"": 0.73 }\n\n    ],\n\n    [\n\n        { ""id"": 1, ""score"": 0.92 },\n\n        { ""id"": 3, ""score"": 0.89 },\n\n        { ""id"": 9, ""score"": 0.75 }\n\n    ]\n\n  ],\n\n  ""status"": ""ok"",\n\n  ""time"": 0.001\n\n}\n\n```\n\n\n\n## Pagination\n\n\n\n*Available as of v0.8.3*\n\n\n\nSearch and [recommendation](../explore/#recommendation-api) APIs allow to skip first results of the search and return only the result starting from some specified offset:\n\n\n\nExample:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{\n\n    ""vector"": [0.2, 0.1, 0.9, 0.7],\n\n    ""with_vectors"": true,\n\n    ""with_payload"": true,\n\n    ""limit"": 10,\n\n    ""offset"": 100\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.search(\n\n    collection_name=""{collection_name}"",\n\n    query_vector=[0.2, 0.1, 0.9, 0.7],\n\n    with_vectors=True,\n\n    with_payload=True,\n\n    limit=10,\n\n    offset=100,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";']",".build())

    .get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.UpsertAsync(

	collectionName: ""{collection_name}"",

	points: new List<PointStruct>

	{

		new()

		{

			Id = 1,

			Vectors = new[] { 0.9f, 0.1f, 0.1f },

			Payload = { [""city""] = ""red"" }

		},

		new()

		{

			Id = 2,

			Vectors = new[] { 0.1f, 0.9f, 0.1f },

			Payload = { [""city""] = ""green"" }

		},

		new()

		{

			Id = 3,

			Vectors = new[] { 0.1f, 0.1f, 0.9f },

			Payload = { [""city""] = ""blue"" }

		}

	},

	ordering: WriteOrderingType.Strong

);

```



## Listener mode



<aside role=""alert"">This is an experimental feature, its behavior may change in the future.</aside>



In some cases it might be useful to have a Qdrant node that only accumulates data and does not participate in search operations.

There are several scenarios where this can be useful:
```http

POST /collections/{collection_name}/points/recommend

{

  ""positive"": [100, 231],

  ""negative"": [718, [0.2, 0.3, 0.4, 0.5]],

  ""filter"": {

        ""must"": [

            {

                ""key"": ""city"",

                ""match"": {

                    ""value"": ""London""

                }

            }

        ]

  },

  ""strategy"": ""average_vector"",

  ""limit"": 3

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.recommend(

    collection_name=""{collection_name}"",

    positive=[100, 231],

    negative=[718, [0.2, 0.3, 0.4, 0.5]],

    strategy=models.RecommendStrategy.AVERAGE_VECTOR,

    query_filter=models.Filter(

        must=[

            models.FieldCondition(

                key=""city"",

                match=models.MatchValue(

                    value=""London"",

                ),

            )

        ]

    ),

    limit=3,

)

```



```typescript
{ ""id"": 11, ""score"": 0.73 }

    ],

    [

        { ""id"": 1, ""score"": 0.92 },

        { ""id"": 3, ""score"": 0.89 },

        { ""id"": 9, ""score"": 0.75 }

    ]

  ],

  ""status"": ""ok"",

  ""time"": 0.001

}

```



## Pagination



*Available as of v0.8.3*



Search and [recommendation](../explore/#recommendation-api) APIs allow to skip first results of the search and return only the result starting from some specified offset:



Example:



```http

POST /collections/{collection_name}/points/search

{

    ""vector"": [0.2, 0.1, 0.9, 0.7],

    ""with_vectors"": true,

    ""with_payload"": true,

    ""limit"": 10,

    ""offset"": 100

}

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.search(

    collection_name=""{collection_name}"",

    query_vector=[0.2, 0.1, 0.9, 0.7],

    with_vectors=True,

    with_payload=True,

    limit=10,

    offset=100,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";"
How can you enable API key based authentication in Qdrant instances for security purposes?,"To enable API key based authentication in Qdrant instances for security purposes, you must specify a key in the configuration. This can be done by adding the following code snippet to your configuration file:

```yaml
service:
  # Set an api-key.
  # If set, all requests must include a header with the api-key.
  # example header: `api-key: <API-KEY>`
  #
  # If you enable this you should also enable TLS.
  # (Either above or via an external service like nginx.)
  # Sending an api-key over an unencrypted channel is insecure.
  api_key: your_secret_api_key_here
```

By adding this configuration, all requests to your Qdrant instance must include a header with the specified API key. This simple form of client authentication helps secure your instance and is available starting from version 1.2.0","['---\n\ntitle: Security\n\nweight: 165\n\naliases:\n\n  - ../security\n\n---\n\n\n\n# Security\n\n\n\n\n\n\n\nPlease read this page carefully. Although there are various ways to secure your Qdrant instances, **they are unsecured by default**. \n\nYou need to enable security measures before production use. Otherwise, they are completely open to anyone\n\n\n\n## Authentication\n\n\n\n*Available as of v1.2.0*\n\n\n\nQdrant supports a simple form of client authentication using a static API key.\n\nThis can be used to secure your instance.\n\n\n\nTo enable API key based authentication in your own Qdrant instance you must\n\nspecify a key in the configuration:\n\n\n\n```yaml\n\nservice:\n\n  # Set an api-key.\n\n  # If set, all requests must include a header with the api-key.\n\n  # example header: `api-key: <API-KEY>`\n\n  #\n\n  # If you enable this you should also enable TLS.\n\n  # (Either above or via an external service like nginx.)\n\n  # Sending an api-key over an unencrypted channel is insecure.\n\n  api_key: your_secret_api_key_here\n\n```', 'Now that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.\n\nOur official Qdrant clients for Python, TypeScript, Go, Rust, .NET and Java all support the API key parameter. \n\n\n\n```bash\n\ncurl \\\n\n  -X GET https://xyz-example.eu-central.aws.cloud.qdrant.io:6333 \\\n\n  --header \'api-key: <provide-your-own-key>\'\n\n\n\n# Alternatively, you can use the `Authorization` header with the `Bearer` prefix\n\ncurl \\\n\n  -X GET https://xyz-example.eu-central.aws.cloud.qdrant.io:6333 \\\n\n  --header \'Authorization: Bearer <provide-your-own-key>\'\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nqdrant_client = QdrantClient(\n\n    ""xyz-example.eu-central.aws.cloud.qdrant.io"",\n\n    api_key=""<paste-your-api-key-here>"",\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({\n\n  host: ""xyz-example.eu-central.aws.cloud.qdrant.io"",\n\n  apiKey: ""<paste-your-api-key-here>"",\n\n});\n\n```\n\n\n\n```rust', '```bash\n\ncurl \\\n\n  -X GET \'https://xyz-example.eu-central.aws.cloud.qdrant.io:6333\' \\\n\n  --header \'api-key: <paste-your-api-key-here>\'\n\n```\n\nOpen Terminal and run the request. You should get a response that looks like this:\n\n\n\n```bash\n\n{""title"":""qdrant - vector search engine"",""version"":""1.4.1""}\n\n```\n\n> **Note:** The API key needs to be present in the request header every time you make a request via Rest or gRPC interface.\n\n\n\n## Step 3: Authenticate via SDK\n\n\n\nNow that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.\n\nOur official Qdrant clients for Python, TypeScript, Go, Rust, and .NET all support the API key parameter. \n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nqdrant_client = QdrantClient(\n\n    ""xyz-example.eu-central.aws.cloud.qdrant.io"",\n\n    api_key=""<paste-your-api-key-here>"",\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({']","---

title: Security

weight: 165

aliases:

  - ../security

---



# Security







Please read this page carefully. Although there are various ways to secure your Qdrant instances, **they are unsecured by default**. 

You need to enable security measures before production use. Otherwise, they are completely open to anyone



## Authentication



*Available as of v1.2.0*



Qdrant supports a simple form of client authentication using a static API key.

This can be used to secure your instance.



To enable API key based authentication in your own Qdrant instance you must

specify a key in the configuration:



```yaml

service:

  # Set an api-key.

  # If set, all requests must include a header with the api-key.

  # example header: `api-key: <API-KEY>`

  #

  # If you enable this you should also enable TLS.

  # (Either above or via an external service like nginx.)

  # Sending an api-key over an unencrypted channel is insecure.

  api_key: your_secret_api_key_here

```
Now that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.

Our official Qdrant clients for Python, TypeScript, Go, Rust, .NET and Java all support the API key parameter. 



```bash

curl \

  -X GET https://xyz-example.eu-central.aws.cloud.qdrant.io:6333 \

  --header 'api-key: <provide-your-own-key>'



# Alternatively, you can use the `Authorization` header with the `Bearer` prefix

curl \

  -X GET https://xyz-example.eu-central.aws.cloud.qdrant.io:6333 \

  --header 'Authorization: Bearer <provide-your-own-key>'

```



```python

from qdrant_client import QdrantClient



qdrant_client = QdrantClient(

    ""xyz-example.eu-central.aws.cloud.qdrant.io"",

    api_key=""<paste-your-api-key-here>"",

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({

  host: ""xyz-example.eu-central.aws.cloud.qdrant.io"",

  apiKey: ""<paste-your-api-key-here>"",

});

```



```rust
```bash

curl \

  -X GET 'https://xyz-example.eu-central.aws.cloud.qdrant.io:6333' \

  --header 'api-key: <paste-your-api-key-here>'

```

Open Terminal and run the request. You should get a response that looks like this:



```bash

{""title"":""qdrant - vector search engine"",""version"":""1.4.1""}

```

> **Note:** The API key needs to be present in the request header every time you make a request via Rest or gRPC interface.



## Step 3: Authenticate via SDK



Now that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.

Our official Qdrant clients for Python, TypeScript, Go, Rust, and .NET all support the API key parameter. 



```python

from qdrant_client import QdrantClient



qdrant_client = QdrantClient(

    ""xyz-example.eu-central.aws.cloud.qdrant.io"",

    api_key=""<paste-your-api-key-here>"",

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({"
