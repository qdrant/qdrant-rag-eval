success,failure_reason,Completion Time (ms),content.id,content.input_text,content.answer,content.completion,content.context,content.documents,content.formatted_content,content.raw_content.input_text,content.raw_content.answer,content.raw_content.documents,content.raw_content.context,rouge1_precision,rouge1_recall,rouge1_fmeasure,rouge2_precision,rouge2_recall,rouge2_fmeasure,rougeL_precision,rougeL_recall,ROUGE-L,rougeLsum_precision,rougeLsum_recall,rougeLsum_fmeasure,BERT Sentence Similarity,bert_score_precision,bert_score_recall,BERTScore,Faithfulness,Context Relevance,Completion Verbosity,Verbosity Ratio,Chunk Relevance
True,,3570.513,e9de2a33,What is vaccum optimizer ?,"The Vacuum Optimizer is a feature in the Qdrant database designed to manage and improve system performance by removing obsolete data. In Qdrant, when records are deleted, they are not immediately removed from the database. Instead, they are marked as deleted and subsequently ignored in future queries. This approach helps minimize disk access but results in accumulated deleted records that can occupy valuable memory and degrade performance over time.
The Vacuum Optimizer addresses this issue by systematically clearing out these marked-as-deleted records once they reach a certain threshold. This threshold is configurable and includes conditions such as the minimum fraction of deleted vectors in a segment and the minimum number of vectors present in a segment. ","In the `OptimizerConfig` section of the configuration file, several parameters related to segment optimization are defined. 

1. `deleted_threshold`: This parameter specifies the minimal fraction of deleted vectors in a segment required to perform segment optimization. In the example provided, the value is set to 0.2, indicating that if at least 20% of the vectors in a segment are deleted, optimization can be triggered for that segment.

2. `vacuum_min_vector_number`: This parameter sets the minimal number of vectors in a segment required to perform segment optimization. In the example, the value is set to 1000, meaning that a segment must contain at least 1000 vectors for optimization to be considered.

These parameters help in determining when a segment should undergo optimization based on the percentage of deleted vectors and the total number of vectors in the segment. By setting appropriate thresholds, the optimizer can effectively manage segment optimization to improve performance and efficiency.","The criteria for starting the optimizer are defined in the configuration file.



Here is an example of parameter values:



```yaml

storage:

  optimizers:

    # The minimal fraction of deleted vectors in a segment, required to perform segment optimization

    deleted_threshold: 0.2

    # The minimal number of vectors in a segment, required to perform segment optimization

    vacuum_min_vector_number: 1000

```



## Merge Optimizer



The service may require the creation of temporary segments.
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
config: Some(Config::Params(VectorParams {

                size: 768,

                distance: Distance::Cosine.into(),

                ..Default::default()

            })),

        }),

        optimizers_config: Some(OptimizersConfigDiff {

            memmap_threshold: Some(20000),

            ..Default::default()

        }),

        hnsw_config: Some(HnswConfigDiff {

            on_disk: Some(true),

            ..Default::default()

        }),

        ..Default::default()
default_segment_number=5,

        indexing_threshold=0,

    ),

    quantization_config=models.BinaryQuantization(

        binary=models.BinaryQuantizationConfig(always_ram=True),

    ),

)

```



#### What is happening in the OptimizerConfig? 



We're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded
#### Optimization with Web Workers","['The criteria for starting the optimizer are defined in the configuration file.\n\n\n\nHere is an example of parameter values:\n\n\n\n```yaml\n\nstorage:\n\n  optimizers:\n\n    # The minimal fraction of deleted vectors in a segment, required to perform segment optimization\n\n    deleted_threshold: 0.2\n\n    # The minimal number of vectors in a segment, required to perform segment optimization\n\n    vacuum_min_vector_number: 1000\n\n```\n\n\n\n## Merge Optimizer\n\n\n\nThe service may require the creation of temporary segments.', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', 'config: Some(Config::Params(VectorParams {\n\n                size: 768,\n\n                distance: Distance::Cosine.into(),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        optimizers_config: Some(OptimizersConfigDiff {\n\n            memmap_threshold: Some(20000),\n\n            ..Default::default()\n\n        }),\n\n        hnsw_config: Some(HnswConfigDiff {\n\n            on_disk: Some(true),\n\n            ..Default::default()\n\n        }),\n\n        ..Default::default()', ""default_segment_number=5,\n\n        indexing_threshold=0,\n\n    ),\n\n    quantization_config=models.BinaryQuantization(\n\n        binary=models.BinaryQuantizationConfig(always_ram=True),\n\n    ),\n\n)\n\n```\n\n\n\n#### What is happening in the OptimizerConfig? \n\n\n\nWe're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded"", '#### Optimization with Web Workers']","Question: What is vaccum optimizer ?\n\nContext: The criteria for starting the optimizer are defined in the configuration file.



Here is an example of parameter values:



```yaml

storage:

optimizers:

# The minimal fraction of deleted vectors in a segment, required to perform segment optimization

deleted_threshold: 0.2

# The minimal number of vectors in a segment, required to perform segment optimization

vacuum_min_vector_number: 1000

```



## Merge Optimizer



The service may require the creation of temporary segments.
""on_disk_payload"": false

},

""hnsw_config"": {

""m"": 16,

""ef_construct"": 100,

""full_scan_threshold"": 10000,

""max_indexing_threads"": 0

},

""optimizer_config"": {

""deleted_threshold"": 0.2,

""vacuum_min_vector_number"": 1000,

""default_segment_number"": 0,

""max_segment_size"": null,
config: Some(Config::Params(VectorParams {

size: 768,

distance: Distance::Cosine.into(),

..Default::default()

})),

}),

optimizers_config: Some(OptimizersConfigDiff {

memmap_threshold: Some(20000),

..Default::default()

}),

hnsw_config: Some(HnswConfigDiff {

on_disk: Some(true),

..Default::default()

}),

..Default::default()
default_segment_number=5,

indexing_threshold=0,

),

quantization_config=models.BinaryQuantization(

binary=models.BinaryQuantizationConfig(always_ram=True),

),

)

```



#### What is happening in the OptimizerConfig? 



We're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded
#### Optimization with Web Workers\n\nAnswer:",What is vaccum optimizer ?,"The Vacuum Optimizer is a feature in the Qdrant database designed to manage and improve system performance by removing obsolete data. In Qdrant, when records are deleted, they are not immediately removed from the database. Instead, they are marked as deleted and subsequently ignored in future queries. This approach helps minimize disk access but results in accumulated deleted records that can occupy valuable memory and degrade performance over time.
The Vacuum Optimizer addresses this issue by systematically clearing out these marked-as-deleted records once they reach a certain threshold. This threshold is configurable and includes conditions such as the minimum fraction of deleted vectors in a segment and the minimum number of vectors present in a segment. ","['The criteria for starting the optimizer are defined in the configuration file.\n\n\n\nHere is an example of parameter values:\n\n\n\n```yaml\n\nstorage:\n\n  optimizers:\n\n    # The minimal fraction of deleted vectors in a segment, required to perform segment optimization\n\n    deleted_threshold: 0.2\n\n    # The minimal number of vectors in a segment, required to perform segment optimization\n\n    vacuum_min_vector_number: 1000\n\n```\n\n\n\n## Merge Optimizer\n\n\n\nThe service may require the creation of temporary segments.', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', 'config: Some(Config::Params(VectorParams {\n\n                size: 768,\n\n                distance: Distance::Cosine.into(),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        optimizers_config: Some(OptimizersConfigDiff {\n\n            memmap_threshold: Some(20000),\n\n            ..Default::default()\n\n        }),\n\n        hnsw_config: Some(HnswConfigDiff {\n\n            on_disk: Some(true),\n\n            ..Default::default()\n\n        }),\n\n        ..Default::default()', ""default_segment_number=5,\n\n        indexing_threshold=0,\n\n    ),\n\n    quantization_config=models.BinaryQuantization(\n\n        binary=models.BinaryQuantizationConfig(always_ram=True),\n\n    ),\n\n)\n\n```\n\n\n\n#### What is happening in the OptimizerConfig? \n\n\n\nWe're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded"", '#### Optimization with Web Workers']","The criteria for starting the optimizer are defined in the configuration file.



Here is an example of parameter values:



```yaml

storage:

  optimizers:

    # The minimal fraction of deleted vectors in a segment, required to perform segment optimization

    deleted_threshold: 0.2

    # The minimal number of vectors in a segment, required to perform segment optimization

    vacuum_min_vector_number: 1000

```



## Merge Optimizer



The service may require the creation of temporary segments.
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
config: Some(Config::Params(VectorParams {

                size: 768,

                distance: Distance::Cosine.into(),

                ..Default::default()

            })),

        }),

        optimizers_config: Some(OptimizersConfigDiff {

            memmap_threshold: Some(20000),

            ..Default::default()

        }),

        hnsw_config: Some(HnswConfigDiff {

            on_disk: Some(true),

            ..Default::default()

        }),

        ..Default::default()
default_segment_number=5,

        indexing_threshold=0,

    ),

    quantization_config=models.BinaryQuantization(

        binary=models.BinaryQuantizationConfig(always_ram=True),

    ),

)

```



#### What is happening in the OptimizerConfig? 



We're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded
#### Optimization with Web Workers",0.3188405797101449,0.4074074074074074,0.3577235772357724,0.072992700729927,0.09345794392523364,0.0819672131147541,0.15942028985507245,0.2037037037037037,0.1788617886178862,0.2971014492753623,0.37962962962962965,0.3333333333333333,0.483131468296051,0.5704728960990906,0.5759592056274414,0.573202908039093,0.8115865092517601,0.4514002054929733,151,1.3017241379310345,0.7339827429013557
True,,3052.766,0b387f5c,Tell me about ‘always_ram’ parameter?,"In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times.","The `always_ram` parameter is used in the quantization configuration of Qdrant to specify whether the quantization should always be kept in RAM (Random Access Memory) or not. When `always_ram` is set to `true`, it indicates that the quantization should always be stored and accessed in RAM. Conversely, setting `always_ram` to `false` means that the quantization can be stored in a manner that allows it to be offloaded from RAM when needed, such as using mmap storage. 

In the provided code snippets, you can see examples of how the `always_ram` parameter is used in different programming languages such as Rust, Java, HTTP request, Python, etc., within the context of creating collections with specific quantization configurations.","always_ram: Some(true),

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CompressionRatio;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.ProductQuantization;
always_ram: Some(true),

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.BinaryQuantization;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.QuantizationConfig;
always_ram: true,

    },

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, BinaryQuantization,

        CreateCollection, Distance, QuantizationConfig, VectorParams, VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".to_string(),
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": false

        }

    }

}

```



```python

from qdrant_client import QdrantClient, models
type: ""int8"",

      always_ram: false,

    },

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,

        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,

        VectorParams, VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {","['always_ram: Some(true),\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.CompressionRatio;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.ProductQuantization;', 'always_ram: Some(true),\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.BinaryQuantization;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.QuantizationConfig;', 'always_ram: true,\n\n    },\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, BinaryQuantization,\n\n        CreateCollection, Distance, QuantizationConfig, VectorParams, VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".to_string(),', 'This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models', 'type: ""int8"",\n\n      always_ram: false,\n\n    },\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,\n\n        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,\n\n        VectorParams, VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {']","Question: Tell me about ‘always_ram’ parameter?\n\nContext: always_ram: Some(true),

})),

}),

..Default::default()

})

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CompressionRatio;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.ProductQuantization;
always_ram: Some(true),

})),

}),

..Default::default()

})

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.BinaryQuantization;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.QuantizationConfig;
always_ram: true,

},

},

});

```



```rust

use qdrant_client::{

client::QdrantClient,

qdrant::{

quantization_config::Quantization, vectors_config::Config, BinaryQuantization,

CreateCollection, Distance, QuantizationConfig, VectorParams, VectorsConfig,

},

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

.create_collection(&CreateCollection {

collection_name: ""{collection_name}"".to_string(),
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""optimizers_config"": {

""memmap_threshold"": 20000

},

""quantization_config"": {

""scalar"": {

""type"": ""int8"",

""always_ram"": false

}

}

}

```



```python

from qdrant_client import QdrantClient, models
type: ""int8"",

always_ram: false,

},

},

});

```



```rust

use qdrant_client::{

client::QdrantClient,

qdrant::{

quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,

OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,

VectorParams, VectorsConfig,

},

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

.create_collection(&CreateCollection {\n\nAnswer:",Tell me about ‘always_ram’ parameter?,"In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times.","['always_ram: Some(true),\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.CompressionRatio;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.ProductQuantization;', 'always_ram: Some(true),\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.BinaryQuantization;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.QuantizationConfig;', 'always_ram: true,\n\n    },\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, BinaryQuantization,\n\n        CreateCollection, Distance, QuantizationConfig, VectorParams, VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".to_string(),', 'This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models', 'type: ""int8"",\n\n      always_ram: false,\n\n    },\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,\n\n        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,\n\n        VectorParams, VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {']","always_ram: Some(true),

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CompressionRatio;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.ProductQuantization;
always_ram: Some(true),

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.BinaryQuantization;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.QuantizationConfig;
always_ram: true,

    },

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, BinaryQuantization,

        CreateCollection, Distance, QuantizationConfig, VectorParams, VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".to_string(),
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": false

        }

    }

}

```



```python

from qdrant_client import QdrantClient, models
type: ""int8"",

      always_ram: false,

    },

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,

        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,

        VectorParams, VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {",0.3119266055045872,0.6415094339622641,0.4197530864197531,0.12037037037037036,0.25,0.1625,0.21100917431192662,0.4339622641509434,0.28395061728395066,0.26605504587155965,0.5471698113207547,0.35802469135802467,0.9049118757247925,0.6183180809020996,0.6857461929321289,0.6502888798713684,0.7988645955920219,0.6065895085533459,114,2.0357142857142856,0.6628022536495946
True,,6120.6449999999995,238cb61d,What is difference between scalar and product quantization?,"While both methods aim to reduce the memory footprint and potentially speed up operations, scalar quantization offers faster processing with SIMD-friendly operations and minimal accuracy loss, making it suitable for scenarios where speed is critical. Product quantization achieves higher compression rates and is better suited for handling very high-dimensional vectors where memory space is more of a concern than computation speed.","Scalar quantization and product quantization are two different methods used for compressing vectors in order to minimize memory usage. Here is the difference between the two:

1. **Scalar Quantization**:
   - Scalar quantization is a compression technique that reduces the number of bits used to represent each vector component.
   - In scalar quantization, each component of the vector is quantized individually by reducing the number of bits used to represent it.
   - For example, if the original vector components are represented using 32-bit floating-point numbers, scalar quantization can reduce the number of bits used to 8 by converting them to `uint8`.
   - Scalar quantization is simpler and more straightforward compared to product quantization.

2. **Product Quantization**:
   - Product quantization is a method of compressing vectors by dividing them into chunks and quantizing each segment individually.
   - In this method, vectors are divided into subvectors or chunks, and each chunk is quantized separately.
   - Product quantization can achieve higher compression ratios compared to scalar quantization, but it comes with some tradeoffs.
   - Product quantization can result in a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.
   - Distance calculations in product quantization are not SIMD-friendly, making it slower than scalar quantization in certain scenarios.

In summary, scalar quantization focuses on compressing individual vector components, while product quantization divides vectors into chunks for compression. Each method has its own advantages and tradeoffs, and the choice between them depends on the specific requirements of the application, such as accuracy, speed, and memory usage.","But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method
- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself

- Situations where the dimensionality of the original vectors is sufficiently high

- Cases where indexing speed is not a critical factor



In circumstances that do not align with the above, Scalar Quantization should be the preferred choice.



Qdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization)
. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
| 0        | 0        | 0                |



</details>



As you can see, both functions are equal up to a constant factor, which makes similarity search equivalent.

Binary quantization makes it efficient to compare vectors using this representation.





## Product Quantization



*Available as of v1.2.0*



Product quantization is a method of compressing vectors to minimize their memory usage by dividing them into 

chunks and quantizing each segment individually.
## Scalar Quantization



*Available as of v1.1.0*



Scalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.





For instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.

In other words, Qdrant performs `float32 -> uint8` conversion for each vector component.","['But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\n\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\n\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n## How to choose the right quantization method', '- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself\n\n- Situations where the dimensionality of the original vectors is sufficiently high\n\n- Cases where indexing speed is not a critical factor\n\n\n\nIn circumstances that do not align with the above, Scalar Quantization should be the preferred choice.\n\n\n\nQdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization)', '. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.', '| 0        | 0        | 0                |\n\n\n\n</details>\n\n\n\nAs you can see, both functions are equal up to a constant factor, which makes similarity search equivalent.\n\nBinary quantization makes it efficient to compare vectors using this representation.\n\n\n\n\n\n## Product Quantization\n\n\n\n*Available as of v1.2.0*\n\n\n\nProduct quantization is a method of compressing vectors to minimize their memory usage by dividing them into \n\nchunks and quantizing each segment individually.', '## Scalar Quantization\n\n\n\n*Available as of v1.1.0*\n\n\n\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\n\n\n\n\n\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\n\nIn other words, Qdrant performs `float32 -> uint8` conversion for each vector component.']","Question: What is difference between scalar and product quantization?\n\nContext: But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method
- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself

- Situations where the dimensionality of the original vectors is sufficiently high

- Cases where indexing speed is not a critical factor



In circumstances that do not align with the above, Scalar Quantization should be the preferred choice.



Qdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization)
. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
| 0        | 0        | 0                |



</details>



As you can see, both functions are equal up to a constant factor, which makes similarity search equivalent.

Binary quantization makes it efficient to compare vectors using this representation.





## Product Quantization



*Available as of v1.2.0*



Product quantization is a method of compressing vectors to minimize their memory usage by dividing them into 

chunks and quantizing each segment individually.
## Scalar Quantization



*Available as of v1.1.0*



Scalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.





For instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.

In other words, Qdrant performs `float32 -> uint8` conversion for each vector component.\n\nAnswer:",What is difference between scalar and product quantization?,"While both methods aim to reduce the memory footprint and potentially speed up operations, scalar quantization offers faster processing with SIMD-friendly operations and minimal accuracy loss, making it suitable for scenarios where speed is critical. Product quantization achieves higher compression rates and is better suited for handling very high-dimensional vectors where memory space is more of a concern than computation speed.","['But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\n\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\n\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n## How to choose the right quantization method', '- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself\n\n- Situations where the dimensionality of the original vectors is sufficiently high\n\n- Cases where indexing speed is not a critical factor\n\n\n\nIn circumstances that do not align with the above, Scalar Quantization should be the preferred choice.\n\n\n\nQdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization)', '. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.', '| 0        | 0        | 0                |\n\n\n\n</details>\n\n\n\nAs you can see, both functions are equal up to a constant factor, which makes similarity search equivalent.\n\nBinary quantization makes it efficient to compare vectors using this representation.\n\n\n\n\n\n## Product Quantization\n\n\n\n*Available as of v1.2.0*\n\n\n\nProduct quantization is a method of compressing vectors to minimize their memory usage by dividing them into \n\nchunks and quantizing each segment individually.', '## Scalar Quantization\n\n\n\n*Available as of v1.1.0*\n\n\n\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\n\n\n\n\n\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\n\nIn other words, Qdrant performs `float32 -> uint8` conversion for each vector component.']","But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method
- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself

- Situations where the dimensionality of the original vectors is sufficiently high

- Cases where indexing speed is not a critical factor



In circumstances that do not align with the above, Scalar Quantization should be the preferred choice.



Qdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization)
. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
| 0        | 0        | 0                |



</details>



As you can see, both functions are equal up to a constant factor, which makes similarity search equivalent.

Binary quantization makes it efficient to compare vectors using this representation.





## Product Quantization



*Available as of v1.2.0*



Product quantization is a method of compressing vectors to minimize their memory usage by dividing them into 

chunks and quantizing each segment individually.
## Scalar Quantization



*Available as of v1.1.0*



Scalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.





For instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.

In other words, Qdrant performs `float32 -> uint8` conversion for each vector component.",0.15611814345991562,0.6065573770491803,0.24832214765100674,0.03389830508474576,0.13333333333333333,0.05405405405405406,0.08438818565400844,0.32786885245901637,0.13422818791946306,0.14345991561181434,0.5573770491803278,0.22818791946308722,0.8398666381835938,0.5158668160438538,0.6362668871879578,0.5697758793830872,0.8251795855300901,0.9457131903618574,246,4.032786885245901,0.3707686920853407
True,,3912.15,58ef3b55,What is ‘best_score’ strategy?,"The `best_score` strategy is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided.","The `best_score` strategy is a new strategy introduced in version 1.6.0 of the recommendation system. It is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative example. This strategy considers each example separately, resulting in a richer variety of results compared to the old strategy.

The way the `best_score` strategy works is by measuring each candidate against every example provided, then selecting the best positive and best negative scores. The final score is determined using a step formula that compares the best positive score with the best negative score. The formula is as follows:

```rust
let score = if best_positive_score > best_negative_score {
                best_positive_score;
            } else {
                -(best_negative_score * best_negative_score);
            };
```

It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided.

To use the `best_score` strategy in a recommendation request, you can set the `""strategy"": ""best_score""` in the request parameters along with specifying the positive and negative examples either by their IDs or embeddings.","This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
##### One positive and one negative example



<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>



The more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there 

is just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making 

the variety richer.
The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:



```rust

let score = if best_positive_score > best_negative_score {

    best_positive_score;

} else {

    -(best_negative_score * best_negative_score);

};

```



<aside role=""alert"">

The performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.

</aside>
},

  ""strategy"": ""average_vector"",

  ""limit"": 3

}

```



There are two key changes in the request. First of all, we can adjust the strategy of search and set it to `average_vector` (the 

default) or `best_score`. Moreover, we can pass both IDs (`718`) and embeddings (`[0.2, 0.3, 0.4, 0.5]`) as both positive and 

negative examples. 



## HNSW ANN example and strategy
* Score is always in descending order (larger is better), regardless of the metric used.

* Best possible score is `0.0`, and it is normal that many points get this score.



</aside>","['This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.\n\n\n\n### Best score strategy\n\n\n\n*Available as of v1.6.0*\n\n\n\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.', '##### One positive and one negative example\n\n\n\n<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>\n\n\n\nThe more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there \n\nis just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making \n\nthe variety richer.', 'The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\n\n\n\n```rust\n\nlet score = if best_positive_score > best_negative_score {\n\n    best_positive_score;\n\n} else {\n\n    -(best_negative_score * best_negative_score);\n\n};\n\n```\n\n\n\n<aside role=""alert"">\n\nThe performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.\n\n</aside>', '},\n\n  ""strategy"": ""average_vector"",\n\n  ""limit"": 3\n\n}\n\n```\n\n\n\nThere are two key changes in the request. First of all, we can adjust the strategy of search and set it to `average_vector` (the \n\ndefault) or `best_score`. Moreover, we can pass both IDs (`718`) and embeddings (`[0.2, 0.3, 0.4, 0.5]`) as both positive and \n\nnegative examples. \n\n\n\n## HNSW ANN example and strategy', '* Score is always in descending order (larger is better), regardless of the metric used.\n\n* Best possible score is `0.0`, and it is normal that many points get this score.\n\n\n\n</aside>']","Question: What is ‘best_score’ strategy?\n\nContext: This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
##### One positive and one negative example



<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>



The more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there 

is just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making 

the variety richer.
The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:



```rust

let score = if best_positive_score > best_negative_score {

best_positive_score;

} else {

-(best_negative_score * best_negative_score);

};

```



<aside role=""alert"">

The performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.

</aside>
},

""strategy"": ""average_vector"",

""limit"": 3

}

```



There are two key changes in the request. First of all, we can adjust the strategy of search and set it to `average_vector` (the 

default) or `best_score`. Moreover, we can pass both IDs (`718`) and embeddings (`[0.2, 0.3, 0.4, 0.5]`) as both positive and 

negative examples. 



## HNSW ANN example and strategy
* Score is always in descending order (larger is better), regardless of the metric used.

* Best possible score is `0.0`, and it is normal that many points get this score.



</aside>\n\nAnswer:",What is ‘best_score’ strategy?,"The `best_score` strategy is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided.","['This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.\n\n\n\n### Best score strategy\n\n\n\n*Available as of v1.6.0*\n\n\n\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.', '##### One positive and one negative example\n\n\n\n<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>\n\n\n\nThe more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there \n\nis just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making \n\nthe variety richer.', 'The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\n\n\n\n```rust\n\nlet score = if best_positive_score > best_negative_score {\n\n    best_positive_score;\n\n} else {\n\n    -(best_negative_score * best_negative_score);\n\n};\n\n```\n\n\n\n<aside role=""alert"">\n\nThe performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.\n\n</aside>', '},\n\n  ""strategy"": ""average_vector"",\n\n  ""limit"": 3\n\n}\n\n```\n\n\n\nThere are two key changes in the request. First of all, we can adjust the strategy of search and set it to `average_vector` (the \n\ndefault) or `best_score`. Moreover, we can pass both IDs (`718`) and embeddings (`[0.2, 0.3, 0.4, 0.5]`) as both positive and \n\nnegative examples. \n\n\n\n## HNSW ANN example and strategy', '* Score is always in descending order (larger is better), regardless of the metric used.\n\n* Best possible score is `0.0`, and it is normal that many points get this score.\n\n\n\n</aside>']","This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
##### One positive and one negative example



<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>



The more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there 

is just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making 

the variety richer.
The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:



```rust

let score = if best_positive_score > best_negative_score {

    best_positive_score;

} else {

    -(best_negative_score * best_negative_score);

};

```



<aside role=""alert"">

The performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.

</aside>
},

  ""strategy"": ""average_vector"",

  ""limit"": 3

}

```



There are two key changes in the request. First of all, we can adjust the strategy of search and set it to `average_vector` (the 

default) or `best_score`. Moreover, we can pass both IDs (`718`) and embeddings (`[0.2, 0.3, 0.4, 0.5]`) as both positive and 

negative examples. 



## HNSW ANN example and strategy
* Score is always in descending order (larger is better), regardless of the metric used.

* Best possible score is `0.0`, and it is normal that many points get this score.



</aside>",0.5443786982248521,0.9019607843137255,0.6789667896678967,0.42857142857142855,0.7128712871287128,0.5353159851301115,0.4970414201183432,0.8235294117647058,0.6199261992619925,0.5325443786982249,0.8823529411764706,0.6642066420664207,0.852627158164978,0.7103137969970703,0.8257551789283752,0.763696551322937,0.93789511183942,0.9868802974466234,176,1.4915254237288136,0.4433013035363896
True,,1831.247,562b2102,How does oversampling helps?,"Defines how many extra vectors should be pre-selected using quantized index, and then re-scored using original vectors. For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring. Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.","Oversampling helps in improving the accuracy of search results by equalizing the representation of classes in the training dataset. This leads to more fair and accurate modeling of real-world scenarios. By oversampling, more vectors are pre-selected and then re-scored, allowing for a more precise selection of top vectors during query time. This technique helps in controlling the precision of search in real-time, resulting in better quality results for the user. While oversampling may impact performance slightly, it is a valuable feature for improving accuracy in various applications.","oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.

Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.



## Quantization tips



#### Accuracy tuning



In this section, we will discuss how to tune the search precision.
Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user
IO-induced delays.



Oversampling is a new feature to improve accuracy at the cost of some

performance. It allows setting a factor, which is multiplied with the `limit`

while doing the search. The results are then re-scored using the original vector

and only then the top results up to the limit are selected.



## Discussion



Looking back, disk IO used to be very serialized; re-positioning read-write

heads on moving platter was a slow and messy business. So the system overhead
3. Oversampling: Use an oversampling factor of 3 for the best balance between accuracy and efficiency. This factor is suitable for a wide range of applications.

4. Rescoring: Enable rescoring to improve the accuracy of search results.","['oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.', 'For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.\n\nOversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.\n\n\n\n## Quantization tips\n\n\n\n#### Accuracy tuning\n\n\n\nIn this section, we will discuss how to tune the search precision.', 'Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user', 'IO-induced delays.\n\n\n\nOversampling is a new feature to improve accuracy at the cost of some\n\nperformance. It allows setting a factor, which is multiplied with the `limit`\n\nwhile doing the search. The results are then re-scored using the original vector\n\nand only then the top results up to the limit are selected.\n\n\n\n## Discussion\n\n\n\nLooking back, disk IO used to be very serialized; re-positioning read-write\n\nheads on moving platter was a slow and messy business. So the system overhead', '3. Oversampling: Use an oversampling factor of 3 for the best balance between accuracy and efficiency. This factor is suitable for a wide range of applications.\n\n4. Rescoring: Enable rescoring to improve the accuracy of search results.']","Question: How does oversampling helps?\n\nContext: oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.

Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.



## Quantization tips



#### Accuracy tuning



In this section, we will discuss how to tune the search precision.
Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user
IO-induced delays.



Oversampling is a new feature to improve accuracy at the cost of some

performance. It allows setting a factor, which is multiplied with the `limit`

while doing the search. The results are then re-scored using the original vector

and only then the top results up to the limit are selected.



## Discussion



Looking back, disk IO used to be very serialized; re-positioning read-write

heads on moving platter was a slow and messy business. So the system overhead
3. Oversampling: Use an oversampling factor of 3 for the best balance between accuracy and efficiency. This factor is suitable for a wide range of applications.

4. Rescoring: Enable rescoring to improve the accuracy of search results.\n\nAnswer:",How does oversampling helps?,"Defines how many extra vectors should be pre-selected using quantized index, and then re-scored using original vectors. For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring. Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.","['oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.', 'For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.\n\nOversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.\n\n\n\n## Quantization tips\n\n\n\n#### Accuracy tuning\n\n\n\nIn this section, we will discuss how to tune the search precision.', 'Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user', 'IO-induced delays.\n\n\n\nOversampling is a new feature to improve accuracy at the cost of some\n\nperformance. It allows setting a factor, which is multiplied with the `limit`\n\nwhile doing the search. The results are then re-scored using the original vector\n\nand only then the top results up to the limit are selected.\n\n\n\n## Discussion\n\n\n\nLooking back, disk IO used to be very serialized; re-positioning read-write\n\nheads on moving platter was a slow and messy business. So the system overhead', '3. Oversampling: Use an oversampling factor of 3 for the best balance between accuracy and efficiency. This factor is suitable for a wide range of applications.\n\n4. Rescoring: Enable rescoring to improve the accuracy of search results.']","oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.

Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.



## Quantization tips



#### Accuracy tuning



In this section, we will discuss how to tune the search precision.
Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user
IO-induced delays.



Oversampling is a new feature to improve accuracy at the cost of some

performance. It allows setting a factor, which is multiplied with the `limit`

while doing the search. The results are then re-scored using the original vector

and only then the top results up to the limit are selected.



## Discussion



Looking back, disk IO used to be very serialized; re-positioning read-write

heads on moving platter was a slow and messy business. So the system overhead
3. Oversampling: Use an oversampling factor of 3 for the best balance between accuracy and efficiency. This factor is suitable for a wide range of applications.

4. Rescoring: Enable rescoring to improve the accuracy of search results.",0.2619047619047619,0.3235294117647059,0.2894736842105263,0.060240963855421686,0.07462686567164178,0.06666666666666667,0.15476190476190477,0.19117647058823528,0.17105263157894737,0.23809523809523808,0.29411764705882354,0.2631578947368421,0.6184085607528687,0.5877902507781982,0.6231571435928345,0.6049572229385376,0.9990335365873761,0.9860824002729108,87,1.359375,0.6099458414458645
True,,2897.014,51b22567,What is the purpose of ‘CreatePayloadIndexAsync’?,CreatePayloadIndexAsync is a method in the Qdrant library that enables the creation of a payload index in Qdrant. A payload index is a data structure designed to store supplemental information about the data stored in Qdrant. This method requires a Qdrant instance and a payload index name as input parameters.,"The purpose of the `CreatePayloadIndexAsync` method is to create an index for a specific field in a collection within the Qdrant database. The method allows you to specify the collection name, the field name to index, the schema type of the payload (e.g., text or keyword), and any additional parameters related to the indexing process, such as tokenization settings or text indexing parameters.

By calling this method, you are configuring the Qdrant database to index the specified field according to the provided schema type and parameters, making it easier and faster to search and retrieve data based on that field in the future.","QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createPayloadIndexAsync(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        PayloadSchemaType.Text,

        PayloadIndexParams.newBuilder()

            .setTextIndexParams(

                TextIndexParams.newBuilder()

                    .setTokenizer(TokenizerType.Word)

                    .setMinTokenLen(2)

                    .setMaxTokenLen(10)
client

    .createPayloadIndexAsync(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        PayloadSchemaType.Keyword,

        null,

        null,

        null,

        null)

    .get();

```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");

```



Available field types are:
.setMaxTokenLen(10)

                    .setLowercase(true)

                    .build())

            .build(),

        null,

        null,

        null)

    .get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(

	collectionName: ""{collection_name}"",

	fieldName: ""name_of_the_field_to_index"",

	schemaType: PayloadSchemaType.Text,
client

    .create_field_index(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        FieldType::Keyword,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createPayloadIndexAsync(
FieldType::Keyword,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createPayloadIndexAsync(

        ""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null)

    .get();

```","['QdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        PayloadSchemaType.Text,\n\n        PayloadIndexParams.newBuilder()\n\n            .setTextIndexParams(\n\n                TextIndexParams.newBuilder()\n\n                    .setTokenizer(TokenizerType.Word)\n\n                    .setMinTokenLen(2)\n\n                    .setMaxTokenLen(10)', 'client\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        PayloadSchemaType.Keyword,\n\n        null,\n\n        null,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");\n\n```\n\n\n\nAvailable field types are:', '.setMaxTokenLen(10)\n\n                    .setLowercase(true)\n\n                    .build())\n\n            .build(),\n\n        null,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tfieldName: ""name_of_the_field_to_index"",\n\n\tschemaType: PayloadSchemaType.Text,', 'client\n\n    .create_field_index(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(', 'FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null)\n\n    .get();\n\n```']","Question: What is the purpose of ‘CreatePayloadIndexAsync’?\n\nContext: QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createPayloadIndexAsync(

""{collection_name}"",

""name_of_the_field_to_index"",

PayloadSchemaType.Text,

PayloadIndexParams.newBuilder()

.setTextIndexParams(

TextIndexParams.newBuilder()

.setTokenizer(TokenizerType.Word)

.setMinTokenLen(2)

.setMaxTokenLen(10)
client

.createPayloadIndexAsync(

""{collection_name}"",

""name_of_the_field_to_index"",

PayloadSchemaType.Keyword,

null,

null,

null,

null)

.get();

```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");

```



Available field types are:
.setMaxTokenLen(10)

.setLowercase(true)

.build())

.build(),

null,

null,

null)

.get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(

collectionName: ""{collection_name}"",

fieldName: ""name_of_the_field_to_index"",

schemaType: PayloadSchemaType.Text,
client

.create_field_index(

""{collection_name}"",

""name_of_the_field_to_index"",

FieldType::Keyword,

None,

None,

)

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createPayloadIndexAsync(
FieldType::Keyword,

None,

None,

)

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createPayloadIndexAsync(

""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null)

.get();

```\n\nAnswer:",What is the purpose of ‘CreatePayloadIndexAsync’?,CreatePayloadIndexAsync is a method in the Qdrant library that enables the creation of a payload index in Qdrant. A payload index is a data structure designed to store supplemental information about the data stored in Qdrant. This method requires a Qdrant instance and a payload index name as input parameters.,"['QdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        PayloadSchemaType.Text,\n\n        PayloadIndexParams.newBuilder()\n\n            .setTextIndexParams(\n\n                TextIndexParams.newBuilder()\n\n                    .setTokenizer(TokenizerType.Word)\n\n                    .setMinTokenLen(2)\n\n                    .setMaxTokenLen(10)', 'client\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        PayloadSchemaType.Keyword,\n\n        null,\n\n        null,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");\n\n```\n\n\n\nAvailable field types are:', '.setMaxTokenLen(10)\n\n                    .setLowercase(true)\n\n                    .build())\n\n            .build(),\n\n        null,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tfieldName: ""name_of_the_field_to_index"",\n\n\tschemaType: PayloadSchemaType.Text,', 'client\n\n    .create_field_index(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(', 'FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null)\n\n    .get();\n\n```']","QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createPayloadIndexAsync(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        PayloadSchemaType.Text,

        PayloadIndexParams.newBuilder()

            .setTextIndexParams(

                TextIndexParams.newBuilder()

                    .setTokenizer(TokenizerType.Word)

                    .setMinTokenLen(2)

                    .setMaxTokenLen(10)
client

    .createPayloadIndexAsync(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        PayloadSchemaType.Keyword,

        null,

        null,

        null,

        null)

    .get();

```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");

```



Available field types are:
.setMaxTokenLen(10)

                    .setLowercase(true)

                    .build())

            .build(),

        null,

        null,

        null)

    .get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(

	collectionName: ""{collection_name}"",

	fieldName: ""name_of_the_field_to_index"",

	schemaType: PayloadSchemaType.Text,
client

    .create_field_index(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        FieldType::Keyword,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createPayloadIndexAsync(
FieldType::Keyword,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createPayloadIndexAsync(

        ""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null)

    .get();

```",0.23863636363636365,0.5121951219512195,0.32558139534883723,0.022988505747126436,0.05,0.031496062992125984,0.14772727272727273,0.3170731707317073,0.20155038759689922,0.22727272727272727,0.4878048780487805,0.31007751937984496,0.8117215633392334,0.6088776588439941,0.6842049360275269,0.6443472504615784,0.9417669450243314,0.9709500639388958,103,2.06,0.5731939368168172
True,,2887.81,2bfe7285,What is the purpose of ef_construct in HNSW ?,"In HNSW algorithm the ef_construct parameter is the number of neighbours to consider during the index building. The larger the value, the higher the precision, but the longer the indexing time. The default values of this parameters 100","The purpose of the `ef_construct` parameter in HNSW (Hierarchical Navigable Small World) indexing is to control the number of neighbors considered during the index building process. By increasing the value of `ef_construct`, you can improve the precision of the search at the cost of longer indexing time and more space required.

In the provided context, the default values for `m` and `ef_construct` are `m=16` and `ef_construct=100`. By increasing them to `m=32` and `ef_construct=123`, you are adjusting the parameters to potentially achieve higher precision in search results.

The `ef_construct` parameter influences how many neighbors are taken into account when constructing the index structure, thereby impacting the search efficiency and accuracy. It plays a crucial role in balancing search precision, indexing time, and resource consumption in HNSW indexing.","The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of 

neighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.

The default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and
""m"": 32,

                ""ef_construct"": 123

            },

            ""quantization_config"": {

                ""product"": {

                    ""compression"": ""x32"",

                    ""always_ram"": true

                }

            },

            ""on_disk"": true

        }

    },

    ""hnsw_config"": {

        ""ef_construct"": 123

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""quantile"": 0.8,
""hnsw_config"": {

        ""ef_construct"": 123

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""quantile"": 0.8,

            ""always_ram"": false

        }

    }

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

  -H 'Content-Type: application/json' \

  --data-raw '{

    ""vectors"": {

        ""my_vector"": {

            ""hnsw_config"": {

                ""m"": 32,

                ""ef_construct"": 123
ef_construct: 123,

  },

  quantization_config: {

    scalar: {

      type: ""int8"",

      quantile: 0.8,

      always_ram: true,

    },

  },

});

```



```rust

use qdrant_client::client::QdrantClient;

use qdrant_client::qdrant::{

    quantization_config_diff::Quantization, vectors_config_diff::Config, HnswConfigDiff,

    QuantizationConfigDiff, QuantizationType, ScalarQuantization, VectorParamsDiff,

    VectorsConfigDiff,

};



client

    .update_collection(
(""my_vector"".into()),

                        VectorParamsDiff {

                            hnsw_config: Some(HnswConfigDiff {

                                m: Some(32),

                                ef_construct: Some(123),

                                ..Default::default()

                            }),

                            ..Default::default()

                        },

                    )]),

                },

            )),

        }),","[""The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \n\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\n\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and"", '""m"": 32,\n\n                ""ef_construct"": 123\n\n            },\n\n            ""quantization_config"": {\n\n                ""product"": {\n\n                    ""compression"": ""x32"",\n\n                    ""always_ram"": true\n\n                }\n\n            },\n\n            ""on_disk"": true\n\n        }\n\n    },\n\n    ""hnsw_config"": {\n\n        ""ef_construct"": 123\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""quantile"": 0.8,', '""hnsw_config"": {\n\n        ""ef_construct"": 123\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""quantile"": 0.8,\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""vectors"": {\n\n        ""my_vector"": {\n\n            ""hnsw_config"": {\n\n                ""m"": 32,\n\n                ""ef_construct"": 123', 'ef_construct: 123,\n\n  },\n\n  quantization_config: {\n\n    scalar: {\n\n      type: ""int8"",\n\n      quantile: 0.8,\n\n      always_ram: true,\n\n    },\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::client::QdrantClient;\n\nuse qdrant_client::qdrant::{\n\n    quantization_config_diff::Quantization, vectors_config_diff::Config, HnswConfigDiff,\n\n    QuantizationConfigDiff, QuantizationType, ScalarQuantization, VectorParamsDiff,\n\n    VectorsConfigDiff,\n\n};\n\n\n\nclient\n\n    .update_collection(', '(""my_vector"".into()),\n\n                        VectorParamsDiff {\n\n                            hnsw_config: Some(HnswConfigDiff {\n\n                                m: Some(32),\n\n                                ef_construct: Some(123),\n\n                                ..Default::default()\n\n                            }),\n\n                            ..Default::default()\n\n                        },\n\n                    )]),\n\n                },\n\n            )),\n\n        }),']","Question: What is the purpose of ef_construct in HNSW ?\n\nContext: The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of 

neighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.

The default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and
""m"": 32,

""ef_construct"": 123

},

""quantization_config"": {

""product"": {

""compression"": ""x32"",

""always_ram"": true

}

},

""on_disk"": true

}

},

""hnsw_config"": {

""ef_construct"": 123

},

""quantization_config"": {

""scalar"": {

""type"": ""int8"",

""quantile"": 0.8,
""hnsw_config"": {

""ef_construct"": 123

},

""quantization_config"": {

""scalar"": {

""type"": ""int8"",

""quantile"": 0.8,

""always_ram"": false

}

}

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

-H 'Content-Type: application/json' \

--data-raw '{

""vectors"": {

""my_vector"": {

""hnsw_config"": {

""m"": 32,

""ef_construct"": 123
ef_construct: 123,

},

quantization_config: {

scalar: {

type: ""int8"",

quantile: 0.8,

always_ram: true,

},

},

});

```



```rust

use qdrant_client::client::QdrantClient;

use qdrant_client::qdrant::{

quantization_config_diff::Quantization, vectors_config_diff::Config, HnswConfigDiff,

QuantizationConfigDiff, QuantizationType, ScalarQuantization, VectorParamsDiff,

VectorsConfigDiff,

};



client

.update_collection(
(""my_vector"".into()),

VectorParamsDiff {

hnsw_config: Some(HnswConfigDiff {

m: Some(32),

ef_construct: Some(123),

..Default::default()

}),

..Default::default()

},

)]),

},

)),

}),\n\nAnswer:",What is the purpose of ef_construct in HNSW ?,"In HNSW algorithm the ef_construct parameter is the number of neighbours to consider during the index building. The larger the value, the higher the precision, but the longer the indexing time. The default values of this parameters 100","[""The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \n\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\n\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and"", '""m"": 32,\n\n                ""ef_construct"": 123\n\n            },\n\n            ""quantization_config"": {\n\n                ""product"": {\n\n                    ""compression"": ""x32"",\n\n                    ""always_ram"": true\n\n                }\n\n            },\n\n            ""on_disk"": true\n\n        }\n\n    },\n\n    ""hnsw_config"": {\n\n        ""ef_construct"": 123\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""quantile"": 0.8,', '""hnsw_config"": {\n\n        ""ef_construct"": 123\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""quantile"": 0.8,\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""vectors"": {\n\n        ""my_vector"": {\n\n            ""hnsw_config"": {\n\n                ""m"": 32,\n\n                ""ef_construct"": 123', 'ef_construct: 123,\n\n  },\n\n  quantization_config: {\n\n    scalar: {\n\n      type: ""int8"",\n\n      quantile: 0.8,\n\n      always_ram: true,\n\n    },\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::client::QdrantClient;\n\nuse qdrant_client::qdrant::{\n\n    quantization_config_diff::Quantization, vectors_config_diff::Config, HnswConfigDiff,\n\n    QuantizationConfigDiff, QuantizationType, ScalarQuantization, VectorParamsDiff,\n\n    VectorsConfigDiff,\n\n};\n\n\n\nclient\n\n    .update_collection(', '(""my_vector"".into()),\n\n                        VectorParamsDiff {\n\n                            hnsw_config: Some(HnswConfigDiff {\n\n                                m: Some(32),\n\n                                ef_construct: Some(123),\n\n                                ..Default::default()\n\n                            }),\n\n                            ..Default::default()\n\n                        },\n\n                    )]),\n\n                },\n\n            )),\n\n        }),']","The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of 

neighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.

The default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and
""m"": 32,

                ""ef_construct"": 123

            },

            ""quantization_config"": {

                ""product"": {

                    ""compression"": ""x32"",

                    ""always_ram"": true

                }

            },

            ""on_disk"": true

        }

    },

    ""hnsw_config"": {

        ""ef_construct"": 123

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""quantile"": 0.8,
""hnsw_config"": {

        ""ef_construct"": 123

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""quantile"": 0.8,

            ""always_ram"": false

        }

    }

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

  -H 'Content-Type: application/json' \

  --data-raw '{

    ""vectors"": {

        ""my_vector"": {

            ""hnsw_config"": {

                ""m"": 32,

                ""ef_construct"": 123
ef_construct: 123,

  },

  quantization_config: {

    scalar: {

      type: ""int8"",

      quantile: 0.8,

      always_ram: true,

    },

  },

});

```



```rust

use qdrant_client::client::QdrantClient;

use qdrant_client::qdrant::{

    quantization_config_diff::Quantization, vectors_config_diff::Config, HnswConfigDiff,

    QuantizationConfigDiff, QuantizationType, ScalarQuantization, VectorParamsDiff,

    VectorsConfigDiff,

};



client

    .update_collection(
(""my_vector"".into()),

                        VectorParamsDiff {

                            hnsw_config: Some(HnswConfigDiff {

                                m: Some(32),

                                ef_construct: Some(123),

                                ..Default::default()

                            }),

                            ..Default::default()

                        },

                    )]),

                },

            )),

        }),",0.19834710743801653,0.8275862068965517,0.32,0.1,0.42857142857142855,0.16216216216216217,0.1487603305785124,0.6206896551724138,0.24,0.19834710743801653,0.8275862068965517,0.32,0.8193555474281311,0.5314010381698608,0.648678719997406,0.5842123031616211,0.8331735938166579,0.4816011687119802,126,3.3157894736842106,0.4228661751861532
True,,4564.904,532e6e80,How do you use ‘ordering’ parameter?,"Write ordering can be specified for any write request to serialize it through a single “leader” node, which ensures that all write operations (issued with the same ordering) are performed and observed sequentially. It is of 3 types weak , medium and strong and is used in python with additional param ordering=models.WriteOrdering.STRONG to upsert request.","To use the `ordering` parameter in Qdrant, you need to specify it in the parameters when performing update and delete operations. This parameter ensures that the operations are executed in the same order on all replicas. When `ordering` is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. 

This option is particularly useful to avoid data inconsistency in cases of concurrent updates of the same documents. By enforcing a specific order of operations, you can maintain data integrity across replicas.

In your code snippet, you can add the `ordering` parameter like this:

```
QdrantIngestOperator(
    conn_id=""qdrant_connection"",
    task_id=""qdrant_ingest"",
    collection_name=""<COLLECTION_NAME>"",
    vectors=vectors,
    ids=ids,
    payload=payload,
    parameters={""ordering"": True}  # Specify the ordering parameter
)
```

By including `parameters={""ordering"": True}` in the code above, you are instructing Qdrant to ensure that the updates or deletions are executed in the specified order to prevent data inconsistencies.","- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
WITH ENGINE = ""qdrant"",

PARAMETERS = {

    ""location"": "":memory:"",

    ""collection_config"": {

        ""size"": 386,

        ""distance"": ""Cosine""

    }

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
[0.11, 0.22, 0.33, 0.44],

    [0.55, 0.66, 0.77, 0.88],

    [0.88, 0.11, 0.12, 0.13],

]

ids = [32, 21, ""b626f6a9-b14d-4af9-b7c3-43d8deb719a6""]

payload = [{""meta"": ""data""}, {""meta"": ""data_2""}, {""meta"": ""data_3"", ""extra"": ""data""}]



QdrantIngestOperator(

    conn_id=""qdrant_connection""

    task_id=""qdrant_ingest"",

    collection_name=""<COLLECTION_NAME>"",

    vectors=vectors,

    ids=ids,

    payload=payload,

)

```



## Reference
```http

POST /collections/{collection_name}/points/payload

{

    ""payload"": {

        ""property1"": ""string"",

        ""property2"": ""string""

    },

    ""filter"": {

        ""must"": [

            {

                ""key"": ""color"",

                ""match"": {

                    ""value"": ""red""

                }

            }

        ]

    }

}

```



```python

client.set_payload(

    collection_name=""{collection_name}"",

    payload={

        ""property1"": ""string"",
using: Some(""image"".to_string()),

        limit: 10,

        ..Default::default()

    })

    .await?;

```



```java

import java.util.List;



import static io.qdrant.client.PointIdFactory.id;



import io.qdrant.client.grpc.Points.RecommendPoints;



client

    .recommendAsync(

        RecommendPoints.newBuilder()

            .setCollectionName(""{collection_name}"")

            .addAllPositive(List.of(id(100), id(231)))

            .addAllNegative(List.of(id(718)))","['- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', 'WITH ENGINE = ""qdrant"",\n\nPARAMETERS = {\n\n    ""location"": "":memory:"",\n\n    ""collection_config"": {\n\n        ""size"": 386,\n\n        ""distance"": ""Cosine""\n\n    }\n\n}\n\n```\n\n\n\nThe available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).\n\n\n\n## Creating a new table', '[0.11, 0.22, 0.33, 0.44],\n\n    [0.55, 0.66, 0.77, 0.88],\n\n    [0.88, 0.11, 0.12, 0.13],\n\n]\n\nids = [32, 21, ""b626f6a9-b14d-4af9-b7c3-43d8deb719a6""]\n\npayload = [{""meta"": ""data""}, {""meta"": ""data_2""}, {""meta"": ""data_3"", ""extra"": ""data""}]\n\n\n\nQdrantIngestOperator(\n\n    conn_id=""qdrant_connection""\n\n    task_id=""qdrant_ingest"",\n\n    collection_name=""<COLLECTION_NAME>"",\n\n    vectors=vectors,\n\n    ids=ids,\n\n    payload=payload,\n\n)\n\n```\n\n\n\n## Reference', '```http\n\nPOST /collections/{collection_name}/points/payload\n\n{\n\n    ""payload"": {\n\n        ""property1"": ""string"",\n\n        ""property2"": ""string""\n\n    },\n\n    ""filter"": {\n\n        ""must"": [\n\n            {\n\n                ""key"": ""color"",\n\n                ""match"": {\n\n                    ""value"": ""red""\n\n                }\n\n            }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.set_payload(\n\n    collection_name=""{collection_name}"",\n\n    payload={\n\n        ""property1"": ""string"",', 'using: Some(""image"".to_string()),\n\n        limit: 10,\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport java.util.List;\n\n\n\nimport static io.qdrant.client.PointIdFactory.id;\n\n\n\nimport io.qdrant.client.grpc.Points.RecommendPoints;\n\n\n\nclient\n\n    .recommendAsync(\n\n        RecommendPoints.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .addAllPositive(List.of(id(100), id(231)))\n\n            .addAllNegative(List.of(id(718)))']","Question: How do you use ‘ordering’ parameter?\n\nContext: - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
WITH ENGINE = ""qdrant"",

PARAMETERS = {

""location"": "":memory:"",

""collection_config"": {

""size"": 386,

""distance"": ""Cosine""

}

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
[0.11, 0.22, 0.33, 0.44],

[0.55, 0.66, 0.77, 0.88],

[0.88, 0.11, 0.12, 0.13],

]

ids = [32, 21, ""b626f6a9-b14d-4af9-b7c3-43d8deb719a6""]

payload = [{""meta"": ""data""}, {""meta"": ""data_2""}, {""meta"": ""data_3"", ""extra"": ""data""}]



QdrantIngestOperator(

conn_id=""qdrant_connection""

task_id=""qdrant_ingest"",

collection_name=""<COLLECTION_NAME>"",

vectors=vectors,

ids=ids,

payload=payload,

)

```



## Reference
```http

POST /collections/{collection_name}/points/payload

{

""payload"": {

""property1"": ""string"",

""property2"": ""string""

},

""filter"": {

""must"": [

{

""key"": ""color"",

""match"": {

""value"": ""red""

}

}

]

}

}

```



```python

client.set_payload(

collection_name=""{collection_name}"",

payload={

""property1"": ""string"",
using: Some(""image"".to_string()),

limit: 10,

..Default::default()

})

.await?;

```



```java

import java.util.List;



import static io.qdrant.client.PointIdFactory.id;



import io.qdrant.client.grpc.Points.RecommendPoints;



client

.recommendAsync(

RecommendPoints.newBuilder()

.setCollectionName(""{collection_name}"")

.addAllPositive(List.of(id(100), id(231)))

.addAllNegative(List.of(id(718)))\n\nAnswer:",How do you use ‘ordering’ parameter?,"Write ordering can be specified for any write request to serialize it through a single “leader” node, which ensures that all write operations (issued with the same ordering) are performed and observed sequentially. It is of 3 types weak , medium and strong and is used in python with additional param ordering=models.WriteOrdering.STRONG to upsert request.","['- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', 'WITH ENGINE = ""qdrant"",\n\nPARAMETERS = {\n\n    ""location"": "":memory:"",\n\n    ""collection_config"": {\n\n        ""size"": 386,\n\n        ""distance"": ""Cosine""\n\n    }\n\n}\n\n```\n\n\n\nThe available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).\n\n\n\n## Creating a new table', '[0.11, 0.22, 0.33, 0.44],\n\n    [0.55, 0.66, 0.77, 0.88],\n\n    [0.88, 0.11, 0.12, 0.13],\n\n]\n\nids = [32, 21, ""b626f6a9-b14d-4af9-b7c3-43d8deb719a6""]\n\npayload = [{""meta"": ""data""}, {""meta"": ""data_2""}, {""meta"": ""data_3"", ""extra"": ""data""}]\n\n\n\nQdrantIngestOperator(\n\n    conn_id=""qdrant_connection""\n\n    task_id=""qdrant_ingest"",\n\n    collection_name=""<COLLECTION_NAME>"",\n\n    vectors=vectors,\n\n    ids=ids,\n\n    payload=payload,\n\n)\n\n```\n\n\n\n## Reference', '```http\n\nPOST /collections/{collection_name}/points/payload\n\n{\n\n    ""payload"": {\n\n        ""property1"": ""string"",\n\n        ""property2"": ""string""\n\n    },\n\n    ""filter"": {\n\n        ""must"": [\n\n            {\n\n                ""key"": ""color"",\n\n                ""match"": {\n\n                    ""value"": ""red""\n\n                }\n\n            }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.set_payload(\n\n    collection_name=""{collection_name}"",\n\n    payload={\n\n        ""property1"": ""string"",', 'using: Some(""image"".to_string()),\n\n        limit: 10,\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport java.util.List;\n\n\n\nimport static io.qdrant.client.PointIdFactory.id;\n\n\n\nimport io.qdrant.client.grpc.Points.RecommendPoints;\n\n\n\nclient\n\n    .recommendAsync(\n\n        RecommendPoints.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .addAllPositive(List.of(id(100), id(231)))\n\n            .addAllNegative(List.of(id(718)))']","- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
WITH ENGINE = ""qdrant"",

PARAMETERS = {

    ""location"": "":memory:"",

    ""collection_config"": {

        ""size"": 386,

        ""distance"": ""Cosine""

    }

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
[0.11, 0.22, 0.33, 0.44],

    [0.55, 0.66, 0.77, 0.88],

    [0.88, 0.11, 0.12, 0.13],

]

ids = [32, 21, ""b626f6a9-b14d-4af9-b7c3-43d8deb719a6""]

payload = [{""meta"": ""data""}, {""meta"": ""data_2""}, {""meta"": ""data_3"", ""extra"": ""data""}]



QdrantIngestOperator(

    conn_id=""qdrant_connection""

    task_id=""qdrant_ingest"",

    collection_name=""<COLLECTION_NAME>"",

    vectors=vectors,

    ids=ids,

    payload=payload,

)

```



## Reference
```http

POST /collections/{collection_name}/points/payload

{

    ""payload"": {

        ""property1"": ""string"",

        ""property2"": ""string""

    },

    ""filter"": {

        ""must"": [

            {

                ""key"": ""color"",

                ""match"": {

                    ""value"": ""red""

                }

            }

        ]

    }

}

```



```python

client.set_payload(

    collection_name=""{collection_name}"",

    payload={

        ""property1"": ""string"",
using: Some(""image"".to_string()),

        limit: 10,

        ..Default::default()

    })

    .await?;

```



```java

import java.util.List;



import static io.qdrant.client.PointIdFactory.id;



import io.qdrant.client.grpc.Points.RecommendPoints;



client

    .recommendAsync(

        RecommendPoints.newBuilder()

            .setCollectionName(""{collection_name}"")

            .addAllPositive(List.of(id(100), id(231)))

            .addAllNegative(List.of(id(718)))",0.1643835616438356,0.43636363636363634,0.23880597014925375,0.020689655172413793,0.05555555555555555,0.030150753768844223,0.1095890410958904,0.2909090909090909,0.15920398009950248,0.136986301369863,0.36363636363636365,0.1990049751243781,0.4244999885559082,0.536311149597168,0.5963313579559326,0.5647310018539429,0.6923611667783309,0.11615538597106934,153,2.732142857142857,0.24229507601028158
True,,1663.864,7335414f,What is significance of ‘on_disk_payload’ setting?,"The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes.","The significance of the `on_disk_payload` setting is that it defines where the payload data should be stored. If set to `true`, the payload data will be stored on disk only. This setting can be useful for limiting RAM usage, especially in cases where the payload data is large. It allows for efficient storage and management of payload data, offloading it from memory and onto disk to optimize resource usage.","* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.

* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
""on_disk"": true

    }

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),

    hnsw_config=models.HnswConfigDiff(on_disk=True),

)

```



```typescript
""on_disk"": true

        }

    }

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

  -H 'Content-Type: application/json' \

  --data-raw '{

    ""vectors"": {

        ""my_vector"": { 

           ""on_disk"": true 

      }

    }

  }'

```



In the following example the HNSW index and quantization parameters are updated,

both for the whole collection, and for `my_vector` specifically:





```http

PATCH /collections/{collection_name}

{
To enable this, you need to set the `hnsw_config.on_disk` parameter to `true` during collection [creation](../collections/#create-a-collection) or [updating](../collections/#update-collection-parameters).



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""hnsw_config"": {

        ""on_disk"": true

    }

}

```



```python","['* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.\n\n* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\n\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', '""on_disk"": true\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),\n\n    hnsw_config=models.HnswConfigDiff(on_disk=True),\n\n)\n\n```\n\n\n\n```typescript', '""on_disk"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""vectors"": {\n\n        ""my_vector"": { \n\n           ""on_disk"": true \n\n      }\n\n    }\n\n  }\'\n\n```\n\n\n\nIn the following example the HNSW index and quantization parameters are updated,\n\nboth for the whole collection, and for `my_vector` specifically:\n\n\n\n\n\n```http\n\nPATCH /collections/{collection_name}\n\n{', 'To enable this, you need to set the `hnsw_config.on_disk` parameter to `true` during collection [creation](../collections/#create-a-collection) or [updating](../collections/#update-collection-parameters).\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""hnsw_config"": {\n\n        ""on_disk"": true\n\n    }\n\n}\n\n```\n\n\n\n```python']","Question: What is significance of ‘on_disk_payload’ setting?\n\nContext: * `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.

* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.
""on_disk_payload"": false

},

""hnsw_config"": {

""m"": 16,

""ef_construct"": 100,

""full_scan_threshold"": 10000,

""max_indexing_threads"": 0

},

""optimizer_config"": {

""deleted_threshold"": 0.2,

""vacuum_min_vector_number"": 1000,

""default_segment_number"": 0,

""max_segment_size"": null,
""on_disk"": true

}

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),

hnsw_config=models.HnswConfigDiff(on_disk=True),

)

```



```typescript
""on_disk"": true

}

}

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

-H 'Content-Type: application/json' \

--data-raw '{

""vectors"": {

""my_vector"": { 

""on_disk"": true 

}

}

}'

```



In the following example the HNSW index and quantization parameters are updated,

both for the whole collection, and for `my_vector` specifically:





```http

PATCH /collections/{collection_name}

{
To enable this, you need to set the `hnsw_config.on_disk` parameter to `true` during collection [creation](../collections/#create-a-collection) or [updating](../collections/#update-collection-parameters).



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""optimizers_config"": {

""memmap_threshold"": 20000

},

""hnsw_config"": {

""on_disk"": true

}

}

```



```python\n\nAnswer:",What is significance of ‘on_disk_payload’ setting?,"The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes.","['* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.\n\n* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\n\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', '""on_disk"": true\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),\n\n    hnsw_config=models.HnswConfigDiff(on_disk=True),\n\n)\n\n```\n\n\n\n```typescript', '""on_disk"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""vectors"": {\n\n        ""my_vector"": { \n\n           ""on_disk"": true \n\n      }\n\n    }\n\n  }\'\n\n```\n\n\n\nIn the following example the HNSW index and quantization parameters are updated,\n\nboth for the whole collection, and for `my_vector` specifically:\n\n\n\n\n\n```http\n\nPATCH /collections/{collection_name}\n\n{', 'To enable this, you need to set the `hnsw_config.on_disk` parameter to `true` during collection [creation](../collections/#create-a-collection) or [updating](../collections/#update-collection-parameters).\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""hnsw_config"": {\n\n        ""on_disk"": true\n\n    }\n\n}\n\n```\n\n\n\n```python']","* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.

* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
""on_disk"": true

    }

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),

    hnsw_config=models.HnswConfigDiff(on_disk=True),

)

```



```typescript
""on_disk"": true

        }

    }

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

  -H 'Content-Type: application/json' \

  --data-raw '{

    ""vectors"": {

        ""my_vector"": { 

           ""on_disk"": true 

      }

    }

  }'

```



In the following example the HNSW index and quantization parameters are updated,

both for the whole collection, and for `my_vector` specifically:





```http

PATCH /collections/{collection_name}

{
To enable this, you need to set the `hnsw_config.on_disk` parameter to `true` during collection [creation](../collections/#create-a-collection) or [updating](../collections/#update-collection-parameters).



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""hnsw_config"": {

        ""on_disk"": true

    }

}

```



```python",0.6060606060606061,0.425531914893617,0.5,0.2,0.13978494623655913,0.16455696202531644,0.36363636363636365,0.2553191489361702,0.29999999999999993,0.5454545454545454,0.3829787234042553,0.44999999999999996,0.8218528032302856,0.6715302467346191,0.640785813331604,0.6557978987693787,0.9825868369080126,0.5186407808214426,69,0.696969696969697,0.6237824172460477
True,,4596.389,04d1a8f5,What is the impact of ‘write_consistency_factor’ ?,"The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully.","The impact of the `write_consistency_factor` parameter is that it defines the level of consistency required for write operations in a distributed system. When this factor is set to a value greater than 1, it means that a certain number of replicas must acknowledge the write operation before it is considered successful and the client receives a response.

Increasing the `write_consistency_factor` value makes write operations more tolerant to network partitions within the cluster. This means that even if some replicas are temporarily unreachable due to network issues, as long as the required number of replicas are available and acknowledge the write operation, it will be considered successful.

However, setting a higher `write_consistency_factor` also means that a higher number of replicas need to be active and reachable in order to perform write operations. This can impact the availability and performance of the system, as the system must wait for a greater number of replicas to respond before completing the write operation.

In summary, increasing the `write_consistency_factor` provides stronger consistency guarantees for write operations in distributed systems, but it can also introduce trade-offs in terms of availability and performance.","- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

    ""write_consistency_factor"": 2,

}

```



```python
""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

    shard_number=6,

    replication_factor=2,

    write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

	shardNumber: 6,

	replicationFactor: 2,

	writeConsistencyFactor: 2

);

```



Write operations will fail if the number of active replicas is less than the `write_consistency_factor`.



### Read consistency



Read `consistency` can be specified for most read requests and will ensure that the returned result

is consistent across cluster nodes.
replication_factor: Some(2),

        write_consistency_factor: Some(2),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =","['- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.', '### Write consistency factor\n\n\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\n\nIt can be configured at the collection\'s creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python', '""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n\n    shard_number=6,\n\n    replication_factor=2,\n\n    write_consistency_factor=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', 'collectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\n\tshardNumber: 6,\n\n\treplicationFactor: 2,\n\n\twriteConsistencyFactor: 2\n\n);\n\n```\n\n\n\nWrite operations will fail if the number of active replicas is less than the `write_consistency_factor`.\n\n\n\n### Read consistency\n\n\n\nRead `consistency` can be specified for most read requests and will ensure that the returned result\n\nis consistent across cluster nodes.', 'replication_factor: Some(2),\n\n        write_consistency_factor: Some(2),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =']","Question: What is the impact of ‘write_consistency_factor’ ?\n\nContext: - `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 300,

""distance"": ""Cosine""

},

""shard_number"": 6,

""replication_factor"": 2,

""write_consistency_factor"": 2,

}

```



```python
""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

shard_number=6,

replication_factor=2,

write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
collectionName: ""{collection_name}"",

vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

shardNumber: 6,

replicationFactor: 2,

writeConsistencyFactor: 2

);

```



Write operations will fail if the number of active replicas is less than the `write_consistency_factor`.



### Read consistency



Read `consistency` can be specified for most read requests and will ensure that the returned result

is consistent across cluster nodes.
replication_factor: Some(2),

write_consistency_factor: Some(2),

..Default::default()

})

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =\n\nAnswer:",What is the impact of ‘write_consistency_factor’ ?,"The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully.","['- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.', '### Write consistency factor\n\n\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\n\nIt can be configured at the collection\'s creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python', '""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n\n    shard_number=6,\n\n    replication_factor=2,\n\n    write_consistency_factor=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', 'collectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\n\tshardNumber: 6,\n\n\treplicationFactor: 2,\n\n\twriteConsistencyFactor: 2\n\n);\n\n```\n\n\n\nWrite operations will fail if the number of active replicas is less than the `write_consistency_factor`.\n\n\n\n### Read consistency\n\n\n\nRead `consistency` can be specified for most read requests and will ensure that the returned result\n\nis consistent across cluster nodes.', 'replication_factor: Some(2),\n\n        write_consistency_factor: Some(2),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =']","- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

    ""write_consistency_factor"": 2,

}

```



```python
""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

    shard_number=6,

    replication_factor=2,

    write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

	shardNumber: 6,

	replicationFactor: 2,

	writeConsistencyFactor: 2

);

```



Write operations will fail if the number of active replicas is less than the `write_consistency_factor`.



### Read consistency



Read `consistency` can be specified for most read requests and will ensure that the returned result

is consistent across cluster nodes.
replication_factor: Some(2),

        write_consistency_factor: Some(2),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =",0.29310344827586204,0.9107142857142857,0.4434782608695652,0.18497109826589594,0.5818181818181818,0.2807017543859649,0.25287356321839083,0.7857142857142857,0.3826086956521739,0.28160919540229884,0.875,0.42608695652173917,0.8734761476516724,0.628726065158844,0.748778223991394,0.6835207343101501,0.8932702550664544,0.9414428891032003,186,3.0,0.7434092079894886
True,,3956.732,3c10f3b1,What is the purpose of oversampling in Qdrant search process?,Oversampling in Qdrant search process defines how many extra vectors should be pre-selected using quantized index and then re-scored using original vectors to improve search quality.,"The purpose of oversampling in the Qdrant search process is to control the precision of the search in real-time during the query time. By using oversampling, Qdrant can internally retrieve a slightly larger number of vectors from the quantized storage than actually needed. This allows for rescoring with original vectors to assign more precise scores to the retrieved vectors. 

In practical terms, oversampling ensures that more vectors are pre-selected using the quantized index than required, and then the top results are returned after re-scoring. For example, if the oversampling factor is set to 2.4 and the limit is 100, 240 vectors will be pre-selected using quantized index, and then the top 100 will be returned after re-scoring.

Oversampling is a useful technique for fine-tuning the trade-off between search speed and search quality during the query time in the Qdrant search process.","### Oversampling and Rescoring



A distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.

This enables the best combination of speed, accuracy, and RAM usage.



Qdrant stores the original vectors, so it is possible to rescore the top-k results with

the original vectors after doing the neighbours search in quantized space. That obviously
{

		Quantization = new QuantizationSearchParams

		{

			Ignore = false,

			Rescore = true,

			Oversampling = 2.0

		}

	},

	limit: 10

);

```



`ignore` - Toggle whether to ignore quantized vectors during the search process. By default, Qdrant will use quantized vectors if they are available.



`rescore` - Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors.
Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user
For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.

Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.



## Quantization tips



#### Accuracy tuning



In this section, we will discuss how to tune the search precision.
.setOversampling(2.0)

                            .build())

                    .build())

            .setLimit(10)

            .build())

    .get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.SearchAsync(

	collectionName: ""{collection_name}"",

	vector: new float[] { 0.2f, 0.1f, 0.9f, 0.7f },

	searchParams: new SearchParams

	{

		Quantization = new QuantizationSearchParams

		{","['### Oversampling and Rescoring\n\n\n\nA distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.\n\nThis enables the best combination of speed, accuracy, and RAM usage.\n\n\n\nQdrant stores the original vectors, so it is possible to rescore the top-k results with\n\nthe original vectors after doing the neighbours search in quantized space. That obviously', '{\n\n\t\tQuantization = new QuantizationSearchParams\n\n\t\t{\n\n\t\t\tIgnore = false,\n\n\t\t\tRescore = true,\n\n\t\t\tOversampling = 2.0\n\n\t\t}\n\n\t},\n\n\tlimit: 10\n\n);\n\n```\n\n\n\n`ignore` - Toggle whether to ignore quantized vectors during the search process. By default, Qdrant will use quantized vectors if they are available.\n\n\n\n`rescore` - Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors.', 'Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user', 'For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.\n\nOversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.\n\n\n\n## Quantization tips\n\n\n\n#### Accuracy tuning\n\n\n\nIn this section, we will discuss how to tune the search precision.', '.setOversampling(2.0)\n\n                            .build())\n\n                    .build())\n\n            .setLimit(10)\n\n            .build())\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.SearchAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tvector: new float[] { 0.2f, 0.1f, 0.9f, 0.7f },\n\n\tsearchParams: new SearchParams\n\n\t{\n\n\t\tQuantization = new QuantizationSearchParams\n\n\t\t{']","Question: What is the purpose of oversampling in Qdrant search process?\n\nContext: ### Oversampling and Rescoring



A distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.

This enables the best combination of speed, accuracy, and RAM usage.



Qdrant stores the original vectors, so it is possible to rescore the top-k results with

the original vectors after doing the neighbours search in quantized space. That obviously
{

Quantization = new QuantizationSearchParams

{

Ignore = false,

Rescore = true,

Oversampling = 2.0

}

},

limit: 10

);

```



`ignore` - Toggle whether to ignore quantized vectors during the search process. By default, Qdrant will use quantized vectors if they are available.



`rescore` - Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors.
Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user
For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.

Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.



## Quantization tips



#### Accuracy tuning



In this section, we will discuss how to tune the search precision.
.setOversampling(2.0)

.build())

.build())

.setLimit(10)

.build())

.get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.SearchAsync(

collectionName: ""{collection_name}"",

vector: new float[] { 0.2f, 0.1f, 0.9f, 0.7f },

searchParams: new SearchParams

{

Quantization = new QuantizationSearchParams

{\n\nAnswer:",What is the purpose of oversampling in Qdrant search process?,Oversampling in Qdrant search process defines how many extra vectors should be pre-selected using quantized index and then re-scored using original vectors to improve search quality.,"['### Oversampling and Rescoring\n\n\n\nA distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.\n\nThis enables the best combination of speed, accuracy, and RAM usage.\n\n\n\nQdrant stores the original vectors, so it is possible to rescore the top-k results with\n\nthe original vectors after doing the neighbours search in quantized space. That obviously', '{\n\n\t\tQuantization = new QuantizationSearchParams\n\n\t\t{\n\n\t\t\tIgnore = false,\n\n\t\t\tRescore = true,\n\n\t\t\tOversampling = 2.0\n\n\t\t}\n\n\t},\n\n\tlimit: 10\n\n);\n\n```\n\n\n\n`ignore` - Toggle whether to ignore quantized vectors during the search process. By default, Qdrant will use quantized vectors if they are available.\n\n\n\n`rescore` - Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors.', 'Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user', 'For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.\n\nOversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.\n\n\n\n## Quantization tips\n\n\n\n#### Accuracy tuning\n\n\n\nIn this section, we will discuss how to tune the search precision.', '.setOversampling(2.0)\n\n                            .build())\n\n                    .build())\n\n            .setLimit(10)\n\n            .build())\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.SearchAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tvector: new float[] { 0.2f, 0.1f, 0.9f, 0.7f },\n\n\tsearchParams: new SearchParams\n\n\t{\n\n\t\tQuantization = new QuantizationSearchParams\n\n\t\t{']","### Oversampling and Rescoring



A distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.

This enables the best combination of speed, accuracy, and RAM usage.



Qdrant stores the original vectors, so it is possible to rescore the top-k results with

the original vectors after doing the neighbours search in quantized space. That obviously
{

		Quantization = new QuantizationSearchParams

		{

			Ignore = false,

			Rescore = true,

			Oversampling = 2.0

		}

	},

	limit: 10

);

```



`ignore` - Toggle whether to ignore quantized vectors during the search process. By default, Qdrant will use quantized vectors if they are available.



`rescore` - Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors.
Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user
For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.

Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.



## Quantization tips



#### Accuracy tuning



In this section, we will discuss how to tune the search precision.
.setOversampling(2.0)

                            .build())

                    .build())

            .setLimit(10)

            .build())

    .get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.SearchAsync(

	collectionName: ""{collection_name}"",

	vector: new float[] { 0.2f, 0.1f, 0.9f, 0.7f },

	searchParams: new SearchParams

	{

		Quantization = new QuantizationSearchParams

		{",0.16666666666666666,0.7857142857142857,0.27499999999999997,0.11450381679389313,0.5555555555555556,0.189873417721519,0.14393939393939395,0.6785714285714286,0.23750000000000004,0.14393939393939395,0.6785714285714286,0.23750000000000004,0.9180518984794617,0.5572621822357178,0.673227846622467,0.6097805500030518,0.9982784066426879,0.9804209694266319,141,5.423076923076923,0.4216660939396358
True,,3977.4379999999996,8ffa68eb,How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?,"Qdrant uses a different approach that does not require pre- or post-filtering, effectively addressing the accuracy problem while maintaining search efficiency.","Qdrant addresses the search accuracy problem in comparison to other search engines by using a custom modification of the HNSW algorithm for Approximate Nearest Neighbor Search (ANN). This modification allows for querying the results with state-of-the-art speed and applying filters without compromising on the accuracy of the results.

The HNSW algorithm is chosen for its compatibility with filters during a search and its reputation as one of the most accurate and fastest algorithms according to public benchmarks. By fine-tuning the HNSW parameters on a collection and named vector level, Qdrant can optimize search performance.

The unique modification of the HNSW algorithm in Qdrant ensures that when a user queries the database, the query is converted into a vector (query vector). The vector search starts at the top layer of the HNSW index, quickly identifying the area of the graph likely to contain vectors closest to the query vector. By comparing the query vector to all others using metrics like distance or similarity, Qdrant ensures accurate and efficient matching.

Overall, Qdrant's use of the HNSW algorithm, along with its custom modifications and ability to apply filters during searches, helps optimize search accuracy and speed, making it a suitable solution for high-throughput applications with real-time latency requirements.","HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements
2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.

3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.

4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.

5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.
The way it works is, when the user queries the database, this query is also converted into a vector (the query vector). The [vector search](https://qdrant.tech/documentation/overview/vector-search/) starts at the top layer of the HNSW index, where the algorithm quickly identifies the area of the graph likely to contain vectors closest to the query vector. The algorithm compares your query vector to all the others, using metrics like ""distance"" or ""similarity"" to gauge how close they are.
In this episode, Rishabh dives into the nitty-gritty of creating a high-performance hotel matching solution with Qdrant, covering everything from data inconsistency challenges to the speed and accuracy enhancements achieved through the HNSW algorithm.



5 Keys to Learning from the Episode:



1. Discover the importance of data consistency and the challenges it poses when dealing with multiple sources and languages.","['HNSW is chosen for several reasons.\n\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\n\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\n\n\n\n*Available as of v1.1.1*\n\n\n\nThe HNSW parameters can also be configured on a collection and named vector\n\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\n\nperformance.', 'Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements', '2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.\n\n3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.\n\n4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.\n\n5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.', 'The way it works is, when the user queries the database, this query is also converted into a vector (the query vector). The [vector search](https://qdrant.tech/documentation/overview/vector-search/) starts at the top layer of the HNSW index, where the algorithm quickly identifies the area of the graph likely to contain vectors closest to the query vector. The algorithm compares your query vector to all the others, using metrics like ""distance"" or ""similarity"" to gauge how close they are.', 'In this episode, Rishabh dives into the nitty-gritty of creating a high-performance hotel matching solution with Qdrant, covering everything from data inconsistency challenges to the speed and accuracy enhancements achieved through the HNSW algorithm.\n\n\n\n5 Keys to Learning from the Episode:\n\n\n\n1. Discover the importance of data consistency and the challenges it poses when dealing with multiple sources and languages.']","Question: How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?\n\nContext: HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements
2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.

3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.

4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.

5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.
The way it works is, when the user queries the database, this query is also converted into a vector (the query vector). The [vector search](https://qdrant.tech/documentation/overview/vector-search/) starts at the top layer of the HNSW index, where the algorithm quickly identifies the area of the graph likely to contain vectors closest to the query vector. The algorithm compares your query vector to all the others, using metrics like ""distance"" or ""similarity"" to gauge how close they are.
In this episode, Rishabh dives into the nitty-gritty of creating a high-performance hotel matching solution with Qdrant, covering everything from data inconsistency challenges to the speed and accuracy enhancements achieved through the HNSW algorithm.



5 Keys to Learning from the Episode:



1. Discover the importance of data consistency and the challenges it poses when dealing with multiple sources and languages.\n\nAnswer:",How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?,"Qdrant uses a different approach that does not require pre- or post-filtering, effectively addressing the accuracy problem while maintaining search efficiency.","['HNSW is chosen for several reasons.\n\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\n\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\n\n\n\n*Available as of v1.1.1*\n\n\n\nThe HNSW parameters can also be configured on a collection and named vector\n\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\n\nperformance.', 'Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements', '2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.\n\n3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.\n\n4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.\n\n5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.', 'The way it works is, when the user queries the database, this query is also converted into a vector (the query vector). The [vector search](https://qdrant.tech/documentation/overview/vector-search/) starts at the top layer of the HNSW index, where the algorithm quickly identifies the area of the graph likely to contain vectors closest to the query vector. The algorithm compares your query vector to all the others, using metrics like ""distance"" or ""similarity"" to gauge how close they are.', 'In this episode, Rishabh dives into the nitty-gritty of creating a high-performance hotel matching solution with Qdrant, covering everything from data inconsistency challenges to the speed and accuracy enhancements achieved through the HNSW algorithm.\n\n\n\n5 Keys to Learning from the Episode:\n\n\n\n1. Discover the importance of data consistency and the challenges it poses when dealing with multiple sources and languages.']","HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements
2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.

3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.

4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.

5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.
The way it works is, when the user queries the database, this query is also converted into a vector (the query vector). The [vector search](https://qdrant.tech/documentation/overview/vector-search/) starts at the top layer of the HNSW index, where the algorithm quickly identifies the area of the graph likely to contain vectors closest to the query vector. The algorithm compares your query vector to all the others, using metrics like ""distance"" or ""similarity"" to gauge how close they are.
In this episode, Rishabh dives into the nitty-gritty of creating a high-performance hotel matching solution with Qdrant, covering everything from data inconsistency challenges to the speed and accuracy enhancements achieved through the HNSW algorithm.



5 Keys to Learning from the Episode:



1. Discover the importance of data consistency and the challenges it poses when dealing with multiple sources and languages.",0.05945945945945946,0.55,0.1073170731707317,0.005434782608695652,0.05263157894736842,0.009852216748768475,0.032432432432432434,0.3,0.058536585365853655,0.05405405405405406,0.5,0.0975609756097561,0.658210039138794,0.4909214973449707,0.5946347117424011,0.5378237366676331,0.990798610568163,0.018901407718658447,206,9.80952380952381,0.35782658550183705
True,,5276.95,8aeb9e0c,What is the difference between regular and neural search?,"Regular full-text search involves searching for keywords within a document, while neural search considers the real meaning of the query and documents, allowing for more accurate results.","Regular search and neural search differ in their approach to information retrieval and understanding. 

Regular full-text search, like Google's search engine, primarily involves searching for keywords within a document. This method does not consider the true meaning or context of the query and documents, leading to potential limitations in finding relevant information when different wording is used. On the other hand, neural search utilizes neural networks to understand the semantics and context of the search query and documents, enabling a more sophisticated and accurate retrieval of information.

Neural networks used for search can vary depending on the specific task and requirements. Commonly used neural networks for search tasks include models like BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), and other deep learning architectures designed for natural language processing and information retrieval.

Neural network search is particularly useful in tasks where understanding the context and meaning of the query and documents is crucial for accurate information retrieval. This includes applications like question answering, semantic search, personalized recommendations, and other scenarios where traditional keyword-based search falls short in capturing the nuances of language and user intent.

Overall, neural search offers a more advanced and nuanced approach to information retrieval compared to traditional keyword-based search methods, leveraging the power of neural networks to enhance search accuracy and relevance.","In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



**What is neural search?**
These days, search technology is the heart of a variety of applications.

From web-pages search to product recommendations.

For many years, this technology didn't get much change until neural networks came into play.



In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?
**What is neural search?**



A regular full-text search, such as Google’s, consists of searching for keywords inside a document. For this reason, the algorithm can not take into account the real meaning of the query and documents. Many documents that might be of interest to the user are not found because they use different wording.
* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



## What is neural search?



A regular full-text search, such as Google's, consists of searching for keywords inside a document.

For this reason, the algorithm can not take into account the real meaning of the query and documents.

Many documents that might be of interest to the user are not found because they use different wording.
The demo contains a switch that selects between neural and full-text searches.

You can turn neural search on and off to compare the result with regular full-text search.

Try to use startup description to find similar ones. 



## Conclusion



In this tutorial, I have tried to give minimal information about neural search, but enough to start using it.

Many potential applications are not mentioned here, this is a space to go further into the subject.","['In this tutorial we are going to find answers to these questions:\n\n\n\n* What is the difference between regular and neural search?\n\n* What neural networks could be used for search?\n\n* In what tasks is neural network search useful?\n\n* How to build and deploy own neural search service step-by-step?\n\n\n\n**What is neural search?**', ""These days, search technology is the heart of a variety of applications.\n\nFrom web-pages search to product recommendations.\n\nFor many years, this technology didn't get much change until neural networks came into play.\n\n\n\nIn this tutorial we are going to find answers to these questions:\n\n\n\n* What is the difference between regular and neural search?\n\n* What neural networks could be used for search?\n\n* In what tasks is neural network search useful?"", '**What is neural search?**\n\n\n\nA regular full-text search, such as Google’s, consists of searching for keywords inside a document. For this reason, the algorithm can not take into account the real meaning of the query and documents. Many documents that might be of interest to the user are not found because they use different wording.', ""* In what tasks is neural network search useful?\n\n* How to build and deploy own neural search service step-by-step?\n\n\n\n## What is neural search?\n\n\n\nA regular full-text search, such as Google's, consists of searching for keywords inside a document.\n\nFor this reason, the algorithm can not take into account the real meaning of the query and documents.\n\nMany documents that might be of interest to the user are not found because they use different wording."", 'The demo contains a switch that selects between neural and full-text searches.\n\nYou can turn neural search on and off to compare the result with regular full-text search.\n\nTry to use startup description to find similar ones. \n\n\n\n## Conclusion\n\n\n\nIn this tutorial, I have tried to give minimal information about neural search, but enough to start using it.\n\nMany potential applications are not mentioned here, this is a space to go further into the subject.']","Question: What is the difference between regular and neural search?\n\nContext: In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



**What is neural search?**
These days, search technology is the heart of a variety of applications.

From web-pages search to product recommendations.

For many years, this technology didn't get much change until neural networks came into play.



In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?
**What is neural search?**



A regular full-text search, such as Google’s, consists of searching for keywords inside a document. For this reason, the algorithm can not take into account the real meaning of the query and documents. Many documents that might be of interest to the user are not found because they use different wording.
* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



## What is neural search?



A regular full-text search, such as Google's, consists of searching for keywords inside a document.

For this reason, the algorithm can not take into account the real meaning of the query and documents.

Many documents that might be of interest to the user are not found because they use different wording.
The demo contains a switch that selects between neural and full-text searches.

You can turn neural search on and off to compare the result with regular full-text search.

Try to use startup description to find similar ones. 



## Conclusion



In this tutorial, I have tried to give minimal information about neural search, but enough to start using it.

Many potential applications are not mentioned here, this is a space to go further into the subject.\n\nAnswer:",What is the difference between regular and neural search?,"Regular full-text search involves searching for keywords within a document, while neural search considers the real meaning of the query and documents, allowing for more accurate results.","['In this tutorial we are going to find answers to these questions:\n\n\n\n* What is the difference between regular and neural search?\n\n* What neural networks could be used for search?\n\n* In what tasks is neural network search useful?\n\n* How to build and deploy own neural search service step-by-step?\n\n\n\n**What is neural search?**', ""These days, search technology is the heart of a variety of applications.\n\nFrom web-pages search to product recommendations.\n\nFor many years, this technology didn't get much change until neural networks came into play.\n\n\n\nIn this tutorial we are going to find answers to these questions:\n\n\n\n* What is the difference between regular and neural search?\n\n* What neural networks could be used for search?\n\n* In what tasks is neural network search useful?"", '**What is neural search?**\n\n\n\nA regular full-text search, such as Google’s, consists of searching for keywords inside a document. For this reason, the algorithm can not take into account the real meaning of the query and documents. Many documents that might be of interest to the user are not found because they use different wording.', ""* In what tasks is neural network search useful?\n\n* How to build and deploy own neural search service step-by-step?\n\n\n\n## What is neural search?\n\n\n\nA regular full-text search, such as Google's, consists of searching for keywords inside a document.\n\nFor this reason, the algorithm can not take into account the real meaning of the query and documents.\n\nMany documents that might be of interest to the user are not found because they use different wording."", 'The demo contains a switch that selects between neural and full-text searches.\n\nYou can turn neural search on and off to compare the result with regular full-text search.\n\nTry to use startup description to find similar ones. \n\n\n\n## Conclusion\n\n\n\nIn this tutorial, I have tried to give minimal information about neural search, but enough to start using it.\n\nMany potential applications are not mentioned here, this is a space to go further into the subject.']","In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



**What is neural search?**
These days, search technology is the heart of a variety of applications.

From web-pages search to product recommendations.

For many years, this technology didn't get much change until neural networks came into play.



In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?
**What is neural search?**



A regular full-text search, such as Google’s, consists of searching for keywords inside a document. For this reason, the algorithm can not take into account the real meaning of the query and documents. Many documents that might be of interest to the user are not found because they use different wording.
* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



## What is neural search?



A regular full-text search, such as Google's, consists of searching for keywords inside a document.

For this reason, the algorithm can not take into account the real meaning of the query and documents.

Many documents that might be of interest to the user are not found because they use different wording.
The demo contains a switch that selects between neural and full-text searches.

You can turn neural search on and off to compare the result with regular full-text search.

Try to use startup description to find similar ones. 



## Conclusion



In this tutorial, I have tried to give minimal information about neural search, but enough to start using it.

Many potential applications are not mentioned here, this is a space to go further into the subject.",0.1,0.84,0.17872340425531916,0.06220095693779904,0.5416666666666666,0.11158798283261803,0.09047619047619047,0.76,0.16170212765957445,0.09523809523809523,0.8,0.1702127659574468,0.8414019346237183,0.5242940187454224,0.7137635350227356,0.6045307517051697,0.8836907725393152,0.980741485953331,219,8.11111111111111,0.2202923699638814
True,,5053.009,687f2642,How can I use Qdrant as a vector store in Langchain Go?,"You can use Qdrant as a vector store in Langchain Go by installing the `langchain-go` project dependency and customizing the values for your configuration, such as the Qdrant REST URL and collection name.","To use Qdrant as a vector store in Langchain Go, you can follow these steps:

1. Install the `langchain-go` project dependency using the following command:

```bash
go get -u github.com/tmc/langchaingo
```

2. Customize the configuration values in your code:

```go
""github.com/tmc/langchaingo/vectorstores""
""github.com/tmc/langchaingo/vectorstores/qdrant""
```

3. Initialize Qdrant with your specific configuration values:

```go
llm, err := openai.New()
if err != nil {
    log.Fatal(err)
}

e, err := embeddings.NewEmbedder(llm)
if err != nil {
    log.Fatal(err)
}

url, err := url.Parse(""YOUR_QDRANT_REST_URL"")
if err != nil {
    log.Fatal(err)
}

store, err := qdrant.New(
    qdrant.WithURL(*url),
    qdrant.WithCollectionName(""YOUR_COLLECTION_NAME""),
    qdrant.WithEmbedder(e),
)
if err != nil {
    log.Fatal(err)
}
```

Additionally, you can refer to the Qdrant documentation for more information on how to use it as a vector store in Langchain Go.","---

title: Langchain Go

weight: 120

---



# Langchain Go



[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.



You can use Qdrant as a vector store in Langchain Go.



## Setup



Install the `langchain-go` project dependency



```bash

go get -u github.com/tmc/langchaingo

```



## Usage



Before you use the following code sample, customize the following values for your configuration:
""github.com/tmc/langchaingo/vectorstores""

        ""github.com/tmc/langchaingo/vectorstores/qdrant""

)



 llm, err := openai.New()

 if err != nil {

  log.Fatal(err)

 }



 e, err := embeddings.NewEmbedder(llm)

 if err != nil {

  log.Fatal(err)

 }



 url, err := url.Parse(""YOUR_QDRANT_REST_URL"")

 if err != nil {

  log.Fatal(err)

 }



 store, err := qdrant.New(

  qdrant.WithURL(*url),

  qdrant.WithCollectionName(""YOUR_COLLECTION_NAME""),

  qdrant.WithEmbedder(e),

 )

 if err != nil {
Qdrant is the only vector database with full coverage of async API in Langchain. Their documentation [describes how to use 

it](https://python.langchain.com/docs/modules/data_connection/vectorstores/#asynchronous-operations).
```bash

pip install langchain

```



Qdrant acts as a vector index that may store the embeddings with the documents used to generate them. There are various ways 

how to use it, but calling `Qdrant.from_texts` is probably the most straightforward way how to get started:



```python

from langchain.vectorstores import Qdrant

from langchain.embeddings import HuggingFaceEmbeddings



embeddings = HuggingFaceEmbeddings(

    model_name=""sentence-transformers/all-mpnet-base-v2""

)
might also be found in the [LangChain documentation](https://python.langchain.com/docs/integrations/vectorstores/qdrant).","['---\n\ntitle: Langchain Go\n\nweight: 120\n\n---\n\n\n\n# Langchain Go\n\n\n\n[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.\n\n\n\nYou can use Qdrant as a vector store in Langchain Go.\n\n\n\n## Setup\n\n\n\nInstall the `langchain-go` project dependency\n\n\n\n```bash\n\ngo get -u github.com/tmc/langchaingo\n\n```\n\n\n\n## Usage\n\n\n\nBefore you use the following code sample, customize the following values for your configuration:', '""github.com/tmc/langchaingo/vectorstores""\n\n        ""github.com/tmc/langchaingo/vectorstores/qdrant""\n\n)\n\n\n\n llm, err := openai.New()\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n\n\n e, err := embeddings.NewEmbedder(llm)\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n\n\n url, err := url.Parse(""YOUR_QDRANT_REST_URL"")\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n\n\n store, err := qdrant.New(\n\n  qdrant.WithURL(*url),\n\n  qdrant.WithCollectionName(""YOUR_COLLECTION_NAME""),\n\n  qdrant.WithEmbedder(e),\n\n )\n\n if err != nil {', 'Qdrant is the only vector database with full coverage of async API in Langchain. Their documentation [describes how to use \n\nit](https://python.langchain.com/docs/modules/data_connection/vectorstores/#asynchronous-operations).', '```bash\n\npip install langchain\n\n```\n\n\n\nQdrant acts as a vector index that may store the embeddings with the documents used to generate them. There are various ways \n\nhow to use it, but calling `Qdrant.from_texts` is probably the most straightforward way how to get started:\n\n\n\n```python\n\nfrom langchain.vectorstores import Qdrant\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\n\n\nembeddings = HuggingFaceEmbeddings(\n\n    model_name=""sentence-transformers/all-mpnet-base-v2""\n\n)', 'might also be found in the [LangChain documentation](https://python.langchain.com/docs/integrations/vectorstores/qdrant).']","Question: How can I use Qdrant as a vector store in Langchain Go?\n\nContext: ---

title: Langchain Go

weight: 120

---



# Langchain Go



[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.



You can use Qdrant as a vector store in Langchain Go.



## Setup



Install the `langchain-go` project dependency



```bash

go get -u github.com/tmc/langchaingo

```



## Usage



Before you use the following code sample, customize the following values for your configuration:
""github.com/tmc/langchaingo/vectorstores""

""github.com/tmc/langchaingo/vectorstores/qdrant""

)



llm, err := openai.New()

if err != nil {

log.Fatal(err)

}



e, err := embeddings.NewEmbedder(llm)

if err != nil {

log.Fatal(err)

}



url, err := url.Parse(""YOUR_QDRANT_REST_URL"")

if err != nil {

log.Fatal(err)

}



store, err := qdrant.New(

qdrant.WithURL(*url),

qdrant.WithCollectionName(""YOUR_COLLECTION_NAME""),

qdrant.WithEmbedder(e),

)

if err != nil {
Qdrant is the only vector database with full coverage of async API in Langchain. Their documentation [describes how to use 

it](https://python.langchain.com/docs/modules/data_connection/vectorstores/#asynchronous-operations).
```bash

pip install langchain

```



Qdrant acts as a vector index that may store the embeddings with the documents used to generate them. There are various ways 

how to use it, but calling `Qdrant.from_texts` is probably the most straightforward way how to get started:



```python

from langchain.vectorstores import Qdrant

from langchain.embeddings import HuggingFaceEmbeddings



embeddings = HuggingFaceEmbeddings(

model_name=""sentence-transformers/all-mpnet-base-v2""

)
might also be found in the [LangChain documentation](https://python.langchain.com/docs/integrations/vectorstores/qdrant).\n\nAnswer:",How can I use Qdrant as a vector store in Langchain Go?,"You can use Qdrant as a vector store in Langchain Go by installing the `langchain-go` project dependency and customizing the values for your configuration, such as the Qdrant REST URL and collection name.","['---\n\ntitle: Langchain Go\n\nweight: 120\n\n---\n\n\n\n# Langchain Go\n\n\n\n[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.\n\n\n\nYou can use Qdrant as a vector store in Langchain Go.\n\n\n\n## Setup\n\n\n\nInstall the `langchain-go` project dependency\n\n\n\n```bash\n\ngo get -u github.com/tmc/langchaingo\n\n```\n\n\n\n## Usage\n\n\n\nBefore you use the following code sample, customize the following values for your configuration:', '""github.com/tmc/langchaingo/vectorstores""\n\n        ""github.com/tmc/langchaingo/vectorstores/qdrant""\n\n)\n\n\n\n llm, err := openai.New()\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n\n\n e, err := embeddings.NewEmbedder(llm)\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n\n\n url, err := url.Parse(""YOUR_QDRANT_REST_URL"")\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n\n\n store, err := qdrant.New(\n\n  qdrant.WithURL(*url),\n\n  qdrant.WithCollectionName(""YOUR_COLLECTION_NAME""),\n\n  qdrant.WithEmbedder(e),\n\n )\n\n if err != nil {', 'Qdrant is the only vector database with full coverage of async API in Langchain. Their documentation [describes how to use \n\nit](https://python.langchain.com/docs/modules/data_connection/vectorstores/#asynchronous-operations).', '```bash\n\npip install langchain\n\n```\n\n\n\nQdrant acts as a vector index that may store the embeddings with the documents used to generate them. There are various ways \n\nhow to use it, but calling `Qdrant.from_texts` is probably the most straightforward way how to get started:\n\n\n\n```python\n\nfrom langchain.vectorstores import Qdrant\n\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\n\n\nembeddings = HuggingFaceEmbeddings(\n\n    model_name=""sentence-transformers/all-mpnet-base-v2""\n\n)', 'might also be found in the [LangChain documentation](https://python.langchain.com/docs/integrations/vectorstores/qdrant).']","---

title: Langchain Go

weight: 120

---



# Langchain Go



[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.



You can use Qdrant as a vector store in Langchain Go.



## Setup



Install the `langchain-go` project dependency



```bash

go get -u github.com/tmc/langchaingo

```



## Usage



Before you use the following code sample, customize the following values for your configuration:
""github.com/tmc/langchaingo/vectorstores""

        ""github.com/tmc/langchaingo/vectorstores/qdrant""

)



 llm, err := openai.New()

 if err != nil {

  log.Fatal(err)

 }



 e, err := embeddings.NewEmbedder(llm)

 if err != nil {

  log.Fatal(err)

 }



 url, err := url.Parse(""YOUR_QDRANT_REST_URL"")

 if err != nil {

  log.Fatal(err)

 }



 store, err := qdrant.New(

  qdrant.WithURL(*url),

  qdrant.WithCollectionName(""YOUR_COLLECTION_NAME""),

  qdrant.WithEmbedder(e),

 )

 if err != nil {
Qdrant is the only vector database with full coverage of async API in Langchain. Their documentation [describes how to use 

it](https://python.langchain.com/docs/modules/data_connection/vectorstores/#asynchronous-operations).
```bash

pip install langchain

```



Qdrant acts as a vector index that may store the embeddings with the documents used to generate them. There are various ways 

how to use it, but calling `Qdrant.from_texts` is probably the most straightforward way how to get started:



```python

from langchain.vectorstores import Qdrant

from langchain.embeddings import HuggingFaceEmbeddings



embeddings = HuggingFaceEmbeddings(

    model_name=""sentence-transformers/all-mpnet-base-v2""

)
might also be found in the [LangChain documentation](https://python.langchain.com/docs/integrations/vectorstores/qdrant).",0.19117647058823528,0.8666666666666667,0.3132530120481928,0.1111111111111111,0.5172413793103449,0.18292682926829268,0.16176470588235295,0.7333333333333333,0.26506024096385544,0.16176470588235295,0.7333333333333333,0.26506024096385544,0.9065554141998291,0.5351162552833557,0.68100905418396,0.5993116497993469,0.820122241973877,0.9927338939160109,113,3.4242424242424243,0.6074739853369471
True,,1853.0230000000001,5ffffb99,How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?,Dust leveraged the control of the MMAP payload threshold and Scalar Quantization in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively.,"Dust leveraged compression features in Qdrant, particularly the Scalar Quantization technique, to effectively manage the balance between storing vectors on disk and keeping quantized vectors in RAM. By implementing Scalar Quantization, Dust was able to reduce the memory footprint required to store vectors by a factor of 4. This reduction in memory footprint not only saved on memory costs but also improved the performance of the search process. With this optimization, Dust was able to scale smoothly and reduce their costs significantly.","compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP

payload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage

the balance between storing vectors on disk and keeping quantized vectors in RAM,

more effectively. “This allowed us to scale smoothly from there,” Polu says.
```



While Qdrant offers various options to store some parts of the data on disk, starting 

from version 1.1.0, you can also optimize your memory by compressing the embeddings. 

We've implemented the mechanism of **Scalar Quantization**! It turns out to have not 

only a positive impact on memory but also on the performance. 



## Scalar Quantization



Scalar quantization is a data compression technique that converts floating point values
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
Nirant Kasliwal:

Okay, we are binary quantization aware and they've been trained for it. Like compression awareness is, I think, what it was called. So Qdrant supports that. So please of that might be worth it because it saves about 30 x in memory costs. So that's quite powerful.



Demetrios:

Excellent.



Nirant Kasliwal:
![“We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as

we don’t have to run lots of nodes in parallel. While being memory-bound, we were

able to push the same instances further with the help of quantization. While you

get pressure on MMAP in this case you maintain very good performance even if the

RAM is fully used. With this we were able to reduce our cost by 2x.” - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)","['compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP\n\npayload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage\n\nthe balance between storing vectors on disk and keeping quantized vectors in RAM,\n\nmore effectively. “This allowed us to scale smoothly from there,” Polu says.', ""```\n\n\n\nWhile Qdrant offers various options to store some parts of the data on disk, starting \n\nfrom version 1.1.0, you can also optimize your memory by compressing the embeddings. \n\nWe've implemented the mechanism of **Scalar Quantization**! It turns out to have not \n\nonly a positive impact on memory but also on the performance. \n\n\n\n## Scalar Quantization\n\n\n\nScalar quantization is a data compression technique that converts floating point values"", 'Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.\n\n\n\nIn addition to reducing the memory footprint, scalar quantization also speeds up the search process.\n\nQdrant uses a special SIMD CPU instruction to perform fast vector comparison.\n\nThis instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.', ""Nirant Kasliwal:\n\nOkay, we are binary quantization aware and they've been trained for it. Like compression awareness is, I think, what it was called. So Qdrant supports that. So please of that might be worth it because it saves about 30 x in memory costs. So that's quite powerful.\n\n\n\nDemetrios:\n\nExcellent.\n\n\n\nNirant Kasliwal:"", '![“We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as\n\nwe don’t have to run lots of nodes in parallel. While being memory-bound, we were\n\nable to push the same instances further with the help of quantization. While you\n\nget pressure on MMAP in this case you maintain very good performance even if the\n\nRAM is fully used. With this we were able to reduce our cost by 2x.” - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)']","Question: How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?\n\nContext: compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP

payload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage

the balance between storing vectors on disk and keeping quantized vectors in RAM,

more effectively. “This allowed us to scale smoothly from there,” Polu says.
```



While Qdrant offers various options to store some parts of the data on disk, starting 

from version 1.1.0, you can also optimize your memory by compressing the embeddings. 

We've implemented the mechanism of **Scalar Quantization**! It turns out to have not 

only a positive impact on memory but also on the performance. 



## Scalar Quantization



Scalar quantization is a data compression technique that converts floating point values
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
Nirant Kasliwal:

Okay, we are binary quantization aware and they've been trained for it. Like compression awareness is, I think, what it was called. So Qdrant supports that. So please of that might be worth it because it saves about 30 x in memory costs. So that's quite powerful.



Demetrios:

Excellent.



Nirant Kasliwal:
![“We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as

we don’t have to run lots of nodes in parallel. While being memory-bound, we were

able to push the same instances further with the help of quantization. While you

get pressure on MMAP in this case you maintain very good performance even if the

RAM is fully used. With this we were able to reduce our cost by 2x.” - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)\n\nAnswer:",How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?,Dust leveraged the control of the MMAP payload threshold and Scalar Quantization in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively.,"['compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP\n\npayload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage\n\nthe balance between storing vectors on disk and keeping quantized vectors in RAM,\n\nmore effectively. “This allowed us to scale smoothly from there,” Polu says.', ""```\n\n\n\nWhile Qdrant offers various options to store some parts of the data on disk, starting \n\nfrom version 1.1.0, you can also optimize your memory by compressing the embeddings. \n\nWe've implemented the mechanism of **Scalar Quantization**! It turns out to have not \n\nonly a positive impact on memory but also on the performance. \n\n\n\n## Scalar Quantization\n\n\n\nScalar quantization is a data compression technique that converts floating point values"", 'Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.\n\n\n\nIn addition to reducing the memory footprint, scalar quantization also speeds up the search process.\n\nQdrant uses a special SIMD CPU instruction to perform fast vector comparison.\n\nThis instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.', ""Nirant Kasliwal:\n\nOkay, we are binary quantization aware and they've been trained for it. Like compression awareness is, I think, what it was called. So Qdrant supports that. So please of that might be worth it because it saves about 30 x in memory costs. So that's quite powerful.\n\n\n\nDemetrios:\n\nExcellent.\n\n\n\nNirant Kasliwal:"", '![“We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as\n\nwe don’t have to run lots of nodes in parallel. While being memory-bound, we were\n\nable to push the same instances further with the help of quantization. While you\n\nget pressure on MMAP in this case you maintain very good performance even if the\n\nRAM is fully used. With this we were able to reduce our cost by 2x.” - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)']","compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP

payload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage

the balance between storing vectors on disk and keeping quantized vectors in RAM,

more effectively. “This allowed us to scale smoothly from there,” Polu says.
```



While Qdrant offers various options to store some parts of the data on disk, starting 

from version 1.1.0, you can also optimize your memory by compressing the embeddings. 

We've implemented the mechanism of **Scalar Quantization**! It turns out to have not 

only a positive impact on memory but also on the performance. 



## Scalar Quantization



Scalar quantization is a data compression technique that converts floating point values
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
Nirant Kasliwal:

Okay, we are binary quantization aware and they've been trained for it. Like compression awareness is, I think, what it was called. So Qdrant supports that. So please of that might be worth it because it saves about 30 x in memory costs. So that's quite powerful.



Demetrios:

Excellent.



Nirant Kasliwal:
![“We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as

we don’t have to run lots of nodes in parallel. While being memory-bound, we were

able to push the same instances further with the help of quantization. While you

get pressure on MMAP in this case you maintain very good performance even if the

RAM is fully used. With this we were able to reduce our cost by 2x.” - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)",0.3026315789473684,0.8518518518518519,0.44660194174757284,0.2,0.5769230769230769,0.29702970297029707,0.23684210526315788,0.6666666666666666,0.34951456310679613,0.23684210526315788,0.6666666666666666,0.34951456310679613,0.854612410068512,0.6124807000160217,0.772545576095581,0.6832639575004578,0.9591802201854686,0.6630982160568237,82,2.7333333333333334,0.49151350959257223
True,,4852.902,0ea625a4,Why do we still need keyword search?,"Keyword search is still useful in cases of out-of-domain search, where words are just words regardless of their meaning.","Keyword search is still essential for several reasons:

1. **Historical Usage**: Keyword-based search has been the traditional method for search engines for a long time. Users are familiar with this approach and have been using it effectively to find information.

2. **Simplicity and Efficiency**: Keyword search is simple and efficient. Users can input a few words related to their query and get relevant results quickly. This straightforward process is still preferred by many users.

3. **Specific Queries**: In some cases, users may have very specific queries that require exact keyword matches. For example, searching for a product name, a specific term in a document, or a particular keyword in code. Keyword search is effective in these situations.

4. **Complementing Semantic Search**: While semantic search has its advantages in understanding intent and context, keyword search can complement it by providing a more direct and precise way of searching. Both approaches can work together to enhance the search experience.

5. **User Familiarity**: Many users are accustomed to using keywords to search for information. It is a widely understood method that continues to be relevant even as new search technologies emerge.

In conclusion, keyword search remains a fundamental component of search engines due to its simplicity, efficiency, historical usage, ability to handle specific queries, and its complementarity with semantic search.","2. Vector search with keyword-based search. This one is covered in this article.

3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.



## Why do we still need keyword search?



A keyword-based search was the obvious choice for search engines in the past. It struggled with some

common issues, but since we didn't have any alternatives, we had to overcome them with additional
. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.
needed to find what we’re looking for. Semantic search enables us to find things without knowing the exact 

terminology. For example, if an engineer wanted to understand “*What library is used for payment processing?*” 

a semantic code search engine would be able to retrieve results containing “*Stripe*” or “*PayPal*”. A traditional 

lexical search engine would not.



One peculiarity of this problem is that the **usefulness of the solution increases with the size of the code
on those candidates only. This is an approach that makes it possible to use the power of semantic and keyword based 

   search together.



![The design of all the three experiments](/articles_data/hybrid-search/experiments-design.png)



### Quality metrics



There are various ways of how to measure the performance of search engines, and *[Recommender Systems: Machine Learning
It shows you a set of results and then from that you can actually converse more to it and say, hey, can you remove five or six or reduce this by a certain degree? That is the power of what we have at hand with complex search. And complex search is becoming quickly a great segue to why we need to implement conversational search. We would need to implement large language models in our ecosystem so that we can understand the context of what users have been asking","[""2. Vector search with keyword-based search. This one is covered in this article.\n\n3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.\n\n\n\n## Why do we still need keyword search?\n\n\n\nA keyword-based search was the obvious choice for search engines in the past. It struggled with some\n\ncommon issues, but since we didn't have any alternatives, we had to overcome them with additional"", '. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.', 'needed to find what we’re looking for. Semantic search enables us to find things without knowing the exact \n\nterminology. For example, if an engineer wanted to understand “*What library is used for payment processing?*” \n\na semantic code search engine would be able to retrieve results containing “*Stripe*” or “*PayPal*”. A traditional \n\nlexical search engine would not.\n\n\n\nOne peculiarity of this problem is that the **usefulness of the solution increases with the size of the code', 'on those candidates only. This is an approach that makes it possible to use the power of semantic and keyword based \n\n   search together.\n\n\n\n![The design of all the three experiments](/articles_data/hybrid-search/experiments-design.png)\n\n\n\n### Quality metrics\n\n\n\nThere are various ways of how to measure the performance of search engines, and *[Recommender Systems: Machine Learning', 'It shows you a set of results and then from that you can actually converse more to it and say, hey, can you remove five or six or reduce this by a certain degree? That is the power of what we have at hand with complex search. And complex search is becoming quickly a great segue to why we need to implement conversational search. We would need to implement large language models in our ecosystem so that we can understand the context of what users have been asking']","Question: Why do we still need keyword search?\n\nContext: 2. Vector search with keyword-based search. This one is covered in this article.

3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.



## Why do we still need keyword search?



A keyword-based search was the obvious choice for search engines in the past. It struggled with some

common issues, but since we didn't have any alternatives, we had to overcome them with additional
. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.
needed to find what we’re looking for. Semantic search enables us to find things without knowing the exact 

terminology. For example, if an engineer wanted to understand “*What library is used for payment processing?*” 

a semantic code search engine would be able to retrieve results containing “*Stripe*” or “*PayPal*”. A traditional 

lexical search engine would not.



One peculiarity of this problem is that the **usefulness of the solution increases with the size of the code
on those candidates only. This is an approach that makes it possible to use the power of semantic and keyword based 

search together.



![The design of all the three experiments](/articles_data/hybrid-search/experiments-design.png)



### Quality metrics



There are various ways of how to measure the performance of search engines, and *[Recommender Systems: Machine Learning
It shows you a set of results and then from that you can actually converse more to it and say, hey, can you remove five or six or reduce this by a certain degree? That is the power of what we have at hand with complex search. And complex search is becoming quickly a great segue to why we need to implement conversational search. We would need to implement large language models in our ecosystem so that we can understand the context of what users have been asking\n\nAnswer:",Why do we still need keyword search?,"Keyword search is still useful in cases of out-of-domain search, where words are just words regardless of their meaning.","[""2. Vector search with keyword-based search. This one is covered in this article.\n\n3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.\n\n\n\n## Why do we still need keyword search?\n\n\n\nA keyword-based search was the obvious choice for search engines in the past. It struggled with some\n\ncommon issues, but since we didn't have any alternatives, we had to overcome them with additional"", '. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.', 'needed to find what we’re looking for. Semantic search enables us to find things without knowing the exact \n\nterminology. For example, if an engineer wanted to understand “*What library is used for payment processing?*” \n\na semantic code search engine would be able to retrieve results containing “*Stripe*” or “*PayPal*”. A traditional \n\nlexical search engine would not.\n\n\n\nOne peculiarity of this problem is that the **usefulness of the solution increases with the size of the code', 'on those candidates only. This is an approach that makes it possible to use the power of semantic and keyword based \n\n   search together.\n\n\n\n![The design of all the three experiments](/articles_data/hybrid-search/experiments-design.png)\n\n\n\n### Quality metrics\n\n\n\nThere are various ways of how to measure the performance of search engines, and *[Recommender Systems: Machine Learning', 'It shows you a set of results and then from that you can actually converse more to it and say, hey, can you remove five or six or reduce this by a certain degree? That is the power of what we have at hand with complex search. And complex search is becoming quickly a great segue to why we need to implement conversational search. We would need to implement large language models in our ecosystem so that we can understand the context of what users have been asking']","2. Vector search with keyword-based search. This one is covered in this article.

3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.



## Why do we still need keyword search?



A keyword-based search was the obvious choice for search engines in the past. It struggled with some

common issues, but since we didn't have any alternatives, we had to overcome them with additional
. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.
needed to find what we’re looking for. Semantic search enables us to find things without knowing the exact 

terminology. For example, if an engineer wanted to understand “*What library is used for payment processing?*” 

a semantic code search engine would be able to retrieve results containing “*Stripe*” or “*PayPal*”. A traditional 

lexical search engine would not.



One peculiarity of this problem is that the **usefulness of the solution increases with the size of the code
on those candidates only. This is an approach that makes it possible to use the power of semantic and keyword based 

   search together.



![The design of all the three experiments](/articles_data/hybrid-search/experiments-design.png)



### Quality metrics



There are various ways of how to measure the performance of search engines, and *[Recommender Systems: Machine Learning
It shows you a set of results and then from that you can actually converse more to it and say, hey, can you remove five or six or reduce this by a certain degree? That is the power of what we have at hand with complex search. And complex search is becoming quickly a great segue to why we need to implement conversational search. We would need to implement large language models in our ecosystem so that we can understand the context of what users have been asking",0.06280193236714976,0.6190476190476191,0.11403508771929824,0.014563106796116505,0.15,0.026548672566371685,0.05314009661835749,0.5238095238095238,0.09649122807017545,0.057971014492753624,0.5714285714285714,0.10526315789473682,0.7692171931266785,0.451676607131958,0.5959785580635071,0.5138896703720093,0.6661564816669043,0.004120230674743652,217,11.421052631578947,0.3105853902795469
True,,5401.946,49d85556,What principles did Qdrant follow while designing benchmarks for vector search engines?,"Qdrant followed the principles of doing comparative benchmarks focusing on relative numbers rather than absolute numbers, and using affordable hardware for easy result reproduction.","When designing benchmarks for vector search engines, Qdrant followed several principles to ensure accuracy and effectiveness:

1. **Comparative Benchmarking**: Qdrant aimed to provide the first comparative benchmark and benchmarking framework for vector search engines and vector databases. This involved comparing Qdrant with other similar systems like Milvus, Weaviate, Redis, and ElasticSearch.

2. **Accuracy Metrics**: The benchmarks focused on metrics such as performance, latency, Requests Per Second (RPS), and other relevant performance indicators to evaluate the efficiency and effectiveness of the vector search engines.

3. **Benchmarking Framework**: Qdrant used a structured framework to conduct the benchmarks, ensuring consistency and reliability in the evaluation process. This framework likely included standardized test cases, datasets, and evaluation methodologies.

4. **Vector Search Specificity**: Since Qdrant is designed for similarity search and clustering tasks, the benchmarks likely focused on evaluating the system's performance in these specific use cases. This ensured that the benchmarks were aligned with the intended purpose of the system.

5. **Scalability Testing**: Given the emphasis on scalability by Andrey Vasnetsov, the benchmarks probably included tests to assess how well the system performs as the data volume increases. This is crucial for systems like Qdrant that handle large-scale data efficiently.

Overall, the benchmarks for vector search engines like Qdrant were designed to provide valuable insights into the performance, scalability, and effectiveness of these systems in handling vector data for similarity search tasks.","- Qdrant

  - Vector Search Engine

  - Vector Database

---

> *""For systems like Qdrant, scalability and performance in my opinion, is much more important than transactional consistency, so it should be treated as a search engine rather than database.""*\

-- Andrey Vasnetsov

> 



Discussing core differences between search engines and databases, Andrey underlined the importance of application needs and scalability in database selection for vector search tasks.
## Project Overview



Qdrant is a powerful vector search engine widely used for similarity search and clustering. However, it lacked a user-friendly web-based UI for data visualization and exploration. My project aimed to bridge this gap by developing a web-based user interface that allows users to easily interact with and explore their vector data.



## Milestones and Achievements
All of those engines might be easily used in combination with the vector search offered by Qdrant. But the 

exact way how to combine the results of both algorithms to achieve the best search precision might be still 

unclear. So we need to understand how to do it effectively. We will be using reference datasets to benchmark 

the search quality.



## Why not linear combination?



It's often proposed to use full-text and vector search scores to form a linear combination formula to rerank
Systems like Qdrant exemplify scalability in vector databases. It leverages Rust's efficiency in **memory management** and **performance**, which allows handling of large-scale data with optimized resource usage.





### Efficient Query Processing
---

title: Vector Database Benchmarks

description: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.

keywords:

  - vector databases comparative benchmark

  - ANN Benchmark

  - Qdrant vs Milvus

  - Qdrant vs Weaviate

  - Qdrant vs Redis

  - Qdrant vs ElasticSearch

  - benchmark

  - performance

  - latency

  - RPS

  - comparison

  - vector search

  - embedding

preview_image: /benchmarks/benchmark-1.png","['- Qdrant\n\n  - Vector Search Engine\n\n  - Vector Database\n\n---\n\n> *""For systems like Qdrant, scalability and performance in my opinion, is much more important than transactional consistency, so it should be treated as a search engine rather than database.""*\\\n\n-- Andrey Vasnetsov\n\n> \n\n\n\nDiscussing core differences between search engines and databases, Andrey underlined the importance of application needs and scalability in database selection for vector search tasks.', '## Project Overview\n\n\n\nQdrant is a powerful vector search engine widely used for similarity search and clustering. However, it lacked a user-friendly web-based UI for data visualization and exploration. My project aimed to bridge this gap by developing a web-based user interface that allows users to easily interact with and explore their vector data.\n\n\n\n## Milestones and Achievements', ""All of those engines might be easily used in combination with the vector search offered by Qdrant. But the \n\nexact way how to combine the results of both algorithms to achieve the best search precision might be still \n\nunclear. So we need to understand how to do it effectively. We will be using reference datasets to benchmark \n\nthe search quality.\n\n\n\n## Why not linear combination?\n\n\n\nIt's often proposed to use full-text and vector search scores to form a linear combination formula to rerank"", ""Systems like Qdrant exemplify scalability in vector databases. It leverages Rust's efficiency in **memory management** and **performance**, which allows handling of large-scale data with optimized resource usage.\n\n\n\n\n\n### Efficient Query Processing"", '---\n\ntitle: Vector Database Benchmarks\n\ndescription: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.\n\nkeywords:\n\n  - vector databases comparative benchmark\n\n  - ANN Benchmark\n\n  - Qdrant vs Milvus\n\n  - Qdrant vs Weaviate\n\n  - Qdrant vs Redis\n\n  - Qdrant vs ElasticSearch\n\n  - benchmark\n\n  - performance\n\n  - latency\n\n  - RPS\n\n  - comparison\n\n  - vector search\n\n  - embedding\n\npreview_image: /benchmarks/benchmark-1.png']","Question: What principles did Qdrant follow while designing benchmarks for vector search engines?\n\nContext: - Qdrant

- Vector Search Engine

- Vector Database

---

> *""For systems like Qdrant, scalability and performance in my opinion, is much more important than transactional consistency, so it should be treated as a search engine rather than database.""*\

-- Andrey Vasnetsov

> 



Discussing core differences between search engines and databases, Andrey underlined the importance of application needs and scalability in database selection for vector search tasks.
## Project Overview



Qdrant is a powerful vector search engine widely used for similarity search and clustering. However, it lacked a user-friendly web-based UI for data visualization and exploration. My project aimed to bridge this gap by developing a web-based user interface that allows users to easily interact with and explore their vector data.



## Milestones and Achievements
All of those engines might be easily used in combination with the vector search offered by Qdrant. But the 

exact way how to combine the results of both algorithms to achieve the best search precision might be still 

unclear. So we need to understand how to do it effectively. We will be using reference datasets to benchmark 

the search quality.



## Why not linear combination?



It's often proposed to use full-text and vector search scores to form a linear combination formula to rerank
Systems like Qdrant exemplify scalability in vector databases. It leverages Rust's efficiency in **memory management** and **performance**, which allows handling of large-scale data with optimized resource usage.





### Efficient Query Processing
---

title: Vector Database Benchmarks

description: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.

keywords:

- vector databases comparative benchmark

- ANN Benchmark

- Qdrant vs Milvus

- Qdrant vs Weaviate

- Qdrant vs Redis

- Qdrant vs ElasticSearch

- benchmark

- performance

- latency

- RPS

- comparison

- vector search

- embedding

preview_image: /benchmarks/benchmark-1.png\n\nAnswer:",What principles did Qdrant follow while designing benchmarks for vector search engines?,"Qdrant followed the principles of doing comparative benchmarks focusing on relative numbers rather than absolute numbers, and using affordable hardware for easy result reproduction.","['- Qdrant\n\n  - Vector Search Engine\n\n  - Vector Database\n\n---\n\n> *""For systems like Qdrant, scalability and performance in my opinion, is much more important than transactional consistency, so it should be treated as a search engine rather than database.""*\\\n\n-- Andrey Vasnetsov\n\n> \n\n\n\nDiscussing core differences between search engines and databases, Andrey underlined the importance of application needs and scalability in database selection for vector search tasks.', '## Project Overview\n\n\n\nQdrant is a powerful vector search engine widely used for similarity search and clustering. However, it lacked a user-friendly web-based UI for data visualization and exploration. My project aimed to bridge this gap by developing a web-based user interface that allows users to easily interact with and explore their vector data.\n\n\n\n## Milestones and Achievements', ""All of those engines might be easily used in combination with the vector search offered by Qdrant. But the \n\nexact way how to combine the results of both algorithms to achieve the best search precision might be still \n\nunclear. So we need to understand how to do it effectively. We will be using reference datasets to benchmark \n\nthe search quality.\n\n\n\n## Why not linear combination?\n\n\n\nIt's often proposed to use full-text and vector search scores to form a linear combination formula to rerank"", ""Systems like Qdrant exemplify scalability in vector databases. It leverages Rust's efficiency in **memory management** and **performance**, which allows handling of large-scale data with optimized resource usage.\n\n\n\n\n\n### Efficient Query Processing"", '---\n\ntitle: Vector Database Benchmarks\n\ndescription: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.\n\nkeywords:\n\n  - vector databases comparative benchmark\n\n  - ANN Benchmark\n\n  - Qdrant vs Milvus\n\n  - Qdrant vs Weaviate\n\n  - Qdrant vs Redis\n\n  - Qdrant vs ElasticSearch\n\n  - benchmark\n\n  - performance\n\n  - latency\n\n  - RPS\n\n  - comparison\n\n  - vector search\n\n  - embedding\n\npreview_image: /benchmarks/benchmark-1.png']","- Qdrant

  - Vector Search Engine

  - Vector Database

---

> *""For systems like Qdrant, scalability and performance in my opinion, is much more important than transactional consistency, so it should be treated as a search engine rather than database.""*\

-- Andrey Vasnetsov

> 



Discussing core differences between search engines and databases, Andrey underlined the importance of application needs and scalability in database selection for vector search tasks.
## Project Overview



Qdrant is a powerful vector search engine widely used for similarity search and clustering. However, it lacked a user-friendly web-based UI for data visualization and exploration. My project aimed to bridge this gap by developing a web-based user interface that allows users to easily interact with and explore their vector data.



## Milestones and Achievements
All of those engines might be easily used in combination with the vector search offered by Qdrant. But the 

exact way how to combine the results of both algorithms to achieve the best search precision might be still 

unclear. So we need to understand how to do it effectively. We will be using reference datasets to benchmark 

the search quality.



## Why not linear combination?



It's often proposed to use full-text and vector search scores to form a linear combination formula to rerank
Systems like Qdrant exemplify scalability in vector databases. It leverages Rust's efficiency in **memory management** and **performance**, which allows handling of large-scale data with optimized resource usage.





### Efficient Query Processing
---

title: Vector Database Benchmarks

description: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.

keywords:

  - vector databases comparative benchmark

  - ANN Benchmark

  - Qdrant vs Milvus

  - Qdrant vs Weaviate

  - Qdrant vs Redis

  - Qdrant vs ElasticSearch

  - benchmark

  - performance

  - latency

  - RPS

  - comparison

  - vector search

  - embedding

preview_image: /benchmarks/benchmark-1.png",0.05188679245283019,0.4782608695652174,0.09361702127659574,0.018957345971563982,0.18181818181818182,0.034334763948497854,0.04716981132075472,0.43478260869565216,0.08510638297872342,0.05188679245283019,0.4782608695652174,0.09361702127659574,0.6699402332305908,0.45371147990226746,0.5650752782821655,0.5033068060874939,0.5479995979617039,0.056031227111816406,229,9.541666666666666,0.4118716163036366
True,,3821.6420000000003,26caa24a,What models does Qdrant support for embedding generation?,"Qdrant supports a reasonable range of models, including a few multilingual ones.","Qdrant supports the following models for embedding generation:

1. Cohere
2. Gemini
3. Jina Embeddings
4. OpenAI
5. Aleph Alpha

These models can be seamlessly integrated with Qdrant for generating embeddings for various use cases.","Selecting the right embedding model for your use case is crucial to your application performance. Qdrant makes it easier by offering seamless integration with the best selection of embedding APIs, including [Cohere](https://qdrant.tech/documentation/embeddings/cohere/), [Gemini](https://qdrant.tech/documentation/embeddings/gemini/), [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/), [OpenAI](https://qdrant.tech/documentation/embeddings/openai/), [Aleph Alpha](https://qdrant
```python

qdrant_client.search(

    collection_name=""GeminiCollection"",

    query=genai.embed_content(

        model=""models/embedding-001"",

        content=""What is the best vector database to use with Gemini?"",

        task_type=""retrieval_query"",

    )[""embedding""],

)

```



## Using Gemini Embedding Models with Binary Quantization
. So this is taken care of. So whatever is your best practices for the Embedding model, make sure you use it when you're using it with Qdrant or just in isolation as well.
query_request = SemanticEmbeddingRequest(**query_params)

    query_response = await client.semantic_embed(

        request=query_request, model=model

    )

    

    qdrant_client.upsert(

        collection_name=""MyCollection"",

        points=Batch(

            ids=[1],

            vectors=[query_response.embedding],

        )

    )

```



If we wanted to create text embeddings with the same model, we wouldn't use `ImagePrompt.from_file`, but simply provide the input
## Common Use Cases:



Qdrant is ideal for deploying applications based on the matching of embeddings produced by neural network encoders. Check out the [Examples](examples/) section to learn more about common use cases. Also, you can visit the [Tutorials](tutorials/) page to learn how to work with Qdrant in different ways. 



| Use Case              | Description                                  | Stack  |   

|-----------------------|----------------------------------------------|--------|","['Selecting the right embedding model for your use case is crucial to your application performance. Qdrant makes it easier by offering seamless integration with the best selection of embedding APIs, including [Cohere](https://qdrant.tech/documentation/embeddings/cohere/), [Gemini](https://qdrant.tech/documentation/embeddings/gemini/), [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/), [OpenAI](https://qdrant.tech/documentation/embeddings/openai/), [Aleph Alpha](https://qdrant', '```python\n\nqdrant_client.search(\n\n    collection_name=""GeminiCollection"",\n\n    query=genai.embed_content(\n\n        model=""models/embedding-001"",\n\n        content=""What is the best vector database to use with Gemini?"",\n\n        task_type=""retrieval_query"",\n\n    )[""embedding""],\n\n)\n\n```\n\n\n\n## Using Gemini Embedding Models with Binary Quantization', "". So this is taken care of. So whatever is your best practices for the Embedding model, make sure you use it when you're using it with Qdrant or just in isolation as well."", 'query_request = SemanticEmbeddingRequest(**query_params)\n\n    query_response = await client.semantic_embed(\n\n        request=query_request, model=model\n\n    )\n\n    \n\n    qdrant_client.upsert(\n\n        collection_name=""MyCollection"",\n\n        points=Batch(\n\n            ids=[1],\n\n            vectors=[query_response.embedding],\n\n        )\n\n    )\n\n```\n\n\n\nIf we wanted to create text embeddings with the same model, we wouldn\'t use `ImagePrompt.from_file`, but simply provide the input', '## Common Use Cases:\n\n\n\nQdrant is ideal for deploying applications based on the matching of embeddings produced by neural network encoders. Check out the [Examples](examples/) section to learn more about common use cases. Also, you can visit the [Tutorials](tutorials/) page to learn how to work with Qdrant in different ways. \n\n\n\n| Use Case              | Description                                  | Stack  |   \n\n|-----------------------|----------------------------------------------|--------|']","Question: What models does Qdrant support for embedding generation?\n\nContext: Selecting the right embedding model for your use case is crucial to your application performance. Qdrant makes it easier by offering seamless integration with the best selection of embedding APIs, including [Cohere](https://qdrant.tech/documentation/embeddings/cohere/), [Gemini](https://qdrant.tech/documentation/embeddings/gemini/), [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/), [OpenAI](https://qdrant.tech/documentation/embeddings/openai/), [Aleph Alpha](https://qdrant
```python

qdrant_client.search(

collection_name=""GeminiCollection"",

query=genai.embed_content(

model=""models/embedding-001"",

content=""What is the best vector database to use with Gemini?"",

task_type=""retrieval_query"",

)[""embedding""],

)

```



## Using Gemini Embedding Models with Binary Quantization
. So this is taken care of. So whatever is your best practices for the Embedding model, make sure you use it when you're using it with Qdrant or just in isolation as well.
query_request = SemanticEmbeddingRequest(**query_params)

query_response = await client.semantic_embed(

request=query_request, model=model

)



qdrant_client.upsert(

collection_name=""MyCollection"",

points=Batch(

ids=[1],

vectors=[query_response.embedding],

)

)

```



If we wanted to create text embeddings with the same model, we wouldn't use `ImagePrompt.from_file`, but simply provide the input
## Common Use Cases:



Qdrant is ideal for deploying applications based on the matching of embeddings produced by neural network encoders. Check out the [Examples](examples/) section to learn more about common use cases. Also, you can visit the [Tutorials](tutorials/) page to learn how to work with Qdrant in different ways. 



| Use Case              | Description                                  | Stack  |   

|-----------------------|----------------------------------------------|--------|\n\nAnswer:",What models does Qdrant support for embedding generation?,"Qdrant supports a reasonable range of models, including a few multilingual ones.","['Selecting the right embedding model for your use case is crucial to your application performance. Qdrant makes it easier by offering seamless integration with the best selection of embedding APIs, including [Cohere](https://qdrant.tech/documentation/embeddings/cohere/), [Gemini](https://qdrant.tech/documentation/embeddings/gemini/), [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/), [OpenAI](https://qdrant.tech/documentation/embeddings/openai/), [Aleph Alpha](https://qdrant', '```python\n\nqdrant_client.search(\n\n    collection_name=""GeminiCollection"",\n\n    query=genai.embed_content(\n\n        model=""models/embedding-001"",\n\n        content=""What is the best vector database to use with Gemini?"",\n\n        task_type=""retrieval_query"",\n\n    )[""embedding""],\n\n)\n\n```\n\n\n\n## Using Gemini Embedding Models with Binary Quantization', "". So this is taken care of. So whatever is your best practices for the Embedding model, make sure you use it when you're using it with Qdrant or just in isolation as well."", 'query_request = SemanticEmbeddingRequest(**query_params)\n\n    query_response = await client.semantic_embed(\n\n        request=query_request, model=model\n\n    )\n\n    \n\n    qdrant_client.upsert(\n\n        collection_name=""MyCollection"",\n\n        points=Batch(\n\n            ids=[1],\n\n            vectors=[query_response.embedding],\n\n        )\n\n    )\n\n```\n\n\n\nIf we wanted to create text embeddings with the same model, we wouldn\'t use `ImagePrompt.from_file`, but simply provide the input', '## Common Use Cases:\n\n\n\nQdrant is ideal for deploying applications based on the matching of embeddings produced by neural network encoders. Check out the [Examples](examples/) section to learn more about common use cases. Also, you can visit the [Tutorials](tutorials/) page to learn how to work with Qdrant in different ways. \n\n\n\n| Use Case              | Description                                  | Stack  |   \n\n|-----------------------|----------------------------------------------|--------|']","Selecting the right embedding model for your use case is crucial to your application performance. Qdrant makes it easier by offering seamless integration with the best selection of embedding APIs, including [Cohere](https://qdrant.tech/documentation/embeddings/cohere/), [Gemini](https://qdrant.tech/documentation/embeddings/gemini/), [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/), [OpenAI](https://qdrant.tech/documentation/embeddings/openai/), [Aleph Alpha](https://qdrant
```python

qdrant_client.search(

    collection_name=""GeminiCollection"",

    query=genai.embed_content(

        model=""models/embedding-001"",

        content=""What is the best vector database to use with Gemini?"",

        task_type=""retrieval_query"",

    )[""embedding""],

)

```



## Using Gemini Embedding Models with Binary Quantization
. So this is taken care of. So whatever is your best practices for the Embedding model, make sure you use it when you're using it with Qdrant or just in isolation as well.
query_request = SemanticEmbeddingRequest(**query_params)

    query_response = await client.semantic_embed(

        request=query_request, model=model

    )

    

    qdrant_client.upsert(

        collection_name=""MyCollection"",

        points=Batch(

            ids=[1],

            vectors=[query_response.embedding],

        )

    )

```



If we wanted to create text embeddings with the same model, we wouldn't use `ImagePrompt.from_file`, but simply provide the input
## Common Use Cases:



Qdrant is ideal for deploying applications based on the matching of embeddings produced by neural network encoders. Check out the [Examples](examples/) section to learn more about common use cases. Also, you can visit the [Tutorials](tutorials/) page to learn how to work with Qdrant in different ways. 



| Use Case              | Description                                  | Stack  |   

|-----------------------|----------------------------------------------|--------|",0.08823529411764706,0.3,0.13636363636363635,0.030303030303030304,0.1111111111111111,0.047619047619047616,0.08823529411764706,0.3,0.13636363636363635,0.08823529411764706,0.3,0.13636363636363635,0.6735039949417114,0.5107621550559998,0.6049492359161377,0.5538800954818726,0.9945713169872761,0.7160230278968811,35,2.9166666666666665,0.5565615623498121
True,,4673.235,bb1a471a,How can you parallelize the upload of a large dataset using shards in Qdrant?,"By creating multiple shards in Qdrant, you can parallelize the upload of a large dataset. It is recommended to have 2 to 4 shards per machine for efficient processing. When creating a collection in Qdrant, you can specify the number of shards to use for that collection. ","To parallelize the upload of a large dataset using shards in Qdrant, you can follow these steps:

1. Create multiple shards for the collection: When creating a collection in Qdrant, specify the number of shards you want to use. For example, in the HTTP request snippet provided, the ""shard_number"" parameter is set to 2. You can increase this number to create more shards for parallel processing.

2. Utilize the Rust client library: Qdrant recommends using the Rust client library for faster performance. The Rust client library is optimized for interacting with Qdrant efficiently.

3. Disable indexing during upload: If you are uploading a large dataset for the first time, consider disabling indexing during the upload process. This will prevent unnecessary indexing of vectors that will be overwritten by subsequent batches.

4. Configure Qdrant for parallel processing: To maximize throughput, configure Qdrant to utilize multiple cores for processing requests in parallel. You can achieve this by setting a minimal number of segments, typically 2, which allows for parallel processing of requests.

5. Consider custom sharding: You can also implement custom sharding strategies based on your specific use case. Custom sharding can help optimize performance based on factors such as data distribution and query patterns.

6. Optimize for latency: If minimizing latency is a priority, configure Qdrant to use as many cores as possible for a single request. Set the number of segments in the collection to match the number of cores in the system to process segments in parallel and obtain faster results.

By following these steps and utilizing the features offered by Qdrant, you can effectively parallelize the upload of a large dataset using shards for improved performance and efficiency.","## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""shard_number"": 2

}

```



```python
We recommend using our [Rust client library](https://github.com/qdrant/rust-client) for this purpose, as it is the fastest client library available for Qdrant.



If you are not using Rust, you might want to consider parallelizing your upload process.



## Disable indexing during upload



In case you are doing an initial upload of a large dataset, you might want to disable indexing during upload.

It will enable to avoid unnecessary indexing of vectors, which will be overwritten by the next batch.
);

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,
. This is called [custom sharding](https://qdrant.tech/documentation/guides/distributed_deployment/#user-defined-sharding).
Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{","['## Parallel upload into multiple shards\n\n\n\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\n\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python', 'We recommend using our [Rust client library](https://github.com/qdrant/rust-client) for this purpose, as it is the fastest client library available for Qdrant.\n\n\n\nIf you are not using Rust, you might want to consider parallelizing your upload process.\n\n\n\n## Disable indexing during upload\n\n\n\nIn case you are doing an initial upload of a large dataset, you might want to disable indexing during upload.\n\nIt will enable to avoid unnecessary indexing of vectors, which will be overwritten by the next batch.', ');\n\n```\n\n\n\nTo prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.\n\nTo do that, you can configure qdrant to use minimal number of segments, which is usually 2.\n\nLarge segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,', '. This is called [custom sharding](https://qdrant.tech/documentation/guides/distributed_deployment/#user-defined-sharding).', 'Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.\n\n\n\nTo prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{']","Question: How can you parallelize the upload of a large dataset using shards in Qdrant?\n\nContext: ## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""shard_number"": 2

}

```



```python
We recommend using our [Rust client library](https://github.com/qdrant/rust-client) for this purpose, as it is the fastest client library available for Qdrant.



If you are not using Rust, you might want to consider parallelizing your upload process.



## Disable indexing during upload



In case you are doing an initial upload of a large dataset, you might want to disable indexing during upload.

It will enable to avoid unnecessary indexing of vectors, which will be overwritten by the next batch.
);

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,
. This is called [custom sharding](https://qdrant.tech/documentation/guides/distributed_deployment/#user-defined-sharding).
Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{\n\nAnswer:",How can you parallelize the upload of a large dataset using shards in Qdrant?,"By creating multiple shards in Qdrant, you can parallelize the upload of a large dataset. It is recommended to have 2 to 4 shards per machine for efficient processing. When creating a collection in Qdrant, you can specify the number of shards to use for that collection. ","['## Parallel upload into multiple shards\n\n\n\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\n\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python', 'We recommend using our [Rust client library](https://github.com/qdrant/rust-client) for this purpose, as it is the fastest client library available for Qdrant.\n\n\n\nIf you are not using Rust, you might want to consider parallelizing your upload process.\n\n\n\n## Disable indexing during upload\n\n\n\nIn case you are doing an initial upload of a large dataset, you might want to disable indexing during upload.\n\nIt will enable to avoid unnecessary indexing of vectors, which will be overwritten by the next batch.', ');\n\n```\n\n\n\nTo prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.\n\nTo do that, you can configure qdrant to use minimal number of segments, which is usually 2.\n\nLarge segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,', '. This is called [custom sharding](https://qdrant.tech/documentation/guides/distributed_deployment/#user-defined-sharding).', 'Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.\n\n\n\nTo prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{']","## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""shard_number"": 2

}

```



```python
We recommend using our [Rust client library](https://github.com/qdrant/rust-client) for this purpose, as it is the fastest client library available for Qdrant.



If you are not using Rust, you might want to consider parallelizing your upload process.



## Disable indexing during upload



In case you are doing an initial upload of a large dataset, you might want to disable indexing during upload.

It will enable to avoid unnecessary indexing of vectors, which will be overwritten by the next batch.
);

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,
. This is called [custom sharding](https://qdrant.tech/documentation/guides/distributed_deployment/#user-defined-sharding).
Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{",0.1511627906976744,0.9069767441860465,0.2591362126245847,0.08171206225680934,0.5,0.14046822742474915,0.09689922480620156,0.5813953488372093,0.16611295681063123,0.1434108527131783,0.8604651162790697,0.24584717607973422,0.9204222559928894,0.5533553957939148,0.6713854074478149,0.606683075428009,0.8835268204163115,0.9959223310579546,280,5.957446808510638,0.49447182219933306
True,,2244.783,89fc919f,What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?,"Maximizing the distance between all points in the response when utilizing vector similarity for diversity search is crucial as it allows for the algorithm to sequentially output dissimilar results. By doing so, a diverse selection of the collection can be made possible without the need for any labeling efforts. This approach ensures that the algorithm can effectively identify and retrieve a wide range of diverse items or data points.","The significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search lies in the ability to ensure that the algorithm sequentially outputs dissimilar results. By maximizing the distance between points, we can achieve a diverse selection of items from the collection without the need for labeling efforts. This approach helps in finding the most distinctive examples in the data and can be particularly useful in scenarios like mislabeling detection, where focusing on non-important features or selecting a lot of irrelevant elements can lead to errors. By performing diversity search with vector similarity, we can mitigate this issue and extract points that are unique and dissimilar, enhancing the overall search and retrieval process.","{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=""Example of similarity-based sampling"" >}}





The power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.

By maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.
{{< figure src=/articles_data/vector-similarity-beyond-search/diversity.png caption=""Diversity Search"" >}}





Some forms of diversity sampling are already used in the industry and are known as [Maximum Margin Relevance](https://python.langchain.com/docs/integrations/vectorstores/qdrant#maximum-marginal-relevance-search-mmr) (MMR). Techniques like this were developed to enhance similarity on a universal search API.

However, there is still room for new ideas, particularly regarding diversity retrieval.
Let's imagine how to get 3 points with similarity search and then with diversity search.



Similarity:

1. Calculate distance matrix

2. Choose your anchor

3. Get a vector corresponding to the distances from the selected anchor from the distance matrix

4. Sort fetched vector

5. Get top-3 embeddings

 

Diversity:

1. Calculate distance matrix

2. Initialize starting point (randomly or according to the certain conditions)
You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. 

To mitigate this issue, you can perform a diversity search.



Diversity search is a method for finding the most distinctive examples in the data.

As similarity search, it also operates on embeddings and measures the distances between them.

The difference lies in deciding which point should be extracted next.
Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. 

It doesn’t rely on keywords or other metadata. 

With vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.



The dissimilarity search can find items in areas where previously no other search could be used.

Let’s look at a few examples.



### Case: Mislabeling Detection","['{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=""Example of similarity-based sampling"" >}}\n\n\n\n\n\nThe power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.\n\nBy maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.', '{{< figure src=/articles_data/vector-similarity-beyond-search/diversity.png caption=""Diversity Search"" >}}\n\n\n\n\n\nSome forms of diversity sampling are already used in the industry and are known as [Maximum Margin Relevance](https://python.langchain.com/docs/integrations/vectorstores/qdrant#maximum-marginal-relevance-search-mmr) (MMR). Techniques like this were developed to enhance similarity on a universal search API.\n\nHowever, there is still room for new ideas, particularly regarding diversity retrieval.', ""Let's imagine how to get 3 points with similarity search and then with diversity search.\n\n\n\nSimilarity:\n\n1. Calculate distance matrix\n\n2. Choose your anchor\n\n3. Get a vector corresponding to the distances from the selected anchor from the distance matrix\n\n4. Sort fetched vector\n\n5. Get top-3 embeddings\n\n \n\nDiversity:\n\n1. Calculate distance matrix\n\n2. Initialize starting point (randomly or according to the certain conditions)"", 'You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. \n\nTo mitigate this issue, you can perform a diversity search.\n\n\n\nDiversity search is a method for finding the most distinctive examples in the data.\n\nAs similarity search, it also operates on embeddings and measures the distances between them.\n\nThe difference lies in deciding which point should be extracted next.', 'Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. \n\nIt doesn’t rely on keywords or other metadata. \n\nWith vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.\n\n\n\nThe dissimilarity search can find items in areas where previously no other search could be used.\n\nLet’s look at a few examples.\n\n\n\n### Case: Mislabeling Detection']","Question: What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?\n\nContext: {{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=""Example of similarity-based sampling"" >}}





The power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.

By maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.
{{< figure src=/articles_data/vector-similarity-beyond-search/diversity.png caption=""Diversity Search"" >}}





Some forms of diversity sampling are already used in the industry and are known as [Maximum Margin Relevance](https://python.langchain.com/docs/integrations/vectorstores/qdrant#maximum-marginal-relevance-search-mmr) (MMR). Techniques like this were developed to enhance similarity on a universal search API.

However, there is still room for new ideas, particularly regarding diversity retrieval.
Let's imagine how to get 3 points with similarity search and then with diversity search.



Similarity:

1. Calculate distance matrix

2. Choose your anchor

3. Get a vector corresponding to the distances from the selected anchor from the distance matrix

4. Sort fetched vector

5. Get top-3 embeddings



Diversity:

1. Calculate distance matrix

2. Initialize starting point (randomly or according to the certain conditions)
You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. 

To mitigate this issue, you can perform a diversity search.



Diversity search is a method for finding the most distinctive examples in the data.

As similarity search, it also operates on embeddings and measures the distances between them.

The difference lies in deciding which point should be extracted next.
Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. 

It doesn’t rely on keywords or other metadata. 

With vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.



The dissimilarity search can find items in areas where previously no other search could be used.

Let’s look at a few examples.



### Case: Mislabeling Detection\n\nAnswer:",What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?,"Maximizing the distance between all points in the response when utilizing vector similarity for diversity search is crucial as it allows for the algorithm to sequentially output dissimilar results. By doing so, a diverse selection of the collection can be made possible without the need for any labeling efforts. This approach ensures that the algorithm can effectively identify and retrieve a wide range of diverse items or data points.","['{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=""Example of similarity-based sampling"" >}}\n\n\n\n\n\nThe power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.\n\nBy maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.', '{{< figure src=/articles_data/vector-similarity-beyond-search/diversity.png caption=""Diversity Search"" >}}\n\n\n\n\n\nSome forms of diversity sampling are already used in the industry and are known as [Maximum Margin Relevance](https://python.langchain.com/docs/integrations/vectorstores/qdrant#maximum-marginal-relevance-search-mmr) (MMR). Techniques like this were developed to enhance similarity on a universal search API.\n\nHowever, there is still room for new ideas, particularly regarding diversity retrieval.', ""Let's imagine how to get 3 points with similarity search and then with diversity search.\n\n\n\nSimilarity:\n\n1. Calculate distance matrix\n\n2. Choose your anchor\n\n3. Get a vector corresponding to the distances from the selected anchor from the distance matrix\n\n4. Sort fetched vector\n\n5. Get top-3 embeddings\n\n \n\nDiversity:\n\n1. Calculate distance matrix\n\n2. Initialize starting point (randomly or according to the certain conditions)"", 'You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. \n\nTo mitigate this issue, you can perform a diversity search.\n\n\n\nDiversity search is a method for finding the most distinctive examples in the data.\n\nAs similarity search, it also operates on embeddings and measures the distances between them.\n\nThe difference lies in deciding which point should be extracted next.', 'Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. \n\nIt doesn’t rely on keywords or other metadata. \n\nWith vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.\n\n\n\nThe dissimilarity search can find items in areas where previously no other search could be used.\n\nLet’s look at a few examples.\n\n\n\n### Case: Mislabeling Detection']","{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=""Example of similarity-based sampling"" >}}





The power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.

By maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.
{{< figure src=/articles_data/vector-similarity-beyond-search/diversity.png caption=""Diversity Search"" >}}





Some forms of diversity sampling are already used in the industry and are known as [Maximum Margin Relevance](https://python.langchain.com/docs/integrations/vectorstores/qdrant#maximum-marginal-relevance-search-mmr) (MMR). Techniques like this were developed to enhance similarity on a universal search API.

However, there is still room for new ideas, particularly regarding diversity retrieval.
Let's imagine how to get 3 points with similarity search and then with diversity search.



Similarity:

1. Calculate distance matrix

2. Choose your anchor

3. Get a vector corresponding to the distances from the selected anchor from the distance matrix

4. Sort fetched vector

5. Get top-3 embeddings

 

Diversity:

1. Calculate distance matrix

2. Initialize starting point (randomly or according to the certain conditions)
You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. 

To mitigate this issue, you can perform a diversity search.



Diversity search is a method for finding the most distinctive examples in the data.

As similarity search, it also operates on embeddings and measures the distances between them.

The difference lies in deciding which point should be extracted next.
Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. 

It doesn’t rely on keywords or other metadata. 

With vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.



The dissimilarity search can find items in areas where previously no other search could be used.

Let’s look at a few examples.



### Case: Mislabeling Detection",0.4166666666666667,0.7377049180327869,0.5325443786982249,0.2616822429906542,0.4666666666666667,0.33532934131736525,0.32407407407407407,0.5737704918032787,0.41420118343195267,0.37037037037037035,0.6557377049180327,0.47337278106508873,0.9480817317962646,0.6570353507995605,0.7210935950279236,0.68757563829422,0.997658697742736,0.9968037049208457,120,1.7391304347826086,0.5697873716593433
True,,3885.962,9bd00c26,How can you ensure that collection shards are replicated in Qdrant after adding a new node to the cluster?,"To ensure that collection shards are replicated in Qdrant after adding a new node to the cluster, you can use the Replicate Shard Operation. This operation allows you to create another copy of the shard on the newly connected node. It's important to note that Qdrant does not automatically balance shards as it is considered an expensive operation.","To ensure that collection shards are replicated in Qdrant after adding a new node to the cluster, you can use the Replicate Shard Operation provided by Qdrant. Here is a general outline of the steps you can take to replicate shards on the new node:

1. After adding the new node to the cluster and ensuring it is synchronized, use the Replicate Shard Operation to create another copy of the shard on the newly connected node. This operation will help in replicating the shards and ensuring data redundancy.

2. By replicating shards on the new node, you are enhancing the fault tolerance and availability of your data in case of node failures or network issues.

3. Remember that Qdrant does not automatically balance shards, so it is important to manually replicate shards when necessary, especially after scaling operations like adding new nodes to the cluster.

By following these steps and utilizing the Replicate Shard Operation, you can ensure that collection shards are properly replicated in Qdrant after adding a new node to the cluster.","Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.

Use the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.
Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.



### Moving shards



*Available as of v0.9.0*
Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.



When you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 300,
Practically, it means that if the cluster is in a transition state - either electing a new leader after a failure or starting up, the collection update operations will be denied.



You may use the cluster [REST API](https://qdrant.github.io/qdrant/redoc/index.html?v=master#tag/cluster) to check the state of the consensus.



## Sharding



A Collection in Qdrant is made of one or more shards.

A shard is an independent store of points which is able to perform all operations provided by collections.
```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

	shardNumber: 6,

	replicationFactor: 2

);

```



This code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.","['Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.\n\nUse the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.', 'Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.\n\n\n\n### Moving shards\n\n\n\n*Available as of v0.9.0*', 'Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.\n\n\n\nWhen you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 300,', 'Practically, it means that if the cluster is in a transition state - either electing a new leader after a failure or starting up, the collection update operations will be denied.\n\n\n\nYou may use the cluster [REST API](https://qdrant.github.io/qdrant/redoc/index.html?v=master#tag/cluster) to check the state of the consensus.\n\n\n\n## Sharding\n\n\n\nA Collection in Qdrant is made of one or more shards.\n\nA shard is an independent store of points which is able to perform all operations provided by collections.', '```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\n\tshardNumber: 6,\n\n\treplicationFactor: 2\n\n);\n\n```\n\n\n\nThis code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.']","Question: How can you ensure that collection shards are replicated in Qdrant after adding a new node to the cluster?\n\nContext: Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.

Use the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.
Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.



### Moving shards



*Available as of v0.9.0*
Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.



When you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 300,
Practically, it means that if the cluster is in a transition state - either electing a new leader after a failure or starting up, the collection update operations will be denied.



You may use the cluster [REST API](https://qdrant.github.io/qdrant/redoc/index.html?v=master#tag/cluster) to check the state of the consensus.



## Sharding



A Collection in Qdrant is made of one or more shards.

A shard is an independent store of points which is able to perform all operations provided by collections.
```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

collectionName: ""{collection_name}"",

vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

shardNumber: 6,

replicationFactor: 2

);

```



This code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.\n\nAnswer:",How can you ensure that collection shards are replicated in Qdrant after adding a new node to the cluster?,"To ensure that collection shards are replicated in Qdrant after adding a new node to the cluster, you can use the Replicate Shard Operation. This operation allows you to create another copy of the shard on the newly connected node. It's important to note that Qdrant does not automatically balance shards as it is considered an expensive operation.","['Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.\n\nUse the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.', 'Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.\n\n\n\n### Moving shards\n\n\n\n*Available as of v0.9.0*', 'Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.\n\n\n\nWhen you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 300,', 'Practically, it means that if the cluster is in a transition state - either electing a new leader after a failure or starting up, the collection update operations will be denied.\n\n\n\nYou may use the cluster [REST API](https://qdrant.github.io/qdrant/redoc/index.html?v=master#tag/cluster) to check the state of the consensus.\n\n\n\n## Sharding\n\n\n\nA Collection in Qdrant is made of one or more shards.\n\nA shard is an independent store of points which is able to perform all operations provided by collections.', '```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\n\tshardNumber: 6,\n\n\treplicationFactor: 2\n\n);\n\n```\n\n\n\nThis code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.']","Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.

Use the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.
Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.



### Moving shards



*Available as of v0.9.0*
Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.



When you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 300,
Practically, it means that if the cluster is in a transition state - either electing a new leader after a failure or starting up, the collection update operations will be denied.



You may use the cluster [REST API](https://qdrant.github.io/qdrant/redoc/index.html?v=master#tag/cluster) to check the state of the consensus.



## Sharding



A Collection in Qdrant is made of one or more shards.

A shard is an independent store of points which is able to perform all operations provided by collections.
```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

	shardNumber: 6,

	replicationFactor: 2

);

```



This code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.",0.30128205128205127,0.8867924528301887,0.44976076555023925,0.24516129032258063,0.7307692307692307,0.3671497584541063,0.2692307692307692,0.7924528301886793,0.40191387559808617,0.28846153846153844,0.8490566037735849,0.430622009569378,0.9749405384063721,0.6586215496063232,0.7739602327346802,0.7116479277610779,0.8974578514074286,0.9922368508802416,174,2.9491525423728815,0.5346853228732715
True,,3012.7619999999997,783e18fa,Why would someone use a vector database?,"A vector database is used for various reasons, primarily for its efficiency in handling complex data structures and enabling advanced search capabilities. In the context of search and discovery, a vector database allows for state-of-the-art vector-search functionalities, making it ideal for applications requiring similarity search, recommendation systems, and content-based search. By leveraging vectors to represent data points, a vector database can efficiently compute similarities between vectors, enabling quick and accurate retrieval of relevant information","Someone would use a vector database for several reasons, including:

1. Efficient storage and indexing of high-dimensional data: Vector databases are optimized for storing and indexing high-dimensional vector data, making them well-suited for applications that deal with complex data structures.

2. Handling large-scale datasets: Vector databases are capable of handling massive amounts of data points, making them suitable for applications that require processing and analyzing extensive datasets.

3. Support for real-time analytics and queries: Vector databases enable real-time processing and querying of data, making them ideal for applications that require quick responses and analysis of data.

4. Advanced data analysis and similarity search: Vector databases provide advanced capabilities for data analysis and similarity search operations, going beyond traditional structured query approaches of conventional databases.

5. Efficient retrieval of unstructured data represented as vectors: By efficiently indexing and searching high-dimensional vector data, vector databases can retrieve unstructured data in a fast and accurate manner, presenting users with the most relevant results to their queries.","aliases: [ /blog/what-is-a-vector-database/ ]

---



> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.





## Why use a Vector Database?



The data flood is real.
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query
Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.





### Starting Your First Vector Database Project
There is no one-size-fits-all approach that would not compromise on performance or flexibility.

So if your use case utilizes vector search in any significant way, it is worth investing in a dedicated vector search engine, aka vector database.","['aliases: [ /blog/what-is-a-vector-database/ ]\n\n---\n\n\n\n> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.\n\n\n\n\n\n## Why use a Vector Database?\n\n\n\nThe data flood is real.', 'advantage of their efficient indexing and searching techniques, vector databases enable faster \n\nand more accurate retrieval of unstructured data already represented as vectors, which can \n\nhelp put in front of users the most relevant results to their queries.\n\n\n\nIn addition, other benefits of using vector databases include:\n\n1. Efficient storage and indexing of high-dimensional data.\n\n3. Ability to handle large-scale datasets with billions of data points.\n\n4. Support for real-time analytics and queries.', ""And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query"", 'Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.\n\n\n\n\n\n### Starting Your First Vector Database Project', 'There is no one-size-fits-all approach that would not compromise on performance or flexibility.\n\nSo if your use case utilizes vector search in any significant way, it is worth investing in a dedicated vector search engine, aka vector database.']","Question: Why would someone use a vector database?\n\nContext: aliases: [ /blog/what-is-a-vector-database/ ]

---



> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.





## Why use a Vector Database?



The data flood is real.
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query
Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.





### Starting Your First Vector Database Project
There is no one-size-fits-all approach that would not compromise on performance or flexibility.

So if your use case utilizes vector search in any significant way, it is worth investing in a dedicated vector search engine, aka vector database.\n\nAnswer:",Why would someone use a vector database?,"A vector database is used for various reasons, primarily for its efficiency in handling complex data structures and enabling advanced search capabilities. In the context of search and discovery, a vector database allows for state-of-the-art vector-search functionalities, making it ideal for applications requiring similarity search, recommendation systems, and content-based search. By leveraging vectors to represent data points, a vector database can efficiently compute similarities between vectors, enabling quick and accurate retrieval of relevant information","['aliases: [ /blog/what-is-a-vector-database/ ]\n\n---\n\n\n\n> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.\n\n\n\n\n\n## Why use a Vector Database?\n\n\n\nThe data flood is real.', 'advantage of their efficient indexing and searching techniques, vector databases enable faster \n\nand more accurate retrieval of unstructured data already represented as vectors, which can \n\nhelp put in front of users the most relevant results to their queries.\n\n\n\nIn addition, other benefits of using vector databases include:\n\n1. Efficient storage and indexing of high-dimensional data.\n\n3. Ability to handle large-scale datasets with billions of data points.\n\n4. Support for real-time analytics and queries.', ""And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query"", 'Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.\n\n\n\n\n\n### Starting Your First Vector Database Project', 'There is no one-size-fits-all approach that would not compromise on performance or flexibility.\n\nSo if your use case utilizes vector search in any significant way, it is worth investing in a dedicated vector search engine, aka vector database.']","aliases: [ /blog/what-is-a-vector-database/ ]

---



> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.





## Why use a Vector Database?



The data flood is real.
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query
Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.





### Starting Your First Vector Database Project
There is no one-size-fits-all approach that would not compromise on performance or flexibility.

So if your use case utilizes vector search in any significant way, it is worth investing in a dedicated vector search engine, aka vector database.",0.30538922155688625,0.6891891891891891,0.4232365145228216,0.07228915662650602,0.1643835616438356,0.10041841004184099,0.19161676646706588,0.43243243243243246,0.2655601659751038,0.25748502994011974,0.581081081081081,0.3568464730290456,0.679102897644043,0.6367596387863159,0.6924497485160828,0.663438081741333,0.8116180503857322,0.993952860958719,163,2.2027027027027026,0.4895165683973922
True,,3382.328,c67141ec,What benefits does Qdrant Cloud on Microsoft Azure offer for rapid application development?,"Qdrant Cloud on Microsoft Azure offers the benefit of rapid application development by allowing users to deploy their own cluster through the Qdrant Cloud Console within seconds. This means that users can set up their environment on Azure quickly, reducing deployment time and enabling them to scale their resources as needed. This rapid deployment capability enables users to hit the ground running with their development projects, facilitating faster development cycles and improved scalability.","Qdrant Cloud on Microsoft Azure offers several benefits for rapid application development, including:

1. **Instant Deployment**: You can deploy your own cluster through the Qdrant Cloud Console within seconds, allowing you to start development quickly.

2. **Scalability**: With Microsoft Azure's infrastructure, you can easily scale your resources as needed, enabling you to handle large-scale datasets with billions of vectors efficiently.

3. **Horizontal Scaling**: Leveraging Qdrant's features like horizontal scaling and binary quantization on Microsoft Azure's scalable infrastructure enhances the performance and efficiency of your applications.

Overall, Qdrant Cloud on Microsoft Azure provides a robust and flexible environment for rapid application development, allowing you to focus on building and scaling your applications without worrying about infrastructure management.","What this means for you:



- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.

- **Billion vector scale**: Seamlessly grow and handle large-scale datasets with billions of vectors. Leverage Qdrant features like horizontal scaling and binary quantization with Microsoft Azure's scalable infrastructure.
---

draft: false

title: Introducing Qdrant Cloud on Microsoft Azure

slug: qdrant-cloud-on-microsoft-azure

short_description: Qdrant Cloud is now available on Microsoft Azure

description: ""Learn the benefits of Qdrant Cloud on Azure.""

preview_image: /blog/from_cms/qdrant-azure-2-1.png

date: 2024-01-17T08:40:42Z

author: Manuel Meyer

featured: false

tags:

  - Data Science

  - Vector Database

  - Machine Learning

  - Information Retrieval

  - Cloud

  - Azure

---
- Information Retrieval

  - Cloud

  - Azure

---

Great news! We've expanded Qdrant's managed vector database offering — [Qdrant Cloud](https://cloud.qdrant.io/) — to be available on Microsoft Azure. 

You can now effortlessly set up your environment on Azure, which reduces deployment time, so you can hit the ground running.



[Get started](https://cloud.qdrant.io/)



What this means for you:
- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:
Qdrant Cloud is the hosted and managed version of the Qdrant engine, our open-source solution. It is accessible as a Software as a Service (“SaaS”) through the following link [https://cloud.qdrant.io](https://cloud.qdrant.io) 



By using the Qdrant Cloud, you agree to comply with the following general terms and conditions of use and sale (the “T&Cs”), which form a binding contract between you and the COmpany, giving you access to both the Solution and its website (the “Website”).","[""What this means for you:\n\n\n\n- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.\n\n- **Billion vector scale**: Seamlessly grow and handle large-scale datasets with billions of vectors. Leverage Qdrant features like horizontal scaling and binary quantization with Microsoft Azure's scalable infrastructure."", '---\n\ndraft: false\n\ntitle: Introducing Qdrant Cloud on Microsoft Azure\n\nslug: qdrant-cloud-on-microsoft-azure\n\nshort_description: Qdrant Cloud is now available on Microsoft Azure\n\ndescription: ""Learn the benefits of Qdrant Cloud on Azure.""\n\npreview_image: /blog/from_cms/qdrant-azure-2-1.png\n\ndate: 2024-01-17T08:40:42Z\n\nauthor: Manuel Meyer\n\nfeatured: false\n\ntags:\n\n  - Data Science\n\n  - Vector Database\n\n  - Machine Learning\n\n  - Information Retrieval\n\n  - Cloud\n\n  - Azure\n\n---', ""- Information Retrieval\n\n  - Cloud\n\n  - Azure\n\n---\n\nGreat news! We've expanded Qdrant's managed vector database offering — [Qdrant Cloud](https://cloud.qdrant.io/) — to be available on Microsoft Azure. \n\nYou can now effortlessly set up your environment on Azure, which reduces deployment time, so you can hit the ground running.\n\n\n\n[Get started](https://cloud.qdrant.io/)\n\n\n\nWhat this means for you:"", '- Upgradeable to later versions of Qdrant as they are released.\n\n- Equipped with monitoring and logging to observe the health of each cluster. \n\n- Accessible through the Qdrant Cloud Console.\n\n- Vertically scalable.\n\n- Offered on AWS and GCP, with Azure currently in development. \n\n\n\n### Getting started with Qdrant Cloud\n\n\n\nTo use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:', 'Qdrant Cloud is the hosted and managed version of the Qdrant engine, our open-source solution. It is accessible as a Software as a Service (“SaaS”) through the following link [https://cloud.qdrant.io](https://cloud.qdrant.io) \n\n\n\nBy using the Qdrant Cloud, you agree to comply with the following general terms and conditions of use and sale (the “T&Cs”), which form a binding contract between you and the COmpany, giving you access to both the Solution and its website (the “Website”).']","Question: What benefits does Qdrant Cloud on Microsoft Azure offer for rapid application development?\n\nContext: What this means for you:



- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.

- **Billion vector scale**: Seamlessly grow and handle large-scale datasets with billions of vectors. Leverage Qdrant features like horizontal scaling and binary quantization with Microsoft Azure's scalable infrastructure.
---

draft: false

title: Introducing Qdrant Cloud on Microsoft Azure

slug: qdrant-cloud-on-microsoft-azure

short_description: Qdrant Cloud is now available on Microsoft Azure

description: ""Learn the benefits of Qdrant Cloud on Azure.""

preview_image: /blog/from_cms/qdrant-azure-2-1.png

date: 2024-01-17T08:40:42Z

author: Manuel Meyer

featured: false

tags:

- Data Science

- Vector Database

- Machine Learning

- Information Retrieval

- Cloud

- Azure

---
- Information Retrieval

- Cloud

- Azure

---

Great news! We've expanded Qdrant's managed vector database offering — [Qdrant Cloud](https://cloud.qdrant.io/) — to be available on Microsoft Azure. 

You can now effortlessly set up your environment on Azure, which reduces deployment time, so you can hit the ground running.



[Get started](https://cloud.qdrant.io/)



What this means for you:
- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:
Qdrant Cloud is the hosted and managed version of the Qdrant engine, our open-source solution. It is accessible as a Software as a Service (“SaaS”) through the following link [https://cloud.qdrant.io](https://cloud.qdrant.io) 



By using the Qdrant Cloud, you agree to comply with the following general terms and conditions of use and sale (the “T&Cs”), which form a binding contract between you and the COmpany, giving you access to both the Solution and its website (the “Website”).\n\nAnswer:",What benefits does Qdrant Cloud on Microsoft Azure offer for rapid application development?,"Qdrant Cloud on Microsoft Azure offers the benefit of rapid application development by allowing users to deploy their own cluster through the Qdrant Cloud Console within seconds. This means that users can set up their environment on Azure quickly, reducing deployment time and enabling them to scale their resources as needed. This rapid deployment capability enables users to hit the ground running with their development projects, facilitating faster development cycles and improved scalability.","[""What this means for you:\n\n\n\n- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.\n\n- **Billion vector scale**: Seamlessly grow and handle large-scale datasets with billions of vectors. Leverage Qdrant features like horizontal scaling and binary quantization with Microsoft Azure's scalable infrastructure."", '---\n\ndraft: false\n\ntitle: Introducing Qdrant Cloud on Microsoft Azure\n\nslug: qdrant-cloud-on-microsoft-azure\n\nshort_description: Qdrant Cloud is now available on Microsoft Azure\n\ndescription: ""Learn the benefits of Qdrant Cloud on Azure.""\n\npreview_image: /blog/from_cms/qdrant-azure-2-1.png\n\ndate: 2024-01-17T08:40:42Z\n\nauthor: Manuel Meyer\n\nfeatured: false\n\ntags:\n\n  - Data Science\n\n  - Vector Database\n\n  - Machine Learning\n\n  - Information Retrieval\n\n  - Cloud\n\n  - Azure\n\n---', ""- Information Retrieval\n\n  - Cloud\n\n  - Azure\n\n---\n\nGreat news! We've expanded Qdrant's managed vector database offering — [Qdrant Cloud](https://cloud.qdrant.io/) — to be available on Microsoft Azure. \n\nYou can now effortlessly set up your environment on Azure, which reduces deployment time, so you can hit the ground running.\n\n\n\n[Get started](https://cloud.qdrant.io/)\n\n\n\nWhat this means for you:"", '- Upgradeable to later versions of Qdrant as they are released.\n\n- Equipped with monitoring and logging to observe the health of each cluster. \n\n- Accessible through the Qdrant Cloud Console.\n\n- Vertically scalable.\n\n- Offered on AWS and GCP, with Azure currently in development. \n\n\n\n### Getting started with Qdrant Cloud\n\n\n\nTo use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:', 'Qdrant Cloud is the hosted and managed version of the Qdrant engine, our open-source solution. It is accessible as a Software as a Service (“SaaS”) through the following link [https://cloud.qdrant.io](https://cloud.qdrant.io) \n\n\n\nBy using the Qdrant Cloud, you agree to comply with the following general terms and conditions of use and sale (the “T&Cs”), which form a binding contract between you and the COmpany, giving you access to both the Solution and its website (the “Website”).']","What this means for you:



- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.

- **Billion vector scale**: Seamlessly grow and handle large-scale datasets with billions of vectors. Leverage Qdrant features like horizontal scaling and binary quantization with Microsoft Azure's scalable infrastructure.
---

draft: false

title: Introducing Qdrant Cloud on Microsoft Azure

slug: qdrant-cloud-on-microsoft-azure

short_description: Qdrant Cloud is now available on Microsoft Azure

description: ""Learn the benefits of Qdrant Cloud on Azure.""

preview_image: /blog/from_cms/qdrant-azure-2-1.png

date: 2024-01-17T08:40:42Z

author: Manuel Meyer

featured: false

tags:

  - Data Science

  - Vector Database

  - Machine Learning

  - Information Retrieval

  - Cloud

  - Azure

---
- Information Retrieval

  - Cloud

  - Azure

---

Great news! We've expanded Qdrant's managed vector database offering — [Qdrant Cloud](https://cloud.qdrant.io/) — to be available on Microsoft Azure. 

You can now effortlessly set up your environment on Azure, which reduces deployment time, so you can hit the ground running.



[Get started](https://cloud.qdrant.io/)



What this means for you:
- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:
Qdrant Cloud is the hosted and managed version of the Qdrant engine, our open-source solution. It is accessible as a Software as a Service (“SaaS”) through the following link [https://cloud.qdrant.io](https://cloud.qdrant.io) 



By using the Qdrant Cloud, you agree to comply with the following general terms and conditions of use and sale (the “T&Cs”), which form a binding contract between you and the COmpany, giving you access to both the Solution and its website (the “Website”).",0.358974358974359,0.6,0.44919786096256686,0.13793103448275862,0.2318840579710145,0.17297297297297298,0.24786324786324787,0.4142857142857143,0.3101604278074866,0.3333333333333333,0.5571428571428572,0.4171122994652407,0.9407100677490234,0.6511015892028809,0.7231172323226929,0.6852224469184875,0.7077985677335943,0.9971100793530544,119,1.63013698630137,0.5059814129500914
True,,1562.355,53cab913,What is the purpose of the `groupBy` parameter?,"The `groupBy` parameter is used to specify how the results should be grouped based on a specific field, like ""document_id"". By setting the `groupBy` parameter to ""document_id"", the results are organized into groups where each group contains all the hits associated with a particular document_id.","The `groupBy` parameter is used in the grouping API request to specify the field by which the search results should be grouped. It determines how the search results will be organized into distinct groups based on the values of the specified field. In the provided code snippets, the `groupBy` parameter is used to group the search results by the `document_id` field.","#### Adding the parameter to grouping API request:



When using the grouping API, add the `with_lookup` parameter to bring the information from those points into each group:



```http

POST /collections/chunks/points/search/groups

{

    // Same as in the regular search API

    ""vector"": [1.1],

    ...,



    // Grouping parameters

    ""group_by"": ""document_id"",  

    ""limit"": 2,                 

    ""group_size"": 2,            



    // Lookup parameters

    ""with_lookup"": {
{

    // Same as in the regular search API

    ""vector"": [1.1],



    // Grouping parameters

    ""group_by"": ""document_id"",

    ""limit"": 2,

    ""group_size"": 2,



    // Lookup parameters

    ""with_lookup"": {

        // Name of the collection to look up points in

        ""collection"": ""documents"",



        // Options for specifying what to bring from the payload 

        // of the looked up point, true by default

        ""with_payload"": [""title"", ""text""],
""with_payload"": [""title"", ""text""],



        // Options for specifying what to bring from the vector(s) 

        // of the looked up point, true by default

        ""with_vectors: false

    }

}

```



```python

client.search_groups(

    collection_name=""chunks"",

    # Same as in the regular search() API

    query_vector=[1.1],

    # Grouping parameters

    group_by=""document_id"",  # Path of the field to group by

    limit=2,  # Max amount of groups
vector: new float[] { 1.1f },

	groupBy: ""document_id"",

	limit: 4,

	groupSize: 2

);

```



The output of a ***groups*** call looks like this:



```json

{

    ""result"": {

        ""groups"": [

            {

                ""id"": ""a"",

                ""hits"": [

                    { ""id"": 0, ""score"": 0.91 },

                    { ""id"": 1, ""score"": 0.85 }

                ]

            },

            {

                ""id"": ""b"",

                ""hits"": [
client

    .search_groups(&SearchPointGroups {

        collection_name: ""{collection_name}"".to_string(),

        vector: vec![1.1],

        group_by: ""document_id"".to_string(),

        limit: 2,

        group_size: 2,

        with_lookup: Some(WithLookup {

            collection: ""documents"".to_string(),

            with_payload: Some(vec![""title"", ""text""].into()),

            with_vectors: Some(false.into()),

        }),

        ..Default::default()

    })

    .await?;

```



```java","['#### Adding the parameter to grouping API request:\n\n\n\nWhen using the grouping API, add the `with_lookup` parameter to bring the information from those points into each group:\n\n\n\n```http\n\nPOST /collections/chunks/points/search/groups\n\n{\n\n    // Same as in the regular search API\n\n    ""vector"": [1.1],\n\n    ...,\n\n\n\n    // Grouping parameters\n\n    ""group_by"": ""document_id"",  \n\n    ""limit"": 2,                 \n\n    ""group_size"": 2,            \n\n\n\n    // Lookup parameters\n\n    ""with_lookup"": {', '{\n\n    // Same as in the regular search API\n\n    ""vector"": [1.1],\n\n\n\n    // Grouping parameters\n\n    ""group_by"": ""document_id"",\n\n    ""limit"": 2,\n\n    ""group_size"": 2,\n\n\n\n    // Lookup parameters\n\n    ""with_lookup"": {\n\n        // Name of the collection to look up points in\n\n        ""collection"": ""documents"",\n\n\n\n        // Options for specifying what to bring from the payload \n\n        // of the looked up point, true by default\n\n        ""with_payload"": [""title"", ""text""],', '""with_payload"": [""title"", ""text""],\n\n\n\n        // Options for specifying what to bring from the vector(s) \n\n        // of the looked up point, true by default\n\n        ""with_vectors: false\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.search_groups(\n\n    collection_name=""chunks"",\n\n    # Same as in the regular search() API\n\n    query_vector=[1.1],\n\n    # Grouping parameters\n\n    group_by=""document_id"",  # Path of the field to group by\n\n    limit=2,  # Max amount of groups', 'vector: new float[] { 1.1f },\n\n\tgroupBy: ""document_id"",\n\n\tlimit: 4,\n\n\tgroupSize: 2\n\n);\n\n```\n\n\n\nThe output of a ***groups*** call looks like this:\n\n\n\n```json\n\n{\n\n    ""result"": {\n\n        ""groups"": [\n\n            {\n\n                ""id"": ""a"",\n\n                ""hits"": [\n\n                    { ""id"": 0, ""score"": 0.91 },\n\n                    { ""id"": 1, ""score"": 0.85 }\n\n                ]\n\n            },\n\n            {\n\n                ""id"": ""b"",\n\n                ""hits"": [', 'client\n\n    .search_groups(&SearchPointGroups {\n\n        collection_name: ""{collection_name}"".to_string(),\n\n        vector: vec![1.1],\n\n        group_by: ""document_id"".to_string(),\n\n        limit: 2,\n\n        group_size: 2,\n\n        with_lookup: Some(WithLookup {\n\n            collection: ""documents"".to_string(),\n\n            with_payload: Some(vec![""title"", ""text""].into()),\n\n            with_vectors: Some(false.into()),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java']","Question: What is the purpose of the `groupBy` parameter?\n\nContext: #### Adding the parameter to grouping API request:



When using the grouping API, add the `with_lookup` parameter to bring the information from those points into each group:



```http

POST /collections/chunks/points/search/groups

{

// Same as in the regular search API

""vector"": [1.1],

...,



// Grouping parameters

""group_by"": ""document_id"",  

""limit"": 2,                 

""group_size"": 2,            



// Lookup parameters

""with_lookup"": {
{

// Same as in the regular search API

""vector"": [1.1],



// Grouping parameters

""group_by"": ""document_id"",

""limit"": 2,

""group_size"": 2,



// Lookup parameters

""with_lookup"": {

// Name of the collection to look up points in

""collection"": ""documents"",



// Options for specifying what to bring from the payload 

// of the looked up point, true by default

""with_payload"": [""title"", ""text""],
""with_payload"": [""title"", ""text""],



// Options for specifying what to bring from the vector(s) 

// of the looked up point, true by default

""with_vectors: false

}

}

```



```python

client.search_groups(

collection_name=""chunks"",

# Same as in the regular search() API

query_vector=[1.1],

# Grouping parameters

group_by=""document_id"",  # Path of the field to group by

limit=2,  # Max amount of groups
vector: new float[] { 1.1f },

groupBy: ""document_id"",

limit: 4,

groupSize: 2

);

```



The output of a ***groups*** call looks like this:



```json

{

""result"": {

""groups"": [

{

""id"": ""a"",

""hits"": [

{ ""id"": 0, ""score"": 0.91 },

{ ""id"": 1, ""score"": 0.85 }

]

},

{

""id"": ""b"",

""hits"": [
client

.search_groups(&SearchPointGroups {

collection_name: ""{collection_name}"".to_string(),

vector: vec![1.1],

group_by: ""document_id"".to_string(),

limit: 2,

group_size: 2,

with_lookup: Some(WithLookup {

collection: ""documents"".to_string(),

with_payload: Some(vec![""title"", ""text""].into()),

with_vectors: Some(false.into()),

}),

..Default::default()

})

.await?;

```



```java\n\nAnswer:",What is the purpose of the `groupBy` parameter?,"The `groupBy` parameter is used to specify how the results should be grouped based on a specific field, like ""document_id"". By setting the `groupBy` parameter to ""document_id"", the results are organized into groups where each group contains all the hits associated with a particular document_id.","['#### Adding the parameter to grouping API request:\n\n\n\nWhen using the grouping API, add the `with_lookup` parameter to bring the information from those points into each group:\n\n\n\n```http\n\nPOST /collections/chunks/points/search/groups\n\n{\n\n    // Same as in the regular search API\n\n    ""vector"": [1.1],\n\n    ...,\n\n\n\n    // Grouping parameters\n\n    ""group_by"": ""document_id"",  \n\n    ""limit"": 2,                 \n\n    ""group_size"": 2,            \n\n\n\n    // Lookup parameters\n\n    ""with_lookup"": {', '{\n\n    // Same as in the regular search API\n\n    ""vector"": [1.1],\n\n\n\n    // Grouping parameters\n\n    ""group_by"": ""document_id"",\n\n    ""limit"": 2,\n\n    ""group_size"": 2,\n\n\n\n    // Lookup parameters\n\n    ""with_lookup"": {\n\n        // Name of the collection to look up points in\n\n        ""collection"": ""documents"",\n\n\n\n        // Options for specifying what to bring from the payload \n\n        // of the looked up point, true by default\n\n        ""with_payload"": [""title"", ""text""],', '""with_payload"": [""title"", ""text""],\n\n\n\n        // Options for specifying what to bring from the vector(s) \n\n        // of the looked up point, true by default\n\n        ""with_vectors: false\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.search_groups(\n\n    collection_name=""chunks"",\n\n    # Same as in the regular search() API\n\n    query_vector=[1.1],\n\n    # Grouping parameters\n\n    group_by=""document_id"",  # Path of the field to group by\n\n    limit=2,  # Max amount of groups', 'vector: new float[] { 1.1f },\n\n\tgroupBy: ""document_id"",\n\n\tlimit: 4,\n\n\tgroupSize: 2\n\n);\n\n```\n\n\n\nThe output of a ***groups*** call looks like this:\n\n\n\n```json\n\n{\n\n    ""result"": {\n\n        ""groups"": [\n\n            {\n\n                ""id"": ""a"",\n\n                ""hits"": [\n\n                    { ""id"": 0, ""score"": 0.91 },\n\n                    { ""id"": 1, ""score"": 0.85 }\n\n                ]\n\n            },\n\n            {\n\n                ""id"": ""b"",\n\n                ""hits"": [', 'client\n\n    .search_groups(&SearchPointGroups {\n\n        collection_name: ""{collection_name}"".to_string(),\n\n        vector: vec![1.1],\n\n        group_by: ""document_id"".to_string(),\n\n        limit: 2,\n\n        group_size: 2,\n\n        with_lookup: Some(WithLookup {\n\n            collection: ""documents"".to_string(),\n\n            with_payload: Some(vec![""title"", ""text""].into()),\n\n            with_vectors: Some(false.into()),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java']","#### Adding the parameter to grouping API request:



When using the grouping API, add the `with_lookup` parameter to bring the information from those points into each group:



```http

POST /collections/chunks/points/search/groups

{

    // Same as in the regular search API

    ""vector"": [1.1],

    ...,



    // Grouping parameters

    ""group_by"": ""document_id"",  

    ""limit"": 2,                 

    ""group_size"": 2,            



    // Lookup parameters

    ""with_lookup"": {
{

    // Same as in the regular search API

    ""vector"": [1.1],



    // Grouping parameters

    ""group_by"": ""document_id"",

    ""limit"": 2,

    ""group_size"": 2,



    // Lookup parameters

    ""with_lookup"": {

        // Name of the collection to look up points in

        ""collection"": ""documents"",



        // Options for specifying what to bring from the payload 

        // of the looked up point, true by default

        ""with_payload"": [""title"", ""text""],
""with_payload"": [""title"", ""text""],



        // Options for specifying what to bring from the vector(s) 

        // of the looked up point, true by default

        ""with_vectors: false

    }

}

```



```python

client.search_groups(

    collection_name=""chunks"",

    # Same as in the regular search() API

    query_vector=[1.1],

    # Grouping parameters

    group_by=""document_id"",  # Path of the field to group by

    limit=2,  # Max amount of groups
vector: new float[] { 1.1f },

	groupBy: ""document_id"",

	limit: 4,

	groupSize: 2

);

```



The output of a ***groups*** call looks like this:



```json

{

    ""result"": {

        ""groups"": [

            {

                ""id"": ""a"",

                ""hits"": [

                    { ""id"": 0, ""score"": 0.91 },

                    { ""id"": 1, ""score"": 0.85 }

                ]

            },

            {

                ""id"": ""b"",

                ""hits"": [
client

    .search_groups(&SearchPointGroups {

        collection_name: ""{collection_name}"".to_string(),

        vector: vec![1.1],

        group_by: ""document_id"".to_string(),

        limit: 2,

        group_size: 2,

        with_lookup: Some(WithLookup {

            collection: ""documents"".to_string(),

            with_payload: Some(vec![""title"", ""text""].into()),

            with_vectors: Some(false.into()),

        }),

        ..Default::default()

    })

    .await?;

```



```java",0.49019607843137253,0.6097560975609756,0.5434782608695652,0.26,0.325,0.2888888888888889,0.37254901960784315,0.4634146341463415,0.4130434782608696,0.47058823529411764,0.5853658536585366,0.5217391304347826,0.8670060038566589,0.6834094524383545,0.6904165148735046,0.6868950724601746,0.8287780868510405,0.9116517528891563,61,1.3555555555555556,0.6177086690116638
True,,4065.48,0262df2a,How can one change or correct Qdrant's behavior and default collection settings using configuration files?,"To change or correct Qdrant's behavior and default collection settings, one can utilize configuration files. The default configuration file for Qdrant is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml). If you wish to modify the default configuration, you can add a new configuration file and specify the path using `--config-path path/to/custom_config.yaml","To change or correct Qdrant's behavior and default collection settings using configuration files, you can follow these steps:

1. Locate the default configuration file at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).

2. Add a new configuration file and specify the settings you want to change or correct.

3. To change the default configuration, you can modify the values in the configuration file.

4. If you are using the QdrantDocumentStore, you can customize the default configuration of the collection used under the hood by providing settings when you create an instance of the `QdrantDocumentStore`.

5. For example, if you want to enable Scalar Quantization, you can set this configuration in your code when creating the `QdrantDocumentStore` instance.

6. To change the storage temporary path, access the `config.yaml` file and set the `storage.temp_path` property to the desired directory location.

7. It's important to note that the latest releases of Qdrant may introduce changes to make the system more reliable and efficient. Make sure to keep an eye on the release notes for any important changes that may affect your configurations.

By following these steps, you can effectively change or correct Qdrant's behavior and default collection settings using configuration files.","---

title: Configuration

weight: 160

aliases:

  - ../configuration

---



# Configuration



To change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.



The default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).



To change the default configuration, add a new configuration file and specify
```bash

pip install qdrant-haystack

```



`QdrantDocumentStore` supports [all the configuration properties](/documentation/collections/#create-collection) available in 

the Qdrant Python client. If you want to customize the default configuration of the collection used under the hood, you can

provide that settings when you create an instance of the `QdrantDocumentStore`. For example, if you'd like to enable the

Scalar Quantization, you'd make that in the following way:



```python
To change it, access `config.yaml` and set `storage.temp_path` to another directory location.



## Important changes



The latest release focuses not only on the new features but also introduces some changes making 

Qdrant even more reliable.



### Optimizing group requests
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()

                    .setParams(

                        VectorParams.newBuilder()

                            .setSize(768)
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()

                    .setParams(

                        VectorParams.newBuilder()

                            .setSize(768)","[""---\n\ntitle: Configuration\n\nweight: 160\n\naliases:\n\n  - ../configuration\n\n---\n\n\n\n# Configuration\n\n\n\nTo change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.\n\n\n\nThe default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\n\n\n\nTo change the default configuration, add a new configuration file and specify"", ""```bash\n\npip install qdrant-haystack\n\n```\n\n\n\n`QdrantDocumentStore` supports [all the configuration properties](/documentation/collections/#create-collection) available in \n\nthe Qdrant Python client. If you want to customize the default configuration of the collection used under the hood, you can\n\nprovide that settings when you create an instance of the `QdrantDocumentStore`. For example, if you'd like to enable the\n\nScalar Quantization, you'd make that in the following way:\n\n\n\n```python"", 'To change it, access `config.yaml` and set `storage.temp_path` to another directory location.\n\n\n\n## Important changes\n\n\n\nThe latest release focuses not only on the new features but also introduces some changes making \n\nQdrant even more reliable.\n\n\n\n### Optimizing group requests', 'import io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()\n\n                    .setParams(\n\n                        VectorParams.newBuilder()\n\n                            .setSize(768)', 'import io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()\n\n                    .setParams(\n\n                        VectorParams.newBuilder()\n\n                            .setSize(768)']","Question: How can one change or correct Qdrant's behavior and default collection settings using configuration files?\n\nContext: ---

title: Configuration

weight: 160

aliases:

- ../configuration

---



# Configuration



To change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.



The default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).



To change the default configuration, add a new configuration file and specify
```bash

pip install qdrant-haystack

```



`QdrantDocumentStore` supports [all the configuration properties](/documentation/collections/#create-collection) available in 

the Qdrant Python client. If you want to customize the default configuration of the collection used under the hood, you can

provide that settings when you create an instance of the `QdrantDocumentStore`. For example, if you'd like to enable the

Scalar Quantization, you'd make that in the following way:



```python
To change it, access `config.yaml` and set `storage.temp_path` to another directory location.



## Important changes



The latest release focuses not only on the new features but also introduces some changes making 

Qdrant even more reliable.



### Optimizing group requests
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createCollectionAsync(

CreateCollection.newBuilder()

.setCollectionName(""{collection_name}"")

.setVectorsConfig(

VectorsConfig.newBuilder()

.setParams(

VectorParams.newBuilder()

.setSize(768)
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createCollectionAsync(

CreateCollection.newBuilder()

.setCollectionName(""{collection_name}"")

.setVectorsConfig(

VectorsConfig.newBuilder()

.setParams(

VectorParams.newBuilder()

.setSize(768)\n\nAnswer:",How can one change or correct Qdrant's behavior and default collection settings using configuration files?,"To change or correct Qdrant's behavior and default collection settings, one can utilize configuration files. The default configuration file for Qdrant is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml). If you wish to modify the default configuration, you can add a new configuration file and specify the path using `--config-path path/to/custom_config.yaml","[""---\n\ntitle: Configuration\n\nweight: 160\n\naliases:\n\n  - ../configuration\n\n---\n\n\n\n# Configuration\n\n\n\nTo change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.\n\n\n\nThe default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\n\n\n\nTo change the default configuration, add a new configuration file and specify"", ""```bash\n\npip install qdrant-haystack\n\n```\n\n\n\n`QdrantDocumentStore` supports [all the configuration properties](/documentation/collections/#create-collection) available in \n\nthe Qdrant Python client. If you want to customize the default configuration of the collection used under the hood, you can\n\nprovide that settings when you create an instance of the `QdrantDocumentStore`. For example, if you'd like to enable the\n\nScalar Quantization, you'd make that in the following way:\n\n\n\n```python"", 'To change it, access `config.yaml` and set `storage.temp_path` to another directory location.\n\n\n\n## Important changes\n\n\n\nThe latest release focuses not only on the new features but also introduces some changes making \n\nQdrant even more reliable.\n\n\n\n### Optimizing group requests', 'import io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()\n\n                    .setParams(\n\n                        VectorParams.newBuilder()\n\n                            .setSize(768)', 'import io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()\n\n                    .setParams(\n\n                        VectorParams.newBuilder()\n\n                            .setSize(768)']","---

title: Configuration

weight: 160

aliases:

  - ../configuration

---



# Configuration



To change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.



The default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).



To change the default configuration, add a new configuration file and specify
```bash

pip install qdrant-haystack

```



`QdrantDocumentStore` supports [all the configuration properties](/documentation/collections/#create-collection) available in 

the Qdrant Python client. If you want to customize the default configuration of the collection used under the hood, you can

provide that settings when you create an instance of the `QdrantDocumentStore`. For example, if you'd like to enable the

Scalar Quantization, you'd make that in the following way:



```python
To change it, access `config.yaml` and set `storage.temp_path` to another directory location.



## Important changes



The latest release focuses not only on the new features but also introduces some changes making 

Qdrant even more reliable.



### Optimizing group requests
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()

                    .setParams(

                        VectorParams.newBuilder()

                            .setSize(768)
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()

                    .setParams(

                        VectorParams.newBuilder()

                            .setSize(768)",0.291005291005291,0.9016393442622951,0.43999999999999995,0.19148936170212766,0.6,0.29032258064516125,0.2222222222222222,0.6885245901639344,0.33599999999999997,0.2698412698412698,0.8360655737704918,0.40800000000000003,0.9642991423606873,0.6295756101608276,0.7514517307281494,0.6851359009742737,0.8677071161671646,0.9901619399800742,197,3.94,0.7211102442130934
True,,3147.652,8649a60d,What are the two ways of creating batches supported by the Qdrant API and how do they differ internally?,"The Qdrant API supports two ways of creating batches - record-oriented and column-oriented. Internally, these options do not differ and are made only for the convenience of interaction. This means that both record-oriented and column-oriented batch creation methods achieve the same result in terms of processing and storing data within the Qdrant system.","The two ways of creating batches supported by the Qdrant API are record-oriented and column-oriented. These options do not differ internally and are provided for the convenience of interaction. Record-oriented batch creation involves uploading points in a batch using the `upsert` method with a `Batch` object. On the other hand, column-oriented batch creation involves uploading points batch by batch into Qdrant using a loop and the `upsert` method with a `Batch` object containing ids, vectors, and payloads. Both methods allow for efficient loading of multiple points into Qdrant in one API call, helping to minimize network overhead and improve performance when working with large datasets.","}

	}

);



```



are both possible.



## Upload points



To optimize performance, Qdrant supports batch loading of points. I.e., you can load several points into the service in one API call.

Batching allows you to minimize the overhead of creating a network connection.



The Qdrant API supports two ways of creating batches - record-oriented and column-oriented.

Internally, these options do not differ and are made only for the convenience of interaction.



Create points with batch:



```http
from qdrant_client.http.models import Batch



cohere_client = cohere.Client(""<< your_api_key >>"")

qdrant_client = qdrant_client.QdrantClient()

qdrant_client.upsert(

    collection_name=""MyCollection"",

    points=Batch(

        ids=[1],

        vectors=cohere_client.embed(

            model=""large"",

            texts=[""The best vector database""],

        ).embeddings,

    ),

)

```
batch by batch, into Qdrant:



```python

batch_size = 100



for batch in batched(dataset, batch_size):

    ids = [point.pop(""id"") for point in batch]

    vectors = [point.pop(""vector"") for point in batch]



    client.upsert(

        collection_name=""arxiv-titles-instructorxl-embeddings"",

        points=models.Batch(

            ids=ids,

            vectors=vectors,

            payloads=batch,

        ),

    )

```
to create in a second:



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""http://localhost:6333"")



client.create_collection(

    collection_name=""arxiv-titles-instructorxl-embeddings"",

    vectors_config=models.VectorParams(

        size=768,

        distance=models.Distance.COSINE,

    ),

)

```



It is always a good idea to use batching, while loading a large dataset, so let's do that.

We are going to need a helper function to split the dataset into batches:
## Summary



Batch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!","['}\n\n\t}\n\n);\n\n\n\n```\n\n\n\nare both possible.\n\n\n\n## Upload points\n\n\n\nTo optimize performance, Qdrant supports batch loading of points. I.e., you can load several points into the service in one API call.\n\nBatching allows you to minimize the overhead of creating a network connection.\n\n\n\nThe Qdrant API supports two ways of creating batches - record-oriented and column-oriented.\n\nInternally, these options do not differ and are made only for the convenience of interaction.\n\n\n\nCreate points with batch:\n\n\n\n```http', 'from qdrant_client.http.models import Batch\n\n\n\ncohere_client = cohere.Client(""<< your_api_key >>"")\n\nqdrant_client = qdrant_client.QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""MyCollection"",\n\n    points=Batch(\n\n        ids=[1],\n\n        vectors=cohere_client.embed(\n\n            model=""large"",\n\n            texts=[""The best vector database""],\n\n        ).embeddings,\n\n    ),\n\n)\n\n```', 'batch by batch, into Qdrant:\n\n\n\n```python\n\nbatch_size = 100\n\n\n\nfor batch in batched(dataset, batch_size):\n\n    ids = [point.pop(""id"") for point in batch]\n\n    vectors = [point.pop(""vector"") for point in batch]\n\n\n\n    client.upsert(\n\n        collection_name=""arxiv-titles-instructorxl-embeddings"",\n\n        points=models.Batch(\n\n            ids=ids,\n\n            vectors=vectors,\n\n            payloads=batch,\n\n        ),\n\n    )\n\n```', 'to create in a second:\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""http://localhost:6333"")\n\n\n\nclient.create_collection(\n\n    collection_name=""arxiv-titles-instructorxl-embeddings"",\n\n    vectors_config=models.VectorParams(\n\n        size=768,\n\n        distance=models.Distance.COSINE,\n\n    ),\n\n)\n\n```\n\n\n\nIt is always a good idea to use batching, while loading a large dataset, so let\'s do that.\n\nWe are going to need a helper function to split the dataset into batches:', '## Summary\n\n\n\nBatch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!']","Question: What are the two ways of creating batches supported by the Qdrant API and how do they differ internally?\n\nContext: }

}

);



```



are both possible.



## Upload points



To optimize performance, Qdrant supports batch loading of points. I.e., you can load several points into the service in one API call.

Batching allows you to minimize the overhead of creating a network connection.



The Qdrant API supports two ways of creating batches - record-oriented and column-oriented.

Internally, these options do not differ and are made only for the convenience of interaction.



Create points with batch:



```http
from qdrant_client.http.models import Batch



cohere_client = cohere.Client(""<< your_api_key >>"")

qdrant_client = qdrant_client.QdrantClient()

qdrant_client.upsert(

collection_name=""MyCollection"",

points=Batch(

ids=[1],

vectors=cohere_client.embed(

model=""large"",

texts=[""The best vector database""],

).embeddings,

),

)

```
batch by batch, into Qdrant:



```python

batch_size = 100



for batch in batched(dataset, batch_size):

ids = [point.pop(""id"") for point in batch]

vectors = [point.pop(""vector"") for point in batch]



client.upsert(

collection_name=""arxiv-titles-instructorxl-embeddings"",

points=models.Batch(

ids=ids,

vectors=vectors,

payloads=batch,

),

)

```
to create in a second:



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""http://localhost:6333"")



client.create_collection(

collection_name=""arxiv-titles-instructorxl-embeddings"",

vectors_config=models.VectorParams(

size=768,

distance=models.Distance.COSINE,

),

)

```



It is always a good idea to use batching, while loading a large dataset, so let's do that.

We are going to need a helper function to split the dataset into batches:
## Summary



Batch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!\n\nAnswer:",What are the two ways of creating batches supported by the Qdrant API and how do they differ internally?,"The Qdrant API supports two ways of creating batches - record-oriented and column-oriented. Internally, these options do not differ and are made only for the convenience of interaction. This means that both record-oriented and column-oriented batch creation methods achieve the same result in terms of processing and storing data within the Qdrant system.","['}\n\n\t}\n\n);\n\n\n\n```\n\n\n\nare both possible.\n\n\n\n## Upload points\n\n\n\nTo optimize performance, Qdrant supports batch loading of points. I.e., you can load several points into the service in one API call.\n\nBatching allows you to minimize the overhead of creating a network connection.\n\n\n\nThe Qdrant API supports two ways of creating batches - record-oriented and column-oriented.\n\nInternally, these options do not differ and are made only for the convenience of interaction.\n\n\n\nCreate points with batch:\n\n\n\n```http', 'from qdrant_client.http.models import Batch\n\n\n\ncohere_client = cohere.Client(""<< your_api_key >>"")\n\nqdrant_client = qdrant_client.QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""MyCollection"",\n\n    points=Batch(\n\n        ids=[1],\n\n        vectors=cohere_client.embed(\n\n            model=""large"",\n\n            texts=[""The best vector database""],\n\n        ).embeddings,\n\n    ),\n\n)\n\n```', 'batch by batch, into Qdrant:\n\n\n\n```python\n\nbatch_size = 100\n\n\n\nfor batch in batched(dataset, batch_size):\n\n    ids = [point.pop(""id"") for point in batch]\n\n    vectors = [point.pop(""vector"") for point in batch]\n\n\n\n    client.upsert(\n\n        collection_name=""arxiv-titles-instructorxl-embeddings"",\n\n        points=models.Batch(\n\n            ids=ids,\n\n            vectors=vectors,\n\n            payloads=batch,\n\n        ),\n\n    )\n\n```', 'to create in a second:\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""http://localhost:6333"")\n\n\n\nclient.create_collection(\n\n    collection_name=""arxiv-titles-instructorxl-embeddings"",\n\n    vectors_config=models.VectorParams(\n\n        size=768,\n\n        distance=models.Distance.COSINE,\n\n    ),\n\n)\n\n```\n\n\n\nIt is always a good idea to use batching, while loading a large dataset, so let\'s do that.\n\nWe are going to need a helper function to split the dataset into batches:', '## Summary\n\n\n\nBatch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!']","}

	}

);



```



are both possible.



## Upload points



To optimize performance, Qdrant supports batch loading of points. I.e., you can load several points into the service in one API call.

Batching allows you to minimize the overhead of creating a network connection.



The Qdrant API supports two ways of creating batches - record-oriented and column-oriented.

Internally, these options do not differ and are made only for the convenience of interaction.



Create points with batch:



```http
from qdrant_client.http.models import Batch



cohere_client = cohere.Client(""<< your_api_key >>"")

qdrant_client = qdrant_client.QdrantClient()

qdrant_client.upsert(

    collection_name=""MyCollection"",

    points=Batch(

        ids=[1],

        vectors=cohere_client.embed(

            model=""large"",

            texts=[""The best vector database""],

        ).embeddings,

    ),

)

```
batch by batch, into Qdrant:



```python

batch_size = 100



for batch in batched(dataset, batch_size):

    ids = [point.pop(""id"") for point in batch]

    vectors = [point.pop(""vector"") for point in batch]



    client.upsert(

        collection_name=""arxiv-titles-instructorxl-embeddings"",

        points=models.Batch(

            ids=ids,

            vectors=vectors,

            payloads=batch,

        ),

    )

```
to create in a second:



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""http://localhost:6333"")



client.create_collection(

    collection_name=""arxiv-titles-instructorxl-embeddings"",

    vectors_config=models.VectorParams(

        size=768,

        distance=models.Distance.COSINE,

    ),

)

```



It is always a good idea to use batching, while loading a large dataset, so let's do that.

We are going to need a helper function to split the dataset into batches:
## Summary



Batch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!",0.3838383838383838,0.7307692307692307,0.5033112582781457,0.21428571428571427,0.4117647058823529,0.28187919463087246,0.30303030303030304,0.5769230769230769,0.3973509933774835,0.35353535353535354,0.6730769230769231,0.46357615894039733,0.8668818473815918,0.6444379091262817,0.7281219363212585,0.6837288737297058,0.8308387644588947,0.8876404042045275,105,2.019230769230769,0.5697139839255223
True,,6215.191,d45ccab4,How can you create an index for a specific field in a payload using the Qdrant library?,"To create an index for a specific field in a payload using the Qdrant library, you can utilize the `CreatePayloadIndexAsync` method provided by the client. This method takes in the parameters `collectionName` and `fieldName`. ","To create an index for a specific field in a payload using the Qdrant library, you can follow the code snippets provided. Here is a step-by-step explanation based on the code snippets provided in different programming languages:

1. JavaScript/TypeScript:
```typescript
import { QdrantClient, Schemas } from ""@qdrant/js-client-rest"";
client.create_field_index(
    ""{collection_name}"",
    ""name_of_the_field_to_index"",
    FieldType::Keyword,
    None,
    None,
).await?;
```

2. Rust:
```rust
use qdrant_client::client::QdrantClient;
use qdrant_client::qdrant::{payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams, TokenizerType};

let client = QdrantClient::new(""localhost"", 6333)?;
client.create_payload_index(
    collection_name=""{collection_name}"",
    field_name=""name_of_the_field_to_index"",
    field_schema=models.TextIndexParams(
        type=""text"",
        tokenizer=models.TokenizerType.WORD,
        min_token_len=2,
        max_token_len=15,
        lowercase=True,
    ),
);
```

3. Java:
```java
import io.qdrant.client.QdrantClient;
import io.qdrant.client.QdrantGrpcClient;
import io.qdrant.client.grpc.Collections.PayloadSchemaType;

QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());

client.createPayloadIndexAsync(
    ""{collection_name}"",
    ""name_of_the_field_to_index"",
    PayloadSchemaType.Keyword,
    null,
    null,
    null,
    null
).get();
```

4. C#:
```csharp
using Qdrant.Client;

var client = new QdrantClient(""localhost"", 6334);
await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");
```

These code snippets demonstrate how to create a payload index for a specific field in a collection using the Qdrant library in different programming languages. Make sure to adjust the parameters such as collection name and field name based on your specific use case.","const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createPayloadIndex(""{collection_name}"", {

  field_name: ""name_of_the_field_to_index"",

  field_schema: {

    type: ""text"",

    tokenizer: ""word"",

    min_token_len: 2,

    max_token_len: 15,

    lowercase: true,

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,

        TokenizerType,
client = QdrantClient(host=""localhost"", port=6333)



client.create_payload_index(

    collection_name=""{collection_name}"",

    field_name=""name_of_the_field_to_index"",

    field_schema=models.TextIndexParams(

        type=""text"",

        tokenizer=models.TokenizerType.WORD,

        min_token_len=2,

        max_token_len=15,

        lowercase=True,

    ),

)

```



```typescript

import { QdrantClient, Schemas } from ""@qdrant/js-client-rest"";
client

    .create_field_index(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        FieldType::Keyword,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createPayloadIndexAsync(
client

    .createPayloadIndexAsync(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        PayloadSchemaType.Keyword,

        null,

        null,

        null,

        null)

    .get();

```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");

```



Available field types are:
field_schema=""keyword"",

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createPayloadIndex(""{collection_name}"", {

  field_name: ""name_of_the_field_to_index"",

  field_schema: ""keyword"",

});

```



```rust

use qdrant_client::{client::QdrantClient, qdrant::FieldType};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_field_index(","['const client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createPayloadIndex(""{collection_name}"", {\n\n  field_name: ""name_of_the_field_to_index"",\n\n  field_schema: {\n\n    type: ""text"",\n\n    tokenizer: ""word"",\n\n    min_token_len: 2,\n\n    max_token_len: 15,\n\n    lowercase: true,\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\n\n        TokenizerType,', 'client = QdrantClient(host=""localhost"", port=6333)\n\n\n\nclient.create_payload_index(\n\n    collection_name=""{collection_name}"",\n\n    field_name=""name_of_the_field_to_index"",\n\n    field_schema=models.TextIndexParams(\n\n        type=""text"",\n\n        tokenizer=models.TokenizerType.WORD,\n\n        min_token_len=2,\n\n        max_token_len=15,\n\n        lowercase=True,\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient, Schemas } from ""@qdrant/js-client-rest"";', 'client\n\n    .create_field_index(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(', 'client\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        PayloadSchemaType.Keyword,\n\n        null,\n\n        null,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");\n\n```\n\n\n\nAvailable field types are:', 'field_schema=""keyword"",\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createPayloadIndex(""{collection_name}"", {\n\n  field_name: ""name_of_the_field_to_index"",\n\n  field_schema: ""keyword"",\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{client::QdrantClient, qdrant::FieldType};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_field_index(']","Question: How can you create an index for a specific field in a payload using the Qdrant library?\n\nContext: const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createPayloadIndex(""{collection_name}"", {

field_name: ""name_of_the_field_to_index"",

field_schema: {

type: ""text"",

tokenizer: ""word"",

min_token_len: 2,

max_token_len: 15,

lowercase: true,

},

});

```



```rust

use qdrant_client::{

client::QdrantClient,

qdrant::{

payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,

TokenizerType,
client = QdrantClient(host=""localhost"", port=6333)



client.create_payload_index(

collection_name=""{collection_name}"",

field_name=""name_of_the_field_to_index"",

field_schema=models.TextIndexParams(

type=""text"",

tokenizer=models.TokenizerType.WORD,

min_token_len=2,

max_token_len=15,

lowercase=True,

),

)

```



```typescript

import { QdrantClient, Schemas } from ""@qdrant/js-client-rest"";
client

.create_field_index(

""{collection_name}"",

""name_of_the_field_to_index"",

FieldType::Keyword,

None,

None,

)

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createPayloadIndexAsync(
client

.createPayloadIndexAsync(

""{collection_name}"",

""name_of_the_field_to_index"",

PayloadSchemaType.Keyword,

null,

null,

null,

null)

.get();

```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");

```



Available field types are:
field_schema=""keyword"",

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createPayloadIndex(""{collection_name}"", {

field_name: ""name_of_the_field_to_index"",

field_schema: ""keyword"",

});

```



```rust

use qdrant_client::{client::QdrantClient, qdrant::FieldType};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

.create_field_index(\n\nAnswer:",How can you create an index for a specific field in a payload using the Qdrant library?,"To create an index for a specific field in a payload using the Qdrant library, you can utilize the `CreatePayloadIndexAsync` method provided by the client. This method takes in the parameters `collectionName` and `fieldName`. ","['const client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createPayloadIndex(""{collection_name}"", {\n\n  field_name: ""name_of_the_field_to_index"",\n\n  field_schema: {\n\n    type: ""text"",\n\n    tokenizer: ""word"",\n\n    min_token_len: 2,\n\n    max_token_len: 15,\n\n    lowercase: true,\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\n\n        TokenizerType,', 'client = QdrantClient(host=""localhost"", port=6333)\n\n\n\nclient.create_payload_index(\n\n    collection_name=""{collection_name}"",\n\n    field_name=""name_of_the_field_to_index"",\n\n    field_schema=models.TextIndexParams(\n\n        type=""text"",\n\n        tokenizer=models.TokenizerType.WORD,\n\n        min_token_len=2,\n\n        max_token_len=15,\n\n        lowercase=True,\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient, Schemas } from ""@qdrant/js-client-rest"";', 'client\n\n    .create_field_index(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(', 'client\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        PayloadSchemaType.Keyword,\n\n        null,\n\n        null,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");\n\n```\n\n\n\nAvailable field types are:', 'field_schema=""keyword"",\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createPayloadIndex(""{collection_name}"", {\n\n  field_name: ""name_of_the_field_to_index"",\n\n  field_schema: ""keyword"",\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{client::QdrantClient, qdrant::FieldType};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_field_index(']","const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createPayloadIndex(""{collection_name}"", {

  field_name: ""name_of_the_field_to_index"",

  field_schema: {

    type: ""text"",

    tokenizer: ""word"",

    min_token_len: 2,

    max_token_len: 15,

    lowercase: true,

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,

        TokenizerType,
client = QdrantClient(host=""localhost"", port=6333)



client.create_payload_index(

    collection_name=""{collection_name}"",

    field_name=""name_of_the_field_to_index"",

    field_schema=models.TextIndexParams(

        type=""text"",

        tokenizer=models.TokenizerType.WORD,

        min_token_len=2,

        max_token_len=15,

        lowercase=True,

    ),

)

```



```typescript

import { QdrantClient, Schemas } from ""@qdrant/js-client-rest"";
client

    .create_field_index(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        FieldType::Keyword,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createPayloadIndexAsync(
client

    .createPayloadIndexAsync(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        PayloadSchemaType.Keyword,

        null,

        null,

        null,

        null)

    .get();

```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");

```



Available field types are:
field_schema=""keyword"",

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createPayloadIndex(""{collection_name}"", {

  field_name: ""name_of_the_field_to_index"",

  field_schema: ""keyword"",

});

```



```rust

use qdrant_client::{client::QdrantClient, qdrant::FieldType};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_field_index(",0.09401709401709402,0.8148148148148148,0.1685823754789272,0.05150214592274678,0.46153846153846156,0.09266409266409266,0.0811965811965812,0.7037037037037037,0.14559386973180077,0.08547008547008547,0.7407407407407407,0.15325670498084293,0.8847013115882874,0.4679951071739197,0.6371955275535583,0.5396433472633362,0.7749277591705322,0.9505727142095566,175,5.147058823529412,0.4884398868232858
True,,3277.3979999999997,8383bea0,What is the purpose of the Quantization?,"Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces. In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.","The purpose of quantization, specifically binary quantization as explained in the context provided, is to convert floating-point elements of vectors into binary representation (0 or 1) based on whether the original element is positive or negative. This allows for approximating dot products or cosine similarity with just Hamming distance, a simple computation that uses only two highly optimized CPU instructions. The choice of quantization is often influenced by the available CPU instructions that can be applied efficiently for the computations. 

Binary quantization simplifies computations and enables faster search times by reducing the complexity of operations involved in comparing vectors. It is favored for its simplicity and efficiency compared to other forms of quantization like four-bit quantization. Graph optimizations are also employed in the process without significant dropping of bits, such as reducing from 32 to 16 or 64 to 32 bits only where necessary.","Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.

In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.



There are tradeoffs associated with quantization.

On the one hand, quantization allows for significant reductions in storage requirements and faster search times.
Demetrios:

I see, okay, so basically this is the quantization. And now let's dive into the binary quantization and how it works.



Andrey Vasnetsov:
Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever
Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear
. We also do some graph optimizations and we don't really do a lot of dropping the bits, which is let's say 32 to 16 or 64 to 32 kind of quantization only where required.","['Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\n\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\n\n\n\nThere are tradeoffs associated with quantization.\n\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.', ""Demetrios:\n\nI see, okay, so basically this is the quantization. And now let's dive into the binary quantization and how it works.\n\n\n\nAndrey Vasnetsov:"", ""Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever"", ""Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear"", "". We also do some graph optimizations and we don't really do a lot of dropping the bits, which is let's say 32 to 16 or 64 to 32 kind of quantization only where required.""]","Question: What is the purpose of the Quantization?\n\nContext: Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.

In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.



There are tradeoffs associated with quantization.

On the one hand, quantization allows for significant reductions in storage requirements and faster search times.
Demetrios:

I see, okay, so basically this is the quantization. And now let's dive into the binary quantization and how it works.



Andrey Vasnetsov:
Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever
Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear
. We also do some graph optimizations and we don't really do a lot of dropping the bits, which is let's say 32 to 16 or 64 to 32 kind of quantization only where required.\n\nAnswer:",What is the purpose of the Quantization?,"Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces. In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.","['Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\n\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\n\n\n\nThere are tradeoffs associated with quantization.\n\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.', ""Demetrios:\n\nI see, okay, so basically this is the quantization. And now let's dive into the binary quantization and how it works.\n\n\n\nAndrey Vasnetsov:"", ""Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever"", ""Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear"", "". We also do some graph optimizations and we don't really do a lot of dropping the bits, which is let's say 32 to 16 or 64 to 32 kind of quantization only where required.""]","Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.

In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.



There are tradeoffs associated with quantization.

On the one hand, quantization allows for significant reductions in storage requirements and faster search times.
Demetrios:

I see, okay, so basically this is the quantization. And now let's dive into the binary quantization and how it works.



Andrey Vasnetsov:
Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever
Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear
. We also do some graph optimizations and we don't really do a lot of dropping the bits, which is let's say 32 to 16 or 64 to 32 kind of quantization only where required.",0.15328467153284672,0.5121951219512195,0.2359550561797753,0.014705882352941176,0.05,0.022727272727272724,0.072992700729927,0.24390243902439024,0.11235955056179774,0.11678832116788321,0.3902439024390244,0.17977528089887643,0.7459934949874878,0.555540144443512,0.6129397749900818,0.5828301310539246,0.9992016396330049,0.993038218235597,144,3.130434782608696,0.5840568247865143
True,,5175.691,c4601c8c,How can the retrieval quality of an approximation be measured in the context of semantic search?,"The retrieval quality of an approximation in semantic search can be measured using various quality metrics. Some of these metrics include Precision@k, Mean Reciprocal Rank (MRR), and DCG and NDCG.","In the context of semantic search, the retrieval quality of an approximation can be measured using various metrics. Some common quality metrics include:

1. **Precision@k**: This metric measures the proportion of relevant documents in the top-k search results. A higher Precision@k value indicates better retrieval quality as more relevant documents are retrieved in the top results.

2. **Mean Reciprocal Rank (MRR)**: MRR calculates the average of the reciprocal ranks of the first relevant document retrieved. A higher MRR score signifies better retrieval quality as relevant documents are ranked higher on average.

3. **Mean Average Precision (MAP)**: MAP calculates the average precision at each relevant document in the result set. It provides a single measure of the overall retrieval quality by considering precision at various recall levels.

4. **Normalized Discounted Cumulative Gain (NDCG)**: NDCG considers both the relevance and rank of retrieved documents. It measures the effectiveness of the ranking of relevant documents by assigning higher scores to relevant documents ranked higher in the result list.

By utilizing these quality metrics and comparing the retrieval results with ground truth data, one can effectively measure the retrieval quality of an approximation in semantic search systems.","to do it. 



## Wrapping up



Assessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.

your search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, 

even in an automated way, as part of your CI/CD pipeline.
perform pure kNN search. Instead, they use **Approximate Nearest Neighbors** (ANN) algorithms, which are much faster than the exact search, 

but can return suboptimal results. We can also **measure the retrieval quality of that approximation** which also contributes to the overall

search quality.



### Quality metrics
them with the ground truth. In that setup, **finding the most similar documents is implemented as full kNN search, without any approximation**.

As a result, we can measure the quality of the embeddings themselves, without the influence of the ANN algorithm.



## Retrieval quality



Embeddings quality is indeed the most important factor in the semantic search quality. However, vector search engines, such as Qdrant, do not
---

title: Measure retrieval quality

weight: 21

---



# Measure retrieval quality



| Time: 30 min | Level: Intermediate |  |    |

|--------------|---------------------|--|----|



Semantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might

be far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another
search quality.



### Quality metrics



There are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), 

are based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),","['to do it. \n\n\n\n## Wrapping up\n\n\n\nAssessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.\n\nyour search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, \n\neven in an automated way, as part of your CI/CD pipeline.', 'perform pure kNN search. Instead, they use **Approximate Nearest Neighbors** (ANN) algorithms, which are much faster than the exact search, \n\nbut can return suboptimal results. We can also **measure the retrieval quality of that approximation** which also contributes to the overall\n\nsearch quality.\n\n\n\n### Quality metrics', 'them with the ground truth. In that setup, **finding the most similar documents is implemented as full kNN search, without any approximation**.\n\nAs a result, we can measure the quality of the embeddings themselves, without the influence of the ANN algorithm.\n\n\n\n## Retrieval quality\n\n\n\nEmbeddings quality is indeed the most important factor in the semantic search quality. However, vector search engines, such as Qdrant, do not', '---\n\ntitle: Measure retrieval quality\n\nweight: 21\n\n---\n\n\n\n# Measure retrieval quality\n\n\n\n| Time: 30 min | Level: Intermediate |  |    |\n\n|--------------|---------------------|--|----|\n\n\n\nSemantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might\n\nbe far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another', 'search quality.\n\n\n\n### Quality metrics\n\n\n\nThere are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), \n\nare based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),']","Question: How can the retrieval quality of an approximation be measured in the context of semantic search?\n\nContext: to do it. 



## Wrapping up



Assessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.

your search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, 

even in an automated way, as part of your CI/CD pipeline.
perform pure kNN search. Instead, they use **Approximate Nearest Neighbors** (ANN) algorithms, which are much faster than the exact search, 

but can return suboptimal results. We can also **measure the retrieval quality of that approximation** which also contributes to the overall

search quality.



### Quality metrics
them with the ground truth. In that setup, **finding the most similar documents is implemented as full kNN search, without any approximation**.

As a result, we can measure the quality of the embeddings themselves, without the influence of the ANN algorithm.



## Retrieval quality



Embeddings quality is indeed the most important factor in the semantic search quality. However, vector search engines, such as Qdrant, do not
---

title: Measure retrieval quality

weight: 21

---



# Measure retrieval quality



| Time: 30 min | Level: Intermediate |  |    |

|--------------|---------------------|--|----|



Semantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might

be far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another
search quality.



### Quality metrics



There are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), 

are based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),\n\nAnswer:",How can the retrieval quality of an approximation be measured in the context of semantic search?,"The retrieval quality of an approximation in semantic search can be measured using various quality metrics. Some of these metrics include Precision@k, Mean Reciprocal Rank (MRR), and DCG and NDCG.","['to do it. \n\n\n\n## Wrapping up\n\n\n\nAssessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.\n\nyour search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, \n\neven in an automated way, as part of your CI/CD pipeline.', 'perform pure kNN search. Instead, they use **Approximate Nearest Neighbors** (ANN) algorithms, which are much faster than the exact search, \n\nbut can return suboptimal results. We can also **measure the retrieval quality of that approximation** which also contributes to the overall\n\nsearch quality.\n\n\n\n### Quality metrics', 'them with the ground truth. In that setup, **finding the most similar documents is implemented as full kNN search, without any approximation**.\n\nAs a result, we can measure the quality of the embeddings themselves, without the influence of the ANN algorithm.\n\n\n\n## Retrieval quality\n\n\n\nEmbeddings quality is indeed the most important factor in the semantic search quality. However, vector search engines, such as Qdrant, do not', '---\n\ntitle: Measure retrieval quality\n\nweight: 21\n\n---\n\n\n\n# Measure retrieval quality\n\n\n\n| Time: 30 min | Level: Intermediate |  |    |\n\n|--------------|---------------------|--|----|\n\n\n\nSemantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might\n\nbe far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another', 'search quality.\n\n\n\n### Quality metrics\n\n\n\nThere are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), \n\nare based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),']","to do it. 



## Wrapping up



Assessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.

your search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, 

even in an automated way, as part of your CI/CD pipeline.
perform pure kNN search. Instead, they use **Approximate Nearest Neighbors** (ANN) algorithms, which are much faster than the exact search, 

but can return suboptimal results. We can also **measure the retrieval quality of that approximation** which also contributes to the overall

search quality.



### Quality metrics
them with the ground truth. In that setup, **finding the most similar documents is implemented as full kNN search, without any approximation**.

As a result, we can measure the quality of the embeddings themselves, without the influence of the ANN algorithm.



## Retrieval quality



Embeddings quality is indeed the most important factor in the semantic search quality. However, vector search engines, such as Qdrant, do not
---

title: Measure retrieval quality

weight: 21

---



# Measure retrieval quality



| Time: 30 min | Level: Intermediate |  |    |

|--------------|---------------------|--|----|



Semantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might

be far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another
search quality.



### Quality metrics



There are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), 

are based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),",0.16091954022988506,0.9655172413793104,0.27586206896551724,0.09826589595375723,0.6071428571428571,0.1691542288557214,0.1206896551724138,0.7241379310344828,0.20689655172413793,0.14942528735632185,0.896551724137931,0.2561576354679803,0.9002655744552612,0.5391460657119751,0.6725324988365173,0.5984974503517151,0.5968290295817756,0.8517300420789979,195,6.290322580645161,0.5588481235671164
True,,1837.961,28d4c169,Why does Qdrant deliberately exclude libraries or algorithm implementations in their benchmark comparisons?,"Qdrant excludes libraries or algorithm implementations in their benchmark comparisons because their primary focus is on vector databases. By limiting their comparisons to open-source solutions and avoiding external cloud components, Qdrant ensures hardware parity and minimizes biases. This allows them to provide accurate and unbiased benchmarks specifically tailored to the performance of vector databases, enabling users to make informed decisions based on the data provided.","Qdrant deliberately excludes libraries or algorithm implementations in their benchmark comparisons because their focus is specifically on vector databases. By solely testing open-source vector databases, they aim to provide fair and reproducible benchmarks for users. Excluding other libraries or algorithms helps in maintaining a level playing field for comparison purposes. The source code for the benchmarks is available on Github, allowing anyone to run them on their own hardware and contribute to the process. By sticking to open-source solutions and avoiding biases from external components, Qdrant ensures transparency and accuracy in their benchmarking efforts.","We deliberately don't include libraries or algorithm implementations in our comparisons because our focus is squarely on vector databases. 



Why?
That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.



This is not the final list, and we’ll continue benchmarking as many different engines as possible.



## How to reproduce the benchmark?



The source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.
Qdrant avoids all these problems and also benefits from the speed boost, as it implements an advanced [query planning strategy](/documentation/search/#query-planning).



<aside role=""status"">The Filtering Benchmark is all about changes in performance between filter and un-filtered queries. Please refer to the search benchmark for absolute speed comparison.</aside>
### What Hasn't Changed?



#### Our Principles of Benchmarking



At Qdrant all code stays open-source. We ensure our benchmarks are accessible for everyone, allowing you to run them on your own hardware. Your input matters to us, and contributions and sharing of best practices are welcome!





Our benchmarks are strictly limited to open-source solutions, ensuring hardware parity and avoiding biases from external cloud components.
. Then the benchmarks that I specified were only against PG vector, and we did not benchmark it any further because the speed and the accuracy that Qdrant provides, I think it is already covering our use case and it is way more faster than we thought the solution could be. So right now we did not benchmark against any other vector database or any other solution.","[""We deliberately don't include libraries or algorithm implementations in our comparisons because our focus is squarely on vector databases. \n\n\n\nWhy?"", 'That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.\n\n\n\nThis is not the final list, and we’ll continue benchmarking as many different engines as possible.\n\n\n\n## How to reproduce the benchmark?\n\n\n\nThe source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.', 'Qdrant avoids all these problems and also benefits from the speed boost, as it implements an advanced [query planning strategy](/documentation/search/#query-planning).\n\n\n\n<aside role=""status"">The Filtering Benchmark is all about changes in performance between filter and un-filtered queries. Please refer to the search benchmark for absolute speed comparison.</aside>', ""### What Hasn't Changed?\n\n\n\n#### Our Principles of Benchmarking\n\n\n\nAt Qdrant all code stays open-source. We ensure our benchmarks are accessible for everyone, allowing you to run them on your own hardware. Your input matters to us, and contributions and sharing of best practices are welcome!\n\n\n\n\n\nOur benchmarks are strictly limited to open-source solutions, ensuring hardware parity and avoiding biases from external cloud components."", '. Then the benchmarks that I specified were only against PG vector, and we did not benchmark it any further because the speed and the accuracy that Qdrant provides, I think it is already covering our use case and it is way more faster than we thought the solution could be. So right now we did not benchmark against any other vector database or any other solution.']","Question: Why does Qdrant deliberately exclude libraries or algorithm implementations in their benchmark comparisons?\n\nContext: We deliberately don't include libraries or algorithm implementations in our comparisons because our focus is squarely on vector databases. 



Why?
That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.



This is not the final list, and we’ll continue benchmarking as many different engines as possible.



## How to reproduce the benchmark?



The source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.
Qdrant avoids all these problems and also benefits from the speed boost, as it implements an advanced [query planning strategy](/documentation/search/#query-planning).



<aside role=""status"">The Filtering Benchmark is all about changes in performance between filter and un-filtered queries. Please refer to the search benchmark for absolute speed comparison.</aside>
### What Hasn't Changed?



#### Our Principles of Benchmarking



At Qdrant all code stays open-source. We ensure our benchmarks are accessible for everyone, allowing you to run them on your own hardware. Your input matters to us, and contributions and sharing of best practices are welcome!





Our benchmarks are strictly limited to open-source solutions, ensuring hardware parity and avoiding biases from external cloud components.
. Then the benchmarks that I specified were only against PG vector, and we did not benchmark it any further because the speed and the accuracy that Qdrant provides, I think it is already covering our use case and it is way more faster than we thought the solution could be. So right now we did not benchmark against any other vector database or any other solution.\n\nAnswer:",Why does Qdrant deliberately exclude libraries or algorithm implementations in their benchmark comparisons?,"Qdrant excludes libraries or algorithm implementations in their benchmark comparisons because their primary focus is on vector databases. By limiting their comparisons to open-source solutions and avoiding external cloud components, Qdrant ensures hardware parity and minimizes biases. This allows them to provide accurate and unbiased benchmarks specifically tailored to the performance of vector databases, enabling users to make informed decisions based on the data provided.","[""We deliberately don't include libraries or algorithm implementations in our comparisons because our focus is squarely on vector databases. \n\n\n\nWhy?"", 'That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.\n\n\n\nThis is not the final list, and we’ll continue benchmarking as many different engines as possible.\n\n\n\n## How to reproduce the benchmark?\n\n\n\nThe source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.', 'Qdrant avoids all these problems and also benefits from the speed boost, as it implements an advanced [query planning strategy](/documentation/search/#query-planning).\n\n\n\n<aside role=""status"">The Filtering Benchmark is all about changes in performance between filter and un-filtered queries. Please refer to the search benchmark for absolute speed comparison.</aside>', ""### What Hasn't Changed?\n\n\n\n#### Our Principles of Benchmarking\n\n\n\nAt Qdrant all code stays open-source. We ensure our benchmarks are accessible for everyone, allowing you to run them on your own hardware. Your input matters to us, and contributions and sharing of best practices are welcome!\n\n\n\n\n\nOur benchmarks are strictly limited to open-source solutions, ensuring hardware parity and avoiding biases from external cloud components."", '. Then the benchmarks that I specified were only against PG vector, and we did not benchmark it any further because the speed and the accuracy that Qdrant provides, I think it is already covering our use case and it is way more faster than we thought the solution could be. So right now we did not benchmark against any other vector database or any other solution.']","We deliberately don't include libraries or algorithm implementations in our comparisons because our focus is squarely on vector databases. 



Why?
That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.



This is not the final list, and we’ll continue benchmarking as many different engines as possible.



## How to reproduce the benchmark?



The source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.
Qdrant avoids all these problems and also benefits from the speed boost, as it implements an advanced [query planning strategy](/documentation/search/#query-planning).



<aside role=""status"">The Filtering Benchmark is all about changes in performance between filter and un-filtered queries. Please refer to the search benchmark for absolute speed comparison.</aside>
### What Hasn't Changed?



#### Our Principles of Benchmarking



At Qdrant all code stays open-source. We ensure our benchmarks are accessible for everyone, allowing you to run them on your own hardware. Your input matters to us, and contributions and sharing of best practices are welcome!





Our benchmarks are strictly limited to open-source solutions, ensuring hardware parity and avoiding biases from external cloud components.
. Then the benchmarks that I specified were only against PG vector, and we did not benchmark it any further because the speed and the accuracy that Qdrant provides, I think it is already covering our use case and it is way more faster than we thought the solution could be. So right now we did not benchmark against any other vector database or any other solution.",0.5,0.71875,0.5897435897435898,0.25274725274725274,0.36507936507936506,0.2987012987012987,0.33695652173913043,0.484375,0.39743589743589747,0.4673913043478261,0.671875,0.5512820512820513,0.9665893316268921,0.7087651491165161,0.7514031529426575,0.7294616103172302,0.9989776786707807,0.9970945646152055,94,1.4461538461538461,0.5190060951567853
True,,5096.116,0c31b932,What is the primary purpose of a Vector Database and why would someone choose to use it over traditional databases?,"A Vector Database is a specialized database system that is specifically designed for efficiently indexing, querying, and retrieving high-dimensional vector data. The primary purpose of a Vector Database is to enable advanced data analysis and similarity-search operations that go beyond the capabilities of traditional, structured query approaches used in conventional databases.","The primary purpose of a Vector Database is to efficiently index, query, and retrieve high-dimensional vector data, enabling advanced data analysis and similarity-search operations beyond what traditional databases can offer. 

Someone might choose to use a Vector Database over traditional databases for several reasons, including:

1. **Efficient storage and indexing of high-dimensional data:** Vector databases are designed to handle data in high-dimensional spaces efficiently, making them suitable for applications with complex data structures.

2. **Ability to handle large-scale datasets:** Vector databases can manage datasets with billions of data points, providing scalability for applications that require processing large amounts of data.

3. **Support for real-time analytics and queries:** Vector databases enable real-time analysis and queries, making them suitable for applications that require quick access to insights from data.

4. **Handling vectors derived from complex data types:** Vector databases can work with vectors derived from various data types such as images, videos, and natural language text, allowing for diverse applications in different domains.

5. **Improved performance and reduced latency in machine learning and AI applications:** Vector databases can enhance the performance and reduce latency in machine learning and AI applications by efficiently handling vector operations.

6. **Reduced development and deployment time and cost:** Using a Vector Database can save time and cost compared to building a custom solution for handling high-dimensional vector data, as these databases are specifically designed for such tasks.

Overall, the specialized capabilities of a Vector Database make it a suitable choice for applications that involve high-dimensional vector data, real-time analytics, large-scale datasets, and complex data types.","aliases: [ /blog/what-is-a-vector-database/ ]

---



> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.





## Why use a Vector Database?



The data flood is real.
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.





### Starting Your First Vector Database Project
4. Support for real-time analytics and queries.

5. Ability to handle vectors derived from complex data types such as images, videos, and natural language text.

6. Improved performance and reduced latency in machine learning and AI applications.

7. Reduced development and deployment time and cost compared to building a custom solution.



Keep in mind that the specific benefits of using a vector database may vary depending on the
Those priorities lead to different architectural decisions that are not reproducible in a general-purpose database, even if it has vector index support.





###### Having a dedicated vector database requires duplication of data.



By their very nature, vector embeddings are derivatives of the primary source data.","['aliases: [ /blog/what-is-a-vector-database/ ]\n\n---\n\n\n\n> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.\n\n\n\n\n\n## Why use a Vector Database?\n\n\n\nThe data flood is real.', 'advantage of their efficient indexing and searching techniques, vector databases enable faster \n\nand more accurate retrieval of unstructured data already represented as vectors, which can \n\nhelp put in front of users the most relevant results to their queries.\n\n\n\nIn addition, other benefits of using vector databases include:\n\n1. Efficient storage and indexing of high-dimensional data.\n\n3. Ability to handle large-scale datasets with billions of data points.\n\n4. Support for real-time analytics and queries.', 'Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.\n\n\n\n\n\n### Starting Your First Vector Database Project', '4. Support for real-time analytics and queries.\n\n5. Ability to handle vectors derived from complex data types such as images, videos, and natural language text.\n\n6. Improved performance and reduced latency in machine learning and AI applications.\n\n7. Reduced development and deployment time and cost compared to building a custom solution.\n\n\n\nKeep in mind that the specific benefits of using a vector database may vary depending on the', 'Those priorities lead to different architectural decisions that are not reproducible in a general-purpose database, even if it has vector index support.\n\n\n\n\n\n###### Having a dedicated vector database requires duplication of data.\n\n\n\nBy their very nature, vector embeddings are derivatives of the primary source data.']","Question: What is the primary purpose of a Vector Database and why would someone choose to use it over traditional databases?\n\nContext: aliases: [ /blog/what-is-a-vector-database/ ]

---



> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.





## Why use a Vector Database?



The data flood is real.
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.





### Starting Your First Vector Database Project
4. Support for real-time analytics and queries.

5. Ability to handle vectors derived from complex data types such as images, videos, and natural language text.

6. Improved performance and reduced latency in machine learning and AI applications.

7. Reduced development and deployment time and cost compared to building a custom solution.



Keep in mind that the specific benefits of using a vector database may vary depending on the
Those priorities lead to different architectural decisions that are not reproducible in a general-purpose database, even if it has vector index support.





###### Having a dedicated vector database requires duplication of data.



By their very nature, vector embeddings are derivatives of the primary source data.\n\nAnswer:",What is the primary purpose of a Vector Database and why would someone choose to use it over traditional databases?,"A Vector Database is a specialized database system that is specifically designed for efficiently indexing, querying, and retrieving high-dimensional vector data. The primary purpose of a Vector Database is to enable advanced data analysis and similarity-search operations that go beyond the capabilities of traditional, structured query approaches used in conventional databases.","['aliases: [ /blog/what-is-a-vector-database/ ]\n\n---\n\n\n\n> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.\n\n\n\n\n\n## Why use a Vector Database?\n\n\n\nThe data flood is real.', 'advantage of their efficient indexing and searching techniques, vector databases enable faster \n\nand more accurate retrieval of unstructured data already represented as vectors, which can \n\nhelp put in front of users the most relevant results to their queries.\n\n\n\nIn addition, other benefits of using vector databases include:\n\n1. Efficient storage and indexing of high-dimensional data.\n\n3. Ability to handle large-scale datasets with billions of data points.\n\n4. Support for real-time analytics and queries.', 'Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.\n\n\n\n\n\n### Starting Your First Vector Database Project', '4. Support for real-time analytics and queries.\n\n5. Ability to handle vectors derived from complex data types such as images, videos, and natural language text.\n\n6. Improved performance and reduced latency in machine learning and AI applications.\n\n7. Reduced development and deployment time and cost compared to building a custom solution.\n\n\n\nKeep in mind that the specific benefits of using a vector database may vary depending on the', 'Those priorities lead to different architectural decisions that are not reproducible in a general-purpose database, even if it has vector index support.\n\n\n\n\n\n###### Having a dedicated vector database requires duplication of data.\n\n\n\nBy their very nature, vector embeddings are derivatives of the primary source data.']","aliases: [ /blog/what-is-a-vector-database/ ]

---



> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.





## Why use a Vector Database?



The data flood is real.
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.





### Starting Your First Vector Database Project
4. Support for real-time analytics and queries.

5. Ability to handle vectors derived from complex data types such as images, videos, and natural language text.

6. Improved performance and reduced latency in machine learning and AI applications.

7. Reduced development and deployment time and cost compared to building a custom solution.



Keep in mind that the specific benefits of using a vector database may vary depending on the
Those priorities lead to different architectural decisions that are not reproducible in a general-purpose database, even if it has vector index support.





###### Having a dedicated vector database requires duplication of data.



By their very nature, vector embeddings are derivatives of the primary source data.",0.16153846153846155,0.875,0.2727272727272727,0.09652509652509653,0.5319148936170213,0.1633986928104575,0.1,0.5416666666666666,0.16883116883116886,0.1423076923076923,0.7708333333333334,0.24025974025974023,0.9300588369369507,0.5741633772850037,0.7226590514183044,0.6399092674255371,0.7727015796699561,0.998056348238606,258,5.0588235294117645,0.402763237342973
True,,3158.962,e63d395e,"How does oversampling impact the performance of machine learning models, especially in the context of imbalanced datasets?","Oversampling is a technique used in machine learning to address imbalances in datasets, where one class significantly outnumbers others. This imbalance can lead to skewed model performance, as the model may favor the majority class at the expense of minority classes. By generating additional samples from the minority classes, oversampling helps to equalize the representation of different classes in the training dataset.","Influence of Search Limit:

The search limit parameter plays a crucial role in the context of oversampling and its impact on machine learning models. By controlling the search limit, you can determine the extent to which oversampling is applied in the training process. A higher search limit allows for more oversampling, which can lead to a better balance in the dataset and potentially improve model performance, especially in the case of imbalanced datasets.

However, it is important to note that the influence of search limit on model performance may vary depending on the specific dataset and the characteristics of the problem being addressed. In some cases, increasing the search limit may not necessarily result in significant improvements in model accuracy, especially if the dataset is already well-balanced or if the model is not sensitive to class imbalances.

Therefore, when experimenting with oversampling and adjusting the search limit parameter, it is essential to monitor the performance metrics of the model closely to determine the optimal balance between oversampling levels and model accuracy. This iterative process of tuning the search limit can help in achieving the best possible performance for machine learning models, particularly when dealing with imbalanced datasets.","acc = average_accuracy.pivot(

        index=""limit"", columns=[""oversampling"", ""rescore""], values=""accuracy""

    )

    print(acc)

```



#### Impact of Oversampling



You can use oversampling in machine learning to counteract imbalances in datasets.

It works well when one class significantly outnumbers others. This imbalance

can skew the performance of models, which favors the majority class at the

expense of others. By creating additional samples from the minority classes,
The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one.
Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects.



![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png)



### Leveraging Binary Quantization: Best Practices
oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
In contrast, for lower dimension models (such as text-embedding-3-small with 512 dimensions), the incremental accuracy gains from increased oversampling levels are less significant, even with rescoring enabled. This suggests a diminishing return on accuracy improvement with higher oversampling in lower dimension spaces.



3. **Influence of Search Limit**:","['acc = average_accuracy.pivot(\n\n        index=""limit"", columns=[""oversampling"", ""rescore""], values=""accuracy""\n\n    )\n\n    print(acc)\n\n```\n\n\n\n#### Impact of Oversampling\n\n\n\nYou can use oversampling in machine learning to counteract imbalances in datasets.\n\nIt works well when one class significantly outnumbers others. This imbalance\n\ncan skew the performance of models, which favors the majority class at the\n\nexpense of others. By creating additional samples from the minority classes,', ""The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one."", 'Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects.\n\n\n\n![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png)\n\n\n\n### Leveraging Binary Quantization: Best Practices', 'oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.', 'In contrast, for lower dimension models (such as text-embedding-3-small with 512 dimensions), the incremental accuracy gains from increased oversampling levels are less significant, even with rescoring enabled. This suggests a diminishing return on accuracy improvement with higher oversampling in lower dimension spaces.\n\n\n\n3. **Influence of Search Limit**:']","Question: How does oversampling impact the performance of machine learning models, especially in the context of imbalanced datasets?\n\nContext: acc = average_accuracy.pivot(

index=""limit"", columns=[""oversampling"", ""rescore""], values=""accuracy""

)

print(acc)

```



#### Impact of Oversampling



You can use oversampling in machine learning to counteract imbalances in datasets.

It works well when one class significantly outnumbers others. This imbalance

can skew the performance of models, which favors the majority class at the

expense of others. By creating additional samples from the minority classes,
The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one.
Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects.



![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png)



### Leveraging Binary Quantization: Best Practices
oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
In contrast, for lower dimension models (such as text-embedding-3-small with 512 dimensions), the incremental accuracy gains from increased oversampling levels are less significant, even with rescoring enabled. This suggests a diminishing return on accuracy improvement with higher oversampling in lower dimension spaces.



3. **Influence of Search Limit**:\n\nAnswer:","How does oversampling impact the performance of machine learning models, especially in the context of imbalanced datasets?","Oversampling is a technique used in machine learning to address imbalances in datasets, where one class significantly outnumbers others. This imbalance can lead to skewed model performance, as the model may favor the majority class at the expense of minority classes. By generating additional samples from the minority classes, oversampling helps to equalize the representation of different classes in the training dataset.","['acc = average_accuracy.pivot(\n\n        index=""limit"", columns=[""oversampling"", ""rescore""], values=""accuracy""\n\n    )\n\n    print(acc)\n\n```\n\n\n\n#### Impact of Oversampling\n\n\n\nYou can use oversampling in machine learning to counteract imbalances in datasets.\n\nIt works well when one class significantly outnumbers others. This imbalance\n\ncan skew the performance of models, which favors the majority class at the\n\nexpense of others. By creating additional samples from the minority classes,', ""The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one."", 'Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects.\n\n\n\n![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png)\n\n\n\n### Leveraging Binary Quantization: Best Practices', 'oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.', 'In contrast, for lower dimension models (such as text-embedding-3-small with 512 dimensions), the incremental accuracy gains from increased oversampling levels are less significant, even with rescoring enabled. This suggests a diminishing return on accuracy improvement with higher oversampling in lower dimension spaces.\n\n\n\n3. **Influence of Search Limit**:']","acc = average_accuracy.pivot(

        index=""limit"", columns=[""oversampling"", ""rescore""], values=""accuracy""

    )

    print(acc)

```



#### Impact of Oversampling



You can use oversampling in machine learning to counteract imbalances in datasets.

It works well when one class significantly outnumbers others. This imbalance

can skew the performance of models, which favors the majority class at the

expense of others. By creating additional samples from the minority classes,
The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one.
Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects.



![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png)



### Leveraging Binary Quantization: Best Practices
oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
In contrast, for lower dimension models (such as text-embedding-3-small with 512 dimensions), the incremental accuracy gains from increased oversampling levels are less significant, even with rescoring enabled. This suggests a diminishing return on accuracy improvement with higher oversampling in lower dimension spaces.



3. **Influence of Search Limit**:",0.16,0.509090909090909,0.2434782608695652,0.040229885057471264,0.12962962962962962,0.061403508771929814,0.10857142857142857,0.34545454545454546,0.16521739130434784,0.15428571428571428,0.4909090909090909,0.23478260869565215,0.6380773782730103,0.547292947769165,0.5955076813697815,0.5703832507133484,0.933696406760386,0.9997340876482971,197,3.1774193548387095,0.6438293083642594
True,,5253.7339999999995,2b1a657a,How does binary quantization work in the context of vector comparisons?,"Binary quantization is a method used in indexing and data compression, particularly by Qdrant, that involves splitting a data point's vector in half at a certain point. This process essentially divides the vector into two parts, marking everything above the split point as ""1"" and everything below as ""0"". The result is a string of bits that represents the original vector in a compressed form. This quantized code is much smaller and easier to compare. Especially for OpenAI embeddings, this type of quantization has proven to achieve a massive performance improvement at a lower cost of accuracy.","Binary quantization in the context of vector comparisons involves converting the floating point elements of a vector into a binary representation, where each element is represented as either zero or one based on its sign (positive or negative). This process allows for the approximation of dot product or cosine similarity between vectors using Hamming distance, which is a simple metric that measures the number of differing bits between two binary vectors. 

By quantizing vectors into binary form, vector comparison can be performed with a few optimized CPU instructions, making it a very fast quantization method. This approach can result in significant speedups, up to 40 times faster than working with the original vectors.

It's important to note that binary quantization is most efficient for high-dimensional vectors and requires a centered distribution of vector components. This means that the vector components should be distributed around zero for optimal performance.

Overall, binary quantization is a powerful technique for reducing storage size and boosting speed in tasks such as proximity graph searching. However, it may not be compatible with all models, and its effectiveness can vary based on the specific characteristics of the data being analyzed.","Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever
This is the fastest quantization method, since it lets you perform a vector comparison with a few CPU instructions.



Binary quantization can achieve up to a **40x** speedup compared to the original vectors.



However, binary quantization is only efficient for high-dimensional vectors and require a centered distribution of vector components. 



At the moment, binary quantization shows good accuracy results with the following models:
Think of each data point as a ruler. Binary quantization splits this ruler in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This [binarization](https://deepai.org/machine-learning-glossary-and-terms/binarization) process results in a string of bits, representing the original vector.







![](/articles_data/what-is-a-vector-database/Binary-Quant.png)
> -- Andrey Vasnetsov



Ever wonder why we need quantization for vector indexes? Andrey Vasnetsov explains the complexities and challenges of searching through proximity graphs. Binary quantization reduces storage size and boosts speed by 30x, but not all models are compatible.
Demetrios:

I see, okay, so basically this is the quantization. And now let's dive into the binary quantization and how it works.



Andrey Vasnetsov:","[""Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever"", 'This is the fastest quantization method, since it lets you perform a vector comparison with a few CPU instructions.\n\n\n\nBinary quantization can achieve up to a **40x** speedup compared to the original vectors.\n\n\n\nHowever, binary quantization is only efficient for high-dimensional vectors and require a centered distribution of vector components. \n\n\n\nAt the moment, binary quantization shows good accuracy results with the following models:', 'Think of each data point as a ruler. Binary quantization splits this ruler in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This [binarization](https://deepai.org/machine-learning-glossary-and-terms/binarization) process results in a string of bits, representing the original vector.\n\n\n\n\n\n\n\n![](/articles_data/what-is-a-vector-database/Binary-Quant.png)', '> -- Andrey Vasnetsov\n\n\n\nEver wonder why we need quantization for vector indexes? Andrey Vasnetsov explains the complexities and challenges of searching through proximity graphs. Binary quantization reduces storage size and boosts speed by 30x, but not all models are compatible.', ""Demetrios:\n\nI see, okay, so basically this is the quantization. And now let's dive into the binary quantization and how it works.\n\n\n\nAndrey Vasnetsov:""]","Question: How does binary quantization work in the context of vector comparisons?\n\nContext: Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever
This is the fastest quantization method, since it lets you perform a vector comparison with a few CPU instructions.



Binary quantization can achieve up to a **40x** speedup compared to the original vectors.



However, binary quantization is only efficient for high-dimensional vectors and require a centered distribution of vector components. 



At the moment, binary quantization shows good accuracy results with the following models:
Think of each data point as a ruler. Binary quantization splits this ruler in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This [binarization](https://deepai.org/machine-learning-glossary-and-terms/binarization) process results in a string of bits, representing the original vector.







![](/articles_data/what-is-a-vector-database/Binary-Quant.png)
> -- Andrey Vasnetsov



Ever wonder why we need quantization for vector indexes? Andrey Vasnetsov explains the complexities and challenges of searching through proximity graphs. Binary quantization reduces storage size and boosts speed by 30x, but not all models are compatible.
Demetrios:

I see, okay, so basically this is the quantization. And now let's dive into the binary quantization and how it works.



Andrey Vasnetsov:\n\nAnswer:",How does binary quantization work in the context of vector comparisons?,"Binary quantization is a method used in indexing and data compression, particularly by Qdrant, that involves splitting a data point's vector in half at a certain point. This process essentially divides the vector into two parts, marking everything above the split point as ""1"" and everything below as ""0"". The result is a string of bits that represents the original vector in a compressed form. This quantized code is much smaller and easier to compare. Especially for OpenAI embeddings, this type of quantization has proven to achieve a massive performance improvement at a lower cost of accuracy.","[""Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever"", 'This is the fastest quantization method, since it lets you perform a vector comparison with a few CPU instructions.\n\n\n\nBinary quantization can achieve up to a **40x** speedup compared to the original vectors.\n\n\n\nHowever, binary quantization is only efficient for high-dimensional vectors and require a centered distribution of vector components. \n\n\n\nAt the moment, binary quantization shows good accuracy results with the following models:', 'Think of each data point as a ruler. Binary quantization splits this ruler in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This [binarization](https://deepai.org/machine-learning-glossary-and-terms/binarization) process results in a string of bits, representing the original vector.\n\n\n\n\n\n\n\n![](/articles_data/what-is-a-vector-database/Binary-Quant.png)', '> -- Andrey Vasnetsov\n\n\n\nEver wonder why we need quantization for vector indexes? Andrey Vasnetsov explains the complexities and challenges of searching through proximity graphs. Binary quantization reduces storage size and boosts speed by 30x, but not all models are compatible.', ""Demetrios:\n\nI see, okay, so basically this is the quantization. And now let's dive into the binary quantization and how it works.\n\n\n\nAndrey Vasnetsov:""]","Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever
This is the fastest quantization method, since it lets you perform a vector comparison with a few CPU instructions.



Binary quantization can achieve up to a **40x** speedup compared to the original vectors.



However, binary quantization is only efficient for high-dimensional vectors and require a centered distribution of vector components. 



At the moment, binary quantization shows good accuracy results with the following models:
Think of each data point as a ruler. Binary quantization splits this ruler in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This [binarization](https://deepai.org/machine-learning-glossary-and-terms/binarization) process results in a string of bits, representing the original vector.







![](/articles_data/what-is-a-vector-database/Binary-Quant.png)
> -- Andrey Vasnetsov



Ever wonder why we need quantization for vector indexes? Andrey Vasnetsov explains the complexities and challenges of searching through proximity graphs. Binary quantization reduces storage size and boosts speed by 30x, but not all models are compatible.
Demetrios:

I see, okay, so basically this is the quantization. And now let's dive into the binary quantization and how it works.



Andrey Vasnetsov:",0.25,0.5172413793103449,0.33707865168539325,0.027932960893854747,0.05813953488372093,0.03773584905660377,0.11666666666666667,0.2413793103448276,0.15730337078651688,0.23333333333333334,0.4827586206896552,0.31460674157303375,0.733643651008606,0.6144776940345764,0.6482250690460205,0.6309004426002502,0.997266956645035,0.9551611753180623,194,1.9795918367346939,0.37148766914104026
True,,2822.175,9767f689,What is the significance of the 'always_ram' parameter in the context of vector quantization in Qdrant?,"In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times.","The 'always_ram' parameter in the context of vector quantization in Qdrant determines whether quantized vectors should always be cached in RAM or not. When set to true, quantized vectors are always kept in RAM, providing faster access but potentially consuming more memory. When set to false, quantized vectors are loaded in the same way as the original vectors, which may be more memory-efficient but could impact search quality.

In summary, the significance of the 'always_ram' parameter lies in its impact on the trade-off between speed and memory consumption in the context of vector quantization in Qdrant.","Using quantiles lower than `1.0` might be useful if there are outliers in your vector components.

This parameter only affects the resulting precision and not the memory footprint.

It might be worth tuning this parameter if you experience a significant decrease in search quality.



`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": false

        }

    }

}

```



```python

from qdrant_client import QdrantClient, models
""size"": 1536,

      ""distance"": ""Cosine""

    },

    ""quantization_config"": {

        ""binary"": {

            ""always_ram"": true

        }

    }

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),
..Default::default()

        }),

        quantization_config: Some(QuantizationConfig {

            quantization: Some(Quantization::Scalar(ScalarQuantization {

                r#type: QuantizationType::Int8.into(),

                always_ram: Some(true),

                ..Default::default()

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;
..Default::default()

        }),

        quantization_config: Some(QuantizationConfig {

            quantization: Some(Quantization::Scalar(ScalarQuantization {

                r#type: QuantizationType::Int8.into(),

                always_ram: Some(true),

                ..Default::default()

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;","['Using quantiles lower than `1.0` might be useful if there are outliers in your vector components.\n\nThis parameter only affects the resulting precision and not the memory footprint.\n\nIt might be worth tuning this parameter if you experience a significant decrease in search quality.\n\n\n\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.', 'This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models', '""size"": 1536,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""quantization_config"": {\n\n        ""binary"": {\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),', '..Default::default()\n\n        }),\n\n        quantization_config: Some(QuantizationConfig {\n\n            quantization: Some(Quantization::Scalar(ScalarQuantization {\n\n                r#type: QuantizationType::Int8.into(),\n\n                always_ram: Some(true),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;', '..Default::default()\n\n        }),\n\n        quantization_config: Some(QuantizationConfig {\n\n            quantization: Some(Quantization::Scalar(ScalarQuantization {\n\n                r#type: QuantizationType::Int8.into(),\n\n                always_ram: Some(true),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;']","Question: What is the significance of the 'always_ram' parameter in the context of vector quantization in Qdrant?\n\nContext: Using quantiles lower than `1.0` might be useful if there are outliers in your vector components.

This parameter only affects the resulting precision and not the memory footprint.

It might be worth tuning this parameter if you experience a significant decrease in search quality.



`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""optimizers_config"": {

""memmap_threshold"": 20000

},

""quantization_config"": {

""scalar"": {

""type"": ""int8"",

""always_ram"": false

}

}

}

```



```python

from qdrant_client import QdrantClient, models
""size"": 1536,

""distance"": ""Cosine""

},

""quantization_config"": {

""binary"": {

""always_ram"": true

}

}

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),
..Default::default()

}),

quantization_config: Some(QuantizationConfig {

quantization: Some(Quantization::Scalar(ScalarQuantization {

r#type: QuantizationType::Int8.into(),

always_ram: Some(true),

..Default::default()

})),

}),

..Default::default()

})

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;
..Default::default()

}),

quantization_config: Some(QuantizationConfig {

quantization: Some(Quantization::Scalar(ScalarQuantization {

r#type: QuantizationType::Int8.into(),

always_ram: Some(true),

..Default::default()

})),

}),

..Default::default()

})

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;\n\nAnswer:",What is the significance of the 'always_ram' parameter in the context of vector quantization in Qdrant?,"In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times.","['Using quantiles lower than `1.0` might be useful if there are outliers in your vector components.\n\nThis parameter only affects the resulting precision and not the memory footprint.\n\nIt might be worth tuning this parameter if you experience a significant decrease in search quality.\n\n\n\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.', 'This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models', '""size"": 1536,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""quantization_config"": {\n\n        ""binary"": {\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),', '..Default::default()\n\n        }),\n\n        quantization_config: Some(QuantizationConfig {\n\n            quantization: Some(Quantization::Scalar(ScalarQuantization {\n\n                r#type: QuantizationType::Int8.into(),\n\n                always_ram: Some(true),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;', '..Default::default()\n\n        }),\n\n        quantization_config: Some(QuantizationConfig {\n\n            quantization: Some(Quantization::Scalar(ScalarQuantization {\n\n                r#type: QuantizationType::Int8.into(),\n\n                always_ram: Some(true),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;']","Using quantiles lower than `1.0` might be useful if there are outliers in your vector components.

This parameter only affects the resulting precision and not the memory footprint.

It might be worth tuning this parameter if you experience a significant decrease in search quality.



`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": false

        }

    }

}

```



```python

from qdrant_client import QdrantClient, models
""size"": 1536,

      ""distance"": ""Cosine""

    },

    ""quantization_config"": {

        ""binary"": {

            ""always_ram"": true

        }

    }

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),
..Default::default()

        }),

        quantization_config: Some(QuantizationConfig {

            quantization: Some(Quantization::Scalar(ScalarQuantization {

                r#type: QuantizationType::Int8.into(),

                always_ram: Some(true),

                ..Default::default()

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;
..Default::default()

        }),

        quantization_config: Some(QuantizationConfig {

            quantization: Some(Quantization::Scalar(ScalarQuantization {

                r#type: QuantizationType::Int8.into(),

                always_ram: Some(true),

                ..Default::default()

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;",0.4891304347826087,0.8490566037735849,0.6206896551724138,0.34065934065934067,0.5961538461538461,0.4335664335664336,0.33695652173913043,0.5849056603773585,0.4275862068965517,0.45652173913043476,0.7924528301886793,0.5793103448275861,0.9430214762687683,0.647248387336731,0.734707236289978,0.6882103681564331,0.5957298576831818,0.7806211449205875,96,1.7142857142857142,0.6876216685804435
True,,3644.5910000000003,b4dd2524,How can automatic backups be set up for clusters using the Cloud UI?,"Automatic backups for clusters can be set up using the Cloud UI by following the procedures listed on the page. These procedures allow you to configure snapshots on a daily, weekly, or monthly basis. You have the flexibility to keep as many snapshots as needed and can restore a cluster from the snapshot of your choice. It is important to note that during the restoration of a snapshot, the affected cluster will not be available.","To set up automatic backups for clusters using the Cloud UI, follow these steps:

1. Sign in to the Qdrant Cloud Dashboard at https://cloud.qdrant.io.

2. Select ""Clusters"" from the dashboard.

3. Choose the cluster that you want to back up.

4. Navigate to the ""Backups"" tab for the selected cluster.

5. Configure a backup schedule by setting the frequency of backups (daily, weekly, monthly) and the number of days of retention for each snapshot.

6. Optionally, you can choose to take an immediate snapshot by selecting ""Backup now"".

7. To restore a backup, go to the list of ""Available Backups"" on the Cloud UI and choose the backup you want to restore or delete.

By following these steps, you can easily set up automatic backups for your clusters using the Qdrant Cloud UI.","- [Create a cluster](/documentation/cloud/create-cluster/)

- Set up [Authentication](/documentation/cloud/authentication/)

- Configure one or more [Collections](/documentation/concepts/collections/)



## Automatic backups



You can set up automatic backups of your clusters with our Cloud UI. With the

procedures listed in this page, you can set up

snapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you

need. You can restore a cluster from the snapshot of your choice.
[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:



1. Sign in to the dashboard

1. Select Clusters.

1. Select the cluster that you want to back up.

   ![Select a cluster](/documentation/cloud/select-cluster.png)

1. Find and select the **Backups** tab.

1. Now you can set up a backup schedule.

   The **Days of Retention** is the number of days after a backup snapshot is

   deleted.

1. Alternatively, you can select **Backup now** to take an immediate snapshot.
recommend replication. It guarantees the proper cluster functionality as long as

at least one replica is running.



For other use-cases such as disaster recovery, you can set up automatic or

self-service backups.



## Prerequisites



You can back up your Qdrant clusters though the Qdrant Cloud

Dashboard at https://cloud.qdrant.io. This section assumes that you've already

set up your cluster, as described in the following sections:



- [Create a cluster](/documentation/cloud/create-cluster/)
![Configure a cluster backup](/documentation/cloud/backup-schedule.png)



### Restore a backup



If you have a backup, it appears in the list of **Available Backups**. You can

choose to restore or delete the backups of your choice.



![Restore or delete a cluster backup](/documentation/cloud/restore-delete.png)



<!-- I think we should move this to the Snapshot page, but I'll do it later -->



## Backups with a snapshot



Qdrant also offers a snapshot API which allows you to create a snapshot
<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>



### Create full storage snapshot



```http

POST /snapshots

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.create_full_snapshot()

```","['- [Create a cluster](/documentation/cloud/create-cluster/)\n\n- Set up [Authentication](/documentation/cloud/authentication/)\n\n- Configure one or more [Collections](/documentation/concepts/collections/)\n\n\n\n## Automatic backups\n\n\n\nYou can set up automatic backups of your clusters with our Cloud UI. With the\n\nprocedures listed in this page, you can set up\n\nsnapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you\n\nneed. You can restore a cluster from the snapshot of your choice.', '[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:\n\n\n\n1. Sign in to the dashboard\n\n1. Select Clusters.\n\n1. Select the cluster that you want to back up.\n\n   ![Select a cluster](/documentation/cloud/select-cluster.png)\n\n1. Find and select the **Backups** tab.\n\n1. Now you can set up a backup schedule.\n\n   The **Days of Retention** is the number of days after a backup snapshot is\n\n   deleted.\n\n1. Alternatively, you can select **Backup now** to take an immediate snapshot.', ""recommend replication. It guarantees the proper cluster functionality as long as\n\nat least one replica is running.\n\n\n\nFor other use-cases such as disaster recovery, you can set up automatic or\n\nself-service backups.\n\n\n\n## Prerequisites\n\n\n\nYou can back up your Qdrant clusters though the Qdrant Cloud\n\nDashboard at https://cloud.qdrant.io. This section assumes that you've already\n\nset up your cluster, as described in the following sections:\n\n\n\n- [Create a cluster](/documentation/cloud/create-cluster/)"", ""![Configure a cluster backup](/documentation/cloud/backup-schedule.png)\n\n\n\n### Restore a backup\n\n\n\nIf you have a backup, it appears in the list of **Available Backups**. You can\n\nchoose to restore or delete the backups of your choice.\n\n\n\n![Restore or delete a cluster backup](/documentation/cloud/restore-delete.png)\n\n\n\n<!-- I think we should move this to the Snapshot page, but I'll do it later -->\n\n\n\n## Backups with a snapshot\n\n\n\nQdrant also offers a snapshot API which allows you to create a snapshot"", '<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>\n\n\n\n### Create full storage snapshot\n\n\n\n```http\n\nPOST /snapshots\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_full_snapshot()\n\n```']","Question: How can automatic backups be set up for clusters using the Cloud UI?\n\nContext: - [Create a cluster](/documentation/cloud/create-cluster/)

- Set up [Authentication](/documentation/cloud/authentication/)

- Configure one or more [Collections](/documentation/concepts/collections/)



## Automatic backups



You can set up automatic backups of your clusters with our Cloud UI. With the

procedures listed in this page, you can set up

snapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you

need. You can restore a cluster from the snapshot of your choice.
[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:



1. Sign in to the dashboard

1. Select Clusters.

1. Select the cluster that you want to back up.

![Select a cluster](/documentation/cloud/select-cluster.png)

1. Find and select the **Backups** tab.

1. Now you can set up a backup schedule.

The **Days of Retention** is the number of days after a backup snapshot is

deleted.

1. Alternatively, you can select **Backup now** to take an immediate snapshot.
recommend replication. It guarantees the proper cluster functionality as long as

at least one replica is running.



For other use-cases such as disaster recovery, you can set up automatic or

self-service backups.



## Prerequisites



You can back up your Qdrant clusters though the Qdrant Cloud

Dashboard at https://cloud.qdrant.io. This section assumes that you've already

set up your cluster, as described in the following sections:



- [Create a cluster](/documentation/cloud/create-cluster/)
![Configure a cluster backup](/documentation/cloud/backup-schedule.png)



### Restore a backup



If you have a backup, it appears in the list of **Available Backups**. You can

choose to restore or delete the backups of your choice.



![Restore or delete a cluster backup](/documentation/cloud/restore-delete.png)



<!-- I think we should move this to the Snapshot page, but I'll do it later -->



## Backups with a snapshot



Qdrant also offers a snapshot API which allows you to create a snapshot
<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>



### Create full storage snapshot



```http

POST /snapshots

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.create_full_snapshot()

```\n\nAnswer:",How can automatic backups be set up for clusters using the Cloud UI?,"Automatic backups for clusters can be set up using the Cloud UI by following the procedures listed on the page. These procedures allow you to configure snapshots on a daily, weekly, or monthly basis. You have the flexibility to keep as many snapshots as needed and can restore a cluster from the snapshot of your choice. It is important to note that during the restoration of a snapshot, the affected cluster will not be available.","['- [Create a cluster](/documentation/cloud/create-cluster/)\n\n- Set up [Authentication](/documentation/cloud/authentication/)\n\n- Configure one or more [Collections](/documentation/concepts/collections/)\n\n\n\n## Automatic backups\n\n\n\nYou can set up automatic backups of your clusters with our Cloud UI. With the\n\nprocedures listed in this page, you can set up\n\nsnapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you\n\nneed. You can restore a cluster from the snapshot of your choice.', '[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:\n\n\n\n1. Sign in to the dashboard\n\n1. Select Clusters.\n\n1. Select the cluster that you want to back up.\n\n   ![Select a cluster](/documentation/cloud/select-cluster.png)\n\n1. Find and select the **Backups** tab.\n\n1. Now you can set up a backup schedule.\n\n   The **Days of Retention** is the number of days after a backup snapshot is\n\n   deleted.\n\n1. Alternatively, you can select **Backup now** to take an immediate snapshot.', ""recommend replication. It guarantees the proper cluster functionality as long as\n\nat least one replica is running.\n\n\n\nFor other use-cases such as disaster recovery, you can set up automatic or\n\nself-service backups.\n\n\n\n## Prerequisites\n\n\n\nYou can back up your Qdrant clusters though the Qdrant Cloud\n\nDashboard at https://cloud.qdrant.io. This section assumes that you've already\n\nset up your cluster, as described in the following sections:\n\n\n\n- [Create a cluster](/documentation/cloud/create-cluster/)"", ""![Configure a cluster backup](/documentation/cloud/backup-schedule.png)\n\n\n\n### Restore a backup\n\n\n\nIf you have a backup, it appears in the list of **Available Backups**. You can\n\nchoose to restore or delete the backups of your choice.\n\n\n\n![Restore or delete a cluster backup](/documentation/cloud/restore-delete.png)\n\n\n\n<!-- I think we should move this to the Snapshot page, but I'll do it later -->\n\n\n\n## Backups with a snapshot\n\n\n\nQdrant also offers a snapshot API which allows you to create a snapshot"", '<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>\n\n\n\n### Create full storage snapshot\n\n\n\n```http\n\nPOST /snapshots\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_full_snapshot()\n\n```']","- [Create a cluster](/documentation/cloud/create-cluster/)

- Set up [Authentication](/documentation/cloud/authentication/)

- Configure one or more [Collections](/documentation/concepts/collections/)



## Automatic backups



You can set up automatic backups of your clusters with our Cloud UI. With the

procedures listed in this page, you can set up

snapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you

need. You can restore a cluster from the snapshot of your choice.
[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:



1. Sign in to the dashboard

1. Select Clusters.

1. Select the cluster that you want to back up.

   ![Select a cluster](/documentation/cloud/select-cluster.png)

1. Find and select the **Backups** tab.

1. Now you can set up a backup schedule.

   The **Days of Retention** is the number of days after a backup snapshot is

   deleted.

1. Alternatively, you can select **Backup now** to take an immediate snapshot.
recommend replication. It guarantees the proper cluster functionality as long as

at least one replica is running.



For other use-cases such as disaster recovery, you can set up automatic or

self-service backups.



## Prerequisites



You can back up your Qdrant clusters though the Qdrant Cloud

Dashboard at https://cloud.qdrant.io. This section assumes that you've already

set up your cluster, as described in the following sections:



- [Create a cluster](/documentation/cloud/create-cluster/)
![Configure a cluster backup](/documentation/cloud/backup-schedule.png)



### Restore a backup



If you have a backup, it appears in the list of **Available Backups**. You can

choose to restore or delete the backups of your choice.



![Restore or delete a cluster backup](/documentation/cloud/restore-delete.png)



<!-- I think we should move this to the Snapshot page, but I'll do it later -->



## Backups with a snapshot



Qdrant also offers a snapshot API which allows you to create a snapshot
<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>



### Create full storage snapshot



```http

POST /snapshots

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.create_full_snapshot()

```",0.32231404958677684,0.6,0.41935483870967744,0.075,0.140625,0.09782608695652173,0.19008264462809918,0.35384615384615387,0.24731182795698925,0.2727272727272727,0.5076923076923077,0.3548387096774193,0.7978693246841431,0.5681191682815552,0.6139931678771973,0.5901660323143005,0.20910758152604103,0.50227209366858,134,1.7866666666666666,0.48559476906982196
True,,3493.227,8e10fc90,What are snapshots in the context of Qdrant Cloud and how are they used in a distributed setup?,"Snapshots in Qdrant Cloud are `tar` archive files that contain data and configuration of a specific collection on a specific node at a specific time. In a distributed setup with multiple nodes in a cluster, snapshots must be created for each node separately when dealing with a single collection. These snapshots can be used to archive data or easily replicate an existing deployment.","Snapshots in the context of Qdrant Cloud are a way to create full storage snapshots of the data stored in the Qdrant Cloud cluster. These snapshots can be used for archiving data, easily replicating deployments, or disaster recovery purposes. 

In a distributed setup with multiple nodes, creating snapshots involves calling the snapshot creation endpoint on each node of the cluster. This can be done either through direct HTTP requests or using the Qdrant Python SDK. Each node will create a snapshot of its own data.

Once the snapshots are created, they can be downloaded using the REST API endpoint. However, restoring these snapshots can only be done through the Qdrant CLI at startup time. This means that you cannot restore a Qdrant Cloud cluster from a whole storage snapshot directly, but you can use backups for that purpose.

To list the available full storage snapshots, you can make a GET request to the `/snapshots` endpoint. This can be done using HTTP requests or through the Qdrant Python SDK, JavaScript SDK, or Rust SDK.","<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>



### Create full storage snapshot



```http

POST /snapshots

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.create_full_snapshot()

```
This feature can be used to archive data or easily replicate an existing deployment. For disaster recovery, Qdrant Cloud users may prefer to use [Backups](/documentation/cloud/backups/) instead, which are physical disk-level copies of your data.



For a step-by-step guide on how to use snapshots, see our [tutorial](/documentation/tutorials/create-snapshot/).



## Store snapshots
</details>



If you already have a collection, you can skip the previous step and start by [creating a snapshot](#create-and-download-snapshots).



## Create and download snapshots



Qdrant exposes an HTTP endpoint to request creating a snapshot, but we can also call it with the Python SDK.

Our setup consists of 3 nodes, so we need to call the endpoint **on each of them** and create a snapshot on each node. While using Python SDK, that means creating a separate client instance for each node.
```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.ListFullSnapshotsAsync();

```



### Download full storage snapshot



<aside role=""status"">Only available through the REST API for the time being.</aside>



```http

GET /snapshots/{snapshot_name}

```



## Restore full storage snapshot



Restoring snapshots can only be done through the Qdrant CLI at startup time.



For example:



```bash
```



### List full storage snapshots



```http

GET /snapshots

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.list_full_snapshots()

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.listFullSnapshots();

```



```rust

use qdrant_client::client::QdrantClient;","['<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>\n\n\n\n### Create full storage snapshot\n\n\n\n```http\n\nPOST /snapshots\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_full_snapshot()\n\n```', 'This feature can be used to archive data or easily replicate an existing deployment. For disaster recovery, Qdrant Cloud users may prefer to use [Backups](/documentation/cloud/backups/) instead, which are physical disk-level copies of your data.\n\n\n\nFor a step-by-step guide on how to use snapshots, see our [tutorial](/documentation/tutorials/create-snapshot/).\n\n\n\n## Store snapshots', '</details>\n\n\n\nIf you already have a collection, you can skip the previous step and start by [creating a snapshot](#create-and-download-snapshots).\n\n\n\n## Create and download snapshots\n\n\n\nQdrant exposes an HTTP endpoint to request creating a snapshot, but we can also call it with the Python SDK.\n\nOur setup consists of 3 nodes, so we need to call the endpoint **on each of them** and create a snapshot on each node. While using Python SDK, that means creating a separate client instance for each node.', '```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.ListFullSnapshotsAsync();\n\n```\n\n\n\n### Download full storage snapshot\n\n\n\n<aside role=""status"">Only available through the REST API for the time being.</aside>\n\n\n\n```http\n\nGET /snapshots/{snapshot_name}\n\n```\n\n\n\n## Restore full storage snapshot\n\n\n\nRestoring snapshots can only be done through the Qdrant CLI at startup time.\n\n\n\nFor example:\n\n\n\n```bash', '```\n\n\n\n### List full storage snapshots\n\n\n\n```http\n\nGET /snapshots\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.list_full_snapshots()\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.listFullSnapshots();\n\n```\n\n\n\n```rust\n\nuse qdrant_client::client::QdrantClient;']","Question: What are snapshots in the context of Qdrant Cloud and how are they used in a distributed setup?\n\nContext: <aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>



### Create full storage snapshot



```http

POST /snapshots

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.create_full_snapshot()

```
This feature can be used to archive data or easily replicate an existing deployment. For disaster recovery, Qdrant Cloud users may prefer to use [Backups](/documentation/cloud/backups/) instead, which are physical disk-level copies of your data.



For a step-by-step guide on how to use snapshots, see our [tutorial](/documentation/tutorials/create-snapshot/).



## Store snapshots
</details>



If you already have a collection, you can skip the previous step and start by [creating a snapshot](#create-and-download-snapshots).



## Create and download snapshots



Qdrant exposes an HTTP endpoint to request creating a snapshot, but we can also call it with the Python SDK.

Our setup consists of 3 nodes, so we need to call the endpoint **on each of them** and create a snapshot on each node. While using Python SDK, that means creating a separate client instance for each node.
```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.ListFullSnapshotsAsync();

```



### Download full storage snapshot



<aside role=""status"">Only available through the REST API for the time being.</aside>



```http

GET /snapshots/{snapshot_name}

```



## Restore full storage snapshot



Restoring snapshots can only be done through the Qdrant CLI at startup time.



For example:



```bash
```



### List full storage snapshots



```http

GET /snapshots

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.list_full_snapshots()

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.listFullSnapshots();

```



```rust

use qdrant_client::client::QdrantClient;\n\nAnswer:",What are snapshots in the context of Qdrant Cloud and how are they used in a distributed setup?,"Snapshots in Qdrant Cloud are `tar` archive files that contain data and configuration of a specific collection on a specific node at a specific time. In a distributed setup with multiple nodes in a cluster, snapshots must be created for each node separately when dealing with a single collection. These snapshots can be used to archive data or easily replicate an existing deployment.","['<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>\n\n\n\n### Create full storage snapshot\n\n\n\n```http\n\nPOST /snapshots\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_full_snapshot()\n\n```', 'This feature can be used to archive data or easily replicate an existing deployment. For disaster recovery, Qdrant Cloud users may prefer to use [Backups](/documentation/cloud/backups/) instead, which are physical disk-level copies of your data.\n\n\n\nFor a step-by-step guide on how to use snapshots, see our [tutorial](/documentation/tutorials/create-snapshot/).\n\n\n\n## Store snapshots', '</details>\n\n\n\nIf you already have a collection, you can skip the previous step and start by [creating a snapshot](#create-and-download-snapshots).\n\n\n\n## Create and download snapshots\n\n\n\nQdrant exposes an HTTP endpoint to request creating a snapshot, but we can also call it with the Python SDK.\n\nOur setup consists of 3 nodes, so we need to call the endpoint **on each of them** and create a snapshot on each node. While using Python SDK, that means creating a separate client instance for each node.', '```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.ListFullSnapshotsAsync();\n\n```\n\n\n\n### Download full storage snapshot\n\n\n\n<aside role=""status"">Only available through the REST API for the time being.</aside>\n\n\n\n```http\n\nGET /snapshots/{snapshot_name}\n\n```\n\n\n\n## Restore full storage snapshot\n\n\n\nRestoring snapshots can only be done through the Qdrant CLI at startup time.\n\n\n\nFor example:\n\n\n\n```bash', '```\n\n\n\n### List full storage snapshots\n\n\n\n```http\n\nGET /snapshots\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.list_full_snapshots()\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.listFullSnapshots();\n\n```\n\n\n\n```rust\n\nuse qdrant_client::client::QdrantClient;']","<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>



### Create full storage snapshot



```http

POST /snapshots

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.create_full_snapshot()

```
This feature can be used to archive data or easily replicate an existing deployment. For disaster recovery, Qdrant Cloud users may prefer to use [Backups](/documentation/cloud/backups/) instead, which are physical disk-level copies of your data.



For a step-by-step guide on how to use snapshots, see our [tutorial](/documentation/tutorials/create-snapshot/).



## Store snapshots
</details>



If you already have a collection, you can skip the previous step and start by [creating a snapshot](#create-and-download-snapshots).



## Create and download snapshots



Qdrant exposes an HTTP endpoint to request creating a snapshot, but we can also call it with the Python SDK.

Our setup consists of 3 nodes, so we need to call the endpoint **on each of them** and create a snapshot on each node. While using Python SDK, that means creating a separate client instance for each node.
```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.ListFullSnapshotsAsync();

```



### Download full storage snapshot



<aside role=""status"">Only available through the REST API for the time being.</aside>



```http

GET /snapshots/{snapshot_name}

```



## Restore full storage snapshot



Restoring snapshots can only be done through the Qdrant CLI at startup time.



For example:



```bash
```



### List full storage snapshots



```http

GET /snapshots

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.list_full_snapshots()

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.listFullSnapshots();

```



```rust

use qdrant_client::client::QdrantClient;",0.24516129032258063,0.6785714285714286,0.3601895734597157,0.1038961038961039,0.2909090909090909,0.15311004784688995,0.15483870967741936,0.42857142857142855,0.22748815165876776,0.23225806451612904,0.6428571428571429,0.34123222748815163,0.9182628989219666,0.5905989408493042,0.6906086802482605,0.6367005109786987,0.9161209116340615,0.7449572912106912,174,2.761904761904762,0.33165873922420636
True,,1944.647,3114ab4b,What is the significance of the lowercase parameter in the context of text filters in Qdrant?,"The lowercase parameter in Qdrant is used to specify whether the index should be case-insensitive or not. When set to true, Qdrant will convert all the texts to lowercase before indexing them. This means that during searches, the case of the letters in the query will not affect the results.","In the context of text filters in Qdrant, the significance of the lowercase parameter is that if it is set to true, then the index will be case-insensitive. Qdrant will convert all the texts to lowercase before performing any operations, such as searching or filtering. This means that when the lowercase parameter is enabled, the text comparisons will ignore the case of the letters, treating uppercase and lowercase letters as equivalent.","## Using text filters in practice



![](/blog/from_cms/1_pbtd2tzqtjqqlbi61r8czg.webp ""There are also some additional parameters you can provide, such as  min_token_len — minimal length of the token max_token_len — maximal length of the token lowercase — if set to true, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase Using text filters in practice"")
* **min_token_len** — minimal length of the token

* **max_token_len** — maximal length of the token

* **lowercase** — if set to *true*, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase



## Using text filters in practice
This feature is implemented as additional filters during the search and will enable you to incorporate custom logic on top of semantic similarity.



During the filtering, Qdrant will check the conditions over those values that match the type of the filtering condition. If the stored value type does not fit the filtering condition - it will be considered not satisfied.



For example, you will get an empty output if you apply the [range condition](../filtering/#range) on the string data.
negative: [300],

    filter,

    limit: 3,

  },

];



client.recommend_batch(""{collection_name}"", {

  searches,

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{Condition, Filter, RecommendBatchPoints, RecommendPoints},

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



let filter = Filter::must([Condition::matches(""city"", ""London"".to_string())]);



let recommend_queries = vec![

    RecommendPoints {
using static Qdrant.Client.Grpc.Conditions;



var client = new QdrantClient(""localhost"", 6334);



// The ! operator negates the condition(must not)

await client.ScrollAsync(

	collectionName: ""{collection_name}"",

	filter: !(MatchKeyword(""city"", ""London"") & MatchKeyword(""color"", ""red""))

);

```



Filtered points would be:



```json

[

  { ""id"": 5, ""city"": ""Moscow"", ""color"": ""green"" },

  { ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }

]

```","['## Using text filters in practice\n\n\n\n![](/blog/from_cms/1_pbtd2tzqtjqqlbi61r8czg.webp ""There are also some additional parameters you can provide, such as  min_token_len — minimal length of the token max_token_len — maximal length of the token lowercase — if set to true, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase Using text filters in practice"")', '* **min_token_len**\xa0— minimal length of the token\n\n* **max_token_len**\xa0— maximal length of the token\n\n* **lowercase**\xa0— if set to\xa0*true*, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase\n\n\n\n## Using text filters in practice', 'This feature is implemented as additional filters during the search and will enable you to incorporate custom logic on top of semantic similarity.\n\n\n\nDuring the filtering, Qdrant will check the conditions over those values that match the type of the filtering condition. If the stored value type does not fit the filtering condition - it will be considered not satisfied.\n\n\n\nFor example, you will get an empty output if you apply the [range condition](../filtering/#range) on the string data.', 'negative: [300],\n\n    filter,\n\n    limit: 3,\n\n  },\n\n];\n\n\n\nclient.recommend_batch(""{collection_name}"", {\n\n  searches,\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{Condition, Filter, RecommendBatchPoints, RecommendPoints},\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nlet filter = Filter::must([Condition::matches(""city"", ""London"".to_string())]);\n\n\n\nlet recommend_queries = vec![\n\n    RecommendPoints {', 'using static Qdrant.Client.Grpc.Conditions;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\n// The ! operator negates the condition(must not)\n\nawait client.ScrollAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tfilter: !(MatchKeyword(""city"", ""London"") & MatchKeyword(""color"", ""red""))\n\n);\n\n```\n\n\n\nFiltered points would be:\n\n\n\n```json\n\n[\n\n  { ""id"": 5, ""city"": ""Moscow"", ""color"": ""green"" },\n\n  { ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }\n\n]\n\n```']","Question: What is the significance of the lowercase parameter in the context of text filters in Qdrant?\n\nContext: ## Using text filters in practice



![](/blog/from_cms/1_pbtd2tzqtjqqlbi61r8czg.webp ""There are also some additional parameters you can provide, such as  min_token_len — minimal length of the token max_token_len — maximal length of the token lowercase — if set to true, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase Using text filters in practice"")
* **min_token_len** — minimal length of the token

* **max_token_len** — maximal length of the token

* **lowercase** — if set to *true*, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase



## Using text filters in practice
This feature is implemented as additional filters during the search and will enable you to incorporate custom logic on top of semantic similarity.



During the filtering, Qdrant will check the conditions over those values that match the type of the filtering condition. If the stored value type does not fit the filtering condition - it will be considered not satisfied.



For example, you will get an empty output if you apply the [range condition](../filtering/#range) on the string data.
negative: [300],

filter,

limit: 3,

},

];



client.recommend_batch(""{collection_name}"", {

searches,

});

```



```rust

use qdrant_client::{

client::QdrantClient,

qdrant::{Condition, Filter, RecommendBatchPoints, RecommendPoints},

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



let filter = Filter::must([Condition::matches(""city"", ""London"".to_string())]);



let recommend_queries = vec![

RecommendPoints {
using static Qdrant.Client.Grpc.Conditions;



var client = new QdrantClient(""localhost"", 6334);



// The ! operator negates the condition(must not)

await client.ScrollAsync(

collectionName: ""{collection_name}"",

filter: !(MatchKeyword(""city"", ""London"") & MatchKeyword(""color"", ""red""))

);

```



Filtered points would be:



```json

[

{ ""id"": 5, ""city"": ""Moscow"", ""color"": ""green"" },

{ ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }

]

```\n\nAnswer:",What is the significance of the lowercase parameter in the context of text filters in Qdrant?,"The lowercase parameter in Qdrant is used to specify whether the index should be case-insensitive or not. When set to true, Qdrant will convert all the texts to lowercase before indexing them. This means that during searches, the case of the letters in the query will not affect the results.","['## Using text filters in practice\n\n\n\n![](/blog/from_cms/1_pbtd2tzqtjqqlbi61r8czg.webp ""There are also some additional parameters you can provide, such as  min_token_len — minimal length of the token max_token_len — maximal length of the token lowercase — if set to true, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase Using text filters in practice"")', '* **min_token_len**\xa0— minimal length of the token\n\n* **max_token_len**\xa0— maximal length of the token\n\n* **lowercase**\xa0— if set to\xa0*true*, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase\n\n\n\n## Using text filters in practice', 'This feature is implemented as additional filters during the search and will enable you to incorporate custom logic on top of semantic similarity.\n\n\n\nDuring the filtering, Qdrant will check the conditions over those values that match the type of the filtering condition. If the stored value type does not fit the filtering condition - it will be considered not satisfied.\n\n\n\nFor example, you will get an empty output if you apply the [range condition](../filtering/#range) on the string data.', 'negative: [300],\n\n    filter,\n\n    limit: 3,\n\n  },\n\n];\n\n\n\nclient.recommend_batch(""{collection_name}"", {\n\n  searches,\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{Condition, Filter, RecommendBatchPoints, RecommendPoints},\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nlet filter = Filter::must([Condition::matches(""city"", ""London"".to_string())]);\n\n\n\nlet recommend_queries = vec![\n\n    RecommendPoints {', 'using static Qdrant.Client.Grpc.Conditions;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\n// The ! operator negates the condition(must not)\n\nawait client.ScrollAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tfilter: !(MatchKeyword(""city"", ""London"") & MatchKeyword(""color"", ""red""))\n\n);\n\n```\n\n\n\nFiltered points would be:\n\n\n\n```json\n\n[\n\n  { ""id"": 5, ""city"": ""Moscow"", ""color"": ""green"" },\n\n  { ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }\n\n]\n\n```']","## Using text filters in practice



![](/blog/from_cms/1_pbtd2tzqtjqqlbi61r8czg.webp ""There are also some additional parameters you can provide, such as  min_token_len — minimal length of the token max_token_len — maximal length of the token lowercase — if set to true, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase Using text filters in practice"")
* **min_token_len** — minimal length of the token

* **max_token_len** — maximal length of the token

* **lowercase** — if set to *true*, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase



## Using text filters in practice
This feature is implemented as additional filters during the search and will enable you to incorporate custom logic on top of semantic similarity.



During the filtering, Qdrant will check the conditions over those values that match the type of the filtering condition. If the stored value type does not fit the filtering condition - it will be considered not satisfied.



For example, you will get an empty output if you apply the [range condition](../filtering/#range) on the string data.
negative: [300],

    filter,

    limit: 3,

  },

];



client.recommend_batch(""{collection_name}"", {

  searches,

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{Condition, Filter, RecommendBatchPoints, RecommendPoints},

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



let filter = Filter::must([Condition::matches(""city"", ""London"".to_string())]);



let recommend_queries = vec![

    RecommendPoints {
using static Qdrant.Client.Grpc.Conditions;



var client = new QdrantClient(""localhost"", 6334);



// The ! operator negates the condition(must not)

await client.ScrollAsync(

	collectionName: ""{collection_name}"",

	filter: !(MatchKeyword(""city"", ""London"") & MatchKeyword(""color"", ""red""))

);

```



Filtered points would be:



```json

[

  { ""id"": 5, ""city"": ""Moscow"", ""color"": ""green"" },

  { ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }

]

```",0.49206349206349204,0.7045454545454546,0.5794392523364486,0.27419354838709675,0.3953488372093023,0.32380952380952377,0.3492063492063492,0.5,0.411214953271028,0.47619047619047616,0.6818181818181818,0.5607476635514018,0.9178866147994995,0.6951263546943665,0.7309277057647705,0.7125776410102844,0.7656221141417822,0.9484977163374424,71,1.42,0.2900720898827021
True,,4426.484,e5698595,How does adjusting the `write_consistency_factor` parameter impact write operations in a distributed deployment using Qdrant?,"The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully.","Adjusting the `write_consistency_factor` parameter in a distributed deployment using Qdrant impacts write operations in the following way:

1. The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client.
2. Increasing the `write_consistency_factor` value makes write operations more tolerant to network partitions within the cluster.
3. A higher `write_consistency_factor` value requires a greater number of replicas to be active in order to perform write operations successfully.
4. Setting a higher `write_consistency_factor` can improve data consistency and reliability by ensuring that multiple replicas confirm the write operation before it is considered successful.
5. However, increasing the `write_consistency_factor` may also introduce additional latency in write operations due to the need for more replicas to acknowledge the write before completion.

In summary, adjusting the `write_consistency_factor` parameter in a Qdrant distributed deployment affects the level of data consistency, reliability, and latency in write operations based on the number of replicas required to acknowledge the write.","""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

    shard_number=6,

    replication_factor=2,

    write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

    ""write_consistency_factor"": 2,

}

```



```python
* `params` - other collection parameters, including `write_consistency_factor` and `on_disk_payload`. 



Full API specification is available in [schema definitions](https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/update_collection).



Calls to this endpoint may be blocking as it waits for existing optimizers to

finish. We recommended against using this in a production database as it may

introduce huge overhead due to the rebuilding of the index.
replication_factor: Some(2),

        write_consistency_factor: Some(2),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =","['""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n\n    shard_number=6,\n\n    replication_factor=2,\n\n    write_consistency_factor=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', '- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.', '### Write consistency factor\n\n\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\n\nIt can be configured at the collection\'s creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python', '* `params` - other collection parameters, including `write_consistency_factor` and `on_disk_payload`. \n\n\n\nFull API specification is available in [schema definitions](https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/update_collection).\n\n\n\nCalls to this endpoint may be blocking as it waits for existing optimizers to\n\nfinish. We recommended against using this in a production database as it may\n\nintroduce huge overhead due to the rebuilding of the index.', 'replication_factor: Some(2),\n\n        write_consistency_factor: Some(2),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =']","Question: How does adjusting the `write_consistency_factor` parameter impact write operations in a distributed deployment using Qdrant?\n\nContext: ""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

shard_number=6,

replication_factor=2,

write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 300,

""distance"": ""Cosine""

},

""shard_number"": 6,

""replication_factor"": 2,

""write_consistency_factor"": 2,

}

```



```python
* `params` - other collection parameters, including `write_consistency_factor` and `on_disk_payload`. 



Full API specification is available in [schema definitions](https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/update_collection).



Calls to this endpoint may be blocking as it waits for existing optimizers to

finish. We recommended against using this in a production database as it may

introduce huge overhead due to the rebuilding of the index.
replication_factor: Some(2),

write_consistency_factor: Some(2),

..Default::default()

})

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =\n\nAnswer:",How does adjusting the `write_consistency_factor` parameter impact write operations in a distributed deployment using Qdrant?,"The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully.","['""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n\n    shard_number=6,\n\n    replication_factor=2,\n\n    write_consistency_factor=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', '- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.', '### Write consistency factor\n\n\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\n\nIt can be configured at the collection\'s creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python', '* `params` - other collection parameters, including `write_consistency_factor` and `on_disk_payload`. \n\n\n\nFull API specification is available in [schema definitions](https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/update_collection).\n\n\n\nCalls to this endpoint may be blocking as it waits for existing optimizers to\n\nfinish. We recommended against using this in a production database as it may\n\nintroduce huge overhead due to the rebuilding of the index.', 'replication_factor: Some(2),\n\n        write_consistency_factor: Some(2),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =']","""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

    shard_number=6,

    replication_factor=2,

    write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

    ""write_consistency_factor"": 2,

}

```



```python
* `params` - other collection parameters, including `write_consistency_factor` and `on_disk_payload`. 



Full API specification is available in [schema definitions](https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/update_collection).



Calls to this endpoint may be blocking as it waits for existing optimizers to

finish. We recommended against using this in a production database as it may

introduce huge overhead due to the rebuilding of the index.
replication_factor: Some(2),

        write_consistency_factor: Some(2),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =",0.33774834437086093,0.9107142857142857,0.4927536231884057,0.24666666666666667,0.6727272727272727,0.36097560975609755,0.304635761589404,0.8214285714285714,0.4444444444444444,0.33112582781456956,0.8928571428571429,0.48309178743961356,0.951007604598999,0.6339829564094543,0.7715274095535278,0.6960250735282898,0.803143915720284,0.9279033755883574,158,2.5483870967741935,0.7193119359381428
True,,1496.741,5da1565e,What are some of the features and support services that come pre-configured with each instance in Qdrant Cloud?,"Each instance in Qdrant Cloud comes pre-configured with the following tools, features, and support services:1. Automatically created with the latest available version of Qdrant.
2. Upgradeable to later versions of Qdrant as they are released.
3. Equipped with monitoring and logging to observe the health of each cluster.
4. Accessible through the Qdrant Cloud Console.
5. Vertically scalable.
6. Offered on AWS and GCP, with Azure currently in development.
","Some of the features and support services that come pre-configured with each instance in Qdrant Cloud include:

- Upgradeable to later versions of Qdrant as they are released.
- Equipped with monitoring and logging to observe the health of each cluster.
- Accessible through the Qdrant Cloud Console.
- Vertically scalable.
- Offered on AWS and GCP, with Azure currently in development.","It provides horizontal and vertical scaling, one click installation and upgrades, monitoring, logging, as well as backup and disaster recovery. For more information, see the [Qdrant Cloud documentation](/documentation/cloud).



### Kubernetes



You can use a ready-made [Helm Chart](https://helm.sh/docs/) to run Qdrant in your Kubernetes cluster:



```bash

helm repo add qdrant https://qdrant.to/helm

helm install qdrant qdrant/qdrant

```
## Configuration



Qdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.



Example: 

```yaml

qdrant:

    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""

    api_key: ""<your-api-key>""

```



The available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:

| Field        | Description |
- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:
Transitioning from on-premise to the cloud version of Qdrant does not require changing anything in the way you interact with the service. All you have to do is [create a Qdrant Cloud account](https://qdrant.to/cloud) and [provide a new API key]({{< ref ""/documentation/cloud/authentication"" >}}) to each request.
## Prerequisites



Before you start, make sure you have the following:



1. A Qdrant instance to connect to. You can get free cloud instance [cloud.qdrant.io](https://cloud.qdrant.io/). 

2. An account at Make.com. You can register yourself [here](https://www.make.com/en/register).



## Setting up a connection



Navigate to your scenario on the Make dashboard and select a Qdrant app module to start a connection.

![Qdrant Make connection](/documentation/frameworks/make/connection.png)","['It provides horizontal and vertical scaling, one click installation and upgrades, monitoring, logging, as well as backup and disaster recovery. For more information, see the [Qdrant Cloud documentation](/documentation/cloud).\n\n\n\n### Kubernetes\n\n\n\nYou can use a ready-made [Helm Chart](https://helm.sh/docs/) to run Qdrant in your Kubernetes cluster:\n\n\n\n```bash\n\nhelm repo add qdrant https://qdrant.to/helm\n\nhelm install qdrant qdrant/qdrant\n\n```', '## Configuration\n\n\n\nQdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.\n\n\n\nExample: \n\n```yaml\n\nqdrant:\n\n    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""\n\n    api_key: ""<your-api-key>""\n\n```\n\n\n\nThe available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:\n\n| Field        | Description |', '- Upgradeable to later versions of Qdrant as they are released.\n\n- Equipped with monitoring and logging to observe the health of each cluster. \n\n- Accessible through the Qdrant Cloud Console.\n\n- Vertically scalable.\n\n- Offered on AWS and GCP, with Azure currently in development. \n\n\n\n### Getting started with Qdrant Cloud\n\n\n\nTo use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:', 'Transitioning from on-premise to the cloud version of Qdrant does not require changing anything in the way you interact with the service. All you have to do is [create a Qdrant Cloud account](https://qdrant.to/cloud) and [provide a new API key]({{< ref ""/documentation/cloud/authentication"" >}}) to each request.', '## Prerequisites\n\n\n\nBefore you start, make sure you have the following:\n\n\n\n1. A Qdrant instance to connect to. You can get free cloud instance [cloud.qdrant.io](https://cloud.qdrant.io/). \n\n2. An account at Make.com. You can register yourself [here](https://www.make.com/en/register).\n\n\n\n## Setting up a connection\n\n\n\nNavigate to your scenario on the Make dashboard and select a Qdrant app module to start a connection.\n\n![Qdrant Make connection](/documentation/frameworks/make/connection.png)']","Question: What are some of the features and support services that come pre-configured with each instance in Qdrant Cloud?\n\nContext: It provides horizontal and vertical scaling, one click installation and upgrades, monitoring, logging, as well as backup and disaster recovery. For more information, see the [Qdrant Cloud documentation](/documentation/cloud).



### Kubernetes



You can use a ready-made [Helm Chart](https://helm.sh/docs/) to run Qdrant in your Kubernetes cluster:



```bash

helm repo add qdrant https://qdrant.to/helm

helm install qdrant qdrant/qdrant

```
## Configuration



Qdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.



Example: 

```yaml

qdrant:

url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""

api_key: ""<your-api-key>""

```



The available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:

| Field        | Description |
- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:
Transitioning from on-premise to the cloud version of Qdrant does not require changing anything in the way you interact with the service. All you have to do is [create a Qdrant Cloud account](https://qdrant.to/cloud) and [provide a new API key]({{< ref ""/documentation/cloud/authentication"" >}}) to each request.
## Prerequisites



Before you start, make sure you have the following:



1. A Qdrant instance to connect to. You can get free cloud instance [cloud.qdrant.io](https://cloud.qdrant.io/). 

2. An account at Make.com. You can register yourself [here](https://www.make.com/en/register).



## Setting up a connection



Navigate to your scenario on the Make dashboard and select a Qdrant app module to start a connection.

![Qdrant Make connection](/documentation/frameworks/make/connection.png)\n\nAnswer:",What are some of the features and support services that come pre-configured with each instance in Qdrant Cloud?,"Each instance in Qdrant Cloud comes pre-configured with the following tools, features, and support services:1. Automatically created with the latest available version of Qdrant.
2. Upgradeable to later versions of Qdrant as they are released.
3. Equipped with monitoring and logging to observe the health of each cluster.
4. Accessible through the Qdrant Cloud Console.
5. Vertically scalable.
6. Offered on AWS and GCP, with Azure currently in development.
","['It provides horizontal and vertical scaling, one click installation and upgrades, monitoring, logging, as well as backup and disaster recovery. For more information, see the [Qdrant Cloud documentation](/documentation/cloud).\n\n\n\n### Kubernetes\n\n\n\nYou can use a ready-made [Helm Chart](https://helm.sh/docs/) to run Qdrant in your Kubernetes cluster:\n\n\n\n```bash\n\nhelm repo add qdrant https://qdrant.to/helm\n\nhelm install qdrant qdrant/qdrant\n\n```', '## Configuration\n\n\n\nQdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.\n\n\n\nExample: \n\n```yaml\n\nqdrant:\n\n    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""\n\n    api_key: ""<your-api-key>""\n\n```\n\n\n\nThe available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:\n\n| Field        | Description |', '- Upgradeable to later versions of Qdrant as they are released.\n\n- Equipped with monitoring and logging to observe the health of each cluster. \n\n- Accessible through the Qdrant Cloud Console.\n\n- Vertically scalable.\n\n- Offered on AWS and GCP, with Azure currently in development. \n\n\n\n### Getting started with Qdrant Cloud\n\n\n\nTo use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:', 'Transitioning from on-premise to the cloud version of Qdrant does not require changing anything in the way you interact with the service. All you have to do is [create a Qdrant Cloud account](https://qdrant.to/cloud) and [provide a new API key]({{< ref ""/documentation/cloud/authentication"" >}}) to each request.', '## Prerequisites\n\n\n\nBefore you start, make sure you have the following:\n\n\n\n1. A Qdrant instance to connect to. You can get free cloud instance [cloud.qdrant.io](https://cloud.qdrant.io/). \n\n2. An account at Make.com. You can register yourself [here](https://www.make.com/en/register).\n\n\n\n## Setting up a connection\n\n\n\nNavigate to your scenario on the Make dashboard and select a Qdrant app module to start a connection.\n\n![Qdrant Make connection](/documentation/frameworks/make/connection.png)']","It provides horizontal and vertical scaling, one click installation and upgrades, monitoring, logging, as well as backup and disaster recovery. For more information, see the [Qdrant Cloud documentation](/documentation/cloud).



### Kubernetes



You can use a ready-made [Helm Chart](https://helm.sh/docs/) to run Qdrant in your Kubernetes cluster:



```bash

helm repo add qdrant https://qdrant.to/helm

helm install qdrant qdrant/qdrant

```
## Configuration



Qdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.



Example: 

```yaml

qdrant:

    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""

    api_key: ""<your-api-key>""

```



The available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:

| Field        | Description |
- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:
Transitioning from on-premise to the cloud version of Qdrant does not require changing anything in the way you interact with the service. All you have to do is [create a Qdrant Cloud account](https://qdrant.to/cloud) and [provide a new API key]({{< ref ""/documentation/cloud/authentication"" >}}) to each request.
## Prerequisites



Before you start, make sure you have the following:



1. A Qdrant instance to connect to. You can get free cloud instance [cloud.qdrant.io](https://cloud.qdrant.io/). 

2. An account at Make.com. You can register yourself [here](https://www.make.com/en/register).



## Setting up a connection



Navigate to your scenario on the Make dashboard and select a Qdrant app module to start a connection.

![Qdrant Make connection](/documentation/frameworks/make/connection.png)",0.9454545454545454,0.7761194029850746,0.8524590163934428,0.7962962962962963,0.6515151515151515,0.7166666666666667,0.8,0.6567164179104478,0.7213114754098361,0.8363636363636363,0.6865671641791045,0.7540983606557378,0.9616585969924927,0.8756068348884583,0.8163191676139832,0.84492427110672,0.6739231511950493,0.3362896194060644,57,0.8260869565217391,0.5907355612160425
True,,4996.304,2340453a,How is the `must_not` clause used in conjunction with other clauses in filtering data?,"The `must_not` clause in filtering data is used to exclude documents that meet specific criteria. The `must_not` clause can be used in combination with the `must` clause to create a more refined filter. Specifically, the `must_not` clause is used to specify conditions that must not be met for a document to be included in the results.","The `must_not` clause is used in conjunction with other clauses in filtering data to exclude documents that meet certain criteria. It is used to specify conditions that must not be met for a document to be included in the result set.

In the examples provided, the `must_not` clause is combined with the `must` clause to create a more specific filter. For example, in the Python code snippet:

```python
client.scroll(
    collection_name=""{collection_name}"",
    scroll_filter=models.Filter(
        must=[
            models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")),
        ],
        must_not=[
            { ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },
            { ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" }
        ]
    )
)
```

Here, the documents must have a city matching ""London"" in the `city` field, but they must not have either the combination of id=3, city=""London"", color=""blue"" or the combination of id=4, city=""Berlin"", color=""red"".

This allows for fine-tuning the filtering criteria by including conditions that must not be met in addition to conditions that must be met.","{ ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }

]

```



When using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.

In this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.



### Clauses combination



It is also possible to use several clauses simultaneously:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""must"": [
filter: Some(Filter::must_not([

            Condition::matches(""city"", ""london"".to_string()),

            Condition::matches(""color"", ""red"".to_string()),

        ])),

        ..Default::default()

    })

    .await?;

```



```java

import java.util.List;



import static io.qdrant.client.ConditionFactory.matchKeyword;



import io.qdrant.client.grpc.Points.Filter;

import io.qdrant.client.grpc.Points.ScrollPoints;



client

    .scrollAsync(

        ScrollPoints.newBuilder()
{

    ""filter"": {

        ""must"": [

            { ""key"": ""city"", ""match"": { ""value"": ""London"" } }

        ],

        ""must_not"": [

            { ""key"": ""color"", ""match"": { ""value"": ""red"" } }

        ]

    }

}

```



```python

client.scroll(

    collection_name=""{collection_name}"",

    scroll_filter=models.Filter(

        must=[

            models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")),

        ],

        must_not=[
{ ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },

  { ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" }

]

```



When using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied.

In this sense, `should` is equivalent to the operator `OR`.



### Must Not



Example:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""must_not"": [

            { ""key"": ""city"", ""match"": { ""value"": ""London"" } },
],

        must_not=[

            models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")),

        ],

    ),

)

```



```typescript

client.scroll(""{collection_name}"", {

  filter: {

    must: [

      {

        key: ""city"",

        match: { value: ""London"" },

      },

    ],

    must_not: [

      {

        key: ""color"",

        match: { value: ""red"" },

      },

    ],

  },

});

```



```rust

use qdrant_client::qdrant::{Condition, Filter, ScrollPoints};","['{ ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }\n\n]\n\n```\n\n\n\nWhen using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.\n\nIn this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.\n\n\n\n### Clauses combination\n\n\n\nIt is also possible to use several clauses simultaneously:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""must"": [', 'filter: Some(Filter::must_not([\n\n            Condition::matches(""city"", ""london"".to_string()),\n\n            Condition::matches(""color"", ""red"".to_string()),\n\n        ])),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport java.util.List;\n\n\n\nimport static io.qdrant.client.ConditionFactory.matchKeyword;\n\n\n\nimport io.qdrant.client.grpc.Points.Filter;\n\nimport io.qdrant.client.grpc.Points.ScrollPoints;\n\n\n\nclient\n\n    .scrollAsync(\n\n        ScrollPoints.newBuilder()', '{\n\n    ""filter"": {\n\n        ""must"": [\n\n            { ""key"": ""city"", ""match"": { ""value"": ""London"" } }\n\n        ],\n\n        ""must_not"": [\n\n            { ""key"": ""color"", ""match"": { ""value"": ""red"" } }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.scroll(\n\n    collection_name=""{collection_name}"",\n\n    scroll_filter=models.Filter(\n\n        must=[\n\n            models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")),\n\n        ],\n\n        must_not=[', '{ ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },\n\n  { ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" }\n\n]\n\n```\n\n\n\nWhen using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied.\n\nIn this sense, `should` is equivalent to the operator `OR`.\n\n\n\n### Must Not\n\n\n\nExample:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""must_not"": [\n\n            { ""key"": ""city"", ""match"": { ""value"": ""London"" } },', '],\n\n        must_not=[\n\n            models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")),\n\n        ],\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nclient.scroll(""{collection_name}"", {\n\n  filter: {\n\n    must: [\n\n      {\n\n        key: ""city"",\n\n        match: { value: ""London"" },\n\n      },\n\n    ],\n\n    must_not: [\n\n      {\n\n        key: ""color"",\n\n        match: { value: ""red"" },\n\n      },\n\n    ],\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::qdrant::{Condition, Filter, ScrollPoints};']","Question: How is the `must_not` clause used in conjunction with other clauses in filtering data?\n\nContext: { ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }

]

```



When using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.

In this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.



### Clauses combination



It is also possible to use several clauses simultaneously:



```http

POST /collections/{collection_name}/points/scroll

{

""filter"": {

""must"": [
filter: Some(Filter::must_not([

Condition::matches(""city"", ""london"".to_string()),

Condition::matches(""color"", ""red"".to_string()),

])),

..Default::default()

})

.await?;

```



```java

import java.util.List;



import static io.qdrant.client.ConditionFactory.matchKeyword;



import io.qdrant.client.grpc.Points.Filter;

import io.qdrant.client.grpc.Points.ScrollPoints;



client

.scrollAsync(

ScrollPoints.newBuilder()
{

""filter"": {

""must"": [

{ ""key"": ""city"", ""match"": { ""value"": ""London"" } }

],

""must_not"": [

{ ""key"": ""color"", ""match"": { ""value"": ""red"" } }

]

}

}

```



```python

client.scroll(

collection_name=""{collection_name}"",

scroll_filter=models.Filter(

must=[

models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")),

],

must_not=[
{ ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },

{ ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" }

]

```



When using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied.

In this sense, `should` is equivalent to the operator `OR`.



### Must Not



Example:



```http

POST /collections/{collection_name}/points/scroll

{

""filter"": {

""must_not"": [

{ ""key"": ""city"", ""match"": { ""value"": ""London"" } },
],

must_not=[

models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")),

],

),

)

```



```typescript

client.scroll(""{collection_name}"", {

filter: {

must: [

{

key: ""city"",

match: { value: ""London"" },

},

],

must_not: [

{

key: ""color"",

match: { value: ""red"" },

},

],

},

});

```



```rust

use qdrant_client::qdrant::{Condition, Filter, ScrollPoints};\n\nAnswer:",How is the `must_not` clause used in conjunction with other clauses in filtering data?,"The `must_not` clause in filtering data is used to exclude documents that meet specific criteria. The `must_not` clause can be used in combination with the `must` clause to create a more refined filter. Specifically, the `must_not` clause is used to specify conditions that must not be met for a document to be included in the results.","['{ ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }\n\n]\n\n```\n\n\n\nWhen using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.\n\nIn this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.\n\n\n\n### Clauses combination\n\n\n\nIt is also possible to use several clauses simultaneously:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""must"": [', 'filter: Some(Filter::must_not([\n\n            Condition::matches(""city"", ""london"".to_string()),\n\n            Condition::matches(""color"", ""red"".to_string()),\n\n        ])),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport java.util.List;\n\n\n\nimport static io.qdrant.client.ConditionFactory.matchKeyword;\n\n\n\nimport io.qdrant.client.grpc.Points.Filter;\n\nimport io.qdrant.client.grpc.Points.ScrollPoints;\n\n\n\nclient\n\n    .scrollAsync(\n\n        ScrollPoints.newBuilder()', '{\n\n    ""filter"": {\n\n        ""must"": [\n\n            { ""key"": ""city"", ""match"": { ""value"": ""London"" } }\n\n        ],\n\n        ""must_not"": [\n\n            { ""key"": ""color"", ""match"": { ""value"": ""red"" } }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.scroll(\n\n    collection_name=""{collection_name}"",\n\n    scroll_filter=models.Filter(\n\n        must=[\n\n            models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")),\n\n        ],\n\n        must_not=[', '{ ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },\n\n  { ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" }\n\n]\n\n```\n\n\n\nWhen using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied.\n\nIn this sense, `should` is equivalent to the operator `OR`.\n\n\n\n### Must Not\n\n\n\nExample:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""must_not"": [\n\n            { ""key"": ""city"", ""match"": { ""value"": ""London"" } },', '],\n\n        must_not=[\n\n            models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")),\n\n        ],\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nclient.scroll(""{collection_name}"", {\n\n  filter: {\n\n    must: [\n\n      {\n\n        key: ""city"",\n\n        match: { value: ""London"" },\n\n      },\n\n    ],\n\n    must_not: [\n\n      {\n\n        key: ""color"",\n\n        match: { value: ""red"" },\n\n      },\n\n    ],\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::qdrant::{Condition, Filter, ScrollPoints};']","{ ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }

]

```



When using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.

In this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.



### Clauses combination



It is also possible to use several clauses simultaneously:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""must"": [
filter: Some(Filter::must_not([

            Condition::matches(""city"", ""london"".to_string()),

            Condition::matches(""color"", ""red"".to_string()),

        ])),

        ..Default::default()

    })

    .await?;

```



```java

import java.util.List;



import static io.qdrant.client.ConditionFactory.matchKeyword;



import io.qdrant.client.grpc.Points.Filter;

import io.qdrant.client.grpc.Points.ScrollPoints;



client

    .scrollAsync(

        ScrollPoints.newBuilder()
{

    ""filter"": {

        ""must"": [

            { ""key"": ""city"", ""match"": { ""value"": ""London"" } }

        ],

        ""must_not"": [

            { ""key"": ""color"", ""match"": { ""value"": ""red"" } }

        ]

    }

}

```



```python

client.scroll(

    collection_name=""{collection_name}"",

    scroll_filter=models.Filter(

        must=[

            models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")),

        ],

        must_not=[
{ ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },

  { ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" }

]

```



When using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied.

In this sense, `should` is equivalent to the operator `OR`.



### Must Not



Example:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""must_not"": [

            { ""key"": ""city"", ""match"": { ""value"": ""London"" } },
],

        must_not=[

            models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")),

        ],

    ),

)

```



```typescript

client.scroll(""{collection_name}"", {

  filter: {

    must: [

      {

        key: ""city"",

        match: { value: ""London"" },

      },

    ],

    must_not: [

      {

        key: ""color"",

        match: { value: ""red"" },

      },

    ],

  },

});

```



```rust

use qdrant_client::qdrant::{Condition, Filter, ScrollPoints};",0.3157894736842105,0.9230769230769231,0.47058823529411764,0.24503311258278146,0.7254901960784313,0.36633663366336633,0.2236842105263158,0.6538461538461539,0.3333333333333333,0.3157894736842105,0.9230769230769231,0.47058823529411764,0.8194783926010132,0.5824508666992188,0.7784343957901001,0.6663306355476379,0.71455719769001,0.8253212173779806,151,2.6964285714285716,0.6077156051637455
True,,4476.366,4beab824,What are some advantages of vector search over keyword-based search?,"Vector search has clear advantages over keyword-based search in various scenarios as outlined in the document chunk. Some of these advantages include:

1. Multi-lingual & multi-modal search: Vector search is effective in handling searches across multiple languages and modalities.
2. For short texts with typos and ambiguous content-dependent meanings: Vector search excels in situations where the search queries are short, contain typos, or have ambiguous meanings.
3. Specialized domains with tuned encoder models: Vector search is beneficial in specialized domains where encoder models can be fine-tuned for better search results.
4. Document-as-a-Query similarity search: Vector search allows for similarity searches where the entire document can be used as a query to find similar documents.

While vector search offers these advantages, it is important to note that keyword-based search still has its relevance in certain cases. For example, in out-of-domain search.","Some advantages of vector search over keyword-based search include:

1. **Multi-lingual & Multi-modal Search**: Vector search can handle multiple languages and modalities more effectively compared to keyword-based search, which may struggle with language variations and diverse data types.

2. **Handling Short Texts with Typos and Ambiguities**: Vector search is better at interpreting short texts with typos and content-dependent meanings that may confuse keyword-based search algorithms.

3. **Specialized Domains with Tuned Encoder Models**: Vector search can be fine-tuned for specific domains using encoder models, making it more adaptable and accurate in specialized areas compared to keyword-based search.

4. **Document-as-a-Query Similarity Search**: Vector search excels at document similarity search, where the content of a document is used as a query to find similar documents. This is a task where vector search outperforms traditional keyword-based methods.

These advantages make vector search a powerful alternative to keyword-based search, especially in scenarios where precision, flexibility, and accuracy are crucial.","preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has

some clear advantages in the following scenarios:



- 🌍 Multi-lingual & multi-modal search

- 🤔 For short texts with typos and ambiguous content-dependent meanings

- 👨‍🔬 Specialized domains with tuned encoder models

- 📄 Document-as-a-Query similarity search



It doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method

might be useful:
## Next Steps



Vector search is an exciting alternative to sparse methods. It solves the issues we had with the keyword-based search without needing to maintain lots of heuristics manually. It requires an additional component, a neural encoder, to convert text into vectors. 



[**Tutorial 1 - Qdrant for Complete Beginners**](../../tutorials/search-beginners)
In the open-source world, you pay for the resources you use, not the number of different databases you run.

Resources depend more on the optimal solution for each use case.

As a result, running a dedicated vector search engine can be even cheaper, as it allows optimization specifically for vector search use cases.



For instance, Qdrant implements a number of [quantization techniques](documentation/guides/quantization/) that can significantly reduce the memory footprint of embeddings.
</tr>

   </tbody>

</table>





# A wrap up



Each search scenario requires a specialized tool to achieve the best results possible. Still, combining multiple tools with 

minimal overhead is possible to improve the search precision even further. Introducing vector search into an existing search 

stack doesn't need to be a revolution but just one small step at a time. 



You'll never cover all the possible queries with a list of synonyms, so a full-text search may not find all the relevant
are able to benefit the advantages of both methods.



Again, it's worth mentioning that with the 3rd experiment, with cross-encoder reranking, Qdrant returned more than 48.12% of 

the relevant items and BM25 around 66.66%.



## Some anecdotal observations



None of the algorithms works better in all the cases. There might be some specific queries in which keyword-based search

will be a winner and the other way around. The table shows some interesting examples we could find in WANDS dataset","[""preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has\n\nsome clear advantages in the following scenarios:\n\n\n\n- 🌍 Multi-lingual & multi-modal search\n\n- 🤔 For short texts with typos and ambiguous content-dependent meanings\n\n- 👨\u200d🔬 Specialized domains with tuned encoder models\n\n- 📄 Document-as-a-Query similarity search\n\n\n\nIt doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method\n\nmight be useful:"", '## Next Steps\n\n\n\nVector search is an exciting alternative to sparse methods. It solves the issues we had with the keyword-based search without needing to maintain lots of heuristics manually. It requires an additional component, a neural encoder, to convert text into vectors. \n\n\n\n[**Tutorial 1 - Qdrant for Complete Beginners**](../../tutorials/search-beginners)', 'In the open-source world, you pay for the resources you use, not the number of different databases you run.\n\nResources depend more on the optimal solution for each use case.\n\nAs a result, running a dedicated vector search engine can be even cheaper, as it allows optimization specifically for vector search use cases.\n\n\n\nFor instance, Qdrant implements a number of [quantization techniques](documentation/guides/quantization/) that can significantly reduce the memory footprint of embeddings.', ""</tr>\n\n   </tbody>\n\n</table>\n\n\n\n\n\n# A wrap up\n\n\n\nEach search scenario requires a specialized tool to achieve the best results possible. Still, combining multiple tools with \n\nminimal overhead is possible to improve the search precision even further. Introducing vector search into an existing search \n\nstack doesn't need to be a revolution but just one small step at a time. \n\n\n\nYou'll never cover all the possible queries with a list of synonyms, so a full-text search may not find all the relevant"", ""are able to benefit the advantages of both methods.\n\n\n\nAgain, it's worth mentioning that with the 3rd experiment, with cross-encoder reranking, Qdrant returned more than 48.12% of \n\nthe relevant items and BM25 around 66.66%.\n\n\n\n## Some anecdotal observations\n\n\n\nNone of the algorithms works better in all the cases. There might be some specific queries in which keyword-based search\n\nwill be a winner and the other way around. The table shows some interesting examples we could find in WANDS dataset""]","Question: What are some advantages of vector search over keyword-based search?\n\nContext: preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has

some clear advantages in the following scenarios:



- 🌍 Multi-lingual & multi-modal search

- 🤔 For short texts with typos and ambiguous content-dependent meanings

- 👨‍🔬 Specialized domains with tuned encoder models

- 📄 Document-as-a-Query similarity search



It doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method

might be useful:
## Next Steps



Vector search is an exciting alternative to sparse methods. It solves the issues we had with the keyword-based search without needing to maintain lots of heuristics manually. It requires an additional component, a neural encoder, to convert text into vectors. 



[**Tutorial 1 - Qdrant for Complete Beginners**](../../tutorials/search-beginners)
In the open-source world, you pay for the resources you use, not the number of different databases you run.

Resources depend more on the optimal solution for each use case.

As a result, running a dedicated vector search engine can be even cheaper, as it allows optimization specifically for vector search use cases.



For instance, Qdrant implements a number of [quantization techniques](documentation/guides/quantization/) that can significantly reduce the memory footprint of embeddings.
</tr>

</tbody>

</table>





# A wrap up



Each search scenario requires a specialized tool to achieve the best results possible. Still, combining multiple tools with 

minimal overhead is possible to improve the search precision even further. Introducing vector search into an existing search 

stack doesn't need to be a revolution but just one small step at a time. 



You'll never cover all the possible queries with a list of synonyms, so a full-text search may not find all the relevant
are able to benefit the advantages of both methods.



Again, it's worth mentioning that with the 3rd experiment, with cross-encoder reranking, Qdrant returned more than 48.12% of 

the relevant items and BM25 around 66.66%.



## Some anecdotal observations



None of the algorithms works better in all the cases. There might be some specific queries in which keyword-based search

will be a winner and the other way around. The table shows some interesting examples we could find in WANDS dataset\n\nAnswer:",What are some advantages of vector search over keyword-based search?,"Vector search has clear advantages over keyword-based search in various scenarios as outlined in the document chunk. Some of these advantages include:

1. Multi-lingual & multi-modal search: Vector search is effective in handling searches across multiple languages and modalities.
2. For short texts with typos and ambiguous content-dependent meanings: Vector search excels in situations where the search queries are short, contain typos, or have ambiguous meanings.
3. Specialized domains with tuned encoder models: Vector search is beneficial in specialized domains where encoder models can be fine-tuned for better search results.
4. Document-as-a-Query similarity search: Vector search allows for similarity searches where the entire document can be used as a query to find similar documents.

While vector search offers these advantages, it is important to note that keyword-based search still has its relevance in certain cases. For example, in out-of-domain search.","[""preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has\n\nsome clear advantages in the following scenarios:\n\n\n\n- 🌍 Multi-lingual & multi-modal search\n\n- 🤔 For short texts with typos and ambiguous content-dependent meanings\n\n- 👨\u200d🔬 Specialized domains with tuned encoder models\n\n- 📄 Document-as-a-Query similarity search\n\n\n\nIt doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method\n\nmight be useful:"", '## Next Steps\n\n\n\nVector search is an exciting alternative to sparse methods. It solves the issues we had with the keyword-based search without needing to maintain lots of heuristics manually. It requires an additional component, a neural encoder, to convert text into vectors. \n\n\n\n[**Tutorial 1 - Qdrant for Complete Beginners**](../../tutorials/search-beginners)', 'In the open-source world, you pay for the resources you use, not the number of different databases you run.\n\nResources depend more on the optimal solution for each use case.\n\nAs a result, running a dedicated vector search engine can be even cheaper, as it allows optimization specifically for vector search use cases.\n\n\n\nFor instance, Qdrant implements a number of [quantization techniques](documentation/guides/quantization/) that can significantly reduce the memory footprint of embeddings.', ""</tr>\n\n   </tbody>\n\n</table>\n\n\n\n\n\n# A wrap up\n\n\n\nEach search scenario requires a specialized tool to achieve the best results possible. Still, combining multiple tools with \n\nminimal overhead is possible to improve the search precision even further. Introducing vector search into an existing search \n\nstack doesn't need to be a revolution but just one small step at a time. \n\n\n\nYou'll never cover all the possible queries with a list of synonyms, so a full-text search may not find all the relevant"", ""are able to benefit the advantages of both methods.\n\n\n\nAgain, it's worth mentioning that with the 3rd experiment, with cross-encoder reranking, Qdrant returned more than 48.12% of \n\nthe relevant items and BM25 around 66.66%.\n\n\n\n## Some anecdotal observations\n\n\n\nNone of the algorithms works better in all the cases. There might be some specific queries in which keyword-based search\n\nwill be a winner and the other way around. The table shows some interesting examples we could find in WANDS dataset""]","preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has

some clear advantages in the following scenarios:



- 🌍 Multi-lingual & multi-modal search

- 🤔 For short texts with typos and ambiguous content-dependent meanings

- 👨‍🔬 Specialized domains with tuned encoder models

- 📄 Document-as-a-Query similarity search



It doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method

might be useful:
## Next Steps



Vector search is an exciting alternative to sparse methods. It solves the issues we had with the keyword-based search without needing to maintain lots of heuristics manually. It requires an additional component, a neural encoder, to convert text into vectors. 



[**Tutorial 1 - Qdrant for Complete Beginners**](../../tutorials/search-beginners)
In the open-source world, you pay for the resources you use, not the number of different databases you run.

Resources depend more on the optimal solution for each use case.

As a result, running a dedicated vector search engine can be even cheaper, as it allows optimization specifically for vector search use cases.



For instance, Qdrant implements a number of [quantization techniques](documentation/guides/quantization/) that can significantly reduce the memory footprint of embeddings.
</tr>

   </tbody>

</table>





# A wrap up



Each search scenario requires a specialized tool to achieve the best results possible. Still, combining multiple tools with 

minimal overhead is possible to improve the search precision even further. Introducing vector search into an existing search 

stack doesn't need to be a revolution but just one small step at a time. 



You'll never cover all the possible queries with a list of synonyms, so a full-text search may not find all the relevant
are able to benefit the advantages of both methods.



Again, it's worth mentioning that with the 3rd experiment, with cross-encoder reranking, Qdrant returned more than 48.12% of 

the relevant items and BM25 around 66.66%.



## Some anecdotal observations



None of the algorithms works better in all the cases. There might be some specific queries in which keyword-based search

will be a winner and the other way around. The table shows some interesting examples we could find in WANDS dataset",0.64375,0.7103448275862069,0.6754098360655737,0.36477987421383645,0.4027777777777778,0.3828382838283828,0.46875,0.5172413793103449,0.4918032786885246,0.58125,0.6413793103448275,0.6098360655737706,0.9602095484733582,0.737648606300354,0.7485475540161133,0.7430580854415894,0.7454397943802178,0.6206113467134335,153,1.1007194244604317,0.4621321748286394
True,,2531.092,2c03e435,What is the purpose of the DocArray library and how does it relate to the Qdrant engine?,"The DocArray library is designed to define a structure for unstructured data, making it easier to process a collection of documents that may include various data types such as audio, video, and text. This library simplifies the handling of these diverse types of data within a single framework. On the other hand, the Qdrant engine focuses on enabling the scaling of vector search and storage capabilities. By integrating the DocArray library with the Qdrant engine, users can benefit from efficient processing of unstructured data and enhanced scalability for vector search operations. This integration streamlines the handling of different data types while also optimizing the performance of vector search and storage functions.","The purpose of the DocArray library is to define a structure for unstructured data and simplify the processing of a collection of documents, including various data types such as audio, video, text, and more. It provides a Pythonic API for deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer data.

The integration of the Qdrant engine with the DocArray library allows for scalable vector search and storage capabilities. Qdrant serves as a high-performance document store within the DocArray framework, enabling users to experience the convenience of a Pythonic API combined with the performance of the Qdrant engine for their data processing needs.","DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,

including audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.



Read more about the integration by this [link](/documentation/install/#docarray)
---

title: DocArray

weight: 300

aliases: [ ../integrations/docarray/ ]

---



# DocArray

You can use Qdrant natively in DocArray, where Qdrant serves as a high-performance document store to enable scalable vector search.



DocArray is a library from Jina AI for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc.

It allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API.
date: 2022-03-15T15:00:00+03:00

author: Alyona Kavyerina

featured: false

author_link: https://medium.com/@alyona.kavyerina

tags:

  - jina integration

  - docarray

categories:

  - News

---

We are happy to announce that [Jina.AI](https://jina.ai/) integrates Qdrant engine as a storage backend to their [DocArray](https://docarray.jina.ai/) solution.



Now you can experience the convenience of Pythonic API and Rust performance in a single workflow.
WITH ENGINE = ""qdrant"",

PARAMETERS = {

    ""location"": "":memory:"",

    ""collection_config"": {

        ""size"": 386,

        ""distance"": ""Cosine""

    }

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

    texts=[""Qdrant is the best vector database!""],

    model=""nomic-embed-text-v1"",

    task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

    collection_name=""my-collection"",

    points=models.Batch(

        ids=[1],

        vectors=output[""embeddings""],

    ),

)

```



To query the collection, set the `task_type` to `search_query`:



```python","['DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,\n\nincluding audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.\n\n\n\nRead more about the integration by this [link](/documentation/install/#docarray)', '---\n\ntitle: DocArray\n\nweight: 300\n\naliases: [ ../integrations/docarray/ ]\n\n---\n\n\n\n# DocArray\n\nYou can use Qdrant natively in DocArray, where Qdrant serves as a high-performance document store to enable scalable vector search.\n\n\n\nDocArray is a library from Jina AI for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc.\n\nIt allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API.', 'date: 2022-03-15T15:00:00+03:00\n\nauthor: Alyona Kavyerina\n\nfeatured: false\n\nauthor_link: https://medium.com/@alyona.kavyerina\n\ntags:\n\n  - jina integration\n\n  - docarray\n\ncategories:\n\n  - News\n\n---\n\nWe are happy to announce that [Jina.AI](https://jina.ai/) integrates Qdrant engine as a storage backend to their [DocArray](https://docarray.jina.ai/) solution.\n\n\n\nNow you can experience the convenience of Pythonic API and Rust performance in a single workflow.', 'WITH ENGINE = ""qdrant"",\n\nPARAMETERS = {\n\n    ""location"": "":memory:"",\n\n    ""collection_config"": {\n\n        ""size"": 386,\n\n        ""distance"": ""Cosine""\n\n    }\n\n}\n\n```\n\n\n\nThe available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).\n\n\n\n## Creating a new table', '```python\n\nfrom qdrant_client import QdrantClient, models\n\nfrom nomic import embed\n\n\n\noutput = embed.text(\n\n    texts=[""Qdrant is the best vector database!""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_document"",\n\n)\n\n\n\nqdrant_client = QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""my-collection"",\n\n    points=models.Batch(\n\n        ids=[1],\n\n        vectors=output[""embeddings""],\n\n    ),\n\n)\n\n```\n\n\n\nTo query the collection, set the `task_type` to `search_query`:\n\n\n\n```python']","Question: What is the purpose of the DocArray library and how does it relate to the Qdrant engine?\n\nContext: DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,

including audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.



Read more about the integration by this [link](/documentation/install/#docarray)
---

title: DocArray

weight: 300

aliases: [ ../integrations/docarray/ ]

---



# DocArray

You can use Qdrant natively in DocArray, where Qdrant serves as a high-performance document store to enable scalable vector search.



DocArray is a library from Jina AI for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc.

It allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API.
date: 2022-03-15T15:00:00+03:00

author: Alyona Kavyerina

featured: false

author_link: https://medium.com/@alyona.kavyerina

tags:

- jina integration

- docarray

categories:

- News

---

We are happy to announce that [Jina.AI](https://jina.ai/) integrates Qdrant engine as a storage backend to their [DocArray](https://docarray.jina.ai/) solution.



Now you can experience the convenience of Pythonic API and Rust performance in a single workflow.
WITH ENGINE = ""qdrant"",

PARAMETERS = {

""location"": "":memory:"",

""collection_config"": {

""size"": 386,

""distance"": ""Cosine""

}

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

texts=[""Qdrant is the best vector database!""],

model=""nomic-embed-text-v1"",

task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

collection_name=""my-collection"",

points=models.Batch(

ids=[1],

vectors=output[""embeddings""],

),

)

```



To query the collection, set the `task_type` to `search_query`:



```python\n\nAnswer:",What is the purpose of the DocArray library and how does it relate to the Qdrant engine?,"The DocArray library is designed to define a structure for unstructured data, making it easier to process a collection of documents that may include various data types such as audio, video, and text. This library simplifies the handling of these diverse types of data within a single framework. On the other hand, the Qdrant engine focuses on enabling the scaling of vector search and storage capabilities. By integrating the DocArray library with the Qdrant engine, users can benefit from efficient processing of unstructured data and enhanced scalability for vector search operations. This integration streamlines the handling of different data types while also optimizing the performance of vector search and storage functions.","['DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,\n\nincluding audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.\n\n\n\nRead more about the integration by this [link](/documentation/install/#docarray)', '---\n\ntitle: DocArray\n\nweight: 300\n\naliases: [ ../integrations/docarray/ ]\n\n---\n\n\n\n# DocArray\n\nYou can use Qdrant natively in DocArray, where Qdrant serves as a high-performance document store to enable scalable vector search.\n\n\n\nDocArray is a library from Jina AI for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc.\n\nIt allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API.', 'date: 2022-03-15T15:00:00+03:00\n\nauthor: Alyona Kavyerina\n\nfeatured: false\n\nauthor_link: https://medium.com/@alyona.kavyerina\n\ntags:\n\n  - jina integration\n\n  - docarray\n\ncategories:\n\n  - News\n\n---\n\nWe are happy to announce that [Jina.AI](https://jina.ai/) integrates Qdrant engine as a storage backend to their [DocArray](https://docarray.jina.ai/) solution.\n\n\n\nNow you can experience the convenience of Pythonic API and Rust performance in a single workflow.', 'WITH ENGINE = ""qdrant"",\n\nPARAMETERS = {\n\n    ""location"": "":memory:"",\n\n    ""collection_config"": {\n\n        ""size"": 386,\n\n        ""distance"": ""Cosine""\n\n    }\n\n}\n\n```\n\n\n\nThe available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).\n\n\n\n## Creating a new table', '```python\n\nfrom qdrant_client import QdrantClient, models\n\nfrom nomic import embed\n\n\n\noutput = embed.text(\n\n    texts=[""Qdrant is the best vector database!""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_document"",\n\n)\n\n\n\nqdrant_client = QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""my-collection"",\n\n    points=models.Batch(\n\n        ids=[1],\n\n        vectors=output[""embeddings""],\n\n    ),\n\n)\n\n```\n\n\n\nTo query the collection, set the `task_type` to `search_query`:\n\n\n\n```python']","DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,

including audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.



Read more about the integration by this [link](/documentation/install/#docarray)
---

title: DocArray

weight: 300

aliases: [ ../integrations/docarray/ ]

---



# DocArray

You can use Qdrant natively in DocArray, where Qdrant serves as a high-performance document store to enable scalable vector search.



DocArray is a library from Jina AI for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc.

It allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API.
date: 2022-03-15T15:00:00+03:00

author: Alyona Kavyerina

featured: false

author_link: https://medium.com/@alyona.kavyerina

tags:

  - jina integration

  - docarray

categories:

  - News

---

We are happy to announce that [Jina.AI](https://jina.ai/) integrates Qdrant engine as a storage backend to their [DocArray](https://docarray.jina.ai/) solution.



Now you can experience the convenience of Pythonic API and Rust performance in a single workflow.
WITH ENGINE = ""qdrant"",

PARAMETERS = {

    ""location"": "":memory:"",

    ""collection_config"": {

        ""size"": 386,

        ""distance"": ""Cosine""

    }

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

    texts=[""Qdrant is the best vector database!""],

    model=""nomic-embed-text-v1"",

    task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

    collection_name=""my-collection"",

    points=models.Batch(

        ids=[1],

        vectors=output[""embeddings""],

    ),

)

```



To query the collection, set the `task_type` to `search_query`:



```python",0.6373626373626373,0.5858585858585859,0.6105263157894737,0.3,0.2755102040816326,0.28723404255319146,0.3956043956043956,0.36363636363636365,0.3789473684210526,0.5714285714285714,0.5252525252525253,0.5473684210526316,0.8446837663650513,0.6989571452140808,0.6951662302017212,0.6970565319061279,0.9821431557647884,0.9644495218293742,104,0.9369369369369369,0.5632007755015705
True,,2950.5849999999996,d07958f7,What are the search-time parameters that can be used to tune the search accuracy and speed?,"The search-time parameters that can be used to tune the search accuracy and speed are ""hnsw_ef"" and ""exact"". The ""hnsw_ef"" parameter is set to 128, which controls the search accuracy by specifying the number of neighbors to inspect during the search process. The ""exact"" parameter is set to false, indicating that an approximate search method is used for faster retrieval of results. These parameters are essential for optimizing the search process in the context of the QdrantClient for efficient retrieval of relevant data points.","Search-time parameters that can be used to tune the search accuracy and speed include:

1. `exact`: Setting this parameter to `true` will perform an exact search, which may be slower but more accurate. This can be useful for comparing results with different parameters like `hnsw_ef` values against the ground truth.

2. Latency vs Throughput: Two main approaches to measure the speed of search are latency of the request (time from request submission to response received) and throughput (number of requests per second the system can handle).

3. Disk Speed: The speed of the disk can significantly affect search speed. Measuring disk speed with tools like `fio` can help understand how it impacts search performance.

By adjusting these parameters and understanding their impact on search accuracy and speed, one can optimize the search process for better performance.","collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },

	optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 },

	quantizationConfig: new QuantizationConfig

	{

		Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }

	}

);

```



There are also some search-time parameters you can use to tune the search accuracy and speed:



```http

POST /collections/{collection_name}/points/search

{
- `exact` - if set to `true`, will perform exact search, which will be slower, but more accurate. You can use it to compare results of the search with different `hnsw_ef` values versus the ground truth.



## Latency vs Throughput



- There are two main approaches to measure the speed of search:

  - latency of the request - the time from the moment request is submitted to the moment a response is received

  - throughput - the number of requests per second the system can handle
. And that's how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself.
We can serve the search requests with 135mb of RAM, but the speed of the requests makes it impossible to use the system in production.



Let's see how we can improve the speed.





## How to speed up the search





<!--

We need to look into disk parameters and see how they affect the search speed.



Let's measure the disk speed with `fio`:



```

fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=fiotest --filename=testfio --bs=4k --iodepth=64 --size=8G --readwrite=randread

```
<th colspan=""2"">ef = 256</th>

         <th colspan=""2"">ef = 512</th>

      </tr>

      <tr>

         <th></th>

         <th><small>Upload and indexing time</small></th>

         <th><small>Mean search precision</small></th>

         <th><small>Mean search time</small></th>

         <th><small>Mean search precision</small></th>

         <th><small>Mean search time</small></th>

         <th><small>Mean search precision</small></th>

         <th><small>Mean search time</small></th>","['collectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },\n\n\toptimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 },\n\n\tquantizationConfig: new QuantizationConfig\n\n\t{\n\n\t\tScalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }\n\n\t}\n\n);\n\n```\n\n\n\nThere are also some search-time parameters you can use to tune the search accuracy and speed:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{', '- `exact` - if set to `true`, will perform exact search, which will be slower, but more accurate. You can use it to compare results of the search with different `hnsw_ef` values versus the ground truth.\n\n\n\n## Latency vs Throughput\n\n\n\n- There are two main approaches to measure the speed of search:\n\n  - latency of the request - the time from the moment request is submitted to the moment a response is received\n\n  - throughput - the number of requests per second the system can handle', "". And that's how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself."", ""We can serve the search requests with 135mb of RAM, but the speed of the requests makes it impossible to use the system in production.\n\n\n\nLet's see how we can improve the speed.\n\n\n\n\n\n## How to speed up the search\n\n\n\n\n\n<!--\n\nWe need to look into disk parameters and see how they affect the search speed.\n\n\n\nLet's measure the disk speed with `fio`:\n\n\n\n```\n\nfio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=fiotest --filename=testfio --bs=4k --iodepth=64 --size=8G --readwrite=randread\n\n```"", '<th colspan=""2"">ef = 256</th>\n\n         <th colspan=""2"">ef = 512</th>\n\n      </tr>\n\n      <tr>\n\n         <th></th>\n\n         <th><small>Upload and indexing time</small></th>\n\n         <th><small>Mean search precision</small></th>\n\n         <th><small>Mean search time</small></th>\n\n         <th><small>Mean search precision</small></th>\n\n         <th><small>Mean search time</small></th>\n\n         <th><small>Mean search precision</small></th>\n\n         <th><small>Mean search time</small></th>']","Question: What are the search-time parameters that can be used to tune the search accuracy and speed?\n\nContext: collectionName: ""{collection_name}"",

vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },

optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 },

quantizationConfig: new QuantizationConfig

{

Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }

}

);

```



There are also some search-time parameters you can use to tune the search accuracy and speed:



```http

POST /collections/{collection_name}/points/search

{
- `exact` - if set to `true`, will perform exact search, which will be slower, but more accurate. You can use it to compare results of the search with different `hnsw_ef` values versus the ground truth.



## Latency vs Throughput



- There are two main approaches to measure the speed of search:

- latency of the request - the time from the moment request is submitted to the moment a response is received

- throughput - the number of requests per second the system can handle
. And that's how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself.
We can serve the search requests with 135mb of RAM, but the speed of the requests makes it impossible to use the system in production.



Let's see how we can improve the speed.





## How to speed up the search





<!--

We need to look into disk parameters and see how they affect the search speed.



Let's measure the disk speed with `fio`:



```

fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=fiotest --filename=testfio --bs=4k --iodepth=64 --size=8G --readwrite=randread

```
<th colspan=""2"">ef = 256</th>

<th colspan=""2"">ef = 512</th>

</tr>

<tr>

<th></th>

<th><small>Upload and indexing time</small></th>

<th><small>Mean search precision</small></th>

<th><small>Mean search time</small></th>

<th><small>Mean search precision</small></th>

<th><small>Mean search time</small></th>

<th><small>Mean search precision</small></th>

<th><small>Mean search time</small></th>\n\nAnswer:",What are the search-time parameters that can be used to tune the search accuracy and speed?,"The search-time parameters that can be used to tune the search accuracy and speed are ""hnsw_ef"" and ""exact"". The ""hnsw_ef"" parameter is set to 128, which controls the search accuracy by specifying the number of neighbors to inspect during the search process. The ""exact"" parameter is set to false, indicating that an approximate search method is used for faster retrieval of results. These parameters are essential for optimizing the search process in the context of the QdrantClient for efficient retrieval of relevant data points.","['collectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },\n\n\toptimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 },\n\n\tquantizationConfig: new QuantizationConfig\n\n\t{\n\n\t\tScalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }\n\n\t}\n\n);\n\n```\n\n\n\nThere are also some search-time parameters you can use to tune the search accuracy and speed:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{', '- `exact` - if set to `true`, will perform exact search, which will be slower, but more accurate. You can use it to compare results of the search with different `hnsw_ef` values versus the ground truth.\n\n\n\n## Latency vs Throughput\n\n\n\n- There are two main approaches to measure the speed of search:\n\n  - latency of the request - the time from the moment request is submitted to the moment a response is received\n\n  - throughput - the number of requests per second the system can handle', "". And that's how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself."", ""We can serve the search requests with 135mb of RAM, but the speed of the requests makes it impossible to use the system in production.\n\n\n\nLet's see how we can improve the speed.\n\n\n\n\n\n## How to speed up the search\n\n\n\n\n\n<!--\n\nWe need to look into disk parameters and see how they affect the search speed.\n\n\n\nLet's measure the disk speed with `fio`:\n\n\n\n```\n\nfio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=fiotest --filename=testfio --bs=4k --iodepth=64 --size=8G --readwrite=randread\n\n```"", '<th colspan=""2"">ef = 256</th>\n\n         <th colspan=""2"">ef = 512</th>\n\n      </tr>\n\n      <tr>\n\n         <th></th>\n\n         <th><small>Upload and indexing time</small></th>\n\n         <th><small>Mean search precision</small></th>\n\n         <th><small>Mean search time</small></th>\n\n         <th><small>Mean search precision</small></th>\n\n         <th><small>Mean search time</small></th>\n\n         <th><small>Mean search precision</small></th>\n\n         <th><small>Mean search time</small></th>']","collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },

	optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 },

	quantizationConfig: new QuantizationConfig

	{

		Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }

	}

);

```



There are also some search-time parameters you can use to tune the search accuracy and speed:



```http

POST /collections/{collection_name}/points/search

{
- `exact` - if set to `true`, will perform exact search, which will be slower, but more accurate. You can use it to compare results of the search with different `hnsw_ef` values versus the ground truth.



## Latency vs Throughput



- There are two main approaches to measure the speed of search:

  - latency of the request - the time from the moment request is submitted to the moment a response is received

  - throughput - the number of requests per second the system can handle
. And that's how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself.
We can serve the search requests with 135mb of RAM, but the speed of the requests makes it impossible to use the system in production.



Let's see how we can improve the speed.





## How to speed up the search





<!--

We need to look into disk parameters and see how they affect the search speed.



Let's measure the disk speed with `fio`:



```

fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=fiotest --filename=testfio --bs=4k --iodepth=64 --size=8G --readwrite=randread

```
<th colspan=""2"">ef = 256</th>

         <th colspan=""2"">ef = 512</th>

      </tr>

      <tr>

         <th></th>

         <th><small>Upload and indexing time</small></th>

         <th><small>Mean search precision</small></th>

         <th><small>Mean search time</small></th>

         <th><small>Mean search precision</small></th>

         <th><small>Mean search time</small></th>

         <th><small>Mean search precision</small></th>

         <th><small>Mean search time</small></th>",0.3488372093023256,0.5921052631578947,0.43902439024390244,0.1484375,0.25333333333333335,0.18719211822660098,0.21705426356589147,0.3684210526315789,0.2731707317073171,0.31007751937984496,0.5263157894736842,0.3902439024390244,0.6851239204406738,0.6241212487220764,0.6684319376945496,0.6455170512199402,0.9138197423973387,0.508799294475466,136,1.619047619047619,0.41761860194532097
True,,6545.12,cb7c7d42,What is the significance of using named vectors in a collection within the Qdrant system?,"Named vectors in a collection within the Qdrant system allow for the inclusion of multiple vectors in a single point, with each vector having its own dimensionality and metric requirements. This feature enables more flexibility in organizing and structuring data within a collection, as different vectors can represent distinct aspects or features of the data points. By utilizing named vectors, users can better tailor the representation of their data to suit specific analysis or search requirements, enhancing the overall efficiency and effectiveness of the system.","Using named vectors in a collection within the Qdrant system is significant for several reasons:

1. **Data Consistency**: By using named vectors, it becomes easier to maintain consistency in the data being stored and retrieved within the system. Each vector having a unique name helps in organizing and structuring the data effectively.

2. **Ease of Access**: Named vectors provide a way to easily identify and reference specific vectors within a collection. This can be especially useful when dealing with multiple vectors for each data point, as it allows for precise retrieval and manipulation of data.

3. **Compatibility with Existing Codebases**: The use of named vectors aligns with the existing API structure in Qdrant, making it seamless to integrate sparse vectors into codebases that are already utilizing named vectors. This ensures a smoother transition and easier adoption of new functionalities.

4. **Advanced Features Support**: Named vectors enable the application of payload filters, shard keys, and other advanced features within Qdrant. This enhances the versatility and capabilities of the system, allowing for more sophisticated data management and retrieval processes.

5. **Simplification of Upsert Operations**: With named vectors, the indices and values of vectors do not need to be sorted before upsert operations. Qdrant handles the sorting process when persisting the index, streamlining the data input process for users.

In summary, using named vectors in a collection within the Qdrant system enhances data organization, accessibility, compatibility, and efficiency, thereby contributing to a more robust and effective data management framework.","Even with such a system, Qdrant ensures data consistency.



[*Available as of v0.10.0*](#create-vector-name)



If the collection was created with multiple vectors, each vector data can be provided using the vector's name:



```http

PUT /collections/{collection_name}/points

{

    ""points"": [

        {

            ""id"": 1,

            ""vector"": {

                ""image"": [0.9, 0.1, 0.1, 0.2],

                ""text"": [0.4, 0.7, 0.1, 0.8, 0.1, 0.1, 0.9, 0.2]

            }

        },

        {
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
```



## Creating a collection



Adding new functionalities typically requires making some changes to the interfaces, so no surprise we had to do it to enable the multiple vectors support. Currently, if you want to create a collection, you need to define the configuration of all the vectors you want to store for each object. Each vector type has its own name and the distance function used to measure how far the points are.



```python

from qdrant_client import QdrantClient
. So collection are where all your vector are stored in Qdrant, it's like a table in SQL or an index in elasticsearch. And so we decided to split our collection between three nodes.
PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""hnsw_config"": {

        ""on_disk"": true

    }

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),","['Even with such a system, Qdrant ensures data consistency.\n\n\n\n[*Available as of v0.10.0*](#create-vector-name)\n\n\n\nIf the collection was created with multiple vectors, each vector data can be provided using the vector\'s name:\n\n\n\n```http\n\nPUT /collections/{collection_name}/points\n\n{\n\n    ""points"": [\n\n        {\n\n            ""id"": 1,\n\n            ""vector"": {\n\n                ""image"": [0.9, 0.1, 0.1, 0.2],\n\n                ""text"": [0.4, 0.7, 0.1, 0.8, 0.1, 0.1, 0.9, 0.2]\n\n            }\n\n        },\n\n        {', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", '```\n\n\n\n## Creating a collection\n\n\n\nAdding new functionalities typically requires making some changes to the interfaces, so no surprise we had to do it to enable the multiple vectors support. Currently, if you want to create a collection, you need to define the configuration of all the vectors you want to store for each object. Each vector type has its own name and the distance function used to measure how far the points are.\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient', "". So collection are where all your vector are stored in Qdrant, it's like a table in SQL or an index in elasticsearch. And so we decided to split our collection between three nodes."", 'PUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""hnsw_config"": {\n\n        ""on_disk"": true\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),']","Question: What is the significance of using named vectors in a collection within the Qdrant system?\n\nContext: Even with such a system, Qdrant ensures data consistency.



[*Available as of v0.10.0*](#create-vector-name)



If the collection was created with multiple vectors, each vector data can be provided using the vector's name:



```http

PUT /collections/{collection_name}/points

{

""points"": [

{

""id"": 1,

""vector"": {

""image"": [0.9, 0.1, 0.1, 0.2],

""text"": [0.4, 0.7, 0.1, 0.8, 0.1, 0.1, 0.9, 0.2]

}

},

{
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
```



## Creating a collection



Adding new functionalities typically requires making some changes to the interfaces, so no surprise we had to do it to enable the multiple vectors support. Currently, if you want to create a collection, you need to define the configuration of all the vectors you want to store for each object. Each vector type has its own name and the distance function used to measure how far the points are.



```python

from qdrant_client import QdrantClient
. So collection are where all your vector are stored in Qdrant, it's like a table in SQL or an index in elasticsearch. And so we decided to split our collection between three nodes.
PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""optimizers_config"": {

""memmap_threshold"": 20000

},

""hnsw_config"": {

""on_disk"": true

}

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\nAnswer:",What is the significance of using named vectors in a collection within the Qdrant system?,"Named vectors in a collection within the Qdrant system allow for the inclusion of multiple vectors in a single point, with each vector having its own dimensionality and metric requirements. This feature enables more flexibility in organizing and structuring data within a collection, as different vectors can represent distinct aspects or features of the data points. By utilizing named vectors, users can better tailor the representation of their data to suit specific analysis or search requirements, enhancing the overall efficiency and effectiveness of the system.","['Even with such a system, Qdrant ensures data consistency.\n\n\n\n[*Available as of v0.10.0*](#create-vector-name)\n\n\n\nIf the collection was created with multiple vectors, each vector data can be provided using the vector\'s name:\n\n\n\n```http\n\nPUT /collections/{collection_name}/points\n\n{\n\n    ""points"": [\n\n        {\n\n            ""id"": 1,\n\n            ""vector"": {\n\n                ""image"": [0.9, 0.1, 0.1, 0.2],\n\n                ""text"": [0.4, 0.7, 0.1, 0.8, 0.1, 0.1, 0.9, 0.2]\n\n            }\n\n        },\n\n        {', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", '```\n\n\n\n## Creating a collection\n\n\n\nAdding new functionalities typically requires making some changes to the interfaces, so no surprise we had to do it to enable the multiple vectors support. Currently, if you want to create a collection, you need to define the configuration of all the vectors you want to store for each object. Each vector type has its own name and the distance function used to measure how far the points are.\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient', "". So collection are where all your vector are stored in Qdrant, it's like a table in SQL or an index in elasticsearch. And so we decided to split our collection between three nodes."", 'PUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""hnsw_config"": {\n\n        ""on_disk"": true\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),']","Even with such a system, Qdrant ensures data consistency.



[*Available as of v0.10.0*](#create-vector-name)



If the collection was created with multiple vectors, each vector data can be provided using the vector's name:



```http

PUT /collections/{collection_name}/points

{

    ""points"": [

        {

            ""id"": 1,

            ""vector"": {

                ""image"": [0.9, 0.1, 0.1, 0.2],

                ""text"": [0.4, 0.7, 0.1, 0.8, 0.1, 0.1, 0.9, 0.2]

            }

        },

        {
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
```



## Creating a collection



Adding new functionalities typically requires making some changes to the interfaces, so no surprise we had to do it to enable the multiple vectors support. Currently, if you want to create a collection, you need to define the configuration of all the vectors you want to store for each object. Each vector type has its own name and the distance function used to measure how far the points are.



```python

from qdrant_client import QdrantClient
. So collection are where all your vector are stored in Qdrant, it's like a table in SQL or an index in elasticsearch. And so we decided to split our collection between three nodes.
PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""hnsw_config"": {

        ""on_disk"": true

    }

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),",0.22666666666666666,0.6710526315789473,0.33887043189368765,0.10267857142857142,0.30666666666666664,0.15384615384615383,0.14666666666666667,0.4342105263157895,0.21926910299003324,0.19111111111111112,0.5657894736842105,0.28571428571428575,0.8419443964958191,0.6066244840621948,0.6797276139259338,0.6410987973213196,0.8151099516253453,0.9686941908051571,246,2.8941176470588235,0.2916183006008841
True,,4518.525000000001,a98f2d32,What parameters can be configured in the configuration file to improve performance in HNSW indexing?,"In the context of HNSW indexing, the parameters that can be configured in the configuration file to enhance performance are `m`, `ef_construct`, and `ef`. 

- `m`: This parameter represents the number of edges per node in the index graph. A higher value for `m` leads to more accurate search results but also requires more space. By adjusting `m`, you can control the trade-off between search accuracy and space efficiency.

- `ef_construct`: When building the index, `ef_construct` specifies the number of neighbors to consider. Increasing the value of `ef_construct` improves the accuracy of the search but also increases the time required to build the index. 

- `ef`: This parameter is used when searching for targets. It determines the search range for finding nearest neighbors. By setting an appropriate value for `ef`, you can optimize the search process to balance efficiency and accuracy in finding the nearest neighbors within the specified search range.","To improve performance in HNSW indexing, the following parameters can be configured in the configuration file:

1. Increase the `ef` parameter: By increasing the `ef` parameter, you can increase the precision of the search. This parameter controls the number of neighbors to visit during the search process.

2. Increase the `m` parameter: The `m` parameter determines the number of edges per node in the hierarchical graph of HNSW. Increasing this parameter can also help improve performance.

3. Utilize the `on_disk` option: Setting the `on_disk` parameter to true can be beneficial for scenarios where disk IOPS is a critical factor. This can affect the speed at which search operations are performed.

By tweaking these parameters in the configuration file, you can optimize the performance of HNSW indexing for your specific use case.","hnswConfig: new HnswConfigDiff { OnDisk = true }

);

```



In this scenario you can increase the precision of the search by increasing the `ef` and `m` parameters of the HNSW index, even with limited RAM.



```json

...

""hnsw_config"": {

    ""m"": 64,

    ""ef_construct"": 512,

    ""on_disk"": true

}

...

```



The disk IOPS is a critical factor in this scenario, it will determine how fast you can perform search.
```text

avg(precision@5) = 0.9935999999999995

```



As we can see, the precision of the approximate search vs exact search is pretty high. There are, however, some scenarios when we

need higher precision and can accept higher latency. HNSW is pretty tunable, and we can increase the precision by changing its parameters.

  

## Tweaking the HNSW parameters



HNSW is a hierarchical graph, where each node has a set of links to other nodes. The number of edges per node is called the `m` parameter.
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
queries will be executed on the whole collection, we can also change its configuration, so the HNSW 

graph is not built globally. This is also done due to [performance reasons](/documentation/guides/multiple-partitions/#calibrate-performance).

**You should not be changing these parameters, if you know there will be some global search operations

done on the collection.**



```python

client.update_collection(

    collection_name=""my_collection"",
To implement this approach, you should:



1. Set `payload_m` in the HNSW configuration to a non-zero value, such as 16.

2. Set `m` in hnsw config to 0. This will disable building global index for the whole collection.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""hnsw_config"": {

        ""payload_m"": 16,

        ""m"": 0

    }

}

```



```python

from qdrant_client import QdrantClient, models","['hnswConfig: new HnswConfigDiff { OnDisk = true }\n\n);\n\n```\n\n\n\nIn this scenario you can increase the precision of the search by increasing the `ef` and `m` parameters of the HNSW index, even with limited RAM.\n\n\n\n```json\n\n...\n\n""hnsw_config"": {\n\n    ""m"": 64,\n\n    ""ef_construct"": 512,\n\n    ""on_disk"": true\n\n}\n\n...\n\n```\n\n\n\nThe disk IOPS is a critical factor in this scenario, it will determine how fast you can perform search.', '```text\n\navg(precision@5) = 0.9935999999999995\n\n```\n\n\n\nAs we can see, the precision of the approximate search vs exact search is pretty high. There are, however, some scenarios when we\n\nneed higher precision and can accept higher latency. HNSW is pretty tunable, and we can increase the precision by changing its parameters.\n\n  \n\n## Tweaking the HNSW parameters\n\n\n\nHNSW is a hierarchical graph, where each node has a set of links to other nodes. The number of edges per node is called the `m` parameter.', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', 'queries will be executed on the whole collection, we can also change its configuration, so the HNSW \n\ngraph is not built globally. This is also done due to [performance reasons](/documentation/guides/multiple-partitions/#calibrate-performance).\n\n**You should not be changing these parameters, if you know there will be some global search operations\n\ndone on the collection.**\n\n\n\n```python\n\nclient.update_collection(\n\n    collection_name=""my_collection"",', 'To implement this approach, you should:\n\n\n\n1. Set `payload_m` in the HNSW configuration to a non-zero value, such as 16.\n\n2. Set `m` in hnsw config to 0. This will disable building global index for the whole collection.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""hnsw_config"": {\n\n        ""payload_m"": 16,\n\n        ""m"": 0\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models']","Question: What parameters can be configured in the configuration file to improve performance in HNSW indexing?\n\nContext: hnswConfig: new HnswConfigDiff { OnDisk = true }

);

```



In this scenario you can increase the precision of the search by increasing the `ef` and `m` parameters of the HNSW index, even with limited RAM.



```json

...

""hnsw_config"": {

""m"": 64,

""ef_construct"": 512,

""on_disk"": true

}

...

```



The disk IOPS is a critical factor in this scenario, it will determine how fast you can perform search.
```text

avg(precision@5) = 0.9935999999999995

```



As we can see, the precision of the approximate search vs exact search is pretty high. There are, however, some scenarios when we

need higher precision and can accept higher latency. HNSW is pretty tunable, and we can increase the precision by changing its parameters.



## Tweaking the HNSW parameters



HNSW is a hierarchical graph, where each node has a set of links to other nodes. The number of edges per node is called the `m` parameter.
""on_disk_payload"": false

},

""hnsw_config"": {

""m"": 16,

""ef_construct"": 100,

""full_scan_threshold"": 10000,

""max_indexing_threads"": 0

},

""optimizer_config"": {

""deleted_threshold"": 0.2,

""vacuum_min_vector_number"": 1000,

""default_segment_number"": 0,

""max_segment_size"": null,
queries will be executed on the whole collection, we can also change its configuration, so the HNSW 

graph is not built globally. This is also done due to [performance reasons](/documentation/guides/multiple-partitions/#calibrate-performance).

**You should not be changing these parameters, if you know there will be some global search operations

done on the collection.**



```python

client.update_collection(

collection_name=""my_collection"",
To implement this approach, you should:



1. Set `payload_m` in the HNSW configuration to a non-zero value, such as 16.

2. Set `m` in hnsw config to 0. This will disable building global index for the whole collection.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""hnsw_config"": {

""payload_m"": 16,

""m"": 0

}

}

```



```python

from qdrant_client import QdrantClient, models\n\nAnswer:",What parameters can be configured in the configuration file to improve performance in HNSW indexing?,"In the context of HNSW indexing, the parameters that can be configured in the configuration file to enhance performance are `m`, `ef_construct`, and `ef`. 

- `m`: This parameter represents the number of edges per node in the index graph. A higher value for `m` leads to more accurate search results but also requires more space. By adjusting `m`, you can control the trade-off between search accuracy and space efficiency.

- `ef_construct`: When building the index, `ef_construct` specifies the number of neighbors to consider. Increasing the value of `ef_construct` improves the accuracy of the search but also increases the time required to build the index. 

- `ef`: This parameter is used when searching for targets. It determines the search range for finding nearest neighbors. By setting an appropriate value for `ef`, you can optimize the search process to balance efficiency and accuracy in finding the nearest neighbors within the specified search range.","['hnswConfig: new HnswConfigDiff { OnDisk = true }\n\n);\n\n```\n\n\n\nIn this scenario you can increase the precision of the search by increasing the `ef` and `m` parameters of the HNSW index, even with limited RAM.\n\n\n\n```json\n\n...\n\n""hnsw_config"": {\n\n    ""m"": 64,\n\n    ""ef_construct"": 512,\n\n    ""on_disk"": true\n\n}\n\n...\n\n```\n\n\n\nThe disk IOPS is a critical factor in this scenario, it will determine how fast you can perform search.', '```text\n\navg(precision@5) = 0.9935999999999995\n\n```\n\n\n\nAs we can see, the precision of the approximate search vs exact search is pretty high. There are, however, some scenarios when we\n\nneed higher precision and can accept higher latency. HNSW is pretty tunable, and we can increase the precision by changing its parameters.\n\n  \n\n## Tweaking the HNSW parameters\n\n\n\nHNSW is a hierarchical graph, where each node has a set of links to other nodes. The number of edges per node is called the `m` parameter.', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', 'queries will be executed on the whole collection, we can also change its configuration, so the HNSW \n\ngraph is not built globally. This is also done due to [performance reasons](/documentation/guides/multiple-partitions/#calibrate-performance).\n\n**You should not be changing these parameters, if you know there will be some global search operations\n\ndone on the collection.**\n\n\n\n```python\n\nclient.update_collection(\n\n    collection_name=""my_collection"",', 'To implement this approach, you should:\n\n\n\n1. Set `payload_m` in the HNSW configuration to a non-zero value, such as 16.\n\n2. Set `m` in hnsw config to 0. This will disable building global index for the whole collection.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""hnsw_config"": {\n\n        ""payload_m"": 16,\n\n        ""m"": 0\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models']","hnswConfig: new HnswConfigDiff { OnDisk = true }

);

```



In this scenario you can increase the precision of the search by increasing the `ef` and `m` parameters of the HNSW index, even with limited RAM.



```json

...

""hnsw_config"": {

    ""m"": 64,

    ""ef_construct"": 512,

    ""on_disk"": true

}

...

```



The disk IOPS is a critical factor in this scenario, it will determine how fast you can perform search.
```text

avg(precision@5) = 0.9935999999999995

```



As we can see, the precision of the approximate search vs exact search is pretty high. There are, however, some scenarios when we

need higher precision and can accept higher latency. HNSW is pretty tunable, and we can increase the precision by changing its parameters.

  

## Tweaking the HNSW parameters



HNSW is a hierarchical graph, where each node has a set of links to other nodes. The number of edges per node is called the `m` parameter.
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
queries will be executed on the whole collection, we can also change its configuration, so the HNSW 

graph is not built globally. This is also done due to [performance reasons](/documentation/guides/multiple-partitions/#calibrate-performance).

**You should not be changing these parameters, if you know there will be some global search operations

done on the collection.**



```python

client.update_collection(

    collection_name=""my_collection"",
To implement this approach, you should:



1. Set `payload_m` in the HNSW configuration to a non-zero value, such as 16.

2. Set `m` in hnsw config to 0. This will disable building global index for the whole collection.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""hnsw_config"": {

        ""payload_m"": 16,

        ""m"": 0

    }

}

```



```python

from qdrant_client import QdrantClient, models",0.5217391304347826,0.44776119402985076,0.4819277108433735,0.19298245614035087,0.16541353383458646,0.17813765182186234,0.2782608695652174,0.23880597014925373,0.2570281124497992,0.5043478260869565,0.43283582089552236,0.465863453815261,0.8393207788467407,0.6509032249450684,0.6337003111839294,0.6421865820884705,0.7707783414849213,0.6781076088858147,131,0.8851351351351351,0.42299127558620636
True,,5261.888,4dd21f0d,How does product quantization differ from scalar quantization in terms of compression and computational efficiency?,"Product quantization involves dividing data into chunks and quantizing each segment individually, approximating each chunk with a centroid index that represents the original vector component. The positions of these centroids are determined through clustering algorithms like k-means, with Qdrant currently utilizing 256 centroids, allowing each centroid index to be represented by a single byte. Product quantization can achieve higher compression factors compared to scalar quantization. However, there are tradeoffs to consider. Product quantization distance calculations are not SIMD-friendly, resulting in slower computational speed compared to scalar quantization. Additionally, product quantization introduces a loss of accuracy, making it more suitable for high-dimensional vectors. To optimize quantization parameters for specific use cases, it is recommended to refer to the ""Quantization Tips"" section for more information.","Product quantization differs from scalar quantization in terms of compression and computational efficiency in the following ways:

1. **Compression Ratio:** Product quantization typically provides a better compression ratio compared to scalar quantization. This means that product quantization can more efficiently reduce the size of the vectors being compressed.

2. **Accuracy:** Product quantization often involves a significant loss of accuracy compared to scalar quantization. This means that the original data may not be perfectly preserved during compression using product quantization.

3. **Computational Efficiency:** Scalar quantization is generally faster computationally compared to product quantization. Product quantization involves more complex operations, which can lead to slower processing speeds.

4. **Memory Usage:** Product quantization can be more memory-efficient than scalar quantization, especially when the memory footprint is a top priority. This can be beneficial in scenarios where minimizing memory usage is crucial.

5. **Search Speed:** Scalar quantization is usually faster for search operations compared to product quantization. Product quantization may result in slower search times due to the increased complexity of the quantization method.

In summary, while product quantization may offer better compression ratios and memory efficiency, it often comes at the cost of reduced accuracy and slower computational performance, especially in search operations.","* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.



## Setting up Quantization in Qdrant



You can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.
<td>474 s</td>

      </tr>

   </tbody>

</table>



It turns out that in some cases, Product Quantization may not only reduce the memory usage, 

but also the search time.



## Good practices



Compared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.



Product Quantization tends to be favored in certain specific scenarios:
But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method
. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
<td>147 s</td>

         <td>339 s</td>

         <td>217 s</td>

         <td>178 s</td>

      </tr>

   </tbody>

</table>



Product Quantization increases both indexing and searching time. The higher the compression ratio, 

the lower the search precision. The main benefit is undoubtedly the reduced usage of memory.



#### Arxiv-titles-384-angular-no-filters



<table>

   <thead>

      <tr>

         <th></th>

         <th>Original</th>

         <th>1D clusters</th>","['* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.\n\n\n\n## Setting up Quantization in Qdrant\n\n\n\nYou can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.', '<td>474 s</td>\n\n      </tr>\n\n   </tbody>\n\n</table>\n\n\n\nIt turns out that in some cases, Product Quantization may not only reduce the memory usage, \n\nbut also the search time.\n\n\n\n## Good practices\n\n\n\nCompared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\n\n\n\nProduct Quantization tends to be favored in certain specific scenarios:', 'But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\n\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\n\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n## How to choose the right quantization method', '. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.', '<td>147 s</td>\n\n         <td>339 s</td>\n\n         <td>217 s</td>\n\n         <td>178 s</td>\n\n      </tr>\n\n   </tbody>\n\n</table>\n\n\n\nProduct Quantization increases both indexing and searching time. The higher the compression ratio, \n\nthe lower the search precision. The main benefit is undoubtedly the reduced usage of memory.\n\n\n\n#### Arxiv-titles-384-angular-no-filters\n\n\n\n<table>\n\n   <thead>\n\n      <tr>\n\n         <th></th>\n\n         <th>Original</th>\n\n         <th>1D clusters</th>']","Question: How does product quantization differ from scalar quantization in terms of compression and computational efficiency?\n\nContext: * **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.



## Setting up Quantization in Qdrant



You can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.
<td>474 s</td>

</tr>

</tbody>

</table>



It turns out that in some cases, Product Quantization may not only reduce the memory usage, 

but also the search time.



## Good practices



Compared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.



Product Quantization tends to be favored in certain specific scenarios:
But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method
. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
<td>147 s</td>

<td>339 s</td>

<td>217 s</td>

<td>178 s</td>

</tr>

</tbody>

</table>



Product Quantization increases both indexing and searching time. The higher the compression ratio, 

the lower the search precision. The main benefit is undoubtedly the reduced usage of memory.



#### Arxiv-titles-384-angular-no-filters



<table>

<thead>

<tr>

<th></th>

<th>Original</th>

<th>1D clusters</th>\n\nAnswer:",How does product quantization differ from scalar quantization in terms of compression and computational efficiency?,"Product quantization involves dividing data into chunks and quantizing each segment individually, approximating each chunk with a centroid index that represents the original vector component. The positions of these centroids are determined through clustering algorithms like k-means, with Qdrant currently utilizing 256 centroids, allowing each centroid index to be represented by a single byte. Product quantization can achieve higher compression factors compared to scalar quantization. However, there are tradeoffs to consider. Product quantization distance calculations are not SIMD-friendly, resulting in slower computational speed compared to scalar quantization. Additionally, product quantization introduces a loss of accuracy, making it more suitable for high-dimensional vectors. To optimize quantization parameters for specific use cases, it is recommended to refer to the ""Quantization Tips"" section for more information.","['* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.\n\n\n\n## Setting up Quantization in Qdrant\n\n\n\nYou can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.', '<td>474 s</td>\n\n      </tr>\n\n   </tbody>\n\n</table>\n\n\n\nIt turns out that in some cases, Product Quantization may not only reduce the memory usage, \n\nbut also the search time.\n\n\n\n## Good practices\n\n\n\nCompared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\n\n\n\nProduct Quantization tends to be favored in certain specific scenarios:', 'But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\n\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\n\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n## How to choose the right quantization method', '. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.', '<td>147 s</td>\n\n         <td>339 s</td>\n\n         <td>217 s</td>\n\n         <td>178 s</td>\n\n      </tr>\n\n   </tbody>\n\n</table>\n\n\n\nProduct Quantization increases both indexing and searching time. The higher the compression ratio, \n\nthe lower the search precision. The main benefit is undoubtedly the reduced usage of memory.\n\n\n\n#### Arxiv-titles-384-angular-no-filters\n\n\n\n<table>\n\n   <thead>\n\n      <tr>\n\n         <th></th>\n\n         <th>Original</th>\n\n         <th>1D clusters</th>']","* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.



## Setting up Quantization in Qdrant



You can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.
<td>474 s</td>

      </tr>

   </tbody>

</table>



It turns out that in some cases, Product Quantization may not only reduce the memory usage, 

but also the search time.



## Good practices



Compared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.



Product Quantization tends to be favored in certain specific scenarios:
But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method
. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
<td>147 s</td>

         <td>339 s</td>

         <td>217 s</td>

         <td>178 s</td>

      </tr>

   </tbody>

</table>



Product Quantization increases both indexing and searching time. The higher the compression ratio, 

the lower the search precision. The main benefit is undoubtedly the reduced usage of memory.



#### Arxiv-titles-384-angular-no-filters



<table>

   <thead>

      <tr>

         <th></th>

         <th>Original</th>

         <th>1D clusters</th>",0.2578947368421053,0.4083333333333333,0.31612903225806455,0.08994708994708994,0.14285714285714285,0.11038961038961038,0.14736842105263157,0.23333333333333334,0.1806451612903226,0.23157894736842105,0.36666666666666664,0.2838709677419355,0.7459715604782104,0.5885448455810547,0.5747600197792053,0.5815707445144653,0.8034671405985137,0.7612229221600241,200,1.6260162601626016,0.5916897993132693
True,,3862.154,9d907a85,What is the significance of Qdrant's approach to storing multiple vectors per object in data indexing?,"Qdrant's approach to storing multiple vectors per object in data indexing is significant as it opens up new possibilities in data representation and retrieval. By allowing multiple vectors to be associated with a single object, Qdrant enables more nuanced and detailed analysis of the data. This approach can be particularly beneficial in applications such as data science, neural networks, database management, and similarity search. It provides a more comprehensive understanding of the relationships between objects and enhances the accuracy and efficiency of search and retrieval processes. Overall, Qdrant's innovative vector storage strategy contributes to pushing the boundaries of data indexing and offers practical applications and benefits in various fields.","Storing multiple vectors per object in Qdrant holds significant significance in data indexing. This approach allows for the storage of various vectors together per data point, enabling the reuse of payloads instead of creating separate collections and querying them individually. This method offers a more efficient and streamlined process for representing and retrieving data points that can be described using multiple vectors of different dimensionality.

One key advantage of Qdrant's approach is the ability to support optional vectors, which means that vectors are not mandatory for each point as opposed to some other indexing systems like Pinecone. Additionally, Qdrant enables the storage of named vectors per point with varying dimensionality and distance functions for calculating similarity. This flexibility is crucial for real-world scenarios where multiple embeddings per item are required, such as encoding textual data.

Overall, the capability of Qdrant to store multiple vectors per object not only enhances the efficiency of data representation and retrieval but also opens up new possibilities for diverse applications in data indexing and retrieval processes.","---

draft: false

title: Storing multiple vectors per object in Qdrant

slug: storing-multiple-vectors-per-object-in-qdrant

short_description: Qdrant's approach to storing multiple vectors per object,

  unraveling new possibilities in data representation and retrieval.

description: Discover how Qdrant continues to push the boundaries of data

  indexing, providing insights into the practical applications and benefits of

  this novel vector storage strategy.
It is not surprising that a method used for creating neural encoding plays an important role in the search process and its quality. If your data points might be described using several vectors, then the latest release of Qdrant gives you an opportunity to store them together and reuse the payloads, instead of creating several collections and querying them separately.
5. Qdrant allows storing multiple vectors per point, and those might be of a different dimensionality. Pinecone doesn't support anything similar.

6. Vectors in Pinecone are mandatory for each point. Qdrant supports optional vectors.



It is worth mentioning, that **Pinecone will automatically create metadata indexes for all the fields**. Qdrant assumes you know
detail. In a nutshell, you can **reduce the memory requirements by up to 64 times**!



### Optional named vectors



Qdrant has been supporting multiple named vectors per point for quite a long time. Those may have 

utterly different dimensionality and distance functions used to calculate similarity. Having multiple 

embeddings per item is an essential real-world scenario. For example, you might be encoding textual
author_link: https://medium.com/@lukawskikacper

date: 2022-09-19T13:30:00+02:00

draft: false

---



[Qdrant 0.10 is a new version](https://github.com/qdrant/qdrant/releases/tag/v0.10.0) that brings a lot of performance 

improvements, but also some new features which were heavily requested by our users. Here is an overview of what has changed.



## Storing multiple vectors per object","[""---\n\ndraft: false\n\ntitle: Storing multiple vectors per object in Qdrant\n\nslug: storing-multiple-vectors-per-object-in-qdrant\n\nshort_description: Qdrant's approach to storing multiple vectors per object,\n\n  unraveling new possibilities in data representation and retrieval.\n\ndescription: Discover how Qdrant continues to push the boundaries of data\n\n  indexing, providing insights into the practical applications and benefits of\n\n  this novel vector storage strategy."", 'It is not surprising that a method used for creating neural encoding plays an important role in the search process and its quality. If your data points might be described using several vectors, then the latest release of Qdrant gives you an opportunity to store them together and reuse the payloads, instead of creating several collections and querying them separately.', ""5. Qdrant allows storing multiple vectors per point, and those might be of a different dimensionality. Pinecone doesn't support anything similar.\n\n6. Vectors in Pinecone are mandatory for each point. Qdrant supports optional vectors.\n\n\n\nIt is worth mentioning, that **Pinecone will automatically create metadata indexes for all the fields**. Qdrant assumes you know"", 'detail. In a nutshell, you can **reduce the memory requirements by up to 64 times**!\n\n\n\n### Optional named vectors\n\n\n\nQdrant has been supporting multiple named vectors per point for quite a long time. Those may have \n\nutterly different dimensionality and distance functions used to calculate similarity. Having multiple \n\nembeddings per item is an essential real-world scenario. For example, you might be encoding textual', 'author_link: https://medium.com/@lukawskikacper\n\ndate: 2022-09-19T13:30:00+02:00\n\ndraft: false\n\n---\n\n\n\n[Qdrant 0.10 is a new version](https://github.com/qdrant/qdrant/releases/tag/v0.10.0) that brings a lot of performance \n\nimprovements, but also some new features which were heavily requested by our users. Here is an overview of what has changed.\n\n\n\n## Storing multiple vectors per object']","Question: What is the significance of Qdrant's approach to storing multiple vectors per object in data indexing?\n\nContext: ---

draft: false

title: Storing multiple vectors per object in Qdrant

slug: storing-multiple-vectors-per-object-in-qdrant

short_description: Qdrant's approach to storing multiple vectors per object,

unraveling new possibilities in data representation and retrieval.

description: Discover how Qdrant continues to push the boundaries of data

indexing, providing insights into the practical applications and benefits of

this novel vector storage strategy.
It is not surprising that a method used for creating neural encoding plays an important role in the search process and its quality. If your data points might be described using several vectors, then the latest release of Qdrant gives you an opportunity to store them together and reuse the payloads, instead of creating several collections and querying them separately.
5. Qdrant allows storing multiple vectors per point, and those might be of a different dimensionality. Pinecone doesn't support anything similar.

6. Vectors in Pinecone are mandatory for each point. Qdrant supports optional vectors.



It is worth mentioning, that **Pinecone will automatically create metadata indexes for all the fields**. Qdrant assumes you know
detail. In a nutshell, you can **reduce the memory requirements by up to 64 times**!



### Optional named vectors



Qdrant has been supporting multiple named vectors per point for quite a long time. Those may have 

utterly different dimensionality and distance functions used to calculate similarity. Having multiple 

embeddings per item is an essential real-world scenario. For example, you might be encoding textual
author_link: https://medium.com/@lukawskikacper

date: 2022-09-19T13:30:00+02:00

draft: false

---



[Qdrant 0.10 is a new version](https://github.com/qdrant/qdrant/releases/tag/v0.10.0) that brings a lot of performance 

improvements, but also some new features which were heavily requested by our users. Here is an overview of what has changed.



## Storing multiple vectors per object\n\nAnswer:",What is the significance of Qdrant's approach to storing multiple vectors per object in data indexing?,"Qdrant's approach to storing multiple vectors per object in data indexing is significant as it opens up new possibilities in data representation and retrieval. By allowing multiple vectors to be associated with a single object, Qdrant enables more nuanced and detailed analysis of the data. This approach can be particularly beneficial in applications such as data science, neural networks, database management, and similarity search. It provides a more comprehensive understanding of the relationships between objects and enhances the accuracy and efficiency of search and retrieval processes. Overall, Qdrant's innovative vector storage strategy contributes to pushing the boundaries of data indexing and offers practical applications and benefits in various fields.","[""---\n\ndraft: false\n\ntitle: Storing multiple vectors per object in Qdrant\n\nslug: storing-multiple-vectors-per-object-in-qdrant\n\nshort_description: Qdrant's approach to storing multiple vectors per object,\n\n  unraveling new possibilities in data representation and retrieval.\n\ndescription: Discover how Qdrant continues to push the boundaries of data\n\n  indexing, providing insights into the practical applications and benefits of\n\n  this novel vector storage strategy."", 'It is not surprising that a method used for creating neural encoding plays an important role in the search process and its quality. If your data points might be described using several vectors, then the latest release of Qdrant gives you an opportunity to store them together and reuse the payloads, instead of creating several collections and querying them separately.', ""5. Qdrant allows storing multiple vectors per point, and those might be of a different dimensionality. Pinecone doesn't support anything similar.\n\n6. Vectors in Pinecone are mandatory for each point. Qdrant supports optional vectors.\n\n\n\nIt is worth mentioning, that **Pinecone will automatically create metadata indexes for all the fields**. Qdrant assumes you know"", 'detail. In a nutshell, you can **reduce the memory requirements by up to 64 times**!\n\n\n\n### Optional named vectors\n\n\n\nQdrant has been supporting multiple named vectors per point for quite a long time. Those may have \n\nutterly different dimensionality and distance functions used to calculate similarity. Having multiple \n\nembeddings per item is an essential real-world scenario. For example, you might be encoding textual', 'author_link: https://medium.com/@lukawskikacper\n\ndate: 2022-09-19T13:30:00+02:00\n\ndraft: false\n\n---\n\n\n\n[Qdrant 0.10 is a new version](https://github.com/qdrant/qdrant/releases/tag/v0.10.0) that brings a lot of performance \n\nimprovements, but also some new features which were heavily requested by our users. Here is an overview of what has changed.\n\n\n\n## Storing multiple vectors per object']","---

draft: false

title: Storing multiple vectors per object in Qdrant

slug: storing-multiple-vectors-per-object-in-qdrant

short_description: Qdrant's approach to storing multiple vectors per object,

  unraveling new possibilities in data representation and retrieval.

description: Discover how Qdrant continues to push the boundaries of data

  indexing, providing insights into the practical applications and benefits of

  this novel vector storage strategy.
It is not surprising that a method used for creating neural encoding plays an important role in the search process and its quality. If your data points might be described using several vectors, then the latest release of Qdrant gives you an opportunity to store them together and reuse the payloads, instead of creating several collections and querying them separately.
5. Qdrant allows storing multiple vectors per point, and those might be of a different dimensionality. Pinecone doesn't support anything similar.

6. Vectors in Pinecone are mandatory for each point. Qdrant supports optional vectors.



It is worth mentioning, that **Pinecone will automatically create metadata indexes for all the fields**. Qdrant assumes you know
detail. In a nutshell, you can **reduce the memory requirements by up to 64 times**!



### Optional named vectors



Qdrant has been supporting multiple named vectors per point for quite a long time. Those may have 

utterly different dimensionality and distance functions used to calculate similarity. Having multiple 

embeddings per item is an essential real-world scenario. For example, you might be encoding textual
author_link: https://medium.com/@lukawskikacper

date: 2022-09-19T13:30:00+02:00

draft: false

---



[Qdrant 0.10 is a new version](https://github.com/qdrant/qdrant/releases/tag/v0.10.0) that brings a lot of performance 

improvements, but also some new features which were heavily requested by our users. Here is an overview of what has changed.



## Storing multiple vectors per object",0.39759036144578314,0.6285714285714286,0.48708487084870844,0.1696969696969697,0.2692307692307692,0.20817843866171004,0.18674698795180722,0.29523809523809524,0.22878228782287824,0.37349397590361444,0.5904761904761905,0.4575645756457565,0.8812974691390991,0.656735897064209,0.6959086656570435,0.6757550835609436,0.9974963871645741,0.9768661603447981,172,1.5495495495495495,0.5039281602313072
True,,6701.965999999999,f4478b6e,What configuration is required to enable in-memory quantization with on-disk original vectors in Qdrant?,"To configure in-memory quantization with on-disk original vectors in Qdrant, you need to create a collection with the following configuration:

```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 768,

        ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": true

        }

    }

}
```

Additionally, in the Python code snippet provided, you can use the QdrantClient library to create the collection with the specified configuration parameters. This includes defining vector size, distance metric (such as Cosine similarity), memmap threshold for optimization, and scalar quantization configuration with type set to INT8","To enable in-memory quantization with on-disk original vectors in Qdrant, you need to create a collection with the following configuration:

```http
PUT /collections/{collection_name}

{
    ""vectors"": {
        ""size"": 768,
        ""distance"": ""Cosine""
    },
    ""optimizers_config"": {
        ""memmap_threshold"": 20000
    },
    ""quantization_config"": {
        ""scalar"": {
            ""type"": ""int8"",
            ""always_ram"": true
        }
    }
}
```

This configuration specifies that the vectors will be quantized using scalar quantization with type `int8` and will always be kept in RAM. Additionally, the `memmap_threshold` parameter in the `optimizers_config` section sets the threshold for when vectors should be loaded from disk.

If you want to enable in-memory quantization with on-disk original vectors using mmap storage, you can set `always_ram` to `false` in the quantization config:

```http
PUT /collections/{collection_name}

{
    ""vectors"": {
        ""size"": 768,
        ""distance"": ""Cosine""
    },
    ""optimizers_config"": {
        ""memmap_threshold"": 20000
    },
    ""quantization_config"": {
        ""scalar"": {
            ""type"": ""int8"",
            ""always_ram"": false
        }
    }
}
```

This configuration allows for storing the full vectors and payload on disk, loading only the binary quantization index into memory to reduce the memory footprint.

You can refer to the provided HTTP configuration examples to set up in-memory quantization with on-disk original vectors in Qdrant.","To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 768,

        ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": true

        }

    }

}

```



```python
5. RAM: Store the full vectors and payload on disk. Limit what you load from memory to the binary quantization index. This helps reduce the memory footprint and improve the overall efficiency of the system. The incremental latency from the disk read is negligible compared to the latency savings from the binary scoring in Qdrant, which uses SIMD instructions where possible.



Want to discuss these findings and learn more about Binary Quantization? [Join our Discord community.](https://discord.gg/qdrant)
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": false

        }

    }

}

```



```python

from qdrant_client import QdrantClient, models
..Default::default()

        }),

        quantization_config: Some(QuantizationConfig {

            quantization: Some(Quantization::Scalar(ScalarQuantization {

                r#type: QuantizationType::Int8.into(),

                always_ram: Some(false),

                ..Default::default()

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;
""size"": 1536,

      ""distance"": ""Cosine""

    },

    ""quantization_config"": {

        ""binary"": {

            ""always_ram"": true

        }

    }

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),","['To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 768,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python', '5. RAM: Store the full vectors and payload on disk. Limit what you load from memory to the binary quantization index. This helps reduce the memory footprint and improve the overall efficiency of the system. The incremental latency from the disk read is negligible compared to the latency savings from the binary scoring in Qdrant, which uses SIMD instructions where possible.\n\n\n\nWant to discuss these findings and learn more about Binary Quantization? [Join our Discord community.](https://discord.gg/qdrant)', 'This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models', '..Default::default()\n\n        }),\n\n        quantization_config: Some(QuantizationConfig {\n\n            quantization: Some(Quantization::Scalar(ScalarQuantization {\n\n                r#type: QuantizationType::Int8.into(),\n\n                always_ram: Some(false),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;', '""size"": 1536,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""quantization_config"": {\n\n        ""binary"": {\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),']","Question: What configuration is required to enable in-memory quantization with on-disk original vectors in Qdrant?\n\nContext: To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration:



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""optimizers_config"": {

""memmap_threshold"": 20000

},

""quantization_config"": {

""scalar"": {

""type"": ""int8"",

""always_ram"": true

}

}

}

```



```python
5. RAM: Store the full vectors and payload on disk. Limit what you load from memory to the binary quantization index. This helps reduce the memory footprint and improve the overall efficiency of the system. The incremental latency from the disk read is negligible compared to the latency savings from the binary scoring in Qdrant, which uses SIMD instructions where possible.



Want to discuss these findings and learn more about Binary Quantization? [Join our Discord community.](https://discord.gg/qdrant)
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""optimizers_config"": {

""memmap_threshold"": 20000

},

""quantization_config"": {

""scalar"": {

""type"": ""int8"",

""always_ram"": false

}

}

}

```



```python

from qdrant_client import QdrantClient, models
..Default::default()

}),

quantization_config: Some(QuantizationConfig {

quantization: Some(Quantization::Scalar(ScalarQuantization {

r#type: QuantizationType::Int8.into(),

always_ram: Some(false),

..Default::default()

})),

}),

..Default::default()

})

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;
""size"": 1536,

""distance"": ""Cosine""

},

""quantization_config"": {

""binary"": {

""always_ram"": true

}

}

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),\n\nAnswer:",What configuration is required to enable in-memory quantization with on-disk original vectors in Qdrant?,"To configure in-memory quantization with on-disk original vectors in Qdrant, you need to create a collection with the following configuration:

```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 768,

        ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": true

        }

    }

}
```

Additionally, in the Python code snippet provided, you can use the QdrantClient library to create the collection with the specified configuration parameters. This includes defining vector size, distance metric (such as Cosine similarity), memmap threshold for optimization, and scalar quantization configuration with type set to INT8","['To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 768,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python', '5. RAM: Store the full vectors and payload on disk. Limit what you load from memory to the binary quantization index. This helps reduce the memory footprint and improve the overall efficiency of the system. The incremental latency from the disk read is negligible compared to the latency savings from the binary scoring in Qdrant, which uses SIMD instructions where possible.\n\n\n\nWant to discuss these findings and learn more about Binary Quantization? [Join our Discord community.](https://discord.gg/qdrant)', 'This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models', '..Default::default()\n\n        }),\n\n        quantization_config: Some(QuantizationConfig {\n\n            quantization: Some(Quantization::Scalar(ScalarQuantization {\n\n                r#type: QuantizationType::Int8.into(),\n\n                always_ram: Some(false),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;', '""size"": 1536,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""quantization_config"": {\n\n        ""binary"": {\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),']","To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 768,

        ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": true

        }

    }

}

```



```python
5. RAM: Store the full vectors and payload on disk. Limit what you load from memory to the binary quantization index. This helps reduce the memory footprint and improve the overall efficiency of the system. The incremental latency from the disk read is negligible compared to the latency savings from the binary scoring in Qdrant, which uses SIMD instructions where possible.



Want to discuss these findings and learn more about Binary Quantization? [Join our Discord community.](https://discord.gg/qdrant)
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": false

        }

    }

}

```



```python

from qdrant_client import QdrantClient, models
..Default::default()

        }),

        quantization_config: Some(QuantizationConfig {

            quantization: Some(Quantization::Scalar(ScalarQuantization {

                r#type: QuantizationType::Int8.into(),

                always_ram: Some(false),

                ..Default::default()

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;
""size"": 1536,

      ""distance"": ""Cosine""

    },

    ""quantization_config"": {

        ""binary"": {

            ""always_ram"": true

        }

    }

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),",0.41954022988505746,0.8588235294117647,0.5637065637065637,0.2658959537572254,0.5476190476190477,0.35797665369649806,0.3390804597701149,0.6941176470588235,0.4555984555984555,0.3563218390804598,0.7294117647058823,0.4787644787644787,0.9910657405853271,0.6405682563781738,0.6775932312011719,0.6585607528686523,0.5019669234752655,0.4883847236633301,166,1.9761904761904763,0.6659818044445068
True,,4924.561,1a0bcd3d,How can dissimilarity search be used for outlier detection in a dataset?,"Dissimilarity search can be utilized for outlier detection in a dataset by first establishing a set of reference points that are considered ""normal"". These reference points serve as a benchmark for comparison. Subsequently, the dissimilarity search algorithm is applied to identify the data points that are the most dissimilar to the reference set. These identified points are then flagged as potential outliers or anomalies in the dataset. This approach allows for the detection of abnormal data points even in cases where labels are not available, enabling the identification of outliers based on their deviation from the established ""normal"" reference points. This method can be a valuable tool in data analysis and anomaly detection tasks.","Dissimilarity search using vector similarity can be a powerful tool for outlier detection in a dataset, even in the absence of labels. Here's how it can be utilized for this purpose:

1. **Define a Reference Set**: Start by selecting a set of reference points that represent the ""normal"" or expected data points in your dataset. These reference points will serve as the baseline for comparison.

2. **Calculate Dissimilarity Scores**: Use vector similarity to calculate the dissimilarity scores between each data point in the dataset and the reference set. This involves comparing the embeddings of data points to quantify their differences.

3. **Identify Outliers**: Look for data points with the highest dissimilarity scores compared to the reference set. These points are considered outliers as they deviate significantly from the norm and may indicate anomalies or errors in the dataset.

4. **Further Analysis**: Once potential outliers are identified, they can be flagged for further analysis or investigation to determine the reasons behind their unusual characteristics.

By leveraging dissimilarity search through vector similarity, outlier detection becomes a data-driven process that can help uncover irregularities and enhance the overall quality and reliability of the dataset.","### Case: Outlier Detection



In some cases, we might not even have labels, but it is still possible to try to detect anomalies in our dataset.

Dissimilarity search can be used for this purpose as well.



{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/anomaly-detection.png caption=""Anomaly Detection"" >}}



The only thing we need is a bunch of reference points that we consider ""normal"".
Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. 

It doesn’t rely on keywords or other metadata. 

With vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.



The dissimilarity search can find items in areas where previously no other search could be used.

Let’s look at a few examples.



### Case: Mislabeling Detection
Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.





## Diversity Search



Even with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.



The naive approach is to do random sampling. 

However, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.
---

title: Finding errors in datasets with Similarity Search

short_description: Finding errors datasets with distance-based methods

description: Improving quality of text-and-images datasets on the online furniture marketplace example.

preview_dir: /articles_data/dataset-quality/preview

social_preview_image: /articles_data/dataset-quality/preview/social_preview.jpg

small_preview_image: /articles_data/dataset-quality/icon.svg

weight: 8

author: George Panchuk
You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. 

To mitigate this issue, you can perform a diversity search.



Diversity search is a method for finding the most distinctive examples in the data.

As similarity search, it also operates on embeddings and measures the distances between them.

The difference lies in deciding which point should be extracted next.","['### Case: Outlier Detection\n\n\n\nIn some cases, we might not even have labels, but it is still possible to try to detect anomalies in our dataset.\n\nDissimilarity search can be used for this purpose as well.\n\n\n\n{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/anomaly-detection.png caption=""Anomaly Detection"" >}}\n\n\n\nThe only thing we need is a bunch of reference points that we consider ""normal"".', 'Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. \n\nIt doesn’t rely on keywords or other metadata. \n\nWith vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.\n\n\n\nThe dissimilarity search can find items in areas where previously no other search could be used.\n\nLet’s look at a few examples.\n\n\n\n### Case: Mislabeling Detection', 'Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.\n\n\n\n\n\n## Diversity Search\n\n\n\nEven with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.\n\n\n\nThe naive approach is to do random sampling. \n\nHowever, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.', '---\n\ntitle: Finding errors in datasets with Similarity Search\n\nshort_description: Finding errors datasets with distance-based methods\n\ndescription: Improving quality of text-and-images datasets on the online furniture marketplace example.\n\npreview_dir: /articles_data/dataset-quality/preview\n\nsocial_preview_image: /articles_data/dataset-quality/preview/social_preview.jpg\n\nsmall_preview_image: /articles_data/dataset-quality/icon.svg\n\nweight: 8\n\nauthor: George Panchuk', 'You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. \n\nTo mitigate this issue, you can perform a diversity search.\n\n\n\nDiversity search is a method for finding the most distinctive examples in the data.\n\nAs similarity search, it also operates on embeddings and measures the distances between them.\n\nThe difference lies in deciding which point should be extracted next.']","Question: How can dissimilarity search be used for outlier detection in a dataset?\n\nContext: ### Case: Outlier Detection



In some cases, we might not even have labels, but it is still possible to try to detect anomalies in our dataset.

Dissimilarity search can be used for this purpose as well.



{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/anomaly-detection.png caption=""Anomaly Detection"" >}}



The only thing we need is a bunch of reference points that we consider ""normal"".
Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. 

It doesn’t rely on keywords or other metadata. 

With vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.



The dissimilarity search can find items in areas where previously no other search could be used.

Let’s look at a few examples.



### Case: Mislabeling Detection
Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.





## Diversity Search



Even with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.



The naive approach is to do random sampling. 

However, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.
---

title: Finding errors in datasets with Similarity Search

short_description: Finding errors datasets with distance-based methods

description: Improving quality of text-and-images datasets on the online furniture marketplace example.

preview_dir: /articles_data/dataset-quality/preview

social_preview_image: /articles_data/dataset-quality/preview/social_preview.jpg

small_preview_image: /articles_data/dataset-quality/icon.svg

weight: 8

author: George Panchuk
You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. 

To mitigate this issue, you can perform a diversity search.



Diversity search is a method for finding the most distinctive examples in the data.

As similarity search, it also operates on embeddings and measures the distances between them.

The difference lies in deciding which point should be extracted next.\n\nAnswer:",How can dissimilarity search be used for outlier detection in a dataset?,"Dissimilarity search can be utilized for outlier detection in a dataset by first establishing a set of reference points that are considered ""normal"". These reference points serve as a benchmark for comparison. Subsequently, the dissimilarity search algorithm is applied to identify the data points that are the most dissimilar to the reference set. These identified points are then flagged as potential outliers or anomalies in the dataset. This approach allows for the detection of abnormal data points even in cases where labels are not available, enabling the identification of outliers based on their deviation from the established ""normal"" reference points. This method can be a valuable tool in data analysis and anomaly detection tasks.","['### Case: Outlier Detection\n\n\n\nIn some cases, we might not even have labels, but it is still possible to try to detect anomalies in our dataset.\n\nDissimilarity search can be used for this purpose as well.\n\n\n\n{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/anomaly-detection.png caption=""Anomaly Detection"" >}}\n\n\n\nThe only thing we need is a bunch of reference points that we consider ""normal"".', 'Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. \n\nIt doesn’t rely on keywords or other metadata. \n\nWith vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.\n\n\n\nThe dissimilarity search can find items in areas where previously no other search could be used.\n\nLet’s look at a few examples.\n\n\n\n### Case: Mislabeling Detection', 'Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.\n\n\n\n\n\n## Diversity Search\n\n\n\nEven with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.\n\n\n\nThe naive approach is to do random sampling. \n\nHowever, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.', '---\n\ntitle: Finding errors in datasets with Similarity Search\n\nshort_description: Finding errors datasets with distance-based methods\n\ndescription: Improving quality of text-and-images datasets on the online furniture marketplace example.\n\npreview_dir: /articles_data/dataset-quality/preview\n\nsocial_preview_image: /articles_data/dataset-quality/preview/social_preview.jpg\n\nsmall_preview_image: /articles_data/dataset-quality/icon.svg\n\nweight: 8\n\nauthor: George Panchuk', 'You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. \n\nTo mitigate this issue, you can perform a diversity search.\n\n\n\nDiversity search is a method for finding the most distinctive examples in the data.\n\nAs similarity search, it also operates on embeddings and measures the distances between them.\n\nThe difference lies in deciding which point should be extracted next.']","### Case: Outlier Detection



In some cases, we might not even have labels, but it is still possible to try to detect anomalies in our dataset.

Dissimilarity search can be used for this purpose as well.



{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/anomaly-detection.png caption=""Anomaly Detection"" >}}



The only thing we need is a bunch of reference points that we consider ""normal"".
Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. 

It doesn’t rely on keywords or other metadata. 

With vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.



The dissimilarity search can find items in areas where previously no other search could be used.

Let’s look at a few examples.



### Case: Mislabeling Detection
Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.





## Diversity Search



Even with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.



The naive approach is to do random sampling. 

However, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.
---

title: Finding errors in datasets with Similarity Search

short_description: Finding errors datasets with distance-based methods

description: Improving quality of text-and-images datasets on the online furniture marketplace example.

preview_dir: /articles_data/dataset-quality/preview

social_preview_image: /articles_data/dataset-quality/preview/social_preview.jpg

small_preview_image: /articles_data/dataset-quality/icon.svg

weight: 8

author: George Panchuk
You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. 

To mitigate this issue, you can perform a diversity search.



Diversity search is a method for finding the most distinctive examples in the data.

As similarity search, it also operates on embeddings and measures the distances between them.

The difference lies in deciding which point should be extracted next.",0.41954022988505746,0.7156862745098039,0.5289855072463769,0.16184971098265896,0.27722772277227725,0.20437956204379562,0.25287356321839083,0.43137254901960786,0.31884057971014496,0.40804597701149425,0.696078431372549,0.5144927536231884,0.8631415367126465,0.6616132259368896,0.7223613262176514,0.6906540393829346,0.7692176838351701,0.9878052588173887,192,1.6842105263157894,0.437507514984101
True,,5971.558,9241d3cf,How does binary quantization work and what benefits does it offer in terms of query processing times and data compression?,"Binary quantization is a method used by Qdrant for fast indexing and data compression. In this process, each data point is represented as a vector. The quantization splits the vector in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This results in a string of bits that represents the original vector, making it much smaller and easier to compare. By supporting vector comparisons, binary quantization can significantly speed up query processing times, up to 40 times faster. This method is especially effective for OpenAI embeddings, where it has been shown to achieve a massive performance improvement at a lower cost of accuracy.","Binary quantization works by compressing vectors into a more compact representation using binary codes. This method converts continuous values into binary format, reducing the storage footprint of the data significantly. By simplifying distance calculations between vectors to bitwise operations, binary quantization accelerates the search process, enabling real-time querying even in extensive databases.

The benefits of binary quantization in terms of query processing times and data compression include:

1. Reduced storage footprint: Binary quantization helps with large-scale datasets by saving on memory and scaling up to 30x at the same cost. This reduction in storage needs allows applications to scale with lower costs.

2. Enhanced speed of data retrieval: Smaller data sizes resulting from binary quantization generally lead to faster searches. This improvement in data retrieval speed can be crucial for applications that require quick access to information.

3. Accelerated search process: By simplifying distance calculations between vectors to bitwise operations, binary quantization supports potentially real-time searches across vast datasets. This efficiency gain in query processing times can make a significant impact on the overall performance of applications.

While binary quantization offers these benefits, it is essential to note that there may be some recall degradation, which is a tradeoff for the speed and storage improvements. However, with the right implementation, it is possible to balance the tradeoff between speed and recall accuracy at the time of search rather than at the time of index creation.","#### What is Binary Quantization?



Quantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.



[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).
![](/blog/openai/Accuracy_Models.png)



The efficiency gains from Binary Quantization are as follows: 



- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. 

- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. 

- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.
By reducing storage needs, you can scale applications with lower costs. This addresses a critical challenge posed by the original embedding sizes. Binary Quantization also speeds the search process. It simplifies the complex distance calculations between vectors into more manageable bitwise operations, which supports potentially real-time searches across vast datasets.
As is the case with other quantization methods, these benefits come at the cost of recall degradation. However, our implementation lets you balance the tradeoff between speed and recall accuracy at time of search, rather than time of index creation. 



The rest of this article will cover:

1. The importance of binary quantization

2. Basic implementation using our Python client

3. Benchmark analysis and usage recommendations



## What is Binary Quantization?
5 Keys to Learning from the Episode:



1. The necessity of quantization and the complex challenges it helps to overcome.

2. The transformative effects of binary quantization on processing speed and storage size reduction.

3. A detailed exploration of oversampling and its real-time precision control in query search.

4. Understanding the simplicity and effectiveness of binary quantization, especially when compared to more intricate quantization methods.","['#### What is Binary Quantization?\n\n\n\nQuantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.\n\n\n\n[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).', '![](/blog/openai/Accuracy_Models.png)\n\n\n\nThe efficiency gains from Binary Quantization are as follows: \n\n\n\n- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. \n\n- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. \n\n- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.', 'By reducing storage needs, you can scale applications with lower costs. This addresses a critical challenge posed by the original embedding sizes. Binary Quantization also speeds the search process. It simplifies the complex distance calculations between vectors into more manageable bitwise operations, which supports potentially real-time searches across vast datasets.', 'As is the case with other quantization methods, these benefits come at the cost of recall degradation. However, our implementation lets you balance the tradeoff between speed and recall accuracy at time of search, rather than time of index creation. \n\n\n\nThe rest of this article will cover:\n\n1. The importance of binary quantization\n\n2. Basic implementation using our Python client\n\n3. Benchmark analysis and usage recommendations\n\n\n\n## What is Binary Quantization?', '5 Keys to Learning from the Episode:\n\n\n\n1. The necessity of quantization and the complex challenges it helps to overcome.\n\n2. The transformative effects of binary quantization on processing speed and storage size reduction.\n\n3. A detailed exploration of oversampling and its real-time precision control in query search.\n\n4. Understanding the simplicity and effectiveness of binary quantization, especially when compared to more intricate quantization methods.']","Question: How does binary quantization work and what benefits does it offer in terms of query processing times and data compression?\n\nContext: #### What is Binary Quantization?



Quantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.



[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).
![](/blog/openai/Accuracy_Models.png)



The efficiency gains from Binary Quantization are as follows: 



- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. 

- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. 

- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.
By reducing storage needs, you can scale applications with lower costs. This addresses a critical challenge posed by the original embedding sizes. Binary Quantization also speeds the search process. It simplifies the complex distance calculations between vectors into more manageable bitwise operations, which supports potentially real-time searches across vast datasets.
As is the case with other quantization methods, these benefits come at the cost of recall degradation. However, our implementation lets you balance the tradeoff between speed and recall accuracy at time of search, rather than time of index creation. 



The rest of this article will cover:

1. The importance of binary quantization

2. Basic implementation using our Python client

3. Benchmark analysis and usage recommendations



## What is Binary Quantization?
5 Keys to Learning from the Episode:



1. The necessity of quantization and the complex challenges it helps to overcome.

2. The transformative effects of binary quantization on processing speed and storage size reduction.

3. A detailed exploration of oversampling and its real-time precision control in query search.

4. Understanding the simplicity and effectiveness of binary quantization, especially when compared to more intricate quantization methods.\n\nAnswer:",How does binary quantization work and what benefits does it offer in terms of query processing times and data compression?,"Binary quantization is a method used by Qdrant for fast indexing and data compression. In this process, each data point is represented as a vector. The quantization splits the vector in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This results in a string of bits that represents the original vector, making it much smaller and easier to compare. By supporting vector comparisons, binary quantization can significantly speed up query processing times, up to 40 times faster. This method is especially effective for OpenAI embeddings, where it has been shown to achieve a massive performance improvement at a lower cost of accuracy.","['#### What is Binary Quantization?\n\n\n\nQuantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.\n\n\n\n[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).', '![](/blog/openai/Accuracy_Models.png)\n\n\n\nThe efficiency gains from Binary Quantization are as follows: \n\n\n\n- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. \n\n- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. \n\n- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.', 'By reducing storage needs, you can scale applications with lower costs. This addresses a critical challenge posed by the original embedding sizes. Binary Quantization also speeds the search process. It simplifies the complex distance calculations between vectors into more manageable bitwise operations, which supports potentially real-time searches across vast datasets.', 'As is the case with other quantization methods, these benefits come at the cost of recall degradation. However, our implementation lets you balance the tradeoff between speed and recall accuracy at time of search, rather than time of index creation. \n\n\n\nThe rest of this article will cover:\n\n1. The importance of binary quantization\n\n2. Basic implementation using our Python client\n\n3. Benchmark analysis and usage recommendations\n\n\n\n## What is Binary Quantization?', '5 Keys to Learning from the Episode:\n\n\n\n1. The necessity of quantization and the complex challenges it helps to overcome.\n\n2. The transformative effects of binary quantization on processing speed and storage size reduction.\n\n3. A detailed exploration of oversampling and its real-time precision control in query search.\n\n4. Understanding the simplicity and effectiveness of binary quantization, especially when compared to more intricate quantization methods.']","#### What is Binary Quantization?



Quantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.



[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).
![](/blog/openai/Accuracy_Models.png)



The efficiency gains from Binary Quantization are as follows: 



- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. 

- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. 

- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.
By reducing storage needs, you can scale applications with lower costs. This addresses a critical challenge posed by the original embedding sizes. Binary Quantization also speeds the search process. It simplifies the complex distance calculations between vectors into more manageable bitwise operations, which supports potentially real-time searches across vast datasets.
As is the case with other quantization methods, these benefits come at the cost of recall degradation. However, our implementation lets you balance the tradeoff between speed and recall accuracy at time of search, rather than time of index creation. 



The rest of this article will cover:

1. The importance of binary quantization

2. Basic implementation using our Python client

3. Benchmark analysis and usage recommendations



## What is Binary Quantization?
5 Keys to Learning from the Episode:



1. The necessity of quantization and the complex challenges it helps to overcome.

2. The transformative effects of binary quantization on processing speed and storage size reduction.

3. A detailed exploration of oversampling and its real-time precision control in query search.

4. Understanding the simplicity and effectiveness of binary quantization, especially when compared to more intricate quantization methods.",0.26785714285714285,0.6,0.37037037037037035,0.04035874439461883,0.09090909090909091,0.05590062111801242,0.13392857142857142,0.3,0.18518518518518517,0.24107142857142858,0.54,0.33333333333333337,0.8114998936653137,0.5710999965667725,0.611290693283081,0.5905122756958008,0.982830322553954,0.7918338174252616,235,2.1559633027522938,0.5464417655230579
True,,2610.993,b788b4d8,What is the primary function of vector embeddings in the context of machine learning and artificial intelligence?,"Vector embeddings play a crucial role in machine learning and artificial intelligence by converting complex data into a format that machines can easily understand and process. These embeddings represent data points in a multi-dimensional space, where similar data points are closer together and dissimilar data points are farther apart. By capturing the underlying relationships and similarities between data points, vector embeddings enable algorithms to perform tasks such as similarity search, recommendation systems, and natural language processing more effectively. In essence, vector embeddings help in transforming raw data into a structured and meaningful representation that can be utilized by machine learning models to make accurate predictions and decisions.","The primary function of vector embeddings in the context of machine learning and artificial intelligence is to capture the meaning of complex, high-dimensional data (such as text, images, or audio) into vectors. These embeddings enable algorithms to process and analyze the data more efficiently by translating the data into a lower-dimensional vector form that is more suitable for machine learning and data processing applications, typically through neural network models. Vector embeddings make high-dimensional data more manageable, reduce storage requirements, improve computational efficiency, and help make sense of unstructured data. They are particularly useful for representing the nuances of natural language or hidden meanings in large datasets, which are difficult to fit into traditional relational databases. The quality of the vector representations plays a crucial role in driving performance, and the choice of embedding model depends on the specific use case.","- machine-learning

  - artificial intelligence



---



> **Embeddings** are numerical machine learning representations of the semantic of the input data. They capture the meaning of complex, high-dimensional data, like text, images, or audio, into vectors. Enabling algorithms to process and analyze the data more efficiently.
The **creation** of vector data (so we can store this high-dimensional data on our vector database) is primarily done through **embeddings**.



![](/articles_data/what-is-a-vector-database/Vector-Data.jpg)



### How do Embeddings Work?



Embeddings translate this high-dimensional data into a more manageable, **lower-dimensional** vector form that's more suitable for machine learning and data processing applications, typically through **neural network models**.
![How embeddings are applied to perform recommendantions and other use cases](/articles_data/what-are-embeddings/Embeddings-Use-Case.jpg)





They make [high-dimensional](https://www.sciencedirect.com/topics/computer-science/high-dimensional-data) data more manageable. This reduces storage requirements, improves computational efficiency, and makes sense of a ton of **unstructured** data.





## Why Use Vector Embeddings?
## Why Use Vector Embeddings?



The **nuances** of natural language or the hidden **meaning** in large datasets of images, sounds, or user interactions are hard to fit into a table. Traditional relational databases can't efficiently query most types of data being currently used and produced, making the **retrieval** of this information very limited.
> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.





### Creating Vector Embeddings



Embeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.","['- machine-learning\n\n  - artificial intelligence\n\n\n\n---\n\n\n\n> **Embeddings** are numerical machine learning representations of the semantic of the input data. They capture the meaning of complex, high-dimensional data, like text, images, or audio, into vectors. Enabling algorithms to process and analyze the data more efficiently.', ""The **creation** of vector data (so we can store this high-dimensional data on our vector database) is primarily done through **embeddings**.\n\n\n\n![](/articles_data/what-is-a-vector-database/Vector-Data.jpg)\n\n\n\n### How do Embeddings Work?\n\n\n\nEmbeddings translate this high-dimensional data into a more manageable, **lower-dimensional** vector form that's more suitable for machine learning and data processing applications, typically through **neural network models**."", '![How embeddings are applied to perform recommendantions and other use cases](/articles_data/what-are-embeddings/Embeddings-Use-Case.jpg)\n\n\n\n\n\nThey make [high-dimensional](https://www.sciencedirect.com/topics/computer-science/high-dimensional-data) data more manageable. This reduces storage requirements, improves computational efficiency, and makes sense of a ton of **unstructured** data.\n\n\n\n\n\n## Why Use Vector Embeddings?', ""## Why Use Vector Embeddings?\n\n\n\nThe **nuances** of natural language or the hidden **meaning** in large datasets of images, sounds, or user interactions are hard to fit into a table. Traditional relational databases can't efficiently query most types of data being currently used and produced, making the **retrieval** of this information very limited."", '> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.\n\n\n\n\n\n### Creating Vector Embeddings\n\n\n\nEmbeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.']","Question: What is the primary function of vector embeddings in the context of machine learning and artificial intelligence?\n\nContext: - machine-learning

- artificial intelligence



---



> **Embeddings** are numerical machine learning representations of the semantic of the input data. They capture the meaning of complex, high-dimensional data, like text, images, or audio, into vectors. Enabling algorithms to process and analyze the data more efficiently.
The **creation** of vector data (so we can store this high-dimensional data on our vector database) is primarily done through **embeddings**.



![](/articles_data/what-is-a-vector-database/Vector-Data.jpg)



### How do Embeddings Work?



Embeddings translate this high-dimensional data into a more manageable, **lower-dimensional** vector form that's more suitable for machine learning and data processing applications, typically through **neural network models**.
![How embeddings are applied to perform recommendantions and other use cases](/articles_data/what-are-embeddings/Embeddings-Use-Case.jpg)





They make [high-dimensional](https://www.sciencedirect.com/topics/computer-science/high-dimensional-data) data more manageable. This reduces storage requirements, improves computational efficiency, and makes sense of a ton of **unstructured** data.





## Why Use Vector Embeddings?
## Why Use Vector Embeddings?



The **nuances** of natural language or the hidden **meaning** in large datasets of images, sounds, or user interactions are hard to fit into a table. Traditional relational databases can't efficiently query most types of data being currently used and produced, making the **retrieval** of this information very limited.
> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.





### Creating Vector Embeddings



Embeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.\n\nAnswer:",What is the primary function of vector embeddings in the context of machine learning and artificial intelligence?,"Vector embeddings play a crucial role in machine learning and artificial intelligence by converting complex data into a format that machines can easily understand and process. These embeddings represent data points in a multi-dimensional space, where similar data points are closer together and dissimilar data points are farther apart. By capturing the underlying relationships and similarities between data points, vector embeddings enable algorithms to perform tasks such as similarity search, recommendation systems, and natural language processing more effectively. In essence, vector embeddings help in transforming raw data into a structured and meaningful representation that can be utilized by machine learning models to make accurate predictions and decisions.","['- machine-learning\n\n  - artificial intelligence\n\n\n\n---\n\n\n\n> **Embeddings** are numerical machine learning representations of the semantic of the input data. They capture the meaning of complex, high-dimensional data, like text, images, or audio, into vectors. Enabling algorithms to process and analyze the data more efficiently.', ""The **creation** of vector data (so we can store this high-dimensional data on our vector database) is primarily done through **embeddings**.\n\n\n\n![](/articles_data/what-is-a-vector-database/Vector-Data.jpg)\n\n\n\n### How do Embeddings Work?\n\n\n\nEmbeddings translate this high-dimensional data into a more manageable, **lower-dimensional** vector form that's more suitable for machine learning and data processing applications, typically through **neural network models**."", '![How embeddings are applied to perform recommendantions and other use cases](/articles_data/what-are-embeddings/Embeddings-Use-Case.jpg)\n\n\n\n\n\nThey make [high-dimensional](https://www.sciencedirect.com/topics/computer-science/high-dimensional-data) data more manageable. This reduces storage requirements, improves computational efficiency, and makes sense of a ton of **unstructured** data.\n\n\n\n\n\n## Why Use Vector Embeddings?', ""## Why Use Vector Embeddings?\n\n\n\nThe **nuances** of natural language or the hidden **meaning** in large datasets of images, sounds, or user interactions are hard to fit into a table. Traditional relational databases can't efficiently query most types of data being currently used and produced, making the **retrieval** of this information very limited."", '> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.\n\n\n\n\n\n### Creating Vector Embeddings\n\n\n\nEmbeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.']","- machine-learning

  - artificial intelligence



---



> **Embeddings** are numerical machine learning representations of the semantic of the input data. They capture the meaning of complex, high-dimensional data, like text, images, or audio, into vectors. Enabling algorithms to process and analyze the data more efficiently.
The **creation** of vector data (so we can store this high-dimensional data on our vector database) is primarily done through **embeddings**.



![](/articles_data/what-is-a-vector-database/Vector-Data.jpg)



### How do Embeddings Work?



Embeddings translate this high-dimensional data into a more manageable, **lower-dimensional** vector form that's more suitable for machine learning and data processing applications, typically through **neural network models**.
![How embeddings are applied to perform recommendantions and other use cases](/articles_data/what-are-embeddings/Embeddings-Use-Case.jpg)





They make [high-dimensional](https://www.sciencedirect.com/topics/computer-science/high-dimensional-data) data more manageable. This reduces storage requirements, improves computational efficiency, and makes sense of a ton of **unstructured** data.





## Why Use Vector Embeddings?
## Why Use Vector Embeddings?



The **nuances** of natural language or the hidden **meaning** in large datasets of images, sounds, or user interactions are hard to fit into a table. Traditional relational databases can't efficiently query most types of data being currently used and produced, making the **retrieval** of this information very limited.
> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.





### Creating Vector Embeddings



Embeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.",0.4351145038167939,0.5533980582524272,0.4871794871794872,0.13076923076923078,0.16666666666666666,0.14655172413793102,0.21374045801526717,0.27184466019417475,0.2393162393162393,0.3893129770992366,0.49514563106796117,0.4358974358974359,0.7982562780380249,0.6726147532463074,0.703441321849823,0.6876827478408813,0.9988588118168991,0.9926148306403775,140,1.308411214953271,0.5760286664981469
True,,1695.474,63710c20,What is the concept of the `best_score` strategy introduced in version 1.6.0 of the recommendation system?,"The `best_score` strategy, introduced in version 1.6.0 of the recommendation system, is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided.","The `best_score` strategy introduced in version 1.6.0 of the recommendation system is based on the idea of finding similar vectors that are closer to a positive example while avoiding those that are closer to a negative example. This strategy takes into account multiple positive and negative examples individually, leading to a richer variety of results compared to the previous strategy where only a single vector was considered. By using the `best_score` strategy, the recommendations provided are more diverse and tailored to the specific positive and negative examples given.","This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
##### One positive and one negative example



<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>



The more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there 

is just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making 

the variety richer.
Qdrant has provided the [Recommendation API](https://qdrant.tech/documentation/concepts/search/#recommendation-api) for a while, and with the latest release, [Qdrant 1.6](https://github.com/qdrant/qdrant/releases/tag/v1.6.0), 

we're glad to give you more flexibility and control over the Recommendation API. 

Here, we'll discuss some internals and show how they may be used in practice.



### Recap of the old recommendations API
```



Example result of this API would be



```json

{

  ""result"": [

    { ""id"": 10, ""score"": 0.81 },

    { ""id"": 14, ""score"": 0.75 },

    { ""id"": 11, ""score"": 0.73 }

  ],

  ""status"": ""ok"",

  ""time"": 0.001

}

```



The algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.



### Average vector strategy
extended the API, so **you can now choose the strategy of how to find the recommended points**.



```http

POST /collections/{collection_name}/points/recommend

{

  ""positive"": [100, 231],

  ""negative"": [718, [0.2, 0.3, 0.4, 0.5]],

  ""filter"": {

        ""must"": [

            {

                ""key"": ""city"",

                ""match"": {

                    ""value"": ""London""

                }

            }

        ]

  },

  ""strategy"": ""average_vector"",

  ""limit"": 3

}

```","['This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.\n\n\n\n### Best score strategy\n\n\n\n*Available as of v1.6.0*\n\n\n\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.', '##### One positive and one negative example\n\n\n\n<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>\n\n\n\nThe more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there \n\nis just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making \n\nthe variety richer.', ""Qdrant has provided the [Recommendation API](https://qdrant.tech/documentation/concepts/search/#recommendation-api) for a while, and with the latest release, [Qdrant 1.6](https://github.com/qdrant/qdrant/releases/tag/v1.6.0), \n\nwe're glad to give you more flexibility and control over the Recommendation API. \n\nHere, we'll discuss some internals and show how they may be used in practice.\n\n\n\n### Recap of the old recommendations API"", '```\n\n\n\nExample result of this API would be\n\n\n\n```json\n\n{\n\n  ""result"": [\n\n    { ""id"": 10, ""score"": 0.81 },\n\n    { ""id"": 14, ""score"": 0.75 },\n\n    { ""id"": 11, ""score"": 0.73 }\n\n  ],\n\n  ""status"": ""ok"",\n\n  ""time"": 0.001\n\n}\n\n```\n\n\n\nThe algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.\n\n\n\n### Average vector strategy', 'extended the API, so **you can now choose the strategy of how to find the recommended points**.\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/recommend\n\n{\n\n  ""positive"": [100, 231],\n\n  ""negative"": [718, [0.2, 0.3, 0.4, 0.5]],\n\n  ""filter"": {\n\n        ""must"": [\n\n            {\n\n                ""key"": ""city"",\n\n                ""match"": {\n\n                    ""value"": ""London""\n\n                }\n\n            }\n\n        ]\n\n  },\n\n  ""strategy"": ""average_vector"",\n\n  ""limit"": 3\n\n}\n\n```']","Question: What is the concept of the `best_score` strategy introduced in version 1.6.0 of the recommendation system?\n\nContext: This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
##### One positive and one negative example



<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>



The more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there 

is just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making 

the variety richer.
Qdrant has provided the [Recommendation API](https://qdrant.tech/documentation/concepts/search/#recommendation-api) for a while, and with the latest release, [Qdrant 1.6](https://github.com/qdrant/qdrant/releases/tag/v1.6.0), 

we're glad to give you more flexibility and control over the Recommendation API. 

Here, we'll discuss some internals and show how they may be used in practice.



### Recap of the old recommendations API
```



Example result of this API would be



```json

{

""result"": [

{ ""id"": 10, ""score"": 0.81 },

{ ""id"": 14, ""score"": 0.75 },

{ ""id"": 11, ""score"": 0.73 }

],

""status"": ""ok"",

""time"": 0.001

}

```



The algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.



### Average vector strategy
extended the API, so **you can now choose the strategy of how to find the recommended points**.



```http

POST /collections/{collection_name}/points/recommend

{

""positive"": [100, 231],

""negative"": [718, [0.2, 0.3, 0.4, 0.5]],

""filter"": {

""must"": [

{

""key"": ""city"",

""match"": {

""value"": ""London""

}

}

]

},

""strategy"": ""average_vector"",

""limit"": 3

}

```\n\nAnswer:",What is the concept of the `best_score` strategy introduced in version 1.6.0 of the recommendation system?,"The `best_score` strategy, introduced in version 1.6.0 of the recommendation system, is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided.","['This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.\n\n\n\n### Best score strategy\n\n\n\n*Available as of v1.6.0*\n\n\n\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.', '##### One positive and one negative example\n\n\n\n<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>\n\n\n\nThe more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there \n\nis just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making \n\nthe variety richer.', ""Qdrant has provided the [Recommendation API](https://qdrant.tech/documentation/concepts/search/#recommendation-api) for a while, and with the latest release, [Qdrant 1.6](https://github.com/qdrant/qdrant/releases/tag/v1.6.0), \n\nwe're glad to give you more flexibility and control over the Recommendation API. \n\nHere, we'll discuss some internals and show how they may be used in practice.\n\n\n\n### Recap of the old recommendations API"", '```\n\n\n\nExample result of this API would be\n\n\n\n```json\n\n{\n\n  ""result"": [\n\n    { ""id"": 10, ""score"": 0.81 },\n\n    { ""id"": 14, ""score"": 0.75 },\n\n    { ""id"": 11, ""score"": 0.73 }\n\n  ],\n\n  ""status"": ""ok"",\n\n  ""time"": 0.001\n\n}\n\n```\n\n\n\nThe algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.\n\n\n\n### Average vector strategy', 'extended the API, so **you can now choose the strategy of how to find the recommended points**.\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/recommend\n\n{\n\n  ""positive"": [100, 231],\n\n  ""negative"": [718, [0.2, 0.3, 0.4, 0.5]],\n\n  ""filter"": {\n\n        ""must"": [\n\n            {\n\n                ""key"": ""city"",\n\n                ""match"": {\n\n                    ""value"": ""London""\n\n                }\n\n            }\n\n        ]\n\n  },\n\n  ""strategy"": ""average_vector"",\n\n  ""limit"": 3\n\n}\n\n```']","This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
##### One positive and one negative example



<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>



The more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there 

is just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making 

the variety richer.
Qdrant has provided the [Recommendation API](https://qdrant.tech/documentation/concepts/search/#recommendation-api) for a while, and with the latest release, [Qdrant 1.6](https://github.com/qdrant/qdrant/releases/tag/v1.6.0), 

we're glad to give you more flexibility and control over the Recommendation API. 

Here, we'll discuss some internals and show how they may be used in practice.



### Recap of the old recommendations API
```



Example result of this API would be



```json

{

  ""result"": [

    { ""id"": 10, ""score"": 0.81 },

    { ""id"": 14, ""score"": 0.75 },

    { ""id"": 11, ""score"": 0.73 }

  ],

  ""status"": ""ok"",

  ""time"": 0.001

}

```



The algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.



### Average vector strategy
extended the API, so **you can now choose the strategy of how to find the recommended points**.



```http

POST /collections/{collection_name}/points/recommend

{

  ""positive"": [100, 231],

  ""negative"": [718, [0.2, 0.3, 0.4, 0.5]],

  ""filter"": {

        ""must"": [

            {

                ""key"": ""city"",

                ""match"": {

                    ""value"": ""London""

                }

            }

        ]

  },

  ""strategy"": ""average_vector"",

  ""limit"": 3

}

```",0.654320987654321,0.4774774774774775,0.5520833333333334,0.45,0.32727272727272727,0.3789473684210526,0.5555555555555556,0.40540540540540543,0.46875,0.6172839506172839,0.45045045045045046,0.5208333333333334,0.9136089086532593,0.7150770425796509,0.6712996959686279,0.692497193813324,0.9676580579640964,0.7503289729356766,88,0.6984126984126984,0.3142829445092636
True,,4641.7570000000005,c7bae3d2,How does Qdrant handle sparse vectors and what changes have been made to accommodate them?,"Qdrant, until recently, was not able to handle sparse vectors natively. The majority of dimensions in sparse vectors are usually zeros, making them theoretically high dimensional. However, Qdrant stores sparse vectors differently by only keeping track of the non-zero dimensions. Previously, some users attempted to convert sparse vectors to dense vectors as a workaround, but this was not considered the best solution. In response to user demand for a single tool that can handle both sparse and dense vectors, Qdrant has now introduced support for sparse vectors. This change allows users to work with both sparse and dense vectors seamlessly within the Qdrant tool, eliminating the need for alternative solutions or tools for keyword lookup.","Qdrant now has the capability to handle sparse vectors natively. To accommodate sparse vectors, changes have been made to support them effectively. Here are some key points on how Qdrant handles sparse vectors and the changes made to accommodate them:

1. **Creating a Collection with Sparse Vector Support**:
   - When creating a collection in Qdrant, you can specify support for sparse vectors using the `sparse_vectors_config` parameter.
   - Sparse vectors are defined using `models.SparseVectorParams`, which allows you to configure indexing parameters such as whether the index should be stored on disk.

2. **Inserting Sparse Vectors**:
   - Sparse vectors can be inserted into a collection using the Qdrant API.
   - The indices and values of the sparse vector are provided in arrays, where the indices must be unique and the arrays must have the same length.
   - Qdrant handles sorting of the indices internally, so the order of elements in the arrays may not be preserved.
   - Sparse vectors are named and can be uploaded in a similar manner to dense vectors.

3. **Additional Resources**:
   - Qdrant provides documentation and articles on sparse vectors to help users understand the concept and usage of sparse vectors within the platform.
   - Users are encouraged to refer to the sparse vectors article and sparse vectors index documentation for more detailed information on how to leverage sparse vectors effectively in Qdrant.

Overall, Qdrant now supports sparse vectors, and users can take advantage of this feature by following the specified guidelines for inserting and working with sparse vectors in their collections.","Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.
# Insert sparse vector into Qdrant collection

point_id = 1  # Assign a unique ID for the point

```



### 2. Creating a Collection with Sparse Vector Support



```python

client.recreate_collection(

    collection_name=COLLECTION_NAME,

    vectors_config={},

    sparse_vectors_config={

        ""text"": models.SparseVectorParams(

            index=models.SparseIndexParams(

                on_disk=False,

            )

        )

    },

)

```





### 3. Inserting Sparse Vectors
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
```json

{

  ""indices"": [6, 7],

  ""values"": [1.0, 2.0]

}

```



The `indices` and `values` arrays must have the same length.

And the `indices` must be unique.



If the `indices` are not sorted, Qdrant will sort them internally so you may not rely on the order of the elements.



Sparse vectors must be named and can be uploaded in the same way as dense vectors.



```http

PUT /collections/{collection_name}/points

{

    ""points"": [

        {

            ""id"": 1,

            ""vector"": {
Check out the [sparse vectors article](../sparse-vectors/) and [sparse vectors index docs](/documentation/concepts/indexing/#sparse-vector-index) for more details on what this new index means for Qdrant users.



### Discovery API","['Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.', '# Insert sparse vector into Qdrant collection\n\npoint_id = 1  # Assign a unique ID for the point\n\n```\n\n\n\n### 2. Creating a Collection with Sparse Vector Support\n\n\n\n```python\n\nclient.recreate_collection(\n\n    collection_name=COLLECTION_NAME,\n\n    vectors_config={},\n\n    sparse_vectors_config={\n\n        ""text"": models.SparseVectorParams(\n\n            index=models.SparseIndexParams(\n\n                on_disk=False,\n\n            )\n\n        )\n\n    },\n\n)\n\n```\n\n\n\n\n\n### 3. Inserting Sparse Vectors', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", '```json\n\n{\n\n  ""indices"": [6, 7],\n\n  ""values"": [1.0, 2.0]\n\n}\n\n```\n\n\n\nThe `indices` and `values` arrays must have the same length.\n\nAnd the `indices` must be unique.\n\n\n\nIf the `indices` are not sorted, Qdrant will sort them internally so you may not rely on the order of the elements.\n\n\n\nSparse vectors must be named and can be uploaded in the same way as dense vectors.\n\n\n\n```http\n\nPUT /collections/{collection_name}/points\n\n{\n\n    ""points"": [\n\n        {\n\n            ""id"": 1,\n\n            ""vector"": {', 'Check out the [sparse vectors article](../sparse-vectors/) and [sparse vectors index docs](/documentation/concepts/indexing/#sparse-vector-index) for more details on what this new index means for Qdrant users.\n\n\n\n### Discovery API']","Question: How does Qdrant handle sparse vectors and what changes have been made to accommodate them?\n\nContext: Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.
# Insert sparse vector into Qdrant collection

point_id = 1  # Assign a unique ID for the point

```



### 2. Creating a Collection with Sparse Vector Support



```python

client.recreate_collection(

collection_name=COLLECTION_NAME,

vectors_config={},

sparse_vectors_config={

""text"": models.SparseVectorParams(

index=models.SparseIndexParams(

on_disk=False,

)

)

},

)

```





### 3. Inserting Sparse Vectors
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
```json

{

""indices"": [6, 7],

""values"": [1.0, 2.0]

}

```



The `indices` and `values` arrays must have the same length.

And the `indices` must be unique.



If the `indices` are not sorted, Qdrant will sort them internally so you may not rely on the order of the elements.



Sparse vectors must be named and can be uploaded in the same way as dense vectors.



```http

PUT /collections/{collection_name}/points

{

""points"": [

{

""id"": 1,

""vector"": {
Check out the [sparse vectors article](../sparse-vectors/) and [sparse vectors index docs](/documentation/concepts/indexing/#sparse-vector-index) for more details on what this new index means for Qdrant users.



### Discovery API\n\nAnswer:",How does Qdrant handle sparse vectors and what changes have been made to accommodate them?,"Qdrant, until recently, was not able to handle sparse vectors natively. The majority of dimensions in sparse vectors are usually zeros, making them theoretically high dimensional. However, Qdrant stores sparse vectors differently by only keeping track of the non-zero dimensions. Previously, some users attempted to convert sparse vectors to dense vectors as a workaround, but this was not considered the best solution. In response to user demand for a single tool that can handle both sparse and dense vectors, Qdrant has now introduced support for sparse vectors. This change allows users to work with both sparse and dense vectors seamlessly within the Qdrant tool, eliminating the need for alternative solutions or tools for keyword lookup.","['Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.', '# Insert sparse vector into Qdrant collection\n\npoint_id = 1  # Assign a unique ID for the point\n\n```\n\n\n\n### 2. Creating a Collection with Sparse Vector Support\n\n\n\n```python\n\nclient.recreate_collection(\n\n    collection_name=COLLECTION_NAME,\n\n    vectors_config={},\n\n    sparse_vectors_config={\n\n        ""text"": models.SparseVectorParams(\n\n            index=models.SparseIndexParams(\n\n                on_disk=False,\n\n            )\n\n        )\n\n    },\n\n)\n\n```\n\n\n\n\n\n### 3. Inserting Sparse Vectors', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", '```json\n\n{\n\n  ""indices"": [6, 7],\n\n  ""values"": [1.0, 2.0]\n\n}\n\n```\n\n\n\nThe `indices` and `values` arrays must have the same length.\n\nAnd the `indices` must be unique.\n\n\n\nIf the `indices` are not sorted, Qdrant will sort them internally so you may not rely on the order of the elements.\n\n\n\nSparse vectors must be named and can be uploaded in the same way as dense vectors.\n\n\n\n```http\n\nPUT /collections/{collection_name}/points\n\n{\n\n    ""points"": [\n\n        {\n\n            ""id"": 1,\n\n            ""vector"": {', 'Check out the [sparse vectors article](../sparse-vectors/) and [sparse vectors index docs](/documentation/concepts/indexing/#sparse-vector-index) for more details on what this new index means for Qdrant users.\n\n\n\n### Discovery API']","Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.
# Insert sparse vector into Qdrant collection

point_id = 1  # Assign a unique ID for the point

```



### 2. Creating a Collection with Sparse Vector Support



```python

client.recreate_collection(

    collection_name=COLLECTION_NAME,

    vectors_config={},

    sparse_vectors_config={

        ""text"": models.SparseVectorParams(

            index=models.SparseIndexParams(

                on_disk=False,

            )

        )

    },

)

```





### 3. Inserting Sparse Vectors
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
```json

{

  ""indices"": [6, 7],

  ""values"": [1.0, 2.0]

}

```



The `indices` and `values` arrays must have the same length.

And the `indices` must be unique.



If the `indices` are not sorted, Qdrant will sort them internally so you may not rely on the order of the elements.



Sparse vectors must be named and can be uploaded in the same way as dense vectors.



```http

PUT /collections/{collection_name}/points

{

    ""points"": [

        {

            ""id"": 1,

            ""vector"": {
Check out the [sparse vectors article](../sparse-vectors/) and [sparse vectors index docs](/documentation/concepts/indexing/#sparse-vector-index) for more details on what this new index means for Qdrant users.



### Discovery API",0.25,0.5229357798165137,0.3382789317507418,0.06607929515418502,0.1388888888888889,0.08955223880597014,0.15350877192982457,0.3211009174311927,0.20771513353115728,0.2324561403508772,0.48623853211009177,0.314540059347181,0.8464949131011963,0.573069155216217,0.6002587676048279,0.5863489508628845,0.6607610065839253,0.4770784281936358,246,2.139130434782609,0.45178643798585144
True,,2394.1020000000003,a8720c63,What is the purpose of the `prefix` parameter in the configuration options for the Qdrant instance?,"The `prefix` parameter in the configuration options for the Qdrant instance serves the purpose of adding a specific prefix to the REST URL path. For example, setting the `prefix` parameter to `service/v1` would result in the REST API endpoint being accessed at `http://localhost:6333/service/v1/{qdrant-endpoint}`. This allows for customization and organization of the API endpoints under a specific path, making it easier to manage and access different functionalities within the Qdrant instance.","The `prefix` parameter in the configuration options for the Qdrant instance serves as a way to group and organize environment variables. By setting a specific prefix, such as `QDRANT__`, all environment variables related to Qdrant can be easily identified and distinguished from other variables. Additionally, using a prefix helps in maintaining a clean and structured configuration setup.

In the given example, all Qdrant-related environment variables start with `QDRANT__` and are followed by specific configurations like `LOG_LEVEL`, `SERVICE`, `TLS`, etc. This prefixing convention makes it clear that these variables are associated with the Qdrant instance.

When these prefixed environment variables are parsed, they are translated into a structured configuration format, such as YAML, as shown in the example. This structured configuration is easier to read and manage compared to individual environment variables.","(see [ordering](#order-and-priority)).



All environment variables are prefixed with `QDRANT__` and are separated with

`__`.



These variables:



```bash

QDRANT__LOG_LEVEL=INFO

QDRANT__SERVICE__HTTP_PORT=6333

QDRANT__SERVICE__ENABLE_TLS=1

QDRANT__TLS__CERT=./tls/cert.pem

QDRANT__TLS__CERT_TTL=3600

```



result in this configuration:



```yaml

log_level: INFO

service:

  http_port: 6333

  enable_tls: true

tls:

  cert: ./tls/cert.pem

  cert_ttl: 3600

```
WITH ENGINE = ""qdrant"",

PARAMETERS = {

    ""location"": "":memory:"",

    ""collection_config"": {

        ""size"": 386,

        ""distance"": ""Cosine""

    }

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
import io.qdrant.client.grpc.Collections.QuantizationConfig;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()

                    .setParams(

                        VectorParams.newBuilder()

                            .setSize(768)
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()

                    .setParams(

                        VectorParams.newBuilder()

                            .setSize(768)","['(see [ordering](#order-and-priority)).\n\n\n\nAll environment variables are prefixed with `QDRANT__` and are separated with\n\n`__`.\n\n\n\nThese variables:\n\n\n\n```bash\n\nQDRANT__LOG_LEVEL=INFO\n\nQDRANT__SERVICE__HTTP_PORT=6333\n\nQDRANT__SERVICE__ENABLE_TLS=1\n\nQDRANT__TLS__CERT=./tls/cert.pem\n\nQDRANT__TLS__CERT_TTL=3600\n\n```\n\n\n\nresult in this configuration:\n\n\n\n```yaml\n\nlog_level: INFO\n\nservice:\n\n  http_port: 6333\n\n  enable_tls: true\n\ntls:\n\n  cert: ./tls/cert.pem\n\n  cert_ttl: 3600\n\n```', 'WITH ENGINE = ""qdrant"",\n\nPARAMETERS = {\n\n    ""location"": "":memory:"",\n\n    ""collection_config"": {\n\n        ""size"": 386,\n\n        ""distance"": ""Cosine""\n\n    }\n\n}\n\n```\n\n\n\nThe available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).\n\n\n\n## Creating a new table', 'import io.qdrant.client.grpc.Collections.QuantizationConfig;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()', 'import io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()\n\n                    .setParams(\n\n                        VectorParams.newBuilder()\n\n                            .setSize(768)', 'import io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()\n\n                    .setParams(\n\n                        VectorParams.newBuilder()\n\n                            .setSize(768)']","Question: What is the purpose of the `prefix` parameter in the configuration options for the Qdrant instance?\n\nContext: (see [ordering](#order-and-priority)).



All environment variables are prefixed with `QDRANT__` and are separated with

`__`.



These variables:



```bash

QDRANT__LOG_LEVEL=INFO

QDRANT__SERVICE__HTTP_PORT=6333

QDRANT__SERVICE__ENABLE_TLS=1

QDRANT__TLS__CERT=./tls/cert.pem

QDRANT__TLS__CERT_TTL=3600

```



result in this configuration:



```yaml

log_level: INFO

service:

http_port: 6333

enable_tls: true

tls:

cert: ./tls/cert.pem

cert_ttl: 3600

```
WITH ENGINE = ""qdrant"",

PARAMETERS = {

""location"": "":memory:"",

""collection_config"": {

""size"": 386,

""distance"": ""Cosine""

}

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
import io.qdrant.client.grpc.Collections.QuantizationConfig;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createCollectionAsync(

CreateCollection.newBuilder()

.setCollectionName(""{collection_name}"")

.setVectorsConfig(

VectorsConfig.newBuilder()
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createCollectionAsync(

CreateCollection.newBuilder()

.setCollectionName(""{collection_name}"")

.setVectorsConfig(

VectorsConfig.newBuilder()

.setParams(

VectorParams.newBuilder()

.setSize(768)
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createCollectionAsync(

CreateCollection.newBuilder()

.setCollectionName(""{collection_name}"")

.setVectorsConfig(

VectorsConfig.newBuilder()

.setParams(

VectorParams.newBuilder()

.setSize(768)\n\nAnswer:",What is the purpose of the `prefix` parameter in the configuration options for the Qdrant instance?,"The `prefix` parameter in the configuration options for the Qdrant instance serves the purpose of adding a specific prefix to the REST URL path. For example, setting the `prefix` parameter to `service/v1` would result in the REST API endpoint being accessed at `http://localhost:6333/service/v1/{qdrant-endpoint}`. This allows for customization and organization of the API endpoints under a specific path, making it easier to manage and access different functionalities within the Qdrant instance.","['(see [ordering](#order-and-priority)).\n\n\n\nAll environment variables are prefixed with `QDRANT__` and are separated with\n\n`__`.\n\n\n\nThese variables:\n\n\n\n```bash\n\nQDRANT__LOG_LEVEL=INFO\n\nQDRANT__SERVICE__HTTP_PORT=6333\n\nQDRANT__SERVICE__ENABLE_TLS=1\n\nQDRANT__TLS__CERT=./tls/cert.pem\n\nQDRANT__TLS__CERT_TTL=3600\n\n```\n\n\n\nresult in this configuration:\n\n\n\n```yaml\n\nlog_level: INFO\n\nservice:\n\n  http_port: 6333\n\n  enable_tls: true\n\ntls:\n\n  cert: ./tls/cert.pem\n\n  cert_ttl: 3600\n\n```', 'WITH ENGINE = ""qdrant"",\n\nPARAMETERS = {\n\n    ""location"": "":memory:"",\n\n    ""collection_config"": {\n\n        ""size"": 386,\n\n        ""distance"": ""Cosine""\n\n    }\n\n}\n\n```\n\n\n\nThe available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).\n\n\n\n## Creating a new table', 'import io.qdrant.client.grpc.Collections.QuantizationConfig;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()', 'import io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()\n\n                    .setParams(\n\n                        VectorParams.newBuilder()\n\n                            .setSize(768)', 'import io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()\n\n                    .setParams(\n\n                        VectorParams.newBuilder()\n\n                            .setSize(768)']","(see [ordering](#order-and-priority)).



All environment variables are prefixed with `QDRANT__` and are separated with

`__`.



These variables:



```bash

QDRANT__LOG_LEVEL=INFO

QDRANT__SERVICE__HTTP_PORT=6333

QDRANT__SERVICE__ENABLE_TLS=1

QDRANT__TLS__CERT=./tls/cert.pem

QDRANT__TLS__CERT_TTL=3600

```



result in this configuration:



```yaml

log_level: INFO

service:

  http_port: 6333

  enable_tls: true

tls:

  cert: ./tls/cert.pem

  cert_ttl: 3600

```
WITH ENGINE = ""qdrant"",

PARAMETERS = {

    ""location"": "":memory:"",

    ""collection_config"": {

        ""size"": 386,

        ""distance"": ""Cosine""

    }

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
import io.qdrant.client.grpc.Collections.QuantizationConfig;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()

                    .setParams(

                        VectorParams.newBuilder()

                            .setSize(768)
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()

                    .setParams(

                        VectorParams.newBuilder()

                            .setSize(768)",0.26229508196721313,0.48484848484848486,0.34042553191489355,0.10743801652892562,0.2,0.13978494623655915,0.18032786885245902,0.3333333333333333,0.23404255319148934,0.2540983606557377,0.4696969696969697,0.3297872340425532,0.8073009848594666,0.616947591304779,0.650213360786438,0.6331438422203064,0.9779569177688765,0.7358430524667103,131,1.8194444444444444,0.501109818924532
True,,660.186,efc2bec0,What is the default scoring metric used in Qdrant for search operations?,"The default scoring metric used in Qdrant for search operations is `Dot product`. It is not necessary to specify this metric explicitly as it is the default setting. However, Qdrant also supports other distance metrics such as Cosine for scoring. When performing a search operation in Qdrant, if no specific metric is mentioned, the system will automatically use the Dot product as the scoring metric.",The default scoring metric used in Qdrant for search operations is the Dot product.,"| Index| Sparse Query | Dense Query |

| --- | --- | --- | 

| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |

| Search Type | Always exact in Qdrant | HNSW is an approximate NN |

| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |



In general, the speed of the search is proportional to the number of non-zero values in the query vector.



```http
qdrant: &QdrantClient,

) -> Result<Vec<ScoredPoint>> {

    Ok(qdrant.search_points(&SearchPoints {

        collection_name,

        limit: 5, // use what fits your use case here

        with_payload: Some(true.into()),

        vector: embed(client, text, api_key)?,

        ..Default::default()

    }).await?.result)

}

```
search_points: searches,

        read_consistency: None,

        ..Default::default()

    })

    .await?;

```



```java

import java.util.List;



import static io.qdrant.client.ConditionFactory.matchKeyword;



import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Points.Filter;

import io.qdrant.client.grpc.Points.SearchPoints;



QdrantClient client =
to do it. 



## Wrapping up



Assessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.

your search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, 

even in an automated way, as part of your CI/CD pipeline.
""count"": 10,

    ""price"": 11.99,

    ""locations"": [

        {

            ""lon"": 52.5200, 

            ""lat"": 13.4050

        }

    ],

    ""reviews"": [

        {

            ""user"": ""alice"",

            ""score"": 4

        },

        {

            ""user"": ""bob"",

            ""score"": 5

        }

    ]

}

```



## Payload types



In addition to storing payloads, Qdrant also allows you search based on certain kinds of values.","['| Index| Sparse Query | Dense Query |\n\n| --- | --- | --- | \n\n| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |\n\n| Search Type | Always exact in Qdrant | HNSW is an approximate NN |\n\n| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |\n\n\n\nIn general, the speed of the search is proportional to the number of non-zero values in the query vector.\n\n\n\n```http', 'qdrant: &QdrantClient,\n\n) -> Result<Vec<ScoredPoint>> {\n\n    Ok(qdrant.search_points(&SearchPoints {\n\n        collection_name,\n\n        limit: 5, // use what fits your use case here\n\n        with_payload: Some(true.into()),\n\n        vector: embed(client, text, api_key)?,\n\n        ..Default::default()\n\n    }).await?.result)\n\n}\n\n```', 'search_points: searches,\n\n        read_consistency: None,\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport java.util.List;\n\n\n\nimport static io.qdrant.client.ConditionFactory.matchKeyword;\n\n\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Points.Filter;\n\nimport io.qdrant.client.grpc.Points.SearchPoints;\n\n\n\nQdrantClient client =', 'to do it. \n\n\n\n## Wrapping up\n\n\n\nAssessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.\n\nyour search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, \n\neven in an automated way, as part of your CI/CD pipeline.', '""count"": 10,\n\n    ""price"": 11.99,\n\n    ""locations"": [\n\n        {\n\n            ""lon"": 52.5200, \n\n            ""lat"": 13.4050\n\n        }\n\n    ],\n\n    ""reviews"": [\n\n        {\n\n            ""user"": ""alice"",\n\n            ""score"": 4\n\n        },\n\n        {\n\n            ""user"": ""bob"",\n\n            ""score"": 5\n\n        }\n\n    ]\n\n}\n\n```\n\n\n\n## Payload types\n\n\n\nIn addition to storing payloads, Qdrant also allows you search based on certain kinds of values.']","Question: What is the default scoring metric used in Qdrant for search operations?\n\nContext: | Index| Sparse Query | Dense Query |

| --- | --- | --- | 

| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |

| Search Type | Always exact in Qdrant | HNSW is an approximate NN |

| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |



In general, the speed of the search is proportional to the number of non-zero values in the query vector.



```http
qdrant: &QdrantClient,

) -> Result<Vec<ScoredPoint>> {

Ok(qdrant.search_points(&SearchPoints {

collection_name,

limit: 5, // use what fits your use case here

with_payload: Some(true.into()),

vector: embed(client, text, api_key)?,

..Default::default()

}).await?.result)

}

```
search_points: searches,

read_consistency: None,

..Default::default()

})

.await?;

```



```java

import java.util.List;



import static io.qdrant.client.ConditionFactory.matchKeyword;



import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Points.Filter;

import io.qdrant.client.grpc.Points.SearchPoints;



QdrantClient client =
to do it. 



## Wrapping up



Assessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.

your search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, 

even in an automated way, as part of your CI/CD pipeline.
""count"": 10,

""price"": 11.99,

""locations"": [

{

""lon"": 52.5200, 

""lat"": 13.4050

}

],

""reviews"": [

{

""user"": ""alice"",

""score"": 4

},

{

""user"": ""bob"",

""score"": 5

}

]

}

```



## Payload types



In addition to storing payloads, Qdrant also allows you search based on certain kinds of values.\n\nAnswer:",What is the default scoring metric used in Qdrant for search operations?,"The default scoring metric used in Qdrant for search operations is `Dot product`. It is not necessary to specify this metric explicitly as it is the default setting. However, Qdrant also supports other distance metrics such as Cosine for scoring. When performing a search operation in Qdrant, if no specific metric is mentioned, the system will automatically use the Dot product as the scoring metric.","['| Index| Sparse Query | Dense Query |\n\n| --- | --- | --- | \n\n| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |\n\n| Search Type | Always exact in Qdrant | HNSW is an approximate NN |\n\n| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |\n\n\n\nIn general, the speed of the search is proportional to the number of non-zero values in the query vector.\n\n\n\n```http', 'qdrant: &QdrantClient,\n\n) -> Result<Vec<ScoredPoint>> {\n\n    Ok(qdrant.search_points(&SearchPoints {\n\n        collection_name,\n\n        limit: 5, // use what fits your use case here\n\n        with_payload: Some(true.into()),\n\n        vector: embed(client, text, api_key)?,\n\n        ..Default::default()\n\n    }).await?.result)\n\n}\n\n```', 'search_points: searches,\n\n        read_consistency: None,\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport java.util.List;\n\n\n\nimport static io.qdrant.client.ConditionFactory.matchKeyword;\n\n\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Points.Filter;\n\nimport io.qdrant.client.grpc.Points.SearchPoints;\n\n\n\nQdrantClient client =', 'to do it. \n\n\n\n## Wrapping up\n\n\n\nAssessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.\n\nyour search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, \n\neven in an automated way, as part of your CI/CD pipeline.', '""count"": 10,\n\n    ""price"": 11.99,\n\n    ""locations"": [\n\n        {\n\n            ""lon"": 52.5200, \n\n            ""lat"": 13.4050\n\n        }\n\n    ],\n\n    ""reviews"": [\n\n        {\n\n            ""user"": ""alice"",\n\n            ""score"": 4\n\n        },\n\n        {\n\n            ""user"": ""bob"",\n\n            ""score"": 5\n\n        }\n\n    ]\n\n}\n\n```\n\n\n\n## Payload types\n\n\n\nIn addition to storing payloads, Qdrant also allows you search based on certain kinds of values.']","| Index| Sparse Query | Dense Query |

| --- | --- | --- | 

| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |

| Search Type | Always exact in Qdrant | HNSW is an approximate NN |

| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |



In general, the speed of the search is proportional to the number of non-zero values in the query vector.



```http
qdrant: &QdrantClient,

) -> Result<Vec<ScoredPoint>> {

    Ok(qdrant.search_points(&SearchPoints {

        collection_name,

        limit: 5, // use what fits your use case here

        with_payload: Some(true.into()),

        vector: embed(client, text, api_key)?,

        ..Default::default()

    }).await?.result)

}

```
search_points: searches,

        read_consistency: None,

        ..Default::default()

    })

    .await?;

```



```java

import java.util.List;



import static io.qdrant.client.ConditionFactory.matchKeyword;



import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Points.Filter;

import io.qdrant.client.grpc.Points.SearchPoints;



QdrantClient client =
to do it. 



## Wrapping up



Assessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.

your search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, 

even in an automated way, as part of your CI/CD pipeline.
""count"": 10,

    ""price"": 11.99,

    ""locations"": [

        {

            ""lon"": 52.5200, 

            ""lat"": 13.4050

        }

    ],

    ""reviews"": [

        {

            ""user"": ""alice"",

            ""score"": 4

        },

        {

            ""user"": ""bob"",

            ""score"": 5

        }

    ]

}

```



## Payload types



In addition to storing payloads, Qdrant also allows you search based on certain kinds of values.",1.0,0.2033898305084746,0.33802816901408456,1.0,0.1896551724137931,0.3188405797101449,1.0,0.2033898305084746,0.33802816901408456,1.0,0.2033898305084746,0.33802816901408456,0.9470928907394409,0.7628515958786011,0.5855770707130432,0.6625614166259766,0.9989146265434101,0.9954970267281169,14,0.2153846153846154,0.34301803806975684
True,,4189.987999999999,3d347a27,What options are available for using Qdrant if the official clients do not support the language you are using?,"If the official clients for Qdrant do not support the language you are using, you have the option to either use the REST API directly or generate a client for your language using OpenAPI definitions or protobuf definitions. By utilizing the OpenAPI definitions available at [this link](https://github.com/qdrant/qdrant/blob/master/docs/redoc/master/openapi.json) or the protobuf definitions at [this link](https://github.com/qdrant/qdrant/tree/master/lib/api/src/grpc/proto), you can interact with Qdrant's functionalities even if there is no official client available for your specific programming language. This flexibility allows developers to integrate Qdrant into a wide range of applications and environments.","If the official clients of Qdrant do not support the language you are using, you still have a few options available to interact with Qdrant:

1. **Using HTTP API directly**: Qdrant provides a more flexible API in version 1.6, allowing you to interact with Qdrant by sending HTTP requests directly. This method can be language-agnostic as long as you can make HTTP requests in your programming language.

2. **Building a custom client**: If you have a specific language that is not officially supported by Qdrant, you can build a custom client in that language that interacts with Qdrant using the available API endpoints.

3. **Utilizing language bridges or wrappers**: You can explore if there are any existing language bridges or wrappers available that can help you connect your preferred language to Qdrant. These bridges can act as intermediaries between your application and Qdrant, enabling communication even in unsupported languages. 

4. **Contributing to the Qdrant project**: If you have the expertise and willingness, you can contribute to the Qdrant project by adding support for your preferred language. This way, you can help expand the supported languages for Qdrant and benefit the broader community.

By exploring these options, you can still leverage the capabilities of Qdrant even if your preferred language is not officially supported by the provided clients.","**Clients**: Qdrant supports various programming languages for client interaction, such as Python, Go, Rust, and Typescript. This way developers can connect to and interact with Qdrant using the programming language they prefer.





### Vector Database Use Cases
. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.
So one thing which I mean, one is definitely try the different models which we support. We support a reasonable range of models, including a few multilingual ones. Second is while we take care of this when you do use with Qdrants. So, for instance, let's say this is how you would have to manually specify, let's say, passage or query. When you do this, let's say add and query. What we do, we add the passage and query keys while creating the Embeddings for you. So this is taken care of
of code, thanks to the recent integration with Qdrant. We're not even going to work directly with `QdrantClient`, as everything is already done in the background

by LangChain. If you want to get into the source code right away, all the processing is available as a 

[Google Colab notebook](https://colab.research.google.com/drive/19RxxkZdnq_YqBH5kBV10Rt0Rax-kminD?usp=sharing).



## Implementing Question Answering with LangChain and Qdrant



### Configuration
However, if the only information you had was that your user dislikes some items, you couldn't use it directly.



Qdrant 1.6 brings a more flexible API. You can now provide both IDs and vectors of positive and negative examples. You can even 

combine them within a single request. That makes the new implementation backward compatible, so you can easily upgrade an existing

Qdrant instance without any changes in your code. And the default behaviour of the API is still the same as before. However, we","['**Clients**: Qdrant supports various programming languages for client interaction, such as Python, Go, Rust, and Typescript. This way developers can connect to and interact with Qdrant using the programming language they prefer.\n\n\n\n\n\n### Vector Database Use Cases', '. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.', ""So one thing which I mean, one is definitely try the different models which we support. We support a reasonable range of models, including a few multilingual ones. Second is while we take care of this when you do use with Qdrants. So, for instance, let's say this is how you would have to manually specify, let's say, passage or query. When you do this, let's say add and query. What we do, we add the passage and query keys while creating the Embeddings for you. So this is taken care of"", ""of code, thanks to the recent integration with Qdrant. We're not even going to work directly with `QdrantClient`, as everything is already done in the background\n\nby LangChain. If you want to get into the source code right away, all the processing is available as a \n\n[Google Colab notebook](https://colab.research.google.com/drive/19RxxkZdnq_YqBH5kBV10Rt0Rax-kminD?usp=sharing).\n\n\n\n## Implementing Question Answering with LangChain and Qdrant\n\n\n\n### Configuration"", ""However, if the only information you had was that your user dislikes some items, you couldn't use it directly.\n\n\n\nQdrant 1.6 brings a more flexible API. You can now provide both IDs and vectors of positive and negative examples. You can even \n\ncombine them within a single request. That makes the new implementation backward compatible, so you can easily upgrade an existing\n\nQdrant instance without any changes in your code. And the default behaviour of the API is still the same as before. However, we""]","Question: What options are available for using Qdrant if the official clients do not support the language you are using?\n\nContext: **Clients**: Qdrant supports various programming languages for client interaction, such as Python, Go, Rust, and Typescript. This way developers can connect to and interact with Qdrant using the programming language they prefer.





### Vector Database Use Cases
. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.
So one thing which I mean, one is definitely try the different models which we support. We support a reasonable range of models, including a few multilingual ones. Second is while we take care of this when you do use with Qdrants. So, for instance, let's say this is how you would have to manually specify, let's say, passage or query. When you do this, let's say add and query. What we do, we add the passage and query keys while creating the Embeddings for you. So this is taken care of
of code, thanks to the recent integration with Qdrant. We're not even going to work directly with `QdrantClient`, as everything is already done in the background

by LangChain. If you want to get into the source code right away, all the processing is available as a 

[Google Colab notebook](https://colab.research.google.com/drive/19RxxkZdnq_YqBH5kBV10Rt0Rax-kminD?usp=sharing).



## Implementing Question Answering with LangChain and Qdrant



### Configuration
However, if the only information you had was that your user dislikes some items, you couldn't use it directly.



Qdrant 1.6 brings a more flexible API. You can now provide both IDs and vectors of positive and negative examples. You can even 

combine them within a single request. That makes the new implementation backward compatible, so you can easily upgrade an existing

Qdrant instance without any changes in your code. And the default behaviour of the API is still the same as before. However, we\n\nAnswer:",What options are available for using Qdrant if the official clients do not support the language you are using?,"If the official clients for Qdrant do not support the language you are using, you have the option to either use the REST API directly or generate a client for your language using OpenAPI definitions or protobuf definitions. By utilizing the OpenAPI definitions available at [this link](https://github.com/qdrant/qdrant/blob/master/docs/redoc/master/openapi.json) or the protobuf definitions at [this link](https://github.com/qdrant/qdrant/tree/master/lib/api/src/grpc/proto), you can interact with Qdrant's functionalities even if there is no official client available for your specific programming language. This flexibility allows developers to integrate Qdrant into a wide range of applications and environments.","['**Clients**: Qdrant supports various programming languages for client interaction, such as Python, Go, Rust, and Typescript. This way developers can connect to and interact with Qdrant using the programming language they prefer.\n\n\n\n\n\n### Vector Database Use Cases', '. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.', ""So one thing which I mean, one is definitely try the different models which we support. We support a reasonable range of models, including a few multilingual ones. Second is while we take care of this when you do use with Qdrants. So, for instance, let's say this is how you would have to manually specify, let's say, passage or query. When you do this, let's say add and query. What we do, we add the passage and query keys while creating the Embeddings for you. So this is taken care of"", ""of code, thanks to the recent integration with Qdrant. We're not even going to work directly with `QdrantClient`, as everything is already done in the background\n\nby LangChain. If you want to get into the source code right away, all the processing is available as a \n\n[Google Colab notebook](https://colab.research.google.com/drive/19RxxkZdnq_YqBH5kBV10Rt0Rax-kminD?usp=sharing).\n\n\n\n## Implementing Question Answering with LangChain and Qdrant\n\n\n\n### Configuration"", ""However, if the only information you had was that your user dislikes some items, you couldn't use it directly.\n\n\n\nQdrant 1.6 brings a more flexible API. You can now provide both IDs and vectors of positive and negative examples. You can even \n\ncombine them within a single request. That makes the new implementation backward compatible, so you can easily upgrade an existing\n\nQdrant instance without any changes in your code. And the default behaviour of the API is still the same as before. However, we""]","**Clients**: Qdrant supports various programming languages for client interaction, such as Python, Go, Rust, and Typescript. This way developers can connect to and interact with Qdrant using the programming language they prefer.





### Vector Database Use Cases
. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.
So one thing which I mean, one is definitely try the different models which we support. We support a reasonable range of models, including a few multilingual ones. Second is while we take care of this when you do use with Qdrants. So, for instance, let's say this is how you would have to manually specify, let's say, passage or query. When you do this, let's say add and query. What we do, we add the passage and query keys while creating the Embeddings for you. So this is taken care of
of code, thanks to the recent integration with Qdrant. We're not even going to work directly with `QdrantClient`, as everything is already done in the background

by LangChain. If you want to get into the source code right away, all the processing is available as a 

[Google Colab notebook](https://colab.research.google.com/drive/19RxxkZdnq_YqBH5kBV10Rt0Rax-kminD?usp=sharing).



## Implementing Question Answering with LangChain and Qdrant



### Configuration
However, if the only information you had was that your user dislikes some items, you couldn't use it directly.



Qdrant 1.6 brings a more flexible API. You can now provide both IDs and vectors of positive and negative examples. You can even 

combine them within a single request. That makes the new implementation backward compatible, so you can easily upgrade an existing

Qdrant instance without any changes in your code. And the default behaviour of the API is still the same as before. However, we",0.29901960784313725,0.580952380952381,0.394822006472492,0.10837438423645321,0.21153846153846154,0.14332247557003258,0.18137254901960784,0.3523809523809524,0.23948220064724923,0.2549019607843137,0.49523809523809526,0.3365695792880259,0.8972741961479187,0.6251991987228394,0.6158128380775452,0.6204704642295837,0.5696800775670756,0.5862864144146442,217,2.3333333333333335,0.44686468212471897
True,,1916.35,758b5df0,What are the two main interfaces available for interacting with Qdrant and where can the documentation for each be found?,"The two main interfaces available for interacting with Qdrant are REST API and gRPC API. The documentation for REST API can be found in the [OpenAPI Specification](https://qdrant.github.io/qdrant/redoc/index.html), while the documentation for gRPC API can be found [here](https://github.com/qdrant/qdrant/blob/master/docs/grpc/docs.md). The gRPC methods in Qdrant follow the same principles as REST, with each REST endpoint having a corresponding gRPC method. The gRPC interface in Qdrant is available on a specified port as outlined in the configuration file, with the default port being 6334. If choosing to use gRPC, it is necessary to expose the port when starting Qdrant, such as when running the service inside a Docker container.","The two main interfaces available for interacting with Qdrant are:

1. **REST API**: This interface allows interaction with Qdrant via HTTP requests. The documentation for the REST API can be found in the [Qdrant API Reference](https://qdrant.docs.apiary.io/).

2. **Qdrant Web UI**: This interface provides a graphical user interface for managing collections and uploading snapshots in Qdrant. The documentation for accessing the Qdrant Web UI and using it can be found in the Qdrant documentation or by running a Qdrant Docker container and accessing the GUI at `http://localhost:6333/dashboard`.","|![java](/docs/misc/java.webp)|**[Java](https://github.com/qdrant/java-client)**|[Available on Maven Central](https://central.sonatype.com/artifact/io.qdrant/client)|[Latest Release](https://github.com/qdrant/java-client/releases)|





## API Reference



All interaction with Qdrant takes place via the REST API. We recommend using REST API if you are using Qdrant for the first time or if you are working on a prototype.



|API|Documentation|

|-|-|
In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. 



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Accessing the Web UI



First, run the Docker container:



```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```



The GUI is available at `http://localhost:6333/dashboard`
```python

QDRANT_MAIN_URL = ""https://my-cluster.com:6333""

QDRANT_NODES = (

    ""https://node-0.my-cluster.com:6333"",

    ""https://node-1.my-cluster.com:6333"",

    ""https://node-2.my-cluster.com:6333"",

)

QDRANT_API_KEY = ""my-api-key""

```



<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>



We can now create a client instance:



```python

from qdrant_client import QdrantClient
might also be found in the [LangChain documentation](https://python.langchain.com/docs/integrations/vectorstores/qdrant).
You can get started using Qdrant with the Python `qdrant-client`, by pulling the latest docker 

image of `qdrant` and connecting to it locally, or by trying out [Qdrant's Cloud](https://cloud.qdrant.io/) 

free tier option until you are ready to make the full switch.



With that out of the way, let's talk about what are vector databases.



## What Are Vector Databases?



![dbs](https://raw.githubusercontent.com/ramonpzg/mlops-sydney-2023/main/images/databases.png)","['|![java](/docs/misc/java.webp)|**[Java](https://github.com/qdrant/java-client)**|[Available on Maven Central](https://central.sonatype.com/artifact/io.qdrant/client)|[Latest Release](https://github.com/qdrant/java-client/releases)|\n\n\n\n\n\n## API Reference\n\n\n\nAll interaction with Qdrant takes place via the REST API. We recommend using REST API if you are using Qdrant for the first time or if you are working on a prototype.\n\n\n\n|API|Documentation|\n\n|-|-|', 'In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. \n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Accessing the Web UI\n\n\n\nFirst, run the Docker container:\n\n\n\n```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```\n\n\n\nThe GUI is available at `http://localhost:6333/dashboard`', '```python\n\nQDRANT_MAIN_URL = ""https://my-cluster.com:6333""\n\nQDRANT_NODES = (\n\n    ""https://node-0.my-cluster.com:6333"",\n\n    ""https://node-1.my-cluster.com:6333"",\n\n    ""https://node-2.my-cluster.com:6333"",\n\n)\n\nQDRANT_API_KEY = ""my-api-key""\n\n```\n\n\n\n<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>\n\n\n\nWe can now create a client instance:\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient', 'might also be found in the [LangChain documentation](https://python.langchain.com/docs/integrations/vectorstores/qdrant).', ""You can get started using Qdrant with the Python `qdrant-client`, by pulling the latest docker \n\nimage of `qdrant` and connecting to it locally, or by trying out [Qdrant's Cloud](https://cloud.qdrant.io/) \n\nfree tier option until you are ready to make the full switch.\n\n\n\nWith that out of the way, let's talk about what are vector databases.\n\n\n\n## What Are Vector Databases?\n\n\n\n![dbs](https://raw.githubusercontent.com/ramonpzg/mlops-sydney-2023/main/images/databases.png)""]","Question: What are the two main interfaces available for interacting with Qdrant and where can the documentation for each be found?\n\nContext: |![java](/docs/misc/java.webp)|**[Java](https://github.com/qdrant/java-client)**|[Available on Maven Central](https://central.sonatype.com/artifact/io.qdrant/client)|[Latest Release](https://github.com/qdrant/java-client/releases)|





## API Reference



All interaction with Qdrant takes place via the REST API. We recommend using REST API if you are using Qdrant for the first time or if you are working on a prototype.



|API|Documentation|

|-|-|
In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. 



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Accessing the Web UI



First, run the Docker container:



```bash

docker run -p 6333:6333 -p 6334:6334 \

-v $(pwd)/qdrant_storage:/qdrant/storage:z \

qdrant/qdrant

```



The GUI is available at `http://localhost:6333/dashboard`
```python

QDRANT_MAIN_URL = ""https://my-cluster.com:6333""

QDRANT_NODES = (

""https://node-0.my-cluster.com:6333"",

""https://node-1.my-cluster.com:6333"",

""https://node-2.my-cluster.com:6333"",

)

QDRANT_API_KEY = ""my-api-key""

```



<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>



We can now create a client instance:



```python

from qdrant_client import QdrantClient
might also be found in the [LangChain documentation](https://python.langchain.com/docs/integrations/vectorstores/qdrant).
You can get started using Qdrant with the Python `qdrant-client`, by pulling the latest docker 

image of `qdrant` and connecting to it locally, or by trying out [Qdrant's Cloud](https://cloud.qdrant.io/) 

free tier option until you are ready to make the full switch.



With that out of the way, let's talk about what are vector databases.



## What Are Vector Databases?



![dbs](https://raw.githubusercontent.com/ramonpzg/mlops-sydney-2023/main/images/databases.png)\n\nAnswer:",What are the two main interfaces available for interacting with Qdrant and where can the documentation for each be found?,"The two main interfaces available for interacting with Qdrant are REST API and gRPC API. The documentation for REST API can be found in the [OpenAPI Specification](https://qdrant.github.io/qdrant/redoc/index.html), while the documentation for gRPC API can be found [here](https://github.com/qdrant/qdrant/blob/master/docs/grpc/docs.md). The gRPC methods in Qdrant follow the same principles as REST, with each REST endpoint having a corresponding gRPC method. The gRPC interface in Qdrant is available on a specified port as outlined in the configuration file, with the default port being 6334. If choosing to use gRPC, it is necessary to expose the port when starting Qdrant, such as when running the service inside a Docker container.","['|![java](/docs/misc/java.webp)|**[Java](https://github.com/qdrant/java-client)**|[Available on Maven Central](https://central.sonatype.com/artifact/io.qdrant/client)|[Latest Release](https://github.com/qdrant/java-client/releases)|\n\n\n\n\n\n## API Reference\n\n\n\nAll interaction with Qdrant takes place via the REST API. We recommend using REST API if you are using Qdrant for the first time or if you are working on a prototype.\n\n\n\n|API|Documentation|\n\n|-|-|', 'In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. \n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Accessing the Web UI\n\n\n\nFirst, run the Docker container:\n\n\n\n```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```\n\n\n\nThe GUI is available at `http://localhost:6333/dashboard`', '```python\n\nQDRANT_MAIN_URL = ""https://my-cluster.com:6333""\n\nQDRANT_NODES = (\n\n    ""https://node-0.my-cluster.com:6333"",\n\n    ""https://node-1.my-cluster.com:6333"",\n\n    ""https://node-2.my-cluster.com:6333"",\n\n)\n\nQDRANT_API_KEY = ""my-api-key""\n\n```\n\n\n\n<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>\n\n\n\nWe can now create a client instance:\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient', 'might also be found in the [LangChain documentation](https://python.langchain.com/docs/integrations/vectorstores/qdrant).', ""You can get started using Qdrant with the Python `qdrant-client`, by pulling the latest docker \n\nimage of `qdrant` and connecting to it locally, or by trying out [Qdrant's Cloud](https://cloud.qdrant.io/) \n\nfree tier option until you are ready to make the full switch.\n\n\n\nWith that out of the way, let's talk about what are vector databases.\n\n\n\n## What Are Vector Databases?\n\n\n\n![dbs](https://raw.githubusercontent.com/ramonpzg/mlops-sydney-2023/main/images/databases.png)""]","|![java](/docs/misc/java.webp)|**[Java](https://github.com/qdrant/java-client)**|[Available on Maven Central](https://central.sonatype.com/artifact/io.qdrant/client)|[Latest Release](https://github.com/qdrant/java-client/releases)|





## API Reference



All interaction with Qdrant takes place via the REST API. We recommend using REST API if you are using Qdrant for the first time or if you are working on a prototype.



|API|Documentation|

|-|-|
In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. 



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Accessing the Web UI



First, run the Docker container:



```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```



The GUI is available at `http://localhost:6333/dashboard`
```python

QDRANT_MAIN_URL = ""https://my-cluster.com:6333""

QDRANT_NODES = (

    ""https://node-0.my-cluster.com:6333"",

    ""https://node-1.my-cluster.com:6333"",

    ""https://node-2.my-cluster.com:6333"",

)

QDRANT_API_KEY = ""my-api-key""

```



<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>



We can now create a client instance:



```python

from qdrant_client import QdrantClient
might also be found in the [LangChain documentation](https://python.langchain.com/docs/integrations/vectorstores/qdrant).
You can get started using Qdrant with the Python `qdrant-client`, by pulling the latest docker 

image of `qdrant` and connecting to it locally, or by trying out [Qdrant's Cloud](https://cloud.qdrant.io/) 

free tier option until you are ready to make the full switch.



With that out of the way, let's talk about what are vector databases.



## What Are Vector Databases?



![dbs](https://raw.githubusercontent.com/ramonpzg/mlops-sydney-2023/main/images/databases.png)",0.5476190476190477,0.41818181818181815,0.4742268041237113,0.27710843373493976,0.21100917431192662,0.23958333333333334,0.40476190476190477,0.3090909090909091,0.3505154639175258,0.4523809523809524,0.34545454545454546,0.3917525773195877,0.9183550477027893,0.6874704360961914,0.6519089341163635,0.6692176461219788,0.664764183262984,0.5870356298983097,89,0.8165137614678899,0.37787611166564183
True,,4475.121,b1638d7c,When should one use gRPC with Qdrant and what factors should be considered in making this choice?,"gRPC is recommended for use with Qdrant when optimizing the performance of an application is a priority and when the user is already familiar with Qdrant. The decision to use gRPC over the REST API is a trade-off between convenience and speed. gRPC is a binary protocol that offers higher performance compared to REST API, but it can be more challenging to debug. Therefore, if the goal is to maximize performance and the user is comfortable with the potential debugging complexities associated with gRPC, it is advisable to opt for gRPC when interacting with Qdrant.","When deciding whether to use gRPC with Qdrant, there are several factors to consider:

1. **Performance Optimization**: If you are looking to optimize the performance of your application, gRPC can be a good choice. It is a binary protocol that can provide faster communication between client and server.

2. **Familiarity with Qdrant**: If you are already familiar with Qdrant and have experience working with gRPC, it may be easier for you to integrate and use gRPC with Qdrant.

3. **Debugging**: It's important to consider that gRPC can be more challenging to debug compared to REST API. If ease of debugging is a priority for you, you may want to stick with REST API.

4. **Compatibility**: Ensure that your client applications support gRPC if you choose to use it with Qdrant. Make sure that the necessary libraries and dependencies are available for your chosen programming language.

5. **Network Configuration**: When using gRPC, you need to ensure that the necessary ports (e.g., port 6334 for Qdrant's gRPC interface) are accessible and properly configured in your network environment.

Ultimately, the decision to use gRPC with Qdrant should be based on your specific use case, requirements, and familiarity with the technology. If performance optimization is a key factor for your application and you are comfortable with gRPC, it may be a suitable choice.","qdrant/qdrant

```



**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.



## Qdrant Web UI



Qdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points.
As per the [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml), the gRPC interface is available on the specified port.



```yaml

service:

  grpc_port: 6334

```

<aside role=""status"">If you decide to use gRPC, you must expose the port when starting Qdrant.</aside>

 

Running the service inside of Docker will look like this:



```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```
![REST API vs gRPC upload time, sec](/blog/qdrant-v-0-6-0-engine-with-grpc-released/upload_time.png)



Read more about the gRPC interface and whether you should use it by this [link](/documentation/quick_start/#grpc).



The release v0.6.0 includes several bug fixes. More information is available in a [changelog](https://github.com/qdrant/qdrant/releases/tag/v0.6.0).



New version was provided in addition to the REST API that the company keeps supporting due to its easy debugging.
- GRPC API: [localhost:6334](http://localhost:6334)



## Initialize the client



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });

```



```rust

use qdrant_client::client::QdrantClient;



// The Rust client uses Qdrant's GRPC interface
* `6334` - For the [gRPC](/documentation/interfaces/#grpc-interface) API

* `6335` - For [Distributed deployment](/documentation/guides/distributed_deployment/)



All Qdrant instances in a cluster must be able to:



- Communicate with each other over these ports

- Allow incoming connections to ports `6333` and `6334` from clients that use Qdrant.



## Installation options



Qdrant can be installed in different ways depending on your needs:","[""qdrant/qdrant\n\n```\n\n\n\n**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.\n\n\n\n## Qdrant Web UI\n\n\n\nQdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points."", 'As per the [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml), the gRPC interface is available on the specified port.\n\n\n\n```yaml\n\nservice:\n\n  grpc_port: 6334\n\n```\n\n<aside role=""status"">If you decide to use gRPC, you must expose the port when starting Qdrant.</aside>\n\n \n\nRunning the service inside of Docker will look like this:\n\n\n\n```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```', '![REST API vs gRPC upload time, sec](/blog/qdrant-v-0-6-0-engine-with-grpc-released/upload_time.png)\n\n\n\nRead more about the gRPC interface and whether you should use it by this [link](/documentation/quick_start/#grpc).\n\n\n\nThe release v0.6.0 includes several bug fixes. More information is available in a [changelog](https://github.com/qdrant/qdrant/releases/tag/v0.6.0).\n\n\n\nNew version was provided in addition to the REST API that the company keeps supporting due to its easy debugging.', '- GRPC API: [localhost:6334](http://localhost:6334)\n\n\n\n## Initialize the client\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n```\n\n\n\n```rust\n\nuse qdrant_client::client::QdrantClient;\n\n\n\n// The Rust client uses Qdrant\'s GRPC interface', '* `6334` - For the [gRPC](/documentation/interfaces/#grpc-interface) API\n\n* `6335` - For [Distributed deployment](/documentation/guides/distributed_deployment/)\n\n\n\nAll Qdrant instances in a cluster must be able to:\n\n\n\n- Communicate with each other over these ports\n\n- Allow incoming connections to ports `6333` and `6334` from clients that use Qdrant.\n\n\n\n## Installation options\n\n\n\nQdrant can be installed in different ways depending on your needs:']","Question: When should one use gRPC with Qdrant and what factors should be considered in making this choice?\n\nContext: qdrant/qdrant

```



**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.



## Qdrant Web UI



Qdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points.
As per the [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml), the gRPC interface is available on the specified port.



```yaml

service:

grpc_port: 6334

```

<aside role=""status"">If you decide to use gRPC, you must expose the port when starting Qdrant.</aside>



Running the service inside of Docker will look like this:



```bash

docker run -p 6333:6333 -p 6334:6334 \

-v $(pwd)/qdrant_storage:/qdrant/storage:z \

qdrant/qdrant

```
![REST API vs gRPC upload time, sec](/blog/qdrant-v-0-6-0-engine-with-grpc-released/upload_time.png)



Read more about the gRPC interface and whether you should use it by this [link](/documentation/quick_start/#grpc).



The release v0.6.0 includes several bug fixes. More information is available in a [changelog](https://github.com/qdrant/qdrant/releases/tag/v0.6.0).



New version was provided in addition to the REST API that the company keeps supporting due to its easy debugging.
- GRPC API: [localhost:6334](http://localhost:6334)



## Initialize the client



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });

```



```rust

use qdrant_client::client::QdrantClient;



// The Rust client uses Qdrant's GRPC interface
* `6334` - For the [gRPC](/documentation/interfaces/#grpc-interface) API

* `6335` - For [Distributed deployment](/documentation/guides/distributed_deployment/)



All Qdrant instances in a cluster must be able to:



- Communicate with each other over these ports

- Allow incoming connections to ports `6333` and `6334` from clients that use Qdrant.



## Installation options



Qdrant can be installed in different ways depending on your needs:\n\nAnswer:",When should one use gRPC with Qdrant and what factors should be considered in making this choice?,"gRPC is recommended for use with Qdrant when optimizing the performance of an application is a priority and when the user is already familiar with Qdrant. The decision to use gRPC over the REST API is a trade-off between convenience and speed. gRPC is a binary protocol that offers higher performance compared to REST API, but it can be more challenging to debug. Therefore, if the goal is to maximize performance and the user is comfortable with the potential debugging complexities associated with gRPC, it is advisable to opt for gRPC when interacting with Qdrant.","[""qdrant/qdrant\n\n```\n\n\n\n**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.\n\n\n\n## Qdrant Web UI\n\n\n\nQdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points."", 'As per the [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml), the gRPC interface is available on the specified port.\n\n\n\n```yaml\n\nservice:\n\n  grpc_port: 6334\n\n```\n\n<aside role=""status"">If you decide to use gRPC, you must expose the port when starting Qdrant.</aside>\n\n \n\nRunning the service inside of Docker will look like this:\n\n\n\n```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```', '![REST API vs gRPC upload time, sec](/blog/qdrant-v-0-6-0-engine-with-grpc-released/upload_time.png)\n\n\n\nRead more about the gRPC interface and whether you should use it by this [link](/documentation/quick_start/#grpc).\n\n\n\nThe release v0.6.0 includes several bug fixes. More information is available in a [changelog](https://github.com/qdrant/qdrant/releases/tag/v0.6.0).\n\n\n\nNew version was provided in addition to the REST API that the company keeps supporting due to its easy debugging.', '- GRPC API: [localhost:6334](http://localhost:6334)\n\n\n\n## Initialize the client\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n```\n\n\n\n```rust\n\nuse qdrant_client::client::QdrantClient;\n\n\n\n// The Rust client uses Qdrant\'s GRPC interface', '* `6334` - For the [gRPC](/documentation/interfaces/#grpc-interface) API\n\n* `6335` - For [Distributed deployment](/documentation/guides/distributed_deployment/)\n\n\n\nAll Qdrant instances in a cluster must be able to:\n\n\n\n- Communicate with each other over these ports\n\n- Allow incoming connections to ports `6333` and `6334` from clients that use Qdrant.\n\n\n\n## Installation options\n\n\n\nQdrant can be installed in different ways depending on your needs:']","qdrant/qdrant

```



**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.



## Qdrant Web UI



Qdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points.
As per the [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml), the gRPC interface is available on the specified port.



```yaml

service:

  grpc_port: 6334

```

<aside role=""status"">If you decide to use gRPC, you must expose the port when starting Qdrant.</aside>

 

Running the service inside of Docker will look like this:



```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```
![REST API vs gRPC upload time, sec](/blog/qdrant-v-0-6-0-engine-with-grpc-released/upload_time.png)



Read more about the gRPC interface and whether you should use it by this [link](/documentation/quick_start/#grpc).



The release v0.6.0 includes several bug fixes. More information is available in a [changelog](https://github.com/qdrant/qdrant/releases/tag/v0.6.0).



New version was provided in addition to the REST API that the company keeps supporting due to its easy debugging.
- GRPC API: [localhost:6334](http://localhost:6334)



## Initialize the client



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });

```



```rust

use qdrant_client::client::QdrantClient;



// The Rust client uses Qdrant's GRPC interface
* `6334` - For the [gRPC](/documentation/interfaces/#grpc-interface) API

* `6335` - For [Distributed deployment](/documentation/guides/distributed_deployment/)



All Qdrant instances in a cluster must be able to:



- Communicate with each other over these ports

- Allow incoming connections to ports `6333` and `6334` from clients that use Qdrant.



## Installation options



Qdrant can be installed in different ways depending on your needs:",0.2783018867924528,0.6941176470588235,0.3973063973063973,0.12796208530805686,0.32142857142857145,0.18305084745762712,0.16037735849056603,0.4,0.22895622895622897,0.25943396226415094,0.6470588235294118,0.37037037037037035,0.8947303295135498,0.6319311261177063,0.7208476662635803,0.673467218875885,0.7070860280655324,0.9921216022921726,221,2.3263157894736843,0.7141019729482067
True,,2387.5280000000002,482d6ad8,Where can the GUI for Qdrant be accessed?,"The GUI for Qdrant can be accessed at `http://localhost:6333/dashboard`. This web address will lead you to the graphical user interface (GUI) for Qdrant, providing a visual interface for interacting with the Qdrant system. The GUI allows users to perform various operations and tasks related to Qdrant, such as managing indexes, querying data, and configuring settings. By accessing the GUI at the specified URL, users can easily navigate and interact with the Qdrant system in a user-friendly and intuitive manner.","To access the GUI for Qdrant, you can follow these steps:

1. Run the Docker container with the following command:
```bash
docker run -p 6333:6333 -p 6334:6334 \
-v $(pwd)/qdrant_storage:/qdrant/storage:z \
qdrant/qdrant
```

2. Access the GUI at `http://localhost:6333/dashboard`

By running the Docker container and accessing the provided URL, you will be able to interact with the Qdrant GUI.","In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. 



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Accessing the Web UI



First, run the Docker container:



```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```



The GUI is available at `http://localhost:6333/dashboard`
Try it out now! If you have Docker running, you can [quickstart Qdrant](https://qdrant.tech/documentation/quick-start/) and access the Dashboard locally from [http://localhost:6333/dashboard](http://localhost:6333/dashboard). You should see this simple access point to Qdrant:



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Temporary directory for Snapshots
```python

QDRANT_MAIN_URL = ""https://my-cluster.com:6333""

QDRANT_NODES = (

    ""https://node-0.my-cluster.com:6333"",

    ""https://node-1.my-cluster.com:6333"",

    ""https://node-2.my-cluster.com:6333"",

)

QDRANT_API_KEY = ""my-api-key""

```



<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>



We can now create a client instance:



```python

from qdrant_client import QdrantClient
```bash

unstructured-ingest \

  local \

  --input-path ""discord-output"" \

  --embedding-provider ""langchain-huggingface"" \

  qdrant \

  --collection-name ""<COLLECTION_NAME>"" \

  --api-key ""<QDRANT_API_KEY>"" \

  --location ""<QDRANT_URL>""

```
WITH ENGINE = ""qdrant"",

PARAMETERS = {

    ""location"": "":memory:"",

    ""collection_config"": {

        ""size"": 386,

        ""distance"": ""Cosine""

    }

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table","['In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. \n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Accessing the Web UI\n\n\n\nFirst, run the Docker container:\n\n\n\n```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```\n\n\n\nThe GUI is available at `http://localhost:6333/dashboard`', 'Try it out now! If you have Docker running, you can [quickstart Qdrant](https://qdrant.tech/documentation/quick-start/) and access the Dashboard locally from [http://localhost:6333/dashboard](http://localhost:6333/dashboard). You should see this simple access point to Qdrant:\n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Temporary directory for Snapshots', '```python\n\nQDRANT_MAIN_URL = ""https://my-cluster.com:6333""\n\nQDRANT_NODES = (\n\n    ""https://node-0.my-cluster.com:6333"",\n\n    ""https://node-1.my-cluster.com:6333"",\n\n    ""https://node-2.my-cluster.com:6333"",\n\n)\n\nQDRANT_API_KEY = ""my-api-key""\n\n```\n\n\n\n<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>\n\n\n\nWe can now create a client instance:\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient', '```bash\n\nunstructured-ingest \\\n\n  local \\\n\n  --input-path ""discord-output"" \\\n\n  --embedding-provider ""langchain-huggingface"" \\\n\n  qdrant \\\n\n  --collection-name ""<COLLECTION_NAME>"" \\\n\n  --api-key ""<QDRANT_API_KEY>"" \\\n\n  --location ""<QDRANT_URL>""\n\n```', 'WITH ENGINE = ""qdrant"",\n\nPARAMETERS = {\n\n    ""location"": "":memory:"",\n\n    ""collection_config"": {\n\n        ""size"": 386,\n\n        ""distance"": ""Cosine""\n\n    }\n\n}\n\n```\n\n\n\nThe available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).\n\n\n\n## Creating a new table']","Question: Where can the GUI for Qdrant be accessed?\n\nContext: In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. 



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Accessing the Web UI



First, run the Docker container:



```bash

docker run -p 6333:6333 -p 6334:6334 \

-v $(pwd)/qdrant_storage:/qdrant/storage:z \

qdrant/qdrant

```



The GUI is available at `http://localhost:6333/dashboard`
Try it out now! If you have Docker running, you can [quickstart Qdrant](https://qdrant.tech/documentation/quick-start/) and access the Dashboard locally from [http://localhost:6333/dashboard](http://localhost:6333/dashboard). You should see this simple access point to Qdrant:



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Temporary directory for Snapshots
```python

QDRANT_MAIN_URL = ""https://my-cluster.com:6333""

QDRANT_NODES = (

""https://node-0.my-cluster.com:6333"",

""https://node-1.my-cluster.com:6333"",

""https://node-2.my-cluster.com:6333"",

)

QDRANT_API_KEY = ""my-api-key""

```



<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>



We can now create a client instance:



```python

from qdrant_client import QdrantClient
```bash

unstructured-ingest \

local \

--input-path ""discord-output"" \

--embedding-provider ""langchain-huggingface"" \

qdrant \

--collection-name ""<COLLECTION_NAME>"" \

--api-key ""<QDRANT_API_KEY>"" \

--location ""<QDRANT_URL>""

```
WITH ENGINE = ""qdrant"",

PARAMETERS = {

""location"": "":memory:"",

""collection_config"": {

""size"": 386,

""distance"": ""Cosine""

}

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table\n\nAnswer:",Where can the GUI for Qdrant be accessed?,"The GUI for Qdrant can be accessed at `http://localhost:6333/dashboard`. This web address will lead you to the graphical user interface (GUI) for Qdrant, providing a visual interface for interacting with the Qdrant system. The GUI allows users to perform various operations and tasks related to Qdrant, such as managing indexes, querying data, and configuring settings. By accessing the GUI at the specified URL, users can easily navigate and interact with the Qdrant system in a user-friendly and intuitive manner.","['In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. \n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Accessing the Web UI\n\n\n\nFirst, run the Docker container:\n\n\n\n```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```\n\n\n\nThe GUI is available at `http://localhost:6333/dashboard`', 'Try it out now! If you have Docker running, you can [quickstart Qdrant](https://qdrant.tech/documentation/quick-start/) and access the Dashboard locally from [http://localhost:6333/dashboard](http://localhost:6333/dashboard). You should see this simple access point to Qdrant:\n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Temporary directory for Snapshots', '```python\n\nQDRANT_MAIN_URL = ""https://my-cluster.com:6333""\n\nQDRANT_NODES = (\n\n    ""https://node-0.my-cluster.com:6333"",\n\n    ""https://node-1.my-cluster.com:6333"",\n\n    ""https://node-2.my-cluster.com:6333"",\n\n)\n\nQDRANT_API_KEY = ""my-api-key""\n\n```\n\n\n\n<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>\n\n\n\nWe can now create a client instance:\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient', '```bash\n\nunstructured-ingest \\\n\n  local \\\n\n  --input-path ""discord-output"" \\\n\n  --embedding-provider ""langchain-huggingface"" \\\n\n  qdrant \\\n\n  --collection-name ""<COLLECTION_NAME>"" \\\n\n  --api-key ""<QDRANT_API_KEY>"" \\\n\n  --location ""<QDRANT_URL>""\n\n```', 'WITH ENGINE = ""qdrant"",\n\nPARAMETERS = {\n\n    ""location"": "":memory:"",\n\n    ""collection_config"": {\n\n        ""size"": 386,\n\n        ""distance"": ""Cosine""\n\n    }\n\n}\n\n```\n\n\n\nThe available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).\n\n\n\n## Creating a new table']","In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. 



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Accessing the Web UI



First, run the Docker container:



```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```



The GUI is available at `http://localhost:6333/dashboard`
Try it out now! If you have Docker running, you can [quickstart Qdrant](https://qdrant.tech/documentation/quick-start/) and access the Dashboard locally from [http://localhost:6333/dashboard](http://localhost:6333/dashboard). You should see this simple access point to Qdrant:



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Temporary directory for Snapshots
```python

QDRANT_MAIN_URL = ""https://my-cluster.com:6333""

QDRANT_NODES = (

    ""https://node-0.my-cluster.com:6333"",

    ""https://node-1.my-cluster.com:6333"",

    ""https://node-2.my-cluster.com:6333"",

)

QDRANT_API_KEY = ""my-api-key""

```



<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>



We can now create a client instance:



```python

from qdrant_client import QdrantClient
```bash

unstructured-ingest \

  local \

  --input-path ""discord-output"" \

  --embedding-provider ""langchain-huggingface"" \

  qdrant \

  --collection-name ""<COLLECTION_NAME>"" \

  --api-key ""<QDRANT_API_KEY>"" \

  --location ""<QDRANT_URL>""

```
WITH ENGINE = ""qdrant"",

PARAMETERS = {

    ""location"": "":memory:"",

    ""collection_config"": {

        ""size"": 386,

        ""distance"": ""Cosine""

    }

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table",0.48333333333333334,0.3918918918918919,0.43283582089552236,0.1694915254237288,0.136986301369863,0.15151515151515152,0.26666666666666666,0.21621621621621623,0.23880597014925373,0.4166666666666667,0.33783783783783783,0.373134328358209,0.7416749596595764,0.657024621963501,0.6499664187431335,0.6534764170646667,0.8177108317613602,0.8500123154371977,62,0.775,0.4412191037306603
True,,2320.313,90588538,What are the three ways to use Qdrant?,"According to the documentation, there are three ways to use Qdrant:

1. Run a Docker image: This method is recommended for users who do not have a Python development environment. By running a Docker image, users can quickly set up a local Qdrant server and storage.

2. Get the Python client: Users familiar with Python can install the Qdrant client by using `pip install qdrant-client`. The Python client also supports an in-memory database.

3. Spin up a Qdrant Cloud cluster: This method is recommended for running Qdrant in a production environment. Users can set up their first instance by following the Quickstart guide provided in the documentation.","The three ways to use Qdrant are:

1. Running a Docker image: This method is suitable for those who don't have a Python development environment. By running a Docker image, you can set up a local Qdrant server and storage quickly.

2. Getting the Python client: If you are familiar with Python, you can install the Qdrant Python client by using `pip install qdrant-client`. The client also supports an in-memory database and allows you to interact with Qdrant from Python.

3. Using Qdrant in a LangChain-based application: For more advanced use cases, such as integrating Qdrant with LangChain, you can refer to specific articles and documentation for detailed instructions.","## First-Time Users:



There are three ways to use Qdrant:



1. [**Run a Docker image**](quick-start/) if you don't have a Python development environment. Setup a local Qdrant server and storage in a few moments. 

2. [**Get the Python client**](https://github.com/qdrant/qdrant-client) if you're familiar with Python. Just `pip install qdrant-client`. The client also supports an in-memory database.
To interact with Qdrant from python, I recommend using an out-of-the-box client library.



To install it, use the following command



```bash

pip install qdrant-client

```



At this point, we should have startup records in file `startups.json`, encoded vectors in file `startup_vectors.npy`, and running Qdrant on a local machine.

Let's write a script to upload all startup data and vectors into the search engine.



First, let's create a client object for Qdrant.



```python

# Import client library
| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  



## Common Use Cases:
```python

url = ""<---qdrant url here --->""

qdrant = Qdrant.from_documents(

    docs, 

    embeddings, 

    url, 

    prefer_grpc=True, 

    collection_name=""my_documents"",

)

```



## Next steps



If you'd like to know more about running Qdrant in a LangChain-based application, please read our article 

[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information
[0.11, 0.22, 0.33, 0.44],

    [0.55, 0.66, 0.77, 0.88],

    [0.88, 0.11, 0.12, 0.13],

]

ids = [32, 21, ""b626f6a9-b14d-4af9-b7c3-43d8deb719a6""]

payload = [{""meta"": ""data""}, {""meta"": ""data_2""}, {""meta"": ""data_3"", ""extra"": ""data""}]



QdrantIngestOperator(

    conn_id=""qdrant_connection""

    task_id=""qdrant_ingest"",

    collection_name=""<COLLECTION_NAME>"",

    vectors=vectors,

    ids=ids,

    payload=payload,

)

```



## Reference","[""## First-Time Users:\n\n\n\nThere are three ways to use Qdrant:\n\n\n\n1. [**Run a Docker image**](quick-start/) if you don't have a Python development environment. Setup a local Qdrant server and storage in a few moments. \n\n2. [**Get the Python client**](https://github.com/qdrant/qdrant-client) if you're familiar with Python. Just `pip install qdrant-client`. The client also supports an in-memory database."", ""To interact with Qdrant from python, I recommend using an out-of-the-box client library.\n\n\n\nTo install it, use the following command\n\n\n\n```bash\n\npip install qdrant-client\n\n```\n\n\n\nAt this point, we should have startup records in file `startups.json`, encoded vectors in file `startup_vectors.npy`, and running Qdrant on a local machine.\n\nLet's write a script to upload all startup data and vectors into the search engine.\n\n\n\nFirst, let's create a client object for Qdrant.\n\n\n\n```python\n\n# Import client library"", ""| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  \n\n\n\n## Common Use Cases:"", '```python\n\nurl = ""<---qdrant url here --->""\n\nqdrant = Qdrant.from_documents(\n\n    docs, \n\n    embeddings, \n\n    url, \n\n    prefer_grpc=True, \n\n    collection_name=""my_documents"",\n\n)\n\n```\n\n\n\n## Next steps\n\n\n\nIf you\'d like to know more about running Qdrant in a LangChain-based application, please read our article \n\n[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information', '[0.11, 0.22, 0.33, 0.44],\n\n    [0.55, 0.66, 0.77, 0.88],\n\n    [0.88, 0.11, 0.12, 0.13],\n\n]\n\nids = [32, 21, ""b626f6a9-b14d-4af9-b7c3-43d8deb719a6""]\n\npayload = [{""meta"": ""data""}, {""meta"": ""data_2""}, {""meta"": ""data_3"", ""extra"": ""data""}]\n\n\n\nQdrantIngestOperator(\n\n    conn_id=""qdrant_connection""\n\n    task_id=""qdrant_ingest"",\n\n    collection_name=""<COLLECTION_NAME>"",\n\n    vectors=vectors,\n\n    ids=ids,\n\n    payload=payload,\n\n)\n\n```\n\n\n\n## Reference']","Question: What are the three ways to use Qdrant?\n\nContext: ## First-Time Users:



There are three ways to use Qdrant:



1. [**Run a Docker image**](quick-start/) if you don't have a Python development environment. Setup a local Qdrant server and storage in a few moments. 

2. [**Get the Python client**](https://github.com/qdrant/qdrant-client) if you're familiar with Python. Just `pip install qdrant-client`. The client also supports an in-memory database.
To interact with Qdrant from python, I recommend using an out-of-the-box client library.



To install it, use the following command



```bash

pip install qdrant-client

```



At this point, we should have startup records in file `startups.json`, encoded vectors in file `startup_vectors.npy`, and running Qdrant on a local machine.

Let's write a script to upload all startup data and vectors into the search engine.



First, let's create a client object for Qdrant.



```python

# Import client library
| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  



## Common Use Cases:
```python

url = ""<---qdrant url here --->""

qdrant = Qdrant.from_documents(

docs, 

embeddings, 

url, 

prefer_grpc=True, 

collection_name=""my_documents"",

)

```



## Next steps



If you'd like to know more about running Qdrant in a LangChain-based application, please read our article 

[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information
[0.11, 0.22, 0.33, 0.44],

[0.55, 0.66, 0.77, 0.88],

[0.88, 0.11, 0.12, 0.13],

]

ids = [32, 21, ""b626f6a9-b14d-4af9-b7c3-43d8deb719a6""]

payload = [{""meta"": ""data""}, {""meta"": ""data_2""}, {""meta"": ""data_3"", ""extra"": ""data""}]



QdrantIngestOperator(

conn_id=""qdrant_connection""

task_id=""qdrant_ingest"",

collection_name=""<COLLECTION_NAME>"",

vectors=vectors,

ids=ids,

payload=payload,

)

```



## Reference\n\nAnswer:",What are the three ways to use Qdrant?,"According to the documentation, there are three ways to use Qdrant:

1. Run a Docker image: This method is recommended for users who do not have a Python development environment. By running a Docker image, users can quickly set up a local Qdrant server and storage.

2. Get the Python client: Users familiar with Python can install the Qdrant client by using `pip install qdrant-client`. The Python client also supports an in-memory database.

3. Spin up a Qdrant Cloud cluster: This method is recommended for running Qdrant in a production environment. Users can set up their first instance by following the Quickstart guide provided in the documentation.","[""## First-Time Users:\n\n\n\nThere are three ways to use Qdrant:\n\n\n\n1. [**Run a Docker image**](quick-start/) if you don't have a Python development environment. Setup a local Qdrant server and storage in a few moments. \n\n2. [**Get the Python client**](https://github.com/qdrant/qdrant-client) if you're familiar with Python. Just `pip install qdrant-client`. The client also supports an in-memory database."", ""To interact with Qdrant from python, I recommend using an out-of-the-box client library.\n\n\n\nTo install it, use the following command\n\n\n\n```bash\n\npip install qdrant-client\n\n```\n\n\n\nAt this point, we should have startup records in file `startups.json`, encoded vectors in file `startup_vectors.npy`, and running Qdrant on a local machine.\n\nLet's write a script to upload all startup data and vectors into the search engine.\n\n\n\nFirst, let's create a client object for Qdrant.\n\n\n\n```python\n\n# Import client library"", ""| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  \n\n\n\n## Common Use Cases:"", '```python\n\nurl = ""<---qdrant url here --->""\n\nqdrant = Qdrant.from_documents(\n\n    docs, \n\n    embeddings, \n\n    url, \n\n    prefer_grpc=True, \n\n    collection_name=""my_documents"",\n\n)\n\n```\n\n\n\n## Next steps\n\n\n\nIf you\'d like to know more about running Qdrant in a LangChain-based application, please read our article \n\n[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information', '[0.11, 0.22, 0.33, 0.44],\n\n    [0.55, 0.66, 0.77, 0.88],\n\n    [0.88, 0.11, 0.12, 0.13],\n\n]\n\nids = [32, 21, ""b626f6a9-b14d-4af9-b7c3-43d8deb719a6""]\n\npayload = [{""meta"": ""data""}, {""meta"": ""data_2""}, {""meta"": ""data_3"", ""extra"": ""data""}]\n\n\n\nQdrantIngestOperator(\n\n    conn_id=""qdrant_connection""\n\n    task_id=""qdrant_ingest"",\n\n    collection_name=""<COLLECTION_NAME>"",\n\n    vectors=vectors,\n\n    ids=ids,\n\n    payload=payload,\n\n)\n\n```\n\n\n\n## Reference']","## First-Time Users:



There are three ways to use Qdrant:



1. [**Run a Docker image**](quick-start/) if you don't have a Python development environment. Setup a local Qdrant server and storage in a few moments. 

2. [**Get the Python client**](https://github.com/qdrant/qdrant-client) if you're familiar with Python. Just `pip install qdrant-client`. The client also supports an in-memory database.
To interact with Qdrant from python, I recommend using an out-of-the-box client library.



To install it, use the following command



```bash

pip install qdrant-client

```



At this point, we should have startup records in file `startups.json`, encoded vectors in file `startup_vectors.npy`, and running Qdrant on a local machine.

Let's write a script to upload all startup data and vectors into the search engine.



First, let's create a client object for Qdrant.



```python

# Import client library
| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  



## Common Use Cases:
```python

url = ""<---qdrant url here --->""

qdrant = Qdrant.from_documents(

    docs, 

    embeddings, 

    url, 

    prefer_grpc=True, 

    collection_name=""my_documents"",

)

```



## Next steps



If you'd like to know more about running Qdrant in a LangChain-based application, please read our article 

[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information
[0.11, 0.22, 0.33, 0.44],

    [0.55, 0.66, 0.77, 0.88],

    [0.88, 0.11, 0.12, 0.13],

]

ids = [32, 21, ""b626f6a9-b14d-4af9-b7c3-43d8deb719a6""]

payload = [{""meta"": ""data""}, {""meta"": ""data_2""}, {""meta"": ""data_3"", ""extra"": ""data""}]



QdrantIngestOperator(

    conn_id=""qdrant_connection""

    task_id=""qdrant_ingest"",

    collection_name=""<COLLECTION_NAME>"",

    vectors=vectors,

    ids=ids,

    payload=payload,

)

```



## Reference",0.6213592233009708,0.6666666666666666,0.64321608040201,0.43137254901960786,0.4631578947368421,0.4467005076142132,0.5728155339805825,0.6145833333333334,0.592964824120603,0.6116504854368932,0.65625,0.6331658291457286,0.932908833026886,0.7570425868034363,0.781977653503418,0.7693081498146057,0.9576199393719435,0.8282938043897351,110,1.02803738317757,0.5749777862166304
True,,3801.772,3f348d40,What are the recommended ways for Qdrant users to provide feedback or contribute to the project?,"Qdrant encourages feedback and contributions from its users, specifically Data Scientists, ML Engineers, and MLOps professionals. The best way for Qdrant users to contribute is by providing feedback on their experience with Qdrant. This can include reporting problems, unexpected behaviors, or suggesting improvements in documentation. Users can share their feedback through various channels such as creating an issue on the Qdrant GitHub repository, starting a discussion on GitHub, or sending a message on Discord. Additionally, users who use Qdrant or Metric Learning in their projects are encouraged to share their stories, articles, and demos with the Qdrant community. For those familiar with Rust, there is a specific contribution guide available on the Qdrant GitHub repository. If users encounter problems with understanding the code or architecture, they can reach out to the Qdrant team at any time for assistance.","It's great to hear that you are exploring Qdrant and finding the Python client helpful for your use case. If you have any feedback, questions, or suggestions based on your experience with Qdrant, the best way to contribute is to provide feedback directly to the Qdrant team.

You can do this by:
1. Creating an issue on the Qdrant GitHub repository: [Qdrant GitHub Issues](https://github.com/qdrant/qdrant/issues)
2. Starting a discussion on the Qdrant GitHub Discussions page: [Qdrant GitHub Discussions](https://github.com/qdrant/qdrant/discussions)
3. Joining the Qdrant Discord server and dropping a message: [Qdrant Discord](https://discord.gg/tdtYvXjC4h)

Additionally, if you have expertise in Rust and want to contribute to the codebase, you can check out the Qdrant contribution guide on the GitHub repository.

If you are interested in contributing more actively or working with the Qdrant team, you can explore opportunities to work with them through their website.

Your feedback and contributions are valuable in helping improve Qdrant and enhance its features for the community.","---

title: Contribution Guidelines

weight: 35

draft: true

---



# How to contribute



If you are a Qdrant user - Data Scientist, ML Engineer, or MLOps, the best contribution would be the feedback on your experience with Qdrant.

Let us know whenever you have a problem, face an unexpected behavior, or see a lack of documentation.
You can do it in any convenient way - create an [issue](https://github.com/qdrant/qdrant/issues), start a [discussion](https://github.com/qdrant/qdrant/discussions), or drop up a [message](https://discord.gg/tdtYvXjC4h).

If you use Qdrant or Metric Learning in your projects, we'd love to hear your story! Feel free to share articles and demos in our community.



For those familiar with Rust - check out our [contribution guide](https://github.com/qdrant/qdrant/blob/master/CONTRIBUTING.md).
If you have problems with code or architecture understanding - reach us at any time.

Feeling confident and want to contribute more? - Come to [work with us](https://qdrant.join.com/)!
Without a doubt, I'm eager to continue growing alongside this community and contribute to new features and enhancements that elevate the product. I've also become an advocate for Qdrant, introducing this project to numerous coworkers and friends in the tech industry. I'm excited to witness new users and contributors emerge from within my own network!
But I've explored Qdrant and as I said with Spotify web API there are a lot of things to be done with this specific user created recommendations. So with Qdrant, the Python client is quite good. The getting started helps a lot. So I wrote a bit of code. I think for production use cases it's really great. So for my use case here, as you can see on the right, I just read the text from a column and then I encode with the model. So the sentence transformer is the model that I encode with","['---\n\ntitle: Contribution Guidelines\n\nweight: 35\n\ndraft: true\n\n---\n\n\n\n# How to contribute\n\n\n\nIf you are a Qdrant user - Data Scientist, ML Engineer, or MLOps, the best contribution would be the feedback on your experience with Qdrant.\n\nLet us know whenever you have a problem, face an unexpected behavior, or see a lack of documentation.', ""You can do it in any convenient way - create an [issue](https://github.com/qdrant/qdrant/issues), start a [discussion](https://github.com/qdrant/qdrant/discussions), or drop up a [message](https://discord.gg/tdtYvXjC4h).\n\nIf you use Qdrant or Metric Learning in your projects, we'd love to hear your story! Feel free to share articles and demos in our community.\n\n\n\nFor those familiar with Rust - check out our [contribution guide](https://github.com/qdrant/qdrant/blob/master/CONTRIBUTING.md)."", 'If you have problems with code or architecture understanding - reach us at any time.\n\nFeeling confident and want to contribute more? - Come to [work with us](https://qdrant.join.com/)!', ""Without a doubt, I'm eager to continue growing alongside this community and contribute to new features and enhancements that elevate the product. I've also become an advocate for Qdrant, introducing this project to numerous coworkers and friends in the tech industry. I'm excited to witness new users and contributors emerge from within my own network!"", ""But I've explored Qdrant and as I said with Spotify web API there are a lot of things to be done with this specific user created recommendations. So with Qdrant, the Python client is quite good. The getting started helps a lot. So I wrote a bit of code. I think for production use cases it's really great. So for my use case here, as you can see on the right, I just read the text from a column and then I encode with the model. So the sentence transformer is the model that I encode with""]","Question: What are the recommended ways for Qdrant users to provide feedback or contribute to the project?\n\nContext: ---

title: Contribution Guidelines

weight: 35

draft: true

---



# How to contribute



If you are a Qdrant user - Data Scientist, ML Engineer, or MLOps, the best contribution would be the feedback on your experience with Qdrant.

Let us know whenever you have a problem, face an unexpected behavior, or see a lack of documentation.
You can do it in any convenient way - create an [issue](https://github.com/qdrant/qdrant/issues), start a [discussion](https://github.com/qdrant/qdrant/discussions), or drop up a [message](https://discord.gg/tdtYvXjC4h).

If you use Qdrant or Metric Learning in your projects, we'd love to hear your story! Feel free to share articles and demos in our community.



For those familiar with Rust - check out our [contribution guide](https://github.com/qdrant/qdrant/blob/master/CONTRIBUTING.md).
If you have problems with code or architecture understanding - reach us at any time.

Feeling confident and want to contribute more? - Come to [work with us](https://qdrant.join.com/)!
Without a doubt, I'm eager to continue growing alongside this community and contribute to new features and enhancements that elevate the product. I've also become an advocate for Qdrant, introducing this project to numerous coworkers and friends in the tech industry. I'm excited to witness new users and contributors emerge from within my own network!
But I've explored Qdrant and as I said with Spotify web API there are a lot of things to be done with this specific user created recommendations. So with Qdrant, the Python client is quite good. The getting started helps a lot. So I wrote a bit of code. I think for production use cases it's really great. So for my use case here, as you can see on the right, I just read the text from a column and then I encode with the model. So the sentence transformer is the model that I encode with\n\nAnswer:",What are the recommended ways for Qdrant users to provide feedback or contribute to the project?,"Qdrant encourages feedback and contributions from its users, specifically Data Scientists, ML Engineers, and MLOps professionals. The best way for Qdrant users to contribute is by providing feedback on their experience with Qdrant. This can include reporting problems, unexpected behaviors, or suggesting improvements in documentation. Users can share their feedback through various channels such as creating an issue on the Qdrant GitHub repository, starting a discussion on GitHub, or sending a message on Discord. Additionally, users who use Qdrant or Metric Learning in their projects are encouraged to share their stories, articles, and demos with the Qdrant community. For those familiar with Rust, there is a specific contribution guide available on the Qdrant GitHub repository. If users encounter problems with understanding the code or architecture, they can reach out to the Qdrant team at any time for assistance.","['---\n\ntitle: Contribution Guidelines\n\nweight: 35\n\ndraft: true\n\n---\n\n\n\n# How to contribute\n\n\n\nIf you are a Qdrant user - Data Scientist, ML Engineer, or MLOps, the best contribution would be the feedback on your experience with Qdrant.\n\nLet us know whenever you have a problem, face an unexpected behavior, or see a lack of documentation.', ""You can do it in any convenient way - create an [issue](https://github.com/qdrant/qdrant/issues), start a [discussion](https://github.com/qdrant/qdrant/discussions), or drop up a [message](https://discord.gg/tdtYvXjC4h).\n\nIf you use Qdrant or Metric Learning in your projects, we'd love to hear your story! Feel free to share articles and demos in our community.\n\n\n\nFor those familiar with Rust - check out our [contribution guide](https://github.com/qdrant/qdrant/blob/master/CONTRIBUTING.md)."", 'If you have problems with code or architecture understanding - reach us at any time.\n\nFeeling confident and want to contribute more? - Come to [work with us](https://qdrant.join.com/)!', ""Without a doubt, I'm eager to continue growing alongside this community and contribute to new features and enhancements that elevate the product. I've also become an advocate for Qdrant, introducing this project to numerous coworkers and friends in the tech industry. I'm excited to witness new users and contributors emerge from within my own network!"", ""But I've explored Qdrant and as I said with Spotify web API there are a lot of things to be done with this specific user created recommendations. So with Qdrant, the Python client is quite good. The getting started helps a lot. So I wrote a bit of code. I think for production use cases it's really great. So for my use case here, as you can see on the right, I just read the text from a column and then I encode with the model. So the sentence transformer is the model that I encode with""]","---

title: Contribution Guidelines

weight: 35

draft: true

---



# How to contribute



If you are a Qdrant user - Data Scientist, ML Engineer, or MLOps, the best contribution would be the feedback on your experience with Qdrant.

Let us know whenever you have a problem, face an unexpected behavior, or see a lack of documentation.
You can do it in any convenient way - create an [issue](https://github.com/qdrant/qdrant/issues), start a [discussion](https://github.com/qdrant/qdrant/discussions), or drop up a [message](https://discord.gg/tdtYvXjC4h).

If you use Qdrant or Metric Learning in your projects, we'd love to hear your story! Feel free to share articles and demos in our community.



For those familiar with Rust - check out our [contribution guide](https://github.com/qdrant/qdrant/blob/master/CONTRIBUTING.md).
If you have problems with code or architecture understanding - reach us at any time.

Feeling confident and want to contribute more? - Come to [work with us](https://qdrant.join.com/)!
Without a doubt, I'm eager to continue growing alongside this community and contribute to new features and enhancements that elevate the product. I've also become an advocate for Qdrant, introducing this project to numerous coworkers and friends in the tech industry. I'm excited to witness new users and contributors emerge from within my own network!
But I've explored Qdrant and as I said with Spotify web API there are a lot of things to be done with this specific user created recommendations. So with Qdrant, the Python client is quite good. The getting started helps a lot. So I wrote a bit of code. I think for production use cases it's really great. So for my use case here, as you can see on the right, I just read the text from a column and then I encode with the model. So the sentence transformer is the model that I encode with",0.43125,0.5390625,0.4791666666666667,0.15723270440251572,0.1968503937007874,0.17482517482517482,0.2375,0.296875,0.2638888888888889,0.3625,0.453125,0.40277777777777773,0.7397154569625854,0.6812688112258911,0.6993528604507446,0.6901924014091492,0.9453472401481122,0.9993042101975463,164,1.1884057971014492,0.6995091976335561
True,,2146.009,69b7fb39,What is Aleph Alpha and what capabilities does their API offer?,"Aleph Alpha is a multimodal and multilingual embeddings provider. Their API allows for the creation of embeddings for both text and images, placing them in the same latent space. They offer an official Python client that can be installed using pip. The API provides both synchronous and asynchronous clients. Users can obtain embeddings for images and store them into Qdrant by utilizing the provided Python client and following the specified steps in the documentation.","Aleph Alpha is a multimodal and multilingual embeddings provider. Their API allows users to create embeddings for text and images in the same latent space. The API offers capabilities for generating embeddings, performing semantic searches, and integrating with other platforms like Qdrant.

Users can interact with the Aleph Alpha API using the official Python client, which can be installed via pip. Additionally, users need to have a registered Aleph Alpha account and create an API key to access the API functionalities.

The API supports both synchronous and asynchronous clients for different use cases. Users can obtain embeddings for images and store them into platforms like Qdrant using the API.

In summary, Aleph Alpha's API provides the tools and functionalities for generating multimodal embeddings, performing semantic searches, and integrating with other services for various applications.","---

title: Aleph Alpha

weight: 900

aliases: [ ../integrations/aleph-alpha/ ]

---



Aleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both 

in the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be 

installed with pip:



```bash

pip install aleph-alpha-client

```
[Docker is the fastest way to do that](https://qdrant.tech/documentation/quick_start/#installation).

2. You need to have a registered [Aleph Alpha account](https://app.aleph-alpha.com/). 

3. Upon registration, create an API key (see: [API Tokens](https://app.aleph-alpha.com/profile)).



Now you can store the Aleph Alpha API key in a variable and choose the model your are going to use.



```python

aa_token = ""<< your_token >>""

model = ""luminous-base""

```



## Vectorize the dataset
---

title: Aleph Alpha Search

weight: 16

---



# Multimodal Semantic Search with Aleph Alpha



| Time: 30 min | Level: Beginner |  |    |

| --- | ----------- | ----------- |----------- |



This tutorial shows you how to run a proper multimodal semantic search system with a few lines of code, without the need to annotate the data or train your networks.
| [Build a Search with Aleph Alpha](tutorials/aleph-alpha-search/)           | Build a simple semantic search that combines text and image data.                  | Qdrant, Aleph Alpha | 

| [Developing Recommendations Systems](https://githubtocolab.com/qdrant/examples/blob/master/qdrant_101_getting_started/getting_started.ipynb)    | Learn how to get started building semantic search and recommendation systems. | Qdrant |
```bash

pip install aleph-alpha-client

```



There is both synchronous and asynchronous client available. Obtaining the embeddings for an image and storing it into Qdrant might 

be done in the following way:



```python

import qdrant_client



from aleph_alpha_client import (

    Prompt,

    AsyncClient,

    SemanticEmbeddingRequest,

    SemanticRepresentation,

    ImagePrompt

)

from qdrant_client.http.models import Batch



aa_token = ""<< your_token >>""

model = ""luminous-base""","[""---\n\ntitle: Aleph Alpha\n\nweight: 900\n\naliases: [ ../integrations/aleph-alpha/ ]\n\n---\n\n\n\nAleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both \n\nin the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be \n\ninstalled with pip:\n\n\n\n```bash\n\npip install aleph-alpha-client\n\n```"", '[Docker is the fastest way to do that](https://qdrant.tech/documentation/quick_start/#installation).\n\n2. You need to have a registered [Aleph Alpha account](https://app.aleph-alpha.com/). \n\n3. Upon registration, create an API key (see: [API Tokens](https://app.aleph-alpha.com/profile)).\n\n\n\nNow you can store the Aleph Alpha API key in a variable and choose the model your are going to use.\n\n\n\n```python\n\naa_token = ""<< your_token >>""\n\nmodel = ""luminous-base""\n\n```\n\n\n\n## Vectorize the dataset', '---\n\ntitle: Aleph Alpha Search\n\nweight: 16\n\n---\n\n\n\n# Multimodal Semantic Search with Aleph Alpha\n\n\n\n| Time: 30 min | Level: Beginner |  |    |\n\n| --- | ----------- | ----------- |----------- |\n\n\n\nThis tutorial shows you how to run a proper multimodal semantic search system with a few lines of code, without the need to annotate the data or train your networks.', '| [Build a Search with Aleph Alpha](tutorials/aleph-alpha-search/)           | Build a simple semantic search that combines text and image data.                  | Qdrant, Aleph Alpha | \n\n| [Developing Recommendations Systems](https://githubtocolab.com/qdrant/examples/blob/master/qdrant_101_getting_started/getting_started.ipynb)    | Learn how to get started building semantic search and recommendation systems. | Qdrant |', '```bash\n\npip install aleph-alpha-client\n\n```\n\n\n\nThere is both synchronous and asynchronous client available. Obtaining the embeddings for an image and storing it into Qdrant might \n\nbe done in the following way:\n\n\n\n```python\n\nimport qdrant_client\n\n\n\nfrom aleph_alpha_client import (\n\n    Prompt,\n\n    AsyncClient,\n\n    SemanticEmbeddingRequest,\n\n    SemanticRepresentation,\n\n    ImagePrompt\n\n)\n\nfrom qdrant_client.http.models import Batch\n\n\n\naa_token = ""<< your_token >>""\n\nmodel = ""luminous-base""']","Question: What is Aleph Alpha and what capabilities does their API offer?\n\nContext: ---

title: Aleph Alpha

weight: 900

aliases: [ ../integrations/aleph-alpha/ ]

---



Aleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both 

in the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be 

installed with pip:



```bash

pip install aleph-alpha-client

```
[Docker is the fastest way to do that](https://qdrant.tech/documentation/quick_start/#installation).

2. You need to have a registered [Aleph Alpha account](https://app.aleph-alpha.com/). 

3. Upon registration, create an API key (see: [API Tokens](https://app.aleph-alpha.com/profile)).



Now you can store the Aleph Alpha API key in a variable and choose the model your are going to use.



```python

aa_token = ""<< your_token >>""

model = ""luminous-base""

```



## Vectorize the dataset
---

title: Aleph Alpha Search

weight: 16

---



# Multimodal Semantic Search with Aleph Alpha



| Time: 30 min | Level: Beginner |  |    |

| --- | ----------- | ----------- |----------- |



This tutorial shows you how to run a proper multimodal semantic search system with a few lines of code, without the need to annotate the data or train your networks.
| [Build a Search with Aleph Alpha](tutorials/aleph-alpha-search/)           | Build a simple semantic search that combines text and image data.                  | Qdrant, Aleph Alpha | 

| [Developing Recommendations Systems](https://githubtocolab.com/qdrant/examples/blob/master/qdrant_101_getting_started/getting_started.ipynb)    | Learn how to get started building semantic search and recommendation systems. | Qdrant |
```bash

pip install aleph-alpha-client

```



There is both synchronous and asynchronous client available. Obtaining the embeddings for an image and storing it into Qdrant might 

be done in the following way:



```python

import qdrant_client



from aleph_alpha_client import (

Prompt,

AsyncClient,

SemanticEmbeddingRequest,

SemanticRepresentation,

ImagePrompt

)

from qdrant_client.http.models import Batch



aa_token = ""<< your_token >>""

model = ""luminous-base""\n\nAnswer:",What is Aleph Alpha and what capabilities does their API offer?,"Aleph Alpha is a multimodal and multilingual embeddings provider. Their API allows for the creation of embeddings for both text and images, placing them in the same latent space. They offer an official Python client that can be installed using pip. The API provides both synchronous and asynchronous clients. Users can obtain embeddings for images and store them into Qdrant by utilizing the provided Python client and following the specified steps in the documentation.","[""---\n\ntitle: Aleph Alpha\n\nweight: 900\n\naliases: [ ../integrations/aleph-alpha/ ]\n\n---\n\n\n\nAleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both \n\nin the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be \n\ninstalled with pip:\n\n\n\n```bash\n\npip install aleph-alpha-client\n\n```"", '[Docker is the fastest way to do that](https://qdrant.tech/documentation/quick_start/#installation).\n\n2. You need to have a registered [Aleph Alpha account](https://app.aleph-alpha.com/). \n\n3. Upon registration, create an API key (see: [API Tokens](https://app.aleph-alpha.com/profile)).\n\n\n\nNow you can store the Aleph Alpha API key in a variable and choose the model your are going to use.\n\n\n\n```python\n\naa_token = ""<< your_token >>""\n\nmodel = ""luminous-base""\n\n```\n\n\n\n## Vectorize the dataset', '---\n\ntitle: Aleph Alpha Search\n\nweight: 16\n\n---\n\n\n\n# Multimodal Semantic Search with Aleph Alpha\n\n\n\n| Time: 30 min | Level: Beginner |  |    |\n\n| --- | ----------- | ----------- |----------- |\n\n\n\nThis tutorial shows you how to run a proper multimodal semantic search system with a few lines of code, without the need to annotate the data or train your networks.', '| [Build a Search with Aleph Alpha](tutorials/aleph-alpha-search/)           | Build a simple semantic search that combines text and image data.                  | Qdrant, Aleph Alpha | \n\n| [Developing Recommendations Systems](https://githubtocolab.com/qdrant/examples/blob/master/qdrant_101_getting_started/getting_started.ipynb)    | Learn how to get started building semantic search and recommendation systems. | Qdrant |', '```bash\n\npip install aleph-alpha-client\n\n```\n\n\n\nThere is both synchronous and asynchronous client available. Obtaining the embeddings for an image and storing it into Qdrant might \n\nbe done in the following way:\n\n\n\n```python\n\nimport qdrant_client\n\n\n\nfrom aleph_alpha_client import (\n\n    Prompt,\n\n    AsyncClient,\n\n    SemanticEmbeddingRequest,\n\n    SemanticRepresentation,\n\n    ImagePrompt\n\n)\n\nfrom qdrant_client.http.models import Batch\n\n\n\naa_token = ""<< your_token >>""\n\nmodel = ""luminous-base""']","---

title: Aleph Alpha

weight: 900

aliases: [ ../integrations/aleph-alpha/ ]

---



Aleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both 

in the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be 

installed with pip:



```bash

pip install aleph-alpha-client

```
[Docker is the fastest way to do that](https://qdrant.tech/documentation/quick_start/#installation).

2. You need to have a registered [Aleph Alpha account](https://app.aleph-alpha.com/). 

3. Upon registration, create an API key (see: [API Tokens](https://app.aleph-alpha.com/profile)).



Now you can store the Aleph Alpha API key in a variable and choose the model your are going to use.



```python

aa_token = ""<< your_token >>""

model = ""luminous-base""

```



## Vectorize the dataset
---

title: Aleph Alpha Search

weight: 16

---



# Multimodal Semantic Search with Aleph Alpha



| Time: 30 min | Level: Beginner |  |    |

| --- | ----------- | ----------- |----------- |



This tutorial shows you how to run a proper multimodal semantic search system with a few lines of code, without the need to annotate the data or train your networks.
| [Build a Search with Aleph Alpha](tutorials/aleph-alpha-search/)           | Build a simple semantic search that combines text and image data.                  | Qdrant, Aleph Alpha | 

| [Developing Recommendations Systems](https://githubtocolab.com/qdrant/examples/blob/master/qdrant_101_getting_started/getting_started.ipynb)    | Learn how to get started building semantic search and recommendation systems. | Qdrant |
```bash

pip install aleph-alpha-client

```



There is both synchronous and asynchronous client available. Obtaining the embeddings for an image and storing it into Qdrant might 

be done in the following way:



```python

import qdrant_client



from aleph_alpha_client import (

    Prompt,

    AsyncClient,

    SemanticEmbeddingRequest,

    SemanticRepresentation,

    ImagePrompt

)

from qdrant_client.http.models import Batch



aa_token = ""<< your_token >>""

model = ""luminous-base""",0.4032258064516129,0.7575757575757576,0.5263157894736842,0.2764227642276423,0.5230769230769231,0.36170212765957444,0.3790322580645161,0.7121212121212122,0.49473684210526314,0.3951612903225806,0.7424242424242424,0.5157894736842105,0.959823489189148,0.6888642311096191,0.7917697429656982,0.7367409467697144,0.9847144001396373,0.9847881574183702,135,1.8243243243243243,0.6909456287822361
True,,2061.509,edbaed96,What is the purpose of the `task_type` parameter when obtaining Nomic embeddings?,"The `task_type` parameter in the `nomic-embed-text-v1` model is used to define the type of embeddings that are obtained for documents. When obtaining Nomic embeddings for documents, you need to set the `task_type` parameter to `search_document`. This ensures that the embeddings generated are specifically tailored for document search purposes. By specifying the `task_type` as `search_document`, you are instructing the model to encode the text in a way that is optimized for searching and retrieving relevant documents based on the input query. This parameter helps customize the embeddings to suit the specific requirements of document retrieval tasks within the Nomic framework.","The purpose of the `task_type` parameter when obtaining Nomic embeddings is to specify the type of task you want to use the embeddings for. Different values of `task_type` correspond to different tasks or applications where the embeddings will be used. 

For example:

- `search_document`: Used for obtaining embeddings for documents to store in Qdrant.
- `search_query`: Used for obtaining embeddings for search queries to find the most relevant documents.
- `classification`: Used for classification tasks.
- `clustering`: Used for text clustering.

By setting the `task_type` parameter accordingly, you can obtain the appropriate embeddings for your specific use case or task.","Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.



- `input_type=""search_document""` - for documents to store in Qdrant

- `input_type=""search_query""` - for search queries to find the most relevant documents

- `input_type=""classification""` - for classification tasks

- `input_type=""clustering""` - for text clustering



While implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the
```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

    texts=[""Qdrant is the best vector database!""],

    model=""nomic-embed-text-v1"",

    task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

    collection_name=""my-collection"",

    points=models.Batch(

        ids=[1],

        vectors=output[""embeddings""],

    ),

)

```



To query the collection, set the `task_type` to `search_query`:



```python
```python

output = embed.text(

    texts=[""What is the best vector database?""],

    model=""nomic-embed-text-v1"",

    task_type=""search_query"",

)



qdrant_client.search(

    collection_name=""my-collection"",

    query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
The Embedding Model API supports various task types, outlined as follows:



1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.

2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.

3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.

4. `classification`: Specifies that the given text will be classified.

5. `clustering`: Specifies that the embeddings will be used for clustering.","['Once installed, you can configure it with the official Python client or through direct HTTP requests.\n\n\n\n<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>\n\n\n\nYou can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings\n\nare obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.\n\nFor documents, set the `task_type` to `search_document`:\n\n\n\n```python', 'parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.\n\n\n\n- `input_type=""search_document""` - for documents to store in Qdrant\n\n- `input_type=""search_query""` - for search queries to find the most relevant documents\n\n- `input_type=""classification""` - for classification tasks\n\n- `input_type=""clustering""` - for text clustering\n\n\n\nWhile implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the', '```python\n\nfrom qdrant_client import QdrantClient, models\n\nfrom nomic import embed\n\n\n\noutput = embed.text(\n\n    texts=[""Qdrant is the best vector database!""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_document"",\n\n)\n\n\n\nqdrant_client = QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""my-collection"",\n\n    points=models.Batch(\n\n        ids=[1],\n\n        vectors=output[""embeddings""],\n\n    ),\n\n)\n\n```\n\n\n\nTo query the collection, set the `task_type` to `search_query`:\n\n\n\n```python', '```python\n\noutput = embed.text(\n\n    texts=[""What is the best vector database?""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_query"",\n\n)\n\n\n\nqdrant_client.search(\n\n    collection_name=""my-collection"",\n\n    query=output[""embeddings""][0],\n\n)\n\n```\n\n\n\nFor more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).', 'The Embedding Model API supports various task types, outlined as follows:\n\n\n\n1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.\n\n2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.\n\n3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.\n\n4. `classification`: Specifies that the given text will be classified.\n\n5. `clustering`: Specifies that the embeddings will be used for clustering.']","Question: What is the purpose of the `task_type` parameter when obtaining Nomic embeddings?\n\nContext: Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.



- `input_type=""search_document""` - for documents to store in Qdrant

- `input_type=""search_query""` - for search queries to find the most relevant documents

- `input_type=""classification""` - for classification tasks

- `input_type=""clustering""` - for text clustering



While implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the
```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

texts=[""Qdrant is the best vector database!""],

model=""nomic-embed-text-v1"",

task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

collection_name=""my-collection"",

points=models.Batch(

ids=[1],

vectors=output[""embeddings""],

),

)

```



To query the collection, set the `task_type` to `search_query`:



```python
```python

output = embed.text(

texts=[""What is the best vector database?""],

model=""nomic-embed-text-v1"",

task_type=""search_query"",

)



qdrant_client.search(

collection_name=""my-collection"",

query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
The Embedding Model API supports various task types, outlined as follows:



1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.

2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.

3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.

4. `classification`: Specifies that the given text will be classified.

5. `clustering`: Specifies that the embeddings will be used for clustering.\n\nAnswer:",What is the purpose of the `task_type` parameter when obtaining Nomic embeddings?,"The `task_type` parameter in the `nomic-embed-text-v1` model is used to define the type of embeddings that are obtained for documents. When obtaining Nomic embeddings for documents, you need to set the `task_type` parameter to `search_document`. This ensures that the embeddings generated are specifically tailored for document search purposes. By specifying the `task_type` as `search_document`, you are instructing the model to encode the text in a way that is optimized for searching and retrieving relevant documents based on the input query. This parameter helps customize the embeddings to suit the specific requirements of document retrieval tasks within the Nomic framework.","['Once installed, you can configure it with the official Python client or through direct HTTP requests.\n\n\n\n<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>\n\n\n\nYou can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings\n\nare obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.\n\nFor documents, set the `task_type` to `search_document`:\n\n\n\n```python', 'parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.\n\n\n\n- `input_type=""search_document""` - for documents to store in Qdrant\n\n- `input_type=""search_query""` - for search queries to find the most relevant documents\n\n- `input_type=""classification""` - for classification tasks\n\n- `input_type=""clustering""` - for text clustering\n\n\n\nWhile implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the', '```python\n\nfrom qdrant_client import QdrantClient, models\n\nfrom nomic import embed\n\n\n\noutput = embed.text(\n\n    texts=[""Qdrant is the best vector database!""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_document"",\n\n)\n\n\n\nqdrant_client = QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""my-collection"",\n\n    points=models.Batch(\n\n        ids=[1],\n\n        vectors=output[""embeddings""],\n\n    ),\n\n)\n\n```\n\n\n\nTo query the collection, set the `task_type` to `search_query`:\n\n\n\n```python', '```python\n\noutput = embed.text(\n\n    texts=[""What is the best vector database?""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_query"",\n\n)\n\n\n\nqdrant_client.search(\n\n    collection_name=""my-collection"",\n\n    query=output[""embeddings""][0],\n\n)\n\n```\n\n\n\nFor more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).', 'The Embedding Model API supports various task types, outlined as follows:\n\n\n\n1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.\n\n2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.\n\n3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.\n\n4. `classification`: Specifies that the given text will be classified.\n\n5. `clustering`: Specifies that the embeddings will be used for clustering.']","Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.



- `input_type=""search_document""` - for documents to store in Qdrant

- `input_type=""search_query""` - for search queries to find the most relevant documents

- `input_type=""classification""` - for classification tasks

- `input_type=""clustering""` - for text clustering



While implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the
```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

    texts=[""Qdrant is the best vector database!""],

    model=""nomic-embed-text-v1"",

    task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

    collection_name=""my-collection"",

    points=models.Batch(

        ids=[1],

        vectors=output[""embeddings""],

    ),

)

```



To query the collection, set the `task_type` to `search_query`:



```python
```python

output = embed.text(

    texts=[""What is the best vector database?""],

    model=""nomic-embed-text-v1"",

    task_type=""search_query"",

)



qdrant_client.search(

    collection_name=""my-collection"",

    query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
The Embedding Model API supports various task types, outlined as follows:



1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.

2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.

3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.

4. `classification`: Specifies that the given text will be classified.

5. `clustering`: Specifies that the embeddings will be used for clustering.",0.5161290322580645,0.5106382978723404,0.5133689839572193,0.16304347826086957,0.16129032258064516,0.16216216216216214,0.3010752688172043,0.2978723404255319,0.29946524064171126,0.45161290322580644,0.44680851063829785,0.44919786096256686,0.848180890083313,0.6517066955566406,0.6579447984695435,0.654810905456543,0.9574273999979985,0.9727462632581592,96,0.9696969696969697,0.450750884505794
True,,7744.302000000001,05a5e292,How can you use the Nomic library to upsert data into a collection and query it using text embeddings?,"To upsert data into a collection and query it using text embeddings with the Nomic library, you first need to use the `embed.text` method with the `task_type` set to `search_document` to generate embeddings for the text data you want to upsert. After obtaining the embeddings, you can use the `QdrantClient` to upsert the data into a specific collection by providing the collection name and the batch of points containing the IDs and corresponding vectors.

To query the collection, you need to set the `task_type` to `search_query` in the `embed.text` method with the query text. Once you have the query embeddings, you can use the `QdrantClient` to search the collection by providing the collection name and the query embedding to retrieve relevant results based on similarity.

For more detailed information and examples on using text , refer to : https://qdrant.tech/documentation/embeddings/nomic/","To use the Nomic library to upsert data into a collection and query it using text embeddings, you can follow these steps:

1. Import the necessary modules and functions from the Nomic library and Qdrant client.
2. Use the `embed.text` function to generate embeddings for your text data. Ensure to specify the `task_type` as `search_document` when embedding documents and `search_query` when embedding queries.
3. Upsert the embeddings into your Qdrant collection using the Qdrant client's `upsert` method.
4. To query the collection using text embeddings, you need to generate embeddings for your query text using the same approach as step 2.
5. Use the Qdrant client's `search` method to search for similar embeddings in the collection based on your query embeddings.

Here is an example code snippet demonstrating the process:

```python
from qdrant_client import QdrantClient, models
from nomic import embed

# Generate embeddings for a document
output_document = embed.text(
    texts=[""Qdrant is the best vector database!""],
    model=""nomic-embed-text-v1"",
    task_type=""search_document""
)

# Upsert the document embeddings into the collection
qdrant_client = QdrantClient()
qdrant_client.upsert(
    collection_name=""my-collection"",
    points=models.Batch(
        ids=[1],
        vectors=output_document[""embeddings""],
    ),
)

# Generate embeddings for a query
output_query = embed.text(
    texts=[""What is the best vector database?""],
    model=""nomic-embed-text-v1"",
    task_type=""search_query""
)

# Query the collection using the query embeddings
search_results = qdrant_client.search(
    collection_name=""my-collection"",
    query=output_query[""embeddings""][0],
)
```

Remember to replace placeholders like `my-collection` with your actual collection name and configure the Nomic API token for authentication.

This code snippet showcases how to use Nomic text embeddings to interact with the Qdrant client for upserting documents and querying collections based on text embeddings.","```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

    texts=[""Qdrant is the best vector database!""],

    model=""nomic-embed-text-v1"",

    task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

    collection_name=""my-collection"",

    points=models.Batch(

        ids=[1],

        vectors=output[""embeddings""],

    ),

)

```



To query the collection, set the `task_type` to `search_query`:



```python
```python

output = embed.text(

    texts=[""What is the best vector database?""],

    model=""nomic-embed-text-v1"",

    task_type=""search_query"",

)



qdrant_client.search(

    collection_name=""my-collection"",

    query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
collection_name=""MyCollection"",

    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),

)





qdrant_client.upsert(

    collection_name=""MyCollection"",

    points=Batch(

        ids=list(range(len(embeddings))),

        vectors=embeddings,

    ),

)



```
# https://platform.openai.com/docs/models/embeddings

embedding_model = ""text-embedding-ada-002""



openai_client = openai.Client(

    api_key=""<< your_api_key >>""

)

response = openai_client.embeddings.create(

    input=""The best vector database"",

    model=embedding_model,

)



qdrant_client = qdrant_client.QdrantClient()

qdrant_client.upsert(

    collection_name=""MyCollection"",

    points=Batch(

        ids=[1],

        vectors=[response.data[0].embedding],

    ),

)

```","['```python\n\nfrom qdrant_client import QdrantClient, models\n\nfrom nomic import embed\n\n\n\noutput = embed.text(\n\n    texts=[""Qdrant is the best vector database!""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_document"",\n\n)\n\n\n\nqdrant_client = QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""my-collection"",\n\n    points=models.Batch(\n\n        ids=[1],\n\n        vectors=output[""embeddings""],\n\n    ),\n\n)\n\n```\n\n\n\nTo query the collection, set the `task_type` to `search_query`:\n\n\n\n```python', '```python\n\noutput = embed.text(\n\n    texts=[""What is the best vector database?""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_query"",\n\n)\n\n\n\nqdrant_client.search(\n\n    collection_name=""my-collection"",\n\n    query=output[""embeddings""][0],\n\n)\n\n```\n\n\n\nFor more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).', 'Once installed, you can configure it with the official Python client or through direct HTTP requests.\n\n\n\n<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>\n\n\n\nYou can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings\n\nare obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.\n\nFor documents, set the `task_type` to `search_document`:\n\n\n\n```python', 'collection_name=""MyCollection"",\n\n    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),\n\n)\n\n\n\n\n\nqdrant_client.upsert(\n\n    collection_name=""MyCollection"",\n\n    points=Batch(\n\n        ids=list(range(len(embeddings))),\n\n        vectors=embeddings,\n\n    ),\n\n)\n\n\n\n```', '# https://platform.openai.com/docs/models/embeddings\n\nembedding_model = ""text-embedding-ada-002""\n\n\n\nopenai_client = openai.Client(\n\n    api_key=""<< your_api_key >>""\n\n)\n\nresponse = openai_client.embeddings.create(\n\n    input=""The best vector database"",\n\n    model=embedding_model,\n\n)\n\n\n\nqdrant_client = qdrant_client.QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""MyCollection"",\n\n    points=Batch(\n\n        ids=[1],\n\n        vectors=[response.data[0].embedding],\n\n    ),\n\n)\n\n```']","Question: How can you use the Nomic library to upsert data into a collection and query it using text embeddings?\n\nContext: ```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

texts=[""Qdrant is the best vector database!""],

model=""nomic-embed-text-v1"",

task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

collection_name=""my-collection"",

points=models.Batch(

ids=[1],

vectors=output[""embeddings""],

),

)

```



To query the collection, set the `task_type` to `search_query`:



```python
```python

output = embed.text(

texts=[""What is the best vector database?""],

model=""nomic-embed-text-v1"",

task_type=""search_query"",

)



qdrant_client.search(

collection_name=""my-collection"",

query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
collection_name=""MyCollection"",

vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),

)





qdrant_client.upsert(

collection_name=""MyCollection"",

points=Batch(

ids=list(range(len(embeddings))),

vectors=embeddings,

),

)



```
# https://platform.openai.com/docs/models/embeddings

embedding_model = ""text-embedding-ada-002""



openai_client = openai.Client(

api_key=""<< your_api_key >>""

)

response = openai_client.embeddings.create(

input=""The best vector database"",

model=embedding_model,

)



qdrant_client = qdrant_client.QdrantClient()

qdrant_client.upsert(

collection_name=""MyCollection"",

points=Batch(

ids=[1],

vectors=[response.data[0].embedding],

),

)

```\n\nAnswer:",How can you use the Nomic library to upsert data into a collection and query it using text embeddings?,"To upsert data into a collection and query it using text embeddings with the Nomic library, you first need to use the `embed.text` method with the `task_type` set to `search_document` to generate embeddings for the text data you want to upsert. After obtaining the embeddings, you can use the `QdrantClient` to upsert the data into a specific collection by providing the collection name and the batch of points containing the IDs and corresponding vectors.

To query the collection, you need to set the `task_type` to `search_query` in the `embed.text` method with the query text. Once you have the query embeddings, you can use the `QdrantClient` to search the collection by providing the collection name and the query embedding to retrieve relevant results based on similarity.

For more detailed information and examples on using text , refer to : https://qdrant.tech/documentation/embeddings/nomic/","['```python\n\nfrom qdrant_client import QdrantClient, models\n\nfrom nomic import embed\n\n\n\noutput = embed.text(\n\n    texts=[""Qdrant is the best vector database!""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_document"",\n\n)\n\n\n\nqdrant_client = QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""my-collection"",\n\n    points=models.Batch(\n\n        ids=[1],\n\n        vectors=output[""embeddings""],\n\n    ),\n\n)\n\n```\n\n\n\nTo query the collection, set the `task_type` to `search_query`:\n\n\n\n```python', '```python\n\noutput = embed.text(\n\n    texts=[""What is the best vector database?""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_query"",\n\n)\n\n\n\nqdrant_client.search(\n\n    collection_name=""my-collection"",\n\n    query=output[""embeddings""][0],\n\n)\n\n```\n\n\n\nFor more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).', 'Once installed, you can configure it with the official Python client or through direct HTTP requests.\n\n\n\n<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>\n\n\n\nYou can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings\n\nare obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.\n\nFor documents, set the `task_type` to `search_document`:\n\n\n\n```python', 'collection_name=""MyCollection"",\n\n    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),\n\n)\n\n\n\n\n\nqdrant_client.upsert(\n\n    collection_name=""MyCollection"",\n\n    points=Batch(\n\n        ids=list(range(len(embeddings))),\n\n        vectors=embeddings,\n\n    ),\n\n)\n\n\n\n```', '# https://platform.openai.com/docs/models/embeddings\n\nembedding_model = ""text-embedding-ada-002""\n\n\n\nopenai_client = openai.Client(\n\n    api_key=""<< your_api_key >>""\n\n)\n\nresponse = openai_client.embeddings.create(\n\n    input=""The best vector database"",\n\n    model=embedding_model,\n\n)\n\n\n\nqdrant_client = qdrant_client.QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""MyCollection"",\n\n    points=Batch(\n\n        ids=[1],\n\n        vectors=[response.data[0].embedding],\n\n    ),\n\n)\n\n```']","```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

    texts=[""Qdrant is the best vector database!""],

    model=""nomic-embed-text-v1"",

    task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

    collection_name=""my-collection"",

    points=models.Batch(

        ids=[1],

        vectors=output[""embeddings""],

    ),

)

```



To query the collection, set the `task_type` to `search_query`:



```python
```python

output = embed.text(

    texts=[""What is the best vector database?""],

    model=""nomic-embed-text-v1"",

    task_type=""search_query"",

)



qdrant_client.search(

    collection_name=""my-collection"",

    query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
collection_name=""MyCollection"",

    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),

)





qdrant_client.upsert(

    collection_name=""MyCollection"",

    points=Batch(

        ids=list(range(len(embeddings))),

        vectors=embeddings,

    ),

)



```
# https://platform.openai.com/docs/models/embeddings

embedding_model = ""text-embedding-ada-002""



openai_client = openai.Client(

    api_key=""<< your_api_key >>""

)

response = openai_client.embeddings.create(

    input=""The best vector database"",

    model=embedding_model,

)



qdrant_client = qdrant_client.QdrantClient()

qdrant_client.upsert(

    collection_name=""MyCollection"",

    points=Batch(

        ids=[1],

        vectors=[response.data[0].embedding],

    ),

)

```",0.34962406015037595,0.7322834645669292,0.4732824427480916,0.16226415094339622,0.3412698412698413,0.21994884910485935,0.21428571428571427,0.44881889763779526,0.2900763358778626,0.3233082706766917,0.6771653543307087,0.4376590330788803,0.9487036466598511,0.6513408422470093,0.6794589757919312,0.6651028990745544,0.5835213259095327,0.8841950930655003,254,1.8405797101449275,0.38606482546206555
True,,2173.1859999999997,6f73188b,What is the purpose of the `task_type` parameter in the Gemini Embedding Model API?,"The `task_type` parameter in the Gemini Embedding Model API serves to designate the intended purpose for the embeddings utilized. It allows users to specify the type of task they want to perform with the given text. The supported task types include `retrieval_query` for search/retrieval queries, `retrieval_document` for documents from the corpus being searched, `semantic_similarity` for Semantic Text Similarity, and `classification` for text classification. This parameter helps in customizing the functionality of the Gemini Embedding Model API based on the specific task requirements.","The purpose of the `task_type` parameter in the Gemini Embedding Model API is to designate the intended purpose for the embeddings being utilized. It specifies the type of task that the given text will be used for. The `task_type` parameter allows you to specify whether the text is a query in a search/retrieval setting, a document from the corpus being searched, for semantic text similarity, for classification, or for clustering. By setting the `task_type` parameter accordingly, you can ensure that the embeddings generated serve the specific purpose you require, such as retrieval_query, retrieval_document, semantic_similarity, classification, or clustering.","---

title: Gemini

weight: 700

---



# Gemini



Qdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:



Gemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. 



In the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.
task_type=""retrieval_document"",

    title=""Qdrant x Gemini"",

)

```



The returned result is a dictionary with a key: `embedding`. The value of this key is a list of floats representing the embedding of the document.



## Indexing documents with Qdrant



```python

from qdrant_client.http.models import Batch



qdrant_client = qdrant_client.QdrantClient()

qdrant_client.upsert(

    collection_name=""GeminiCollection"",

    points=Batch(

        ids=[1],

        vectors=genai.embed_content(
The Embedding Model API supports various task types, outlined as follows:



1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.

2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.

3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.

4. `classification`: Specifies that the given text will be classified.

5. `clustering`: Specifies that the embeddings will be used for clustering.
ids=[1],

        vectors=genai.embed_content(

            model=""models/embedding-001"",

            content=""Qdrant is the best vector search engine to use with Gemini"",

            task_type=""retrieval_document"",

            title=""Qdrant x Gemini"",

        )[""embedding""],

    ),

)

```



## Searching for documents with Qdrant



Once the documents are indexed, you can search for the most relevant documents using the same model with the `retrieval_query` task type:



```python
The following example shows how to embed a document with the `models/embedding-001` with the `retrieval_document` task type:



## Embedding a document



```python

import pathlib

import google.generativeai as genai

import qdrant_client



GEMINI_API_KEY = ""YOUR GEMINI API KEY""  # add your key here



genai.configure(api_key=GEMINI_API_KEY)



result = genai.embed_content(

    model=""models/embedding-001"",

    content=""Qdrant is the best vector search engine to use with Gemini"",","['---\n\ntitle: Gemini\n\nweight: 700\n\n---\n\n\n\n# Gemini\n\n\n\nQdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:\n\n\n\nGemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. \n\n\n\nIn the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.', 'task_type=""retrieval_document"",\n\n    title=""Qdrant x Gemini"",\n\n)\n\n```\n\n\n\nThe returned result is a dictionary with a key: `embedding`. The value of this key is a list of floats representing the embedding of the document.\n\n\n\n## Indexing documents with Qdrant\n\n\n\n```python\n\nfrom qdrant_client.http.models import Batch\n\n\n\nqdrant_client = qdrant_client.QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""GeminiCollection"",\n\n    points=Batch(\n\n        ids=[1],\n\n        vectors=genai.embed_content(', 'The Embedding Model API supports various task types, outlined as follows:\n\n\n\n1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.\n\n2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.\n\n3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.\n\n4. `classification`: Specifies that the given text will be classified.\n\n5. `clustering`: Specifies that the embeddings will be used for clustering.', 'ids=[1],\n\n        vectors=genai.embed_content(\n\n            model=""models/embedding-001"",\n\n            content=""Qdrant is the best vector search engine to use with Gemini"",\n\n            task_type=""retrieval_document"",\n\n            title=""Qdrant x Gemini"",\n\n        )[""embedding""],\n\n    ),\n\n)\n\n```\n\n\n\n## Searching for documents with Qdrant\n\n\n\nOnce the documents are indexed, you can search for the most relevant documents using the same model with the `retrieval_query` task type:\n\n\n\n```python', 'The following example shows how to embed a document with the `models/embedding-001` with the `retrieval_document` task type:\n\n\n\n## Embedding a document\n\n\n\n```python\n\nimport pathlib\n\nimport google.generativeai as genai\n\nimport qdrant_client\n\n\n\nGEMINI_API_KEY = ""YOUR GEMINI API KEY""  # add your key here\n\n\n\ngenai.configure(api_key=GEMINI_API_KEY)\n\n\n\nresult = genai.embed_content(\n\n    model=""models/embedding-001"",\n\n    content=""Qdrant is the best vector search engine to use with Gemini"",']","Question: What is the purpose of the `task_type` parameter in the Gemini Embedding Model API?\n\nContext: ---

title: Gemini

weight: 700

---



# Gemini



Qdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:



Gemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. 



In the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.
task_type=""retrieval_document"",

title=""Qdrant x Gemini"",

)

```



The returned result is a dictionary with a key: `embedding`. The value of this key is a list of floats representing the embedding of the document.



## Indexing documents with Qdrant



```python

from qdrant_client.http.models import Batch



qdrant_client = qdrant_client.QdrantClient()

qdrant_client.upsert(

collection_name=""GeminiCollection"",

points=Batch(

ids=[1],

vectors=genai.embed_content(
The Embedding Model API supports various task types, outlined as follows:



1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.

2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.

3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.

4. `classification`: Specifies that the given text will be classified.

5. `clustering`: Specifies that the embeddings will be used for clustering.
ids=[1],

vectors=genai.embed_content(

model=""models/embedding-001"",

content=""Qdrant is the best vector search engine to use with Gemini"",

task_type=""retrieval_document"",

title=""Qdrant x Gemini"",

)[""embedding""],

),

)

```



## Searching for documents with Qdrant



Once the documents are indexed, you can search for the most relevant documents using the same model with the `retrieval_query` task type:



```python
The following example shows how to embed a document with the `models/embedding-001` with the `retrieval_document` task type:



## Embedding a document



```python

import pathlib

import google.generativeai as genai

import qdrant_client



GEMINI_API_KEY = ""YOUR GEMINI API KEY""  # add your key here



genai.configure(api_key=GEMINI_API_KEY)



result = genai.embed_content(

model=""models/embedding-001"",

content=""Qdrant is the best vector search engine to use with Gemini"",\n\nAnswer:",What is the purpose of the `task_type` parameter in the Gemini Embedding Model API?,"The `task_type` parameter in the Gemini Embedding Model API serves to designate the intended purpose for the embeddings utilized. It allows users to specify the type of task they want to perform with the given text. The supported task types include `retrieval_query` for search/retrieval queries, `retrieval_document` for documents from the corpus being searched, `semantic_similarity` for Semantic Text Similarity, and `classification` for text classification. This parameter helps in customizing the functionality of the Gemini Embedding Model API based on the specific task requirements.","['---\n\ntitle: Gemini\n\nweight: 700\n\n---\n\n\n\n# Gemini\n\n\n\nQdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:\n\n\n\nGemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. \n\n\n\nIn the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.', 'task_type=""retrieval_document"",\n\n    title=""Qdrant x Gemini"",\n\n)\n\n```\n\n\n\nThe returned result is a dictionary with a key: `embedding`. The value of this key is a list of floats representing the embedding of the document.\n\n\n\n## Indexing documents with Qdrant\n\n\n\n```python\n\nfrom qdrant_client.http.models import Batch\n\n\n\nqdrant_client = qdrant_client.QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""GeminiCollection"",\n\n    points=Batch(\n\n        ids=[1],\n\n        vectors=genai.embed_content(', 'The Embedding Model API supports various task types, outlined as follows:\n\n\n\n1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.\n\n2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.\n\n3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.\n\n4. `classification`: Specifies that the given text will be classified.\n\n5. `clustering`: Specifies that the embeddings will be used for clustering.', 'ids=[1],\n\n        vectors=genai.embed_content(\n\n            model=""models/embedding-001"",\n\n            content=""Qdrant is the best vector search engine to use with Gemini"",\n\n            task_type=""retrieval_document"",\n\n            title=""Qdrant x Gemini"",\n\n        )[""embedding""],\n\n    ),\n\n)\n\n```\n\n\n\n## Searching for documents with Qdrant\n\n\n\nOnce the documents are indexed, you can search for the most relevant documents using the same model with the `retrieval_query` task type:\n\n\n\n```python', 'The following example shows how to embed a document with the `models/embedding-001` with the `retrieval_document` task type:\n\n\n\n## Embedding a document\n\n\n\n```python\n\nimport pathlib\n\nimport google.generativeai as genai\n\nimport qdrant_client\n\n\n\nGEMINI_API_KEY = ""YOUR GEMINI API KEY""  # add your key here\n\n\n\ngenai.configure(api_key=GEMINI_API_KEY)\n\n\n\nresult = genai.embed_content(\n\n    model=""models/embedding-001"",\n\n    content=""Qdrant is the best vector search engine to use with Gemini"",']","---

title: Gemini

weight: 700

---



# Gemini



Qdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:



Gemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. 



In the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.
task_type=""retrieval_document"",

    title=""Qdrant x Gemini"",

)

```



The returned result is a dictionary with a key: `embedding`. The value of this key is a list of floats representing the embedding of the document.



## Indexing documents with Qdrant



```python

from qdrant_client.http.models import Batch



qdrant_client = qdrant_client.QdrantClient()

qdrant_client.upsert(

    collection_name=""GeminiCollection"",

    points=Batch(

        ids=[1],

        vectors=genai.embed_content(
The Embedding Model API supports various task types, outlined as follows:



1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.

2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.

3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.

4. `classification`: Specifies that the given text will be classified.

5. `clustering`: Specifies that the embeddings will be used for clustering.
ids=[1],

        vectors=genai.embed_content(

            model=""models/embedding-001"",

            content=""Qdrant is the best vector search engine to use with Gemini"",

            task_type=""retrieval_document"",

            title=""Qdrant x Gemini"",

        )[""embedding""],

    ),

)

```



## Searching for documents with Qdrant



Once the documents are indexed, you can search for the most relevant documents using the same model with the `retrieval_query` task type:



```python
The following example shows how to embed a document with the `models/embedding-001` with the `retrieval_document` task type:



## Embedding a document



```python

import pathlib

import google.generativeai as genai

import qdrant_client



GEMINI_API_KEY = ""YOUR GEMINI API KEY""  # add your key here



genai.configure(api_key=GEMINI_API_KEY)



result = genai.embed_content(

    model=""models/embedding-001"",

    content=""Qdrant is the best vector search engine to use with Gemini"",",0.6590909090909091,0.7631578947368421,0.7073170731707317,0.367816091954023,0.4266666666666667,0.39506172839506165,0.4772727272727273,0.5526315789473685,0.5121951219512196,0.5795454545454546,0.6710526315789473,0.6219512195121951,0.9091881513595581,0.6988421678543091,0.7144811153411865,0.7065751552581787,0.9888732202816755,0.9872543483506888,97,1.1829268292682926,0.6472236689655769
True,,934.477,d5130449,What is the maximum number of tokens that Jina embeddings allow for model input lengths?,"Jina embeddings allow for model input lengths of up to 8192 tokens. This means that the models utilizing Jina embeddings can process sequences with a maximum length of 8192 tokens, providing flexibility and capability for handling large amounts of text data efficiently.",The maximum number of tokens that Jina embeddings allow for model input lengths is up to 8192 tokens.,"---

title: Jina Embeddings

weight: 800

aliases: [ ../integrations/jina-embeddings/ ]

---



# Jina Embeddings



Qdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.
The default model and several other models have a context window of maximum 512 tokens. This maximum limit comes from the embedding model training and design itself.If you'd like to embed sequences larger than that, we'd recommend using some pooling strategy to get a single vector out of the sequence. For example, you can use the mean of the embeddings of different chunks of a document. This is also what the [SBERT Paper recommends](https://lilianweng.github.io/posts/2021-05-31-contrastive/#sentence-bert)
. The other caveat is that we pass on the token limits or context windows from the embedding model creators themselves. So in the case of this model, which is BGE base, that is 512 BGE tokens.
# Provide Jina API key and choose one of the available models.

# You can get a free trial key here: https://jina.ai/embeddings/

JINA_API_KEY = ""jina_xxxxxxxxxxx""

MODEL = ""jina-embeddings-v2-base-en""  # or ""jina-embeddings-v2-base-en""

EMBEDDING_SIZE = 768  # 512 for small variant



# Get embeddings from the API

url = ""https://api.jina.ai/v1/embeddings""



headers = {

    ""Content-Type"": ""application/json"",

    ""Authorization"": f""Bearer {JINA_API_KEY}"",

}



data = {
This expansion has a direct relationship with what we can control when making a SPLADE model: Sparsity via Regularisation. The number of tokens (BERT wordpieces) we use to represent each document. If we use more tokens, we can represent more terms, but the vectors become denser. This number is typically between 20 to 200 per document. As a reference point, the dense BERT vector is 768 dimensions, OpenAI Embedding is 1536 dimensions, and the sparse vector is 30 dimensions.","['---\n\ntitle: Jina Embeddings\n\nweight: 800\n\naliases: [ ../integrations/jina-embeddings/ ]\n\n---\n\n\n\n# Jina Embeddings\n\n\n\nQdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.', ""The default model and several other models have a context window of maximum 512 tokens. This maximum limit comes from the embedding model training and design itself.If you'd like to embed sequences larger than that, we'd recommend using some pooling strategy to get a single vector out of the sequence. For example, you can use the mean of the embeddings of different chunks of a document. This is also what the [SBERT Paper recommends](https://lilianweng.github.io/posts/2021-05-31-contrastive/#sentence-bert)"", '. The other caveat is that we pass on the token limits or context windows from the embedding model creators themselves. So in the case of this model, which is BGE base, that is 512 BGE tokens.', '# Provide Jina API key and choose one of the available models.\n\n# You can get a free trial key here: https://jina.ai/embeddings/\n\nJINA_API_KEY = ""jina_xxxxxxxxxxx""\n\nMODEL = ""jina-embeddings-v2-base-en""  # or ""jina-embeddings-v2-base-en""\n\nEMBEDDING_SIZE = 768  # 512 for small variant\n\n\n\n# Get embeddings from the API\n\nurl = ""https://api.jina.ai/v1/embeddings""\n\n\n\nheaders = {\n\n    ""Content-Type"": ""application/json"",\n\n    ""Authorization"": f""Bearer {JINA_API_KEY}"",\n\n}\n\n\n\ndata = {', 'This expansion has a direct relationship with what we can control when making a SPLADE model: Sparsity via Regularisation. The number of tokens (BERT wordpieces) we use to represent each document. If we use more tokens, we can represent more terms, but the vectors become denser. This number is typically between 20 to 200 per document. As a reference point, the dense BERT vector is 768 dimensions, OpenAI Embedding is 1536 dimensions, and the sparse vector is 30 dimensions.']","Question: What is the maximum number of tokens that Jina embeddings allow for model input lengths?\n\nContext: ---

title: Jina Embeddings

weight: 800

aliases: [ ../integrations/jina-embeddings/ ]

---



# Jina Embeddings



Qdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.
The default model and several other models have a context window of maximum 512 tokens. This maximum limit comes from the embedding model training and design itself.If you'd like to embed sequences larger than that, we'd recommend using some pooling strategy to get a single vector out of the sequence. For example, you can use the mean of the embeddings of different chunks of a document. This is also what the [SBERT Paper recommends](https://lilianweng.github.io/posts/2021-05-31-contrastive/#sentence-bert)
. The other caveat is that we pass on the token limits or context windows from the embedding model creators themselves. So in the case of this model, which is BGE base, that is 512 BGE tokens.
# Provide Jina API key and choose one of the available models.

# You can get a free trial key here: https://jina.ai/embeddings/

JINA_API_KEY = ""jina_xxxxxxxxxxx""

MODEL = ""jina-embeddings-v2-base-en""  # or ""jina-embeddings-v2-base-en""

EMBEDDING_SIZE = 768  # 512 for small variant



# Get embeddings from the API

url = ""https://api.jina.ai/v1/embeddings""



headers = {

""Content-Type"": ""application/json"",

""Authorization"": f""Bearer {JINA_API_KEY}"",

}



data = {
This expansion has a direct relationship with what we can control when making a SPLADE model: Sparsity via Regularisation. The number of tokens (BERT wordpieces) we use to represent each document. If we use more tokens, we can represent more terms, but the vectors become denser. This number is typically between 20 to 200 per document. As a reference point, the dense BERT vector is 768 dimensions, OpenAI Embedding is 1536 dimensions, and the sparse vector is 30 dimensions.\n\nAnswer:",What is the maximum number of tokens that Jina embeddings allow for model input lengths?,"Jina embeddings allow for model input lengths of up to 8192 tokens. This means that the models utilizing Jina embeddings can process sequences with a maximum length of 8192 tokens, providing flexibility and capability for handling large amounts of text data efficiently.","['---\n\ntitle: Jina Embeddings\n\nweight: 800\n\naliases: [ ../integrations/jina-embeddings/ ]\n\n---\n\n\n\n# Jina Embeddings\n\n\n\nQdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.', ""The default model and several other models have a context window of maximum 512 tokens. This maximum limit comes from the embedding model training and design itself.If you'd like to embed sequences larger than that, we'd recommend using some pooling strategy to get a single vector out of the sequence. For example, you can use the mean of the embeddings of different chunks of a document. This is also what the [SBERT Paper recommends](https://lilianweng.github.io/posts/2021-05-31-contrastive/#sentence-bert)"", '. The other caveat is that we pass on the token limits or context windows from the embedding model creators themselves. So in the case of this model, which is BGE base, that is 512 BGE tokens.', '# Provide Jina API key and choose one of the available models.\n\n# You can get a free trial key here: https://jina.ai/embeddings/\n\nJINA_API_KEY = ""jina_xxxxxxxxxxx""\n\nMODEL = ""jina-embeddings-v2-base-en""  # or ""jina-embeddings-v2-base-en""\n\nEMBEDDING_SIZE = 768  # 512 for small variant\n\n\n\n# Get embeddings from the API\n\nurl = ""https://api.jina.ai/v1/embeddings""\n\n\n\nheaders = {\n\n    ""Content-Type"": ""application/json"",\n\n    ""Authorization"": f""Bearer {JINA_API_KEY}"",\n\n}\n\n\n\ndata = {', 'This expansion has a direct relationship with what we can control when making a SPLADE model: Sparsity via Regularisation. The number of tokens (BERT wordpieces) we use to represent each document. If we use more tokens, we can represent more terms, but the vectors become denser. This number is typically between 20 to 200 per document. As a reference point, the dense BERT vector is 768 dimensions, OpenAI Embedding is 1536 dimensions, and the sparse vector is 30 dimensions.']","---

title: Jina Embeddings

weight: 800

aliases: [ ../integrations/jina-embeddings/ ]

---



# Jina Embeddings



Qdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.
The default model and several other models have a context window of maximum 512 tokens. This maximum limit comes from the embedding model training and design itself.If you'd like to embed sequences larger than that, we'd recommend using some pooling strategy to get a single vector out of the sequence. For example, you can use the mean of the embeddings of different chunks of a document. This is also what the [SBERT Paper recommends](https://lilianweng.github.io/posts/2021-05-31-contrastive/#sentence-bert)
. The other caveat is that we pass on the token limits or context windows from the embedding model creators themselves. So in the case of this model, which is BGE base, that is 512 BGE tokens.
# Provide Jina API key and choose one of the available models.

# You can get a free trial key here: https://jina.ai/embeddings/

JINA_API_KEY = ""jina_xxxxxxxxxxx""

MODEL = ""jina-embeddings-v2-base-en""  # or ""jina-embeddings-v2-base-en""

EMBEDDING_SIZE = 768  # 512 for small variant



# Get embeddings from the API

url = ""https://api.jina.ai/v1/embeddings""



headers = {

    ""Content-Type"": ""application/json"",

    ""Authorization"": f""Bearer {JINA_API_KEY}"",

}



data = {
This expansion has a direct relationship with what we can control when making a SPLADE model: Sparsity via Regularisation. The number of tokens (BERT wordpieces) we use to represent each document. If we use more tokens, we can represent more terms, but the vectors become denser. This number is typically between 20 to 200 per document. As a reference point, the dense BERT vector is 768 dimensions, OpenAI Embedding is 1536 dimensions, and the sparse vector is 30 dimensions.",0.8823529411764706,0.375,0.5263157894736842,0.5625,0.23076923076923078,0.32727272727272727,0.6470588235294118,0.275,0.3859649122807018,0.7647058823529411,0.325,0.456140350877193,0.9264309406280518,0.6830343008041382,0.6052727103233337,0.6418067216873169,0.9966134079732001,0.9928856168407947,18,0.42857142857142855,0.2696752266529241
True,,4777.249000000001,07a4f0f0,Why does Qdrant by default not return vectors in search results?,"By default, Qdrant tries to minimize network traffic and doesn't return vectors in search results. This default behavior is in place to optimize performance and reduce unnecessary data transfer. However, if you specifically require the vectors to be included in the search results, you can override this default setting by setting the `with_vector` parameter of the Search/Scroll to `true`. This allows you to retrieve the vectors along with other search results, providing you with the option to access the vector data when needed.","By default, Qdrant does not return vectors in search results in order to minimize network traffic. However, you can force Qdrant to include vectors in search results by setting the `with_vector` parameter to `true` when making a search request.

If you are seeing `""vector"": null` in your search results, it could be due to issues such as the vector not being in the correct format or errors in how you are calling the upsert method.

If you need to search without a vector, you can utilize the scroll method provided by Qdrant. This method allows you to retrieve records based on filters or iterate over all records in the collection without relying on vectors.

Regarding full-text search or hybrid search capabilities, Qdrant does not natively support full-text search. However, you can implement a form of full-text search by encoding text data into vectors using techniques like Word2Vec or TF-IDF before indexing them in Qdrant. This allows you to search for text-based queries by converting them into vector representations for matching against indexed vectors.

In the provided code snippets, you can see examples of how you can search for points in a Qdrant collection using the Qdrant client in Rust or Python. The Python client also supports a parameter called `exact` which, when set to `true`, performs a full search on the entire vector collection without using indexing.","Please read more about collections, isolation, and multiple users in our [Multitenancy](../../tutorials/multiple-partitions/) tutorial.



### My search results contain vectors with null values. Why?



By default, Qdrant tries to minimize network traffic and doesn't return vectors in search results.

But you can force Qdrant to do so by setting the `with_vector` parameter of the Search/Scroll to `true`.
If you're still seeing `""vector"": null` in your results, it might be that the vector you're passing is not in the correct format, or there's an issue with how you're calling the upsert method.



### How can I search without a vector?



You are likely looking for the [scroll](../../concepts/points/#scroll-points) method. It allows you to retrieve the records based on filters or even iterate over all the records in the collection.



### Does Qdrant support a full-text search or a hybrid search?
qdrant: &QdrantClient,

) -> Result<Vec<ScoredPoint>> {

    Ok(qdrant.search_points(&SearchPoints {

        collection_name,

        limit: 5, // use what fits your use case here

        with_payload: Some(true.into()),

        vector: embed(client, text, api_key)?,

        ..Default::default()

    }).await?.result)

}

```
qdrant_client

        .search_points(&SearchPoints {

            collection_name: COLLECTION_NAME.to_string(),

            vector,

            limit: SEARCH_LIMIT,

            with_payload: Some(true.into()),

            ..Default::default()

        })

        .await

        .map_err(|err| anyhow!(""Failed to search Qdrant: {}"", err))

}

```
. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.","[""Please read more about collections, isolation, and multiple users in our [Multitenancy](../../tutorials/multiple-partitions/) tutorial.\n\n\n\n### My search results contain vectors with null values. Why?\n\n\n\nBy default, Qdrant tries to minimize network traffic and doesn't return vectors in search results.\n\nBut you can force Qdrant to do so by setting the `with_vector` parameter of the Search/Scroll to `true`."", 'If you\'re still seeing `""vector"": null` in your results, it might be that the vector you\'re passing is not in the correct format, or there\'s an issue with how you\'re calling the upsert method.\n\n\n\n### How can I search without a vector?\n\n\n\nYou are likely looking for the [scroll](../../concepts/points/#scroll-points) method. It allows you to retrieve the records based on filters or even iterate over all the records in the collection.\n\n\n\n### Does Qdrant support a full-text search or a hybrid search?', 'qdrant: &QdrantClient,\n\n) -> Result<Vec<ScoredPoint>> {\n\n    Ok(qdrant.search_points(&SearchPoints {\n\n        collection_name,\n\n        limit: 5, // use what fits your use case here\n\n        with_payload: Some(true.into()),\n\n        vector: embed(client, text, api_key)?,\n\n        ..Default::default()\n\n    }).await?.result)\n\n}\n\n```', 'qdrant_client\n\n        .search_points(&SearchPoints {\n\n            collection_name: COLLECTION_NAME.to_string(),\n\n            vector,\n\n            limit: SEARCH_LIMIT,\n\n            with_payload: Some(true.into()),\n\n            ..Default::default()\n\n        })\n\n        .await\n\n        .map_err(|err| anyhow!(""Failed to search Qdrant: {}"", err))\n\n}\n\n```', '. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.']","Question: Why does Qdrant by default not return vectors in search results?\n\nContext: Please read more about collections, isolation, and multiple users in our [Multitenancy](../../tutorials/multiple-partitions/) tutorial.



### My search results contain vectors with null values. Why?



By default, Qdrant tries to minimize network traffic and doesn't return vectors in search results.

But you can force Qdrant to do so by setting the `with_vector` parameter of the Search/Scroll to `true`.
If you're still seeing `""vector"": null` in your results, it might be that the vector you're passing is not in the correct format, or there's an issue with how you're calling the upsert method.



### How can I search without a vector?



You are likely looking for the [scroll](../../concepts/points/#scroll-points) method. It allows you to retrieve the records based on filters or even iterate over all the records in the collection.



### Does Qdrant support a full-text search or a hybrid search?
qdrant: &QdrantClient,

) -> Result<Vec<ScoredPoint>> {

Ok(qdrant.search_points(&SearchPoints {

collection_name,

limit: 5, // use what fits your use case here

with_payload: Some(true.into()),

vector: embed(client, text, api_key)?,

..Default::default()

}).await?.result)

}

```
qdrant_client

.search_points(&SearchPoints {

collection_name: COLLECTION_NAME.to_string(),

vector,

limit: SEARCH_LIMIT,

with_payload: Some(true.into()),

..Default::default()

})

.await

.map_err(|err| anyhow!(""Failed to search Qdrant: {}"", err))

}

```
. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.\n\nAnswer:",Why does Qdrant by default not return vectors in search results?,"By default, Qdrant tries to minimize network traffic and doesn't return vectors in search results. This default behavior is in place to optimize performance and reduce unnecessary data transfer. However, if you specifically require the vectors to be included in the search results, you can override this default setting by setting the `with_vector` parameter of the Search/Scroll to `true`. This allows you to retrieve the vectors along with other search results, providing you with the option to access the vector data when needed.","[""Please read more about collections, isolation, and multiple users in our [Multitenancy](../../tutorials/multiple-partitions/) tutorial.\n\n\n\n### My search results contain vectors with null values. Why?\n\n\n\nBy default, Qdrant tries to minimize network traffic and doesn't return vectors in search results.\n\nBut you can force Qdrant to do so by setting the `with_vector` parameter of the Search/Scroll to `true`."", 'If you\'re still seeing `""vector"": null` in your results, it might be that the vector you\'re passing is not in the correct format, or there\'s an issue with how you\'re calling the upsert method.\n\n\n\n### How can I search without a vector?\n\n\n\nYou are likely looking for the [scroll](../../concepts/points/#scroll-points) method. It allows you to retrieve the records based on filters or even iterate over all the records in the collection.\n\n\n\n### Does Qdrant support a full-text search or a hybrid search?', 'qdrant: &QdrantClient,\n\n) -> Result<Vec<ScoredPoint>> {\n\n    Ok(qdrant.search_points(&SearchPoints {\n\n        collection_name,\n\n        limit: 5, // use what fits your use case here\n\n        with_payload: Some(true.into()),\n\n        vector: embed(client, text, api_key)?,\n\n        ..Default::default()\n\n    }).await?.result)\n\n}\n\n```', 'qdrant_client\n\n        .search_points(&SearchPoints {\n\n            collection_name: COLLECTION_NAME.to_string(),\n\n            vector,\n\n            limit: SEARCH_LIMIT,\n\n            with_payload: Some(true.into()),\n\n            ..Default::default()\n\n        })\n\n        .await\n\n        .map_err(|err| anyhow!(""Failed to search Qdrant: {}"", err))\n\n}\n\n```', '. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.']","Please read more about collections, isolation, and multiple users in our [Multitenancy](../../tutorials/multiple-partitions/) tutorial.



### My search results contain vectors with null values. Why?



By default, Qdrant tries to minimize network traffic and doesn't return vectors in search results.

But you can force Qdrant to do so by setting the `with_vector` parameter of the Search/Scroll to `true`.
If you're still seeing `""vector"": null` in your results, it might be that the vector you're passing is not in the correct format, or there's an issue with how you're calling the upsert method.



### How can I search without a vector?



You are likely looking for the [scroll](../../concepts/points/#scroll-points) method. It allows you to retrieve the records based on filters or even iterate over all the records in the collection.



### Does Qdrant support a full-text search or a hybrid search?
qdrant: &QdrantClient,

) -> Result<Vec<ScoredPoint>> {

    Ok(qdrant.search_points(&SearchPoints {

        collection_name,

        limit: 5, // use what fits your use case here

        with_payload: Some(true.into()),

        vector: embed(client, text, api_key)?,

        ..Default::default()

    }).await?.result)

}

```
qdrant_client

        .search_points(&SearchPoints {

            collection_name: COLLECTION_NAME.to_string(),

            vector,

            limit: SEARCH_LIMIT,

            with_payload: Some(true.into()),

            ..Default::default()

        })

        .await

        .map_err(|err| anyhow!(""Failed to search Qdrant: {}"", err))

}

```
. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.",0.2488479262672811,0.6835443037974683,0.36486486486486486,0.10648148148148148,0.2948717948717949,0.15646258503401364,0.16129032258064516,0.4430379746835443,0.23648648648648646,0.2119815668202765,0.5822784810126582,0.3108108108108108,0.9296765327453613,0.604485034942627,0.7004414796829224,0.6489351987838745,0.8649765916954493,0.9756897175102495,227,2.7023809523809526,0.40424766002852935
True,,560.923,f8eb2dfd,What does Qdrant not plan to support in the future?,"In the future, Qdrant does not plan to support BM25 or other non-vector-based retrieval or ranking functions, built-in ontologies or knowledge graphs, and query analyzers and other NLP tools. Qdrant aims to introduce support for sparse vectors, as used in SPLADE or similar models, but does not intend to incorporate the aforementioned features into its functionality. Users can combine Qdrant with specialized tools, including full-text search engines, to complement its capabilities.",Qdrant does not plan to support code interpretation in the future.,"To keep the human in the loop.

But obviously some stuff that are on our roadmap.

And another thing that we don't support, which is one type of action would.

Be the first we will be working on is obviously code interpretation, which is I think is one of the things that all users ask because they use.

It on Chat GPT.

And so we'll be looking into that as well.





Demetrios:

What made you choose Qdrant?





Stanislas Polu:

So the decision was made, if I.
Other than as expressly set out in these terms, Qdrant does not make any commitments about the Solution and Website’s availability or ability to meet your expectations.





### 15. Liability



In no event shall Qdrant be liable for:





* any indirect damages of any kind, including any potential loss of business;

* any damage or loss which is not caused by a breach of its obligations under the T&Cs;

* disruptions or damage inherent in an electronic communications network;
Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.
* unauthorized access to the Solution by a third-party caused by you, including through your negligence;

* your failure to fulfill your obligations under the T&Cs;

* implementation of any software package, software or operating system not compatible with the Solution;

* failure of the electronic communication networks which is not the fault of Qdrant;

* your refusal to collaborate with Qdrant in the resolution of the anomalies and in particular to answer questions and requests for information;
Demetrios:

Yeah, I think it's more just like you don't have to add it on to Qdrant in any way or this is completely separated.



Nirant Kasliwal:","[""To keep the human in the loop.\n\nBut obviously some stuff that are on our roadmap.\n\nAnd another thing that we don't support, which is one type of action would.\n\nBe the first we will be working on is obviously code interpretation, which is I think is one of the things that all users ask because they use.\n\nIt on Chat GPT.\n\nAnd so we'll be looking into that as well.\n\n\n\n\n\nDemetrios:\n\nWhat made you choose Qdrant?\n\n\n\n\n\nStanislas Polu:\n\nSo the decision was made, if I."", 'Other than as expressly set out in these terms, Qdrant does not make any commitments about the Solution and Website’s availability or ability to meet your expectations.\n\n\n\n\n\n### 15. Liability\n\n\n\nIn no event shall Qdrant be liable for:\n\n\n\n\n\n* any indirect damages of any kind, including any potential loss of business;\n\n* any damage or loss which is not caused by a breach of its obligations under the T&Cs;\n\n* disruptions or damage inherent in an electronic communications network;', 'Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.', '* unauthorized access to the Solution by a third-party caused by you, including through your negligence;\n\n* your failure to fulfill your obligations under the T&Cs;\n\n* implementation of any software package, software or operating system not compatible with the Solution;\n\n* failure of the electronic communication networks which is not the fault of Qdrant;\n\n* your refusal to collaborate with Qdrant in the resolution of the anomalies and in particular to answer questions and requests for information;', ""Demetrios:\n\nYeah, I think it's more just like you don't have to add it on to Qdrant in any way or this is completely separated.\n\n\n\nNirant Kasliwal:""]","Question: What does Qdrant not plan to support in the future?\n\nContext: To keep the human in the loop.

But obviously some stuff that are on our roadmap.

And another thing that we don't support, which is one type of action would.

Be the first we will be working on is obviously code interpretation, which is I think is one of the things that all users ask because they use.

It on Chat GPT.

And so we'll be looking into that as well.





Demetrios:

What made you choose Qdrant?





Stanislas Polu:

So the decision was made, if I.
Other than as expressly set out in these terms, Qdrant does not make any commitments about the Solution and Website’s availability or ability to meet your expectations.





### 15. Liability



In no event shall Qdrant be liable for:





* any indirect damages of any kind, including any potential loss of business;

* any damage or loss which is not caused by a breach of its obligations under the T&Cs;

* disruptions or damage inherent in an electronic communications network;
Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.
* unauthorized access to the Solution by a third-party caused by you, including through your negligence;

* your failure to fulfill your obligations under the T&Cs;

* implementation of any software package, software or operating system not compatible with the Solution;

* failure of the electronic communication networks which is not the fault of Qdrant;

* your refusal to collaborate with Qdrant in the resolution of the anomalies and in particular to answer questions and requests for information;
Demetrios:

Yeah, I think it's more just like you don't have to add it on to Qdrant in any way or this is completely separated.



Nirant Kasliwal:\n\nAnswer:",What does Qdrant not plan to support in the future?,"In the future, Qdrant does not plan to support BM25 or other non-vector-based retrieval or ranking functions, built-in ontologies or knowledge graphs, and query analyzers and other NLP tools. Qdrant aims to introduce support for sparse vectors, as used in SPLADE or similar models, but does not intend to incorporate the aforementioned features into its functionality. Users can combine Qdrant with specialized tools, including full-text search engines, to complement its capabilities.","[""To keep the human in the loop.\n\nBut obviously some stuff that are on our roadmap.\n\nAnd another thing that we don't support, which is one type of action would.\n\nBe the first we will be working on is obviously code interpretation, which is I think is one of the things that all users ask because they use.\n\nIt on Chat GPT.\n\nAnd so we'll be looking into that as well.\n\n\n\n\n\nDemetrios:\n\nWhat made you choose Qdrant?\n\n\n\n\n\nStanislas Polu:\n\nSo the decision was made, if I."", 'Other than as expressly set out in these terms, Qdrant does not make any commitments about the Solution and Website’s availability or ability to meet your expectations.\n\n\n\n\n\n### 15. Liability\n\n\n\nIn no event shall Qdrant be liable for:\n\n\n\n\n\n* any indirect damages of any kind, including any potential loss of business;\n\n* any damage or loss which is not caused by a breach of its obligations under the T&Cs;\n\n* disruptions or damage inherent in an electronic communications network;', 'Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.', '* unauthorized access to the Solution by a third-party caused by you, including through your negligence;\n\n* your failure to fulfill your obligations under the T&Cs;\n\n* implementation of any software package, software or operating system not compatible with the Solution;\n\n* failure of the electronic communication networks which is not the fault of Qdrant;\n\n* your refusal to collaborate with Qdrant in the resolution of the anomalies and in particular to answer questions and requests for information;', ""Demetrios:\n\nYeah, I think it's more just like you don't have to add it on to Qdrant in any way or this is completely separated.\n\n\n\nNirant Kasliwal:""]","To keep the human in the loop.

But obviously some stuff that are on our roadmap.

And another thing that we don't support, which is one type of action would.

Be the first we will be working on is obviously code interpretation, which is I think is one of the things that all users ask because they use.

It on Chat GPT.

And so we'll be looking into that as well.





Demetrios:

What made you choose Qdrant?





Stanislas Polu:

So the decision was made, if I.
Other than as expressly set out in these terms, Qdrant does not make any commitments about the Solution and Website’s availability or ability to meet your expectations.





### 15. Liability



In no event shall Qdrant be liable for:





* any indirect damages of any kind, including any potential loss of business;

* any damage or loss which is not caused by a breach of its obligations under the T&Cs;

* disruptions or damage inherent in an electronic communications network;
Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.
* unauthorized access to the Solution by a third-party caused by you, including through your negligence;

* your failure to fulfill your obligations under the T&Cs;

* implementation of any software package, software or operating system not compatible with the Solution;

* failure of the electronic communication networks which is not the fault of Qdrant;

* your refusal to collaborate with Qdrant in the resolution of the anomalies and in particular to answer questions and requests for information;
Demetrios:

Yeah, I think it's more just like you don't have to add it on to Qdrant in any way or this is completely separated.



Nirant Kasliwal:",0.8,0.1095890410958904,0.19277108433734938,0.6666666666666666,0.08333333333333333,0.14814814814814814,0.7,0.0958904109589041,0.1686746987951807,0.7,0.0958904109589041,0.1686746987951807,0.34840020537376404,0.7050514817237854,0.5335484147071838,0.6074263453483582,0.09272611141204834,0.615872859954834,11,0.15492957746478872,0.38829750366599036
True,,3302.944,08f8d4c2, What is the recommended approach for configuring multitenancy in terms of the number of collections to create?,"In most cases, the recommended approach for configuring multitenancy is to create only a single collection with payload-based partitioning. This approach, known as multitenancy, is efficient for the majority of users. By using a single collection with payload-based partitioning, each user can access their own vectors while ensuring that they cannot see the vectors of other users. However, it is important to note that this approach requires additional configuration to set up properly.","The recommended approach for configuring multitenancy in terms of the number of collections to create depends on your specific requirements:

1. **Single Collection with Payload-Based Partitioning**: In most cases, it is recommended to use a single collection with payload-based partitioning for multitenancy. This approach is efficient for most users and helps in managing a multi-tenant collection where each tenant is segregated. This approach requires additional configuration but is suitable for most scenarios.

2. **Multiple Collections for Limited Users Requiring Isolation**: If you have a limited number of users and require isolation between them, you may consider creating multiple collections. This approach provides flexibility but may be more costly as creating numerous collections can result in resource overhead. It is essential to ensure that these collections do not affect each other in terms of performance or data integrity.

In summary, for most users, a single collection with payload-based partitioning is recommended for multitenancy. However, if you have specific requirements for isolation between users and are willing to manage the additional overhead, creating multiple collections may be more suitable.","## Setting up multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called [multitenancy](https://en.wikipedia.org/wiki/Multitenancy). It is efficient for most of users, but it requires additional configuration. [Learn how to set it up](../../tutorials/multiple-partitions/)
---

title: Multitenancy

weight: 12

aliases:

  - ../tutorials/multiple-partitions

---

# Configure Multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.
**When should you create multiple collections?** When you have a limited number of users and you need isolation. This approach is flexible, but it may be more costly, since creating numerous collections may result in resource overhead. Also, you need to ensure that they do not affect each other in any way, including performance-wise. 



> Note: If you're running `curl` from the command line, the following commands

assume that you have a running instance of Qdrant on `http://localhost:6333`.
In addition to metrics and vector size, each collection uses its own set of parameters that controls collection optimization, index construction, and vacuum.

These settings can be changed at any time by a corresponding request.



## Setting up multitenancy
A clear use-case for this feature is managing a multi-tenant collection, where each tenant (let it be a user or organization) is assumed to be segregated, so they can have their data stored in separate shards.



To enable user-defined sharding, set `sharding_method` to `custom` during collection creation:



```http

PUT /collections/{collection_name}

{

    ""shard_number"": 1,

    ""sharding_method"": ""custom""

    // ... other collection parameters

}

```



```python","['## Setting up multitenancy\n\n\n\n**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called [multitenancy](https://en.wikipedia.org/wiki/Multitenancy). It is efficient for most of users, but it requires additional configuration. [Learn how to set it up](../../tutorials/multiple-partitions/)', '---\n\ntitle: Multitenancy\n\nweight: 12\n\naliases:\n\n  - ../tutorials/multiple-partitions\n\n---\n\n# Configure Multitenancy\n\n\n\n**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.', ""**When should you create multiple collections?** When you have a limited number of users and you need isolation. This approach is flexible, but it may be more costly, since creating numerous collections may result in resource overhead. Also, you need to ensure that they do not affect each other in any way, including performance-wise. \n\n\n\n> Note: If you're running `curl` from the command line, the following commands\n\nassume that you have a running instance of Qdrant on `http://localhost:6333`."", 'In addition to metrics and vector size, each collection uses its own set of parameters that controls collection optimization, index construction, and vacuum.\n\nThese settings can be changed at any time by a corresponding request.\n\n\n\n## Setting up multitenancy', 'A clear use-case for this feature is managing a multi-tenant collection, where each tenant (let it be a user or organization) is assumed to be segregated, so they can have their data stored in separate shards.\n\n\n\nTo enable user-defined sharding, set `sharding_method` to `custom` during collection creation:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""shard_number"": 1,\n\n    ""sharding_method"": ""custom""\n\n    // ... other collection parameters\n\n}\n\n```\n\n\n\n```python']","Question:  What is the recommended approach for configuring multitenancy in terms of the number of collections to create?\n\nContext: ## Setting up multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called [multitenancy](https://en.wikipedia.org/wiki/Multitenancy). It is efficient for most of users, but it requires additional configuration. [Learn how to set it up](../../tutorials/multiple-partitions/)
---

title: Multitenancy

weight: 12

aliases:

- ../tutorials/multiple-partitions

---

# Configure Multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.
**When should you create multiple collections?** When you have a limited number of users and you need isolation. This approach is flexible, but it may be more costly, since creating numerous collections may result in resource overhead. Also, you need to ensure that they do not affect each other in any way, including performance-wise. 



> Note: If you're running `curl` from the command line, the following commands

assume that you have a running instance of Qdrant on `http://localhost:6333`.
In addition to metrics and vector size, each collection uses its own set of parameters that controls collection optimization, index construction, and vacuum.

These settings can be changed at any time by a corresponding request.



## Setting up multitenancy
A clear use-case for this feature is managing a multi-tenant collection, where each tenant (let it be a user or organization) is assumed to be segregated, so they can have their data stored in separate shards.



To enable user-defined sharding, set `sharding_method` to `custom` during collection creation:



```http

PUT /collections/{collection_name}

{

""shard_number"": 1,

""sharding_method"": ""custom""

// ... other collection parameters

}

```



```python\n\nAnswer:", What is the recommended approach for configuring multitenancy in terms of the number of collections to create?,"In most cases, the recommended approach for configuring multitenancy is to create only a single collection with payload-based partitioning. This approach, known as multitenancy, is efficient for the majority of users. By using a single collection with payload-based partitioning, each user can access their own vectors while ensuring that they cannot see the vectors of other users. However, it is important to note that this approach requires additional configuration to set up properly.","['## Setting up multitenancy\n\n\n\n**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called [multitenancy](https://en.wikipedia.org/wiki/Multitenancy). It is efficient for most of users, but it requires additional configuration. [Learn how to set it up](../../tutorials/multiple-partitions/)', '---\n\ntitle: Multitenancy\n\nweight: 12\n\naliases:\n\n  - ../tutorials/multiple-partitions\n\n---\n\n# Configure Multitenancy\n\n\n\n**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.', ""**When should you create multiple collections?** When you have a limited number of users and you need isolation. This approach is flexible, but it may be more costly, since creating numerous collections may result in resource overhead. Also, you need to ensure that they do not affect each other in any way, including performance-wise. \n\n\n\n> Note: If you're running `curl` from the command line, the following commands\n\nassume that you have a running instance of Qdrant on `http://localhost:6333`."", 'In addition to metrics and vector size, each collection uses its own set of parameters that controls collection optimization, index construction, and vacuum.\n\nThese settings can be changed at any time by a corresponding request.\n\n\n\n## Setting up multitenancy', 'A clear use-case for this feature is managing a multi-tenant collection, where each tenant (let it be a user or organization) is assumed to be segregated, so they can have their data stored in separate shards.\n\n\n\nTo enable user-defined sharding, set `sharding_method` to `custom` during collection creation:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""shard_number"": 1,\n\n    ""sharding_method"": ""custom""\n\n    // ... other collection parameters\n\n}\n\n```\n\n\n\n```python']","## Setting up multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called [multitenancy](https://en.wikipedia.org/wiki/Multitenancy). It is efficient for most of users, but it requires additional configuration. [Learn how to set it up](../../tutorials/multiple-partitions/)
---

title: Multitenancy

weight: 12

aliases:

  - ../tutorials/multiple-partitions

---

# Configure Multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.
**When should you create multiple collections?** When you have a limited number of users and you need isolation. This approach is flexible, but it may be more costly, since creating numerous collections may result in resource overhead. Also, you need to ensure that they do not affect each other in any way, including performance-wise. 



> Note: If you're running `curl` from the command line, the following commands

assume that you have a running instance of Qdrant on `http://localhost:6333`.
In addition to metrics and vector size, each collection uses its own set of parameters that controls collection optimization, index construction, and vacuum.

These settings can be changed at any time by a corresponding request.



## Setting up multitenancy
A clear use-case for this feature is managing a multi-tenant collection, where each tenant (let it be a user or organization) is assumed to be segregated, so they can have their data stored in separate shards.



To enable user-defined sharding, set `sharding_method` to `custom` during collection creation:



```http

PUT /collections/{collection_name}

{

    ""shard_number"": 1,

    ""sharding_method"": ""custom""

    // ... other collection parameters

}

```



```python",0.29310344827586204,0.7285714285714285,0.41803278688524587,0.16184971098265896,0.4057971014492754,0.23140495867768596,0.1781609195402299,0.44285714285714284,0.2540983606557377,0.28160919540229884,0.7,0.4016393442622951,0.7552495002746582,0.6119750738143921,0.702049732208252,0.6539251208305359,0.8947348764042544,0.9949205954035278,177,2.391891891891892,0.6341495339057182
True,,5114.975,414d0e7b,What is the purpose of the locking feature in Qdrant administration tools?,"The locking feature in Qdrant administration tools serves the purpose of restricting the possible operations on a Qdrant process. It allows users to control the behavior of a Qdrant instance at runtime without manually changing its configuration. When a lock is applied, specific operations such as creating new collections or adding new data to the existing storage can be disabled. It is important to note that the locking configuration is not persistent, so it needs to be reapplied after a restart. Additionally, the locking feature applies to a single node only, requiring the lock to be set on all desired nodes in a distributed deployment setup. The locking API in Qdrant enables users to manage and control the access and operations on the Qdrant instance effectively.","The purpose of the locking feature in Qdrant administration tools is to restrict the possible operations on a Qdrant process. By using the locking API, users can control and limit the actions that can be performed on a Qdrant instance at runtime. This feature is important for maintaining the integrity and security of the data stored in Qdrant, as it allows for a more controlled and regulated environment for making changes or modifications to the Qdrant instance.","---

title: Administration

weight: 10

aliases:

  - ../administration

---



# Administration



Qdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.



## Locking



A locking API enables users to restrict the possible operations on a qdrant process.

It is important to mention that:

- The configuration is not persistent therefore it is necessary to lock again following a restart.
Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.
## Why Qdrant?
and once the queues are set up, there is no further syscall that would stop

any user thread.



Servers that use this can easily get to over 100k concurrent requests. Today

Linux allows asynchronous IO via io\_uring for network, disk and accessing

other ports, e.g. for printing or recording video.



## And what about Qdrant?



Qdrant can store everything in memory, but not all data sets may fit, which can

require storing  on disk. Before io\_uring, Qdrant used mmap to do its IO. This
],

    shard_key: ""user_1"",

});

```



```rust



use qdrant_client::qdrant::{PointStruct, WriteOrdering, WriteOrderingType};



client

    .upsert_points_blocking(

        ""{collection_name}"",

        Some(vec![shard_key::Key::String(""user_1"".into())]),

        vec![

            PointStruct::new(

                1111,

                vec![0.1, 0.2, 0.3],

                Default::default(),

            ),

        ],

        None,

    )

    .await?;

```



```java","['---\n\ntitle: Administration\n\nweight: 10\n\naliases:\n\n  - ../administration\n\n---\n\n\n\n# Administration\n\n\n\nQdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.\n\n\n\n## Locking\n\n\n\nA locking API enables users to restrict the possible operations on a qdrant process.\n\nIt is important to mention that:\n\n- The configuration is not persistent therefore it is necessary to lock again following a restart.', 'Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.', '## Why Qdrant?', 'and once the queues are set up, there is no further syscall that would stop\n\nany user thread.\n\n\n\nServers that use this can easily get to over 100k concurrent requests. Today\n\nLinux allows asynchronous IO via io\\_uring for network, disk and accessing\n\nother ports, e.g. for printing or recording video.\n\n\n\n## And what about Qdrant?\n\n\n\nQdrant can store everything in memory, but not all data sets may fit, which can\n\nrequire storing  on disk. Before io\\_uring, Qdrant used mmap to do its IO. This', '],\n\n    shard_key: ""user_1"",\n\n});\n\n```\n\n\n\n```rust\n\n\n\nuse qdrant_client::qdrant::{PointStruct, WriteOrdering, WriteOrderingType};\n\n\n\nclient\n\n    .upsert_points_blocking(\n\n        ""{collection_name}"",\n\n        Some(vec![shard_key::Key::String(""user_1"".into())]),\n\n        vec![\n\n            PointStruct::new(\n\n                1111,\n\n                vec![0.1, 0.2, 0.3],\n\n                Default::default(),\n\n            ),\n\n        ],\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java']","Question: What is the purpose of the locking feature in Qdrant administration tools?\n\nContext: ---

title: Administration

weight: 10

aliases:

- ../administration

---



# Administration



Qdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.



## Locking



A locking API enables users to restrict the possible operations on a qdrant process.

It is important to mention that:

- The configuration is not persistent therefore it is necessary to lock again following a restart.
Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.
## Why Qdrant?
and once the queues are set up, there is no further syscall that would stop

any user thread.



Servers that use this can easily get to over 100k concurrent requests. Today

Linux allows asynchronous IO via io\_uring for network, disk and accessing

other ports, e.g. for printing or recording video.



## And what about Qdrant?



Qdrant can store everything in memory, but not all data sets may fit, which can

require storing  on disk. Before io\_uring, Qdrant used mmap to do its IO. This
],

shard_key: ""user_1"",

});

```



```rust



use qdrant_client::qdrant::{PointStruct, WriteOrdering, WriteOrderingType};



client

.upsert_points_blocking(

""{collection_name}"",

Some(vec![shard_key::Key::String(""user_1"".into())]),

vec![

PointStruct::new(

1111,

vec![0.1, 0.2, 0.3],

Default::default(),

),

],

None,

)

.await?;

```



```java\n\nAnswer:",What is the purpose of the locking feature in Qdrant administration tools?,"The locking feature in Qdrant administration tools serves the purpose of restricting the possible operations on a Qdrant process. It allows users to control the behavior of a Qdrant instance at runtime without manually changing its configuration. When a lock is applied, specific operations such as creating new collections or adding new data to the existing storage can be disabled. It is important to note that the locking configuration is not persistent, so it needs to be reapplied after a restart. Additionally, the locking feature applies to a single node only, requiring the lock to be set on all desired nodes in a distributed deployment setup. The locking API in Qdrant enables users to manage and control the access and operations on the Qdrant instance effectively.","['---\n\ntitle: Administration\n\nweight: 10\n\naliases:\n\n  - ../administration\n\n---\n\n\n\n# Administration\n\n\n\nQdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.\n\n\n\n## Locking\n\n\n\nA locking API enables users to restrict the possible operations on a qdrant process.\n\nIt is important to mention that:\n\n- The configuration is not persistent therefore it is necessary to lock again following a restart.', 'Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.', '## Why Qdrant?', 'and once the queues are set up, there is no further syscall that would stop\n\nany user thread.\n\n\n\nServers that use this can easily get to over 100k concurrent requests. Today\n\nLinux allows asynchronous IO via io\\_uring for network, disk and accessing\n\nother ports, e.g. for printing or recording video.\n\n\n\n## And what about Qdrant?\n\n\n\nQdrant can store everything in memory, but not all data sets may fit, which can\n\nrequire storing  on disk. Before io\\_uring, Qdrant used mmap to do its IO. This', '],\n\n    shard_key: ""user_1"",\n\n});\n\n```\n\n\n\n```rust\n\n\n\nuse qdrant_client::qdrant::{PointStruct, WriteOrdering, WriteOrderingType};\n\n\n\nclient\n\n    .upsert_points_blocking(\n\n        ""{collection_name}"",\n\n        Some(vec![shard_key::Key::String(""user_1"".into())]),\n\n        vec![\n\n            PointStruct::new(\n\n                1111,\n\n                vec![0.1, 0.2, 0.3],\n\n                Default::default(),\n\n            ),\n\n        ],\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java']","---

title: Administration

weight: 10

aliases:

  - ../administration

---



# Administration



Qdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.



## Locking



A locking API enables users to restrict the possible operations on a qdrant process.

It is important to mention that:

- The configuration is not persistent therefore it is necessary to lock again following a restart.
Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.
## Why Qdrant?
and once the queues are set up, there is no further syscall that would stop

any user thread.



Servers that use this can easily get to over 100k concurrent requests. Today

Linux allows asynchronous IO via io\_uring for network, disk and accessing

other ports, e.g. for printing or recording video.



## And what about Qdrant?



Qdrant can store everything in memory, but not all data sets may fit, which can

require storing  on disk. Before io\_uring, Qdrant used mmap to do its IO. This
],

    shard_key: ""user_1"",

});

```



```rust



use qdrant_client::qdrant::{PointStruct, WriteOrdering, WriteOrderingType};



client

    .upsert_points_blocking(

        ""{collection_name}"",

        Some(vec![shard_key::Key::String(""user_1"".into())]),

        vec![

            PointStruct::new(

                1111,

                vec![0.1, 0.2, 0.3],

                Default::default(),

            ),

        ],

        None,

    )

    .await?;

```



```java",0.696969696969697,0.42201834862385323,0.5257142857142858,0.3230769230769231,0.19444444444444445,0.2427745664739884,0.3939393939393939,0.23853211009174313,0.29714285714285715,0.6060606060606061,0.3669724770642202,0.4571428571428572,0.9247552752494812,0.7252912521362305,0.6474488377571106,0.684162974357605,0.996935868965617,0.8290516202905565,77,0.6111111111111112,0.39881357722817595
True,,1820.078,b13195c1,What is the significance of the `on_disk_payload` setting?,"The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes.","The `on_disk_payload` setting in the context provided is significant as it determines where the payload data should be stored. When set to `true`, the payload data will be stored on disk only. This setting can be useful for limiting RAM usage, especially in cases where the payload data is large. Storing payload data on disk can help reduce the amount of data that needs to be kept in RAM, which can be beneficial for systems with limited memory resources.","* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.

* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
InMemory payload storage is organized in the same way as in-memory vectors.

The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.

This type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.



In the case of large payload values, it might be better to use OnDisk payload storage.
""on_disk"": true

    }

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),

    hnsw_config=models.HnswConfigDiff(on_disk=True),

)

```



```typescript
""on_disk"": true

        }

    }

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

  -H 'Content-Type: application/json' \

  --data-raw '{

    ""vectors"": {

        ""my_vector"": { 

           ""on_disk"": true 

      }

    }

  }'

```



In the following example the HNSW index and quantization parameters are updated,

both for the whole collection, and for `my_vector` specifically:





```http

PATCH /collections/{collection_name}

{","['* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.\n\n* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\n\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', 'InMemory payload storage is organized in the same way as in-memory vectors.\n\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\n\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\n\n\n\nIn the case of large payload values, it might be better to use OnDisk payload storage.', '""on_disk"": true\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),\n\n    hnsw_config=models.HnswConfigDiff(on_disk=True),\n\n)\n\n```\n\n\n\n```typescript', '""on_disk"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""vectors"": {\n\n        ""my_vector"": { \n\n           ""on_disk"": true \n\n      }\n\n    }\n\n  }\'\n\n```\n\n\n\nIn the following example the HNSW index and quantization parameters are updated,\n\nboth for the whole collection, and for `my_vector` specifically:\n\n\n\n\n\n```http\n\nPATCH /collections/{collection_name}\n\n{']","Question: What is the significance of the `on_disk_payload` setting?\n\nContext: * `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.

* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.
""on_disk_payload"": false

},

""hnsw_config"": {

""m"": 16,

""ef_construct"": 100,

""full_scan_threshold"": 10000,

""max_indexing_threads"": 0

},

""optimizer_config"": {

""deleted_threshold"": 0.2,

""vacuum_min_vector_number"": 1000,

""default_segment_number"": 0,

""max_segment_size"": null,
InMemory payload storage is organized in the same way as in-memory vectors.

The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.

This type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.



In the case of large payload values, it might be better to use OnDisk payload storage.
""on_disk"": true

}

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),

hnsw_config=models.HnswConfigDiff(on_disk=True),

)

```



```typescript
""on_disk"": true

}

}

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

-H 'Content-Type: application/json' \

--data-raw '{

""vectors"": {

""my_vector"": { 

""on_disk"": true 

}

}

}'

```



In the following example the HNSW index and quantization parameters are updated,

both for the whole collection, and for `my_vector` specifically:





```http

PATCH /collections/{collection_name}

{\n\nAnswer:",What is the significance of the `on_disk_payload` setting?,"The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes.","['* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.\n\n* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\n\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', 'InMemory payload storage is organized in the same way as in-memory vectors.\n\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\n\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\n\n\n\nIn the case of large payload values, it might be better to use OnDisk payload storage.', '""on_disk"": true\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),\n\n    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),\n\n    hnsw_config=models.HnswConfigDiff(on_disk=True),\n\n)\n\n```\n\n\n\n```typescript', '""on_disk"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""vectors"": {\n\n        ""my_vector"": { \n\n           ""on_disk"": true \n\n      }\n\n    }\n\n  }\'\n\n```\n\n\n\nIn the following example the HNSW index and quantization parameters are updated,\n\nboth for the whole collection, and for `my_vector` specifically:\n\n\n\n\n\n```http\n\nPATCH /collections/{collection_name}\n\n{']","* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.

* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
InMemory payload storage is organized in the same way as in-memory vectors.

The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.

This type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.



In the case of large payload values, it might be better to use OnDisk payload storage.
""on_disk"": true

    }

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),

    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),

    hnsw_config=models.HnswConfigDiff(on_disk=True),

)

```



```typescript
""on_disk"": true

        }

    }

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

  -H 'Content-Type: application/json' \

  --data-raw '{

    ""vectors"": {

        ""my_vector"": { 

           ""on_disk"": true 

      }

    }

  }'

```



In the following example the HNSW index and quantization parameters are updated,

both for the whole collection, and for `my_vector` specifically:





```http

PATCH /collections/{collection_name}

{",0.5333333333333333,0.425531914893617,0.47337278106508873,0.20270270270270271,0.16129032258064516,0.17964071856287425,0.3333333333333333,0.26595744680851063,0.2958579881656805,0.5066666666666667,0.40425531914893614,0.44970414201183434,0.8328326940536499,0.6645848155021667,0.6369423866271973,0.6504700183868408,0.9508934094919823,0.5356374876573682,79,0.797979797979798,0.5952244512164326
True,,4024.343,ca924ddb,What is the significance of the parameter `max_segment_size_kb` in the context of vector indexation?,"The `max_segment_size_kb` parameter in the context of vector indexation determines the maximum size, in kilobytes, that a segment can have. Segments larger than this specified size might lead to disproportionately long indexation times. Therefore, it is essential to limit the size of segments to optimize the indexation process. The choice of this parameter should be based on the priority between indexation speed and search speed. If indexation speed is more critical, the parameter should be set lower. Conversely, if search speed is more important, the parameter should be set higher. It is important to note that 1Kb is equivalent to 1 vector of size 256. If the `max_segment_size_kb` parameter is not explicitly set, it will be automatically selected based on the number of available CPUs.","In the context of vector indexation, the parameter `max_segment_size_kb` plays a significant role in determining the maximum size of vectors to store in-memory per segment. Segments larger than this threshold will be stored as read-only memmaped files. 

The size specified in `max_segment_size_kb` is important for optimizing search speed and memory efficiency. By setting an appropriate value for `max_segment_size_kb`, you can control how vectors are stored and accessed during the indexing process. 

Increasing the `max_segment_size_kb` parameter can improve search speed by allowing more vectors to be stored in memory, but it may also increase memory usage. On the other hand, setting a lower threshold may lead to more vectors being stored as memmaped files, which can help manage memory usage but may impact search speed.

Therefore, the `max_segment_size_kb` parameter allows you to balance between search speed and memory usage based on your specific requirements and the available resources.","# If search speed is more important - make this parameter higher.

    # Note: 1Kb = 1 vector of size 256

    # If not set, will be automatically selected considering the number of available CPUs.

    max_segment_size_kb: null



    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.

    # Segments larger than this threshold will be stored as read-only memmaped file.

    # To enable memmap storage, lower the threshold

    # Note: 1Kb = 1 vector of size 256
# Segments larger than this threshold will be stored as read-only memmaped file.

    # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value.

    # To disable memmap storage, set this to `0`.

    # Note: 1Kb = 1 vector of size 256

    memmap_threshold_kb: 200000



    # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing
""max_segment_size"": null,

                ""memmap_threshold"": null,

                ""indexing_threshold"": 20000,

                ""flush_interval_sec"": 5,

                ""max_optimization_threads"": 1

            },

            ""wal_config"": {

                ""wal_capacity_mb"": 32,

                ""wal_segments_ahead"": 0

            }

        },

        ""payload_schema"": {}

    },

    ""status"": ""ok"",

    ""time"": 0.00010143

}

```



</details>

<br/>







```csharp
The following command enables indexing for segments that have more than 10000 kB of vectors stored:





```http

PATCH /collections/{collection_name}

{

    ""optimizers_config"": {

        ""indexing_threshold"": 10000

    }

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

  -H 'Content-Type: application/json' \

  --data-raw '{

    ""optimizers_config"": {

        ""indexing_threshold"": 10000

    }

  }'

```



```python

client.update_collection(
We're changing the `default_segment_number` to 5. Segment numbers influence the number of graph nodes in the underlying HNSW index, thereby indirectly influencing the memory efficiency. 



#### Next, we upload our vectors to this and then enable indexing: 



```python

batch_size = 10000

client.upload_collection(

    collection_name=collection_name,

    ids=range(len(dataset)),

    vectors=dataset[""openai""],

    payload=[

        {""text"": x} for x in dataset[""text""]

    ],

    parallel=10,

)","['# If search speed is more important - make this parameter higher.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    # If not set, will be automatically selected considering the number of available CPUs.\n\n    max_segment_size_kb: null\n\n\n\n    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.\n\n    # Segments larger than this threshold will be stored as read-only memmaped file.\n\n    # To enable memmap storage, lower the threshold\n\n    # Note: 1Kb = 1 vector of size 256', '# Segments larger than this threshold will be stored as read-only memmaped file.\n\n    # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value.\n\n    # To disable memmap storage, set this to `0`.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    memmap_threshold_kb: 200000\n\n\n\n    # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing', '""max_segment_size"": null,\n\n                ""memmap_threshold"": null,\n\n                ""indexing_threshold"": 20000,\n\n                ""flush_interval_sec"": 5,\n\n                ""max_optimization_threads"": 1\n\n            },\n\n            ""wal_config"": {\n\n                ""wal_capacity_mb"": 32,\n\n                ""wal_segments_ahead"": 0\n\n            }\n\n        },\n\n        ""payload_schema"": {}\n\n    },\n\n    ""status"": ""ok"",\n\n    ""time"": 0.00010143\n\n}\n\n```\n\n\n\n</details>\n\n<br/>\n\n\n\n\n\n\n\n```csharp', 'The following command enables indexing for segments that have more than 10000 kB of vectors stored:\n\n\n\n\n\n```http\n\nPATCH /collections/{collection_name}\n\n{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 10000\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 10000\n\n    }\n\n  }\'\n\n```\n\n\n\n```python\n\nclient.update_collection(', 'We\'re changing the `default_segment_number` to 5. Segment numbers influence the number of graph nodes in the underlying HNSW index, thereby indirectly influencing the memory efficiency. \n\n\n\n#### Next, we upload our vectors to this and then enable indexing: \n\n\n\n```python\n\nbatch_size = 10000\n\nclient.upload_collection(\n\n    collection_name=collection_name,\n\n    ids=range(len(dataset)),\n\n    vectors=dataset[""openai""],\n\n    payload=[\n\n        {""text"": x} for x in dataset[""text""]\n\n    ],\n\n    parallel=10,\n\n)']","Question: What is the significance of the parameter `max_segment_size_kb` in the context of vector indexation?\n\nContext: # If search speed is more important - make this parameter higher.

# Note: 1Kb = 1 vector of size 256

# If not set, will be automatically selected considering the number of available CPUs.

max_segment_size_kb: null



# Maximum size (in KiloBytes) of vectors to store in-memory per segment.

# Segments larger than this threshold will be stored as read-only memmaped file.

# To enable memmap storage, lower the threshold

# Note: 1Kb = 1 vector of size 256
# Segments larger than this threshold will be stored as read-only memmaped file.

# Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value.

# To disable memmap storage, set this to `0`.

# Note: 1Kb = 1 vector of size 256

memmap_threshold_kb: 200000



# Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing
""max_segment_size"": null,

""memmap_threshold"": null,

""indexing_threshold"": 20000,

""flush_interval_sec"": 5,

""max_optimization_threads"": 1

},

""wal_config"": {

""wal_capacity_mb"": 32,

""wal_segments_ahead"": 0

}

},

""payload_schema"": {}

},

""status"": ""ok"",

""time"": 0.00010143

}

```



</details>

<br/>







```csharp
The following command enables indexing for segments that have more than 10000 kB of vectors stored:





```http

PATCH /collections/{collection_name}

{

""optimizers_config"": {

""indexing_threshold"": 10000

}

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

-H 'Content-Type: application/json' \

--data-raw '{

""optimizers_config"": {

""indexing_threshold"": 10000

}

}'

```



```python

client.update_collection(
We're changing the `default_segment_number` to 5. Segment numbers influence the number of graph nodes in the underlying HNSW index, thereby indirectly influencing the memory efficiency. 



#### Next, we upload our vectors to this and then enable indexing: 



```python

batch_size = 10000

client.upload_collection(

collection_name=collection_name,

ids=range(len(dataset)),

vectors=dataset[""openai""],

payload=[

{""text"": x} for x in dataset[""text""]

],

parallel=10,

)\n\nAnswer:",What is the significance of the parameter `max_segment_size_kb` in the context of vector indexation?,"The `max_segment_size_kb` parameter in the context of vector indexation determines the maximum size, in kilobytes, that a segment can have. Segments larger than this specified size might lead to disproportionately long indexation times. Therefore, it is essential to limit the size of segments to optimize the indexation process. The choice of this parameter should be based on the priority between indexation speed and search speed. If indexation speed is more critical, the parameter should be set lower. Conversely, if search speed is more important, the parameter should be set higher. It is important to note that 1Kb is equivalent to 1 vector of size 256. If the `max_segment_size_kb` parameter is not explicitly set, it will be automatically selected based on the number of available CPUs.","['# If search speed is more important - make this parameter higher.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    # If not set, will be automatically selected considering the number of available CPUs.\n\n    max_segment_size_kb: null\n\n\n\n    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.\n\n    # Segments larger than this threshold will be stored as read-only memmaped file.\n\n    # To enable memmap storage, lower the threshold\n\n    # Note: 1Kb = 1 vector of size 256', '# Segments larger than this threshold will be stored as read-only memmaped file.\n\n    # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value.\n\n    # To disable memmap storage, set this to `0`.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    memmap_threshold_kb: 200000\n\n\n\n    # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing', '""max_segment_size"": null,\n\n                ""memmap_threshold"": null,\n\n                ""indexing_threshold"": 20000,\n\n                ""flush_interval_sec"": 5,\n\n                ""max_optimization_threads"": 1\n\n            },\n\n            ""wal_config"": {\n\n                ""wal_capacity_mb"": 32,\n\n                ""wal_segments_ahead"": 0\n\n            }\n\n        },\n\n        ""payload_schema"": {}\n\n    },\n\n    ""status"": ""ok"",\n\n    ""time"": 0.00010143\n\n}\n\n```\n\n\n\n</details>\n\n<br/>\n\n\n\n\n\n\n\n```csharp', 'The following command enables indexing for segments that have more than 10000 kB of vectors stored:\n\n\n\n\n\n```http\n\nPATCH /collections/{collection_name}\n\n{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 10000\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 10000\n\n    }\n\n  }\'\n\n```\n\n\n\n```python\n\nclient.update_collection(', 'We\'re changing the `default_segment_number` to 5. Segment numbers influence the number of graph nodes in the underlying HNSW index, thereby indirectly influencing the memory efficiency. \n\n\n\n#### Next, we upload our vectors to this and then enable indexing: \n\n\n\n```python\n\nbatch_size = 10000\n\nclient.upload_collection(\n\n    collection_name=collection_name,\n\n    ids=range(len(dataset)),\n\n    vectors=dataset[""openai""],\n\n    payload=[\n\n        {""text"": x} for x in dataset[""text""]\n\n    ],\n\n    parallel=10,\n\n)']","# If search speed is more important - make this parameter higher.

    # Note: 1Kb = 1 vector of size 256

    # If not set, will be automatically selected considering the number of available CPUs.

    max_segment_size_kb: null



    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.

    # Segments larger than this threshold will be stored as read-only memmaped file.

    # To enable memmap storage, lower the threshold

    # Note: 1Kb = 1 vector of size 256
# Segments larger than this threshold will be stored as read-only memmaped file.

    # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value.

    # To disable memmap storage, set this to `0`.

    # Note: 1Kb = 1 vector of size 256

    memmap_threshold_kb: 200000



    # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing
""max_segment_size"": null,

                ""memmap_threshold"": null,

                ""indexing_threshold"": 20000,

                ""flush_interval_sec"": 5,

                ""max_optimization_threads"": 1

            },

            ""wal_config"": {

                ""wal_capacity_mb"": 32,

                ""wal_segments_ahead"": 0

            }

        },

        ""payload_schema"": {}

    },

    ""status"": ""ok"",

    ""time"": 0.00010143

}

```



</details>

<br/>







```csharp
The following command enables indexing for segments that have more than 10000 kB of vectors stored:





```http

PATCH /collections/{collection_name}

{

    ""optimizers_config"": {

        ""indexing_threshold"": 10000

    }

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

  -H 'Content-Type: application/json' \

  --data-raw '{

    ""optimizers_config"": {

        ""indexing_threshold"": 10000

    }

  }'

```



```python

client.update_collection(
We're changing the `default_segment_number` to 5. Segment numbers influence the number of graph nodes in the underlying HNSW index, thereby indirectly influencing the memory efficiency. 



#### Next, we upload our vectors to this and then enable indexing: 



```python

batch_size = 10000

client.upload_collection(

    collection_name=collection_name,

    ids=range(len(dataset)),

    vectors=dataset[""openai""],

    payload=[

        {""text"": x} for x in dataset[""text""]

    ],

    parallel=10,

)",0.4342105263157895,0.5546218487394958,0.4870848708487085,0.17880794701986755,0.2288135593220339,0.20074349442379183,0.24342105263157895,0.31092436974789917,0.27306273062730624,0.39473684210526316,0.5042016806722689,0.4428044280442805,0.8833258152008057,0.6182185411453247,0.6186917424201965,0.6184549927711487,0.6454419067927769,0.7215317359992436,147,1.176,0.29330644538887424
True,,1958.233,740dd135,What is the significance of the `indexing_threshold_kb` parameter?,"The `indexing_threshold_kb` parameter specifies the maximum size, in Kilobytes, allowed for vectors for plain index. The default value for this parameter is set to 20000 KB. It is important to note that 1 KB is equivalent to 1 vector of size 256. This parameter plays a crucial role in determining the maximum size of vectors that can be used for indexing. If the `indexing_threshold_kb` parameter is not explicitly set, the default value will be used. Additionally, setting this parameter to `0` will explicitly disable vector indexing. This parameter directly impacts the efficiency and performance of the indexing process for the given vectors.","The `indexing_threshold_kb` parameter is significant in determining the size threshold at which vector indexing is enabled or disabled. In the provided configuration examples, setting `indexing_threshold_kb` to `20000` means that indexing will be enabled for segments that have more than 20000 kB of vectors stored. On the other hand, setting `indexing_threshold` to `0` in the optimizer configurations as shown in the examples disables indexing to zero, allowing faster uploads of vectors and payloads without indexing. This flexibility in adjusting the indexing threshold helps in optimizing the performance of the indexing process based on the size of the data being handled.","# To explicitly disable vector indexing, set to `0`.

    # If not set, the default value will be used.

    indexing_threshold_kb: 20000



    # Interval between forced flushes.

    flush_interval_sec: 5



    # Max number of threads, which can be used for optimization per collection.

    # Note: Each optimization thread will also use `max_indexing_threads` for index building.

    # So total number of threads used for optimization will be `max_optimization_threads * max_indexing_threads`
Using `memmap_threshold_kb` is not recommended in this case. It would require

the [optimizer](../../concepts/optimizer/) to constantly

transform in-memory segments into memmap segments on disk. This process is

slower, and the optimizer can be a bottleneck when ingesting a large amount of

data.



Read more about this in

[Configuring Memmap Storage](../../concepts/storage/#configuring-memmap-storage).



## Parallel upload into multiple shards
# Default value is 20,000, based on <https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md>.

    # To disable vector indexing, set to `0`.

    # Note: 1kB = 1 vector of size 256.

    indexing_threshold_kb: 20000

```



In addition to the configuration file, you can also set optimizer parameters separately for each [collection](../collections).
The following command enables indexing for segments that have more than 10000 kB of vectors stored:





```http

PATCH /collections/{collection_name}

{

    ""optimizers_config"": {

        ""indexing_threshold"": 10000

    }

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

  -H 'Content-Type: application/json' \

  --data-raw '{

    ""optimizers_config"": {

        ""indexing_threshold"": 10000

    }

  }'

```



```python

client.update_collection(
default_segment_number=5,

        indexing_threshold=0,

    ),

    quantization_config=models.BinaryQuantization(

        binary=models.BinaryQuantizationConfig(always_ram=True),

    ),

)

```



#### What is happening in the OptimizerConfig? 



We're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded","['# To explicitly disable vector indexing, set to `0`.\n\n    # If not set, the default value will be used.\n\n    indexing_threshold_kb: 20000\n\n\n\n    # Interval between forced flushes.\n\n    flush_interval_sec: 5\n\n\n\n    # Max number of threads, which can be used for optimization per collection.\n\n    # Note: Each optimization thread will also use `max_indexing_threads` for index building.\n\n    # So total number of threads used for optimization will be `max_optimization_threads * max_indexing_threads`', 'Using `memmap_threshold_kb` is not recommended in this case. It would require\n\nthe [optimizer](../../concepts/optimizer/) to constantly\n\ntransform in-memory segments into memmap segments on disk. This process is\n\nslower, and the optimizer can be a bottleneck when ingesting a large amount of\n\ndata.\n\n\n\nRead more about this in\n\n[Configuring Memmap Storage](../../concepts/storage/#configuring-memmap-storage).\n\n\n\n## Parallel upload into multiple shards', '# Default value is 20,000, based on <https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md>.\n\n    # To disable vector indexing, set to `0`.\n\n    # Note: 1kB = 1 vector of size 256.\n\n    indexing_threshold_kb: 20000\n\n```\n\n\n\nIn addition to the configuration file, you can also set optimizer parameters separately for each [collection](../collections).', 'The following command enables indexing for segments that have more than 10000 kB of vectors stored:\n\n\n\n\n\n```http\n\nPATCH /collections/{collection_name}\n\n{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 10000\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 10000\n\n    }\n\n  }\'\n\n```\n\n\n\n```python\n\nclient.update_collection(', ""default_segment_number=5,\n\n        indexing_threshold=0,\n\n    ),\n\n    quantization_config=models.BinaryQuantization(\n\n        binary=models.BinaryQuantizationConfig(always_ram=True),\n\n    ),\n\n)\n\n```\n\n\n\n#### What is happening in the OptimizerConfig? \n\n\n\nWe're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded""]","Question: What is the significance of the `indexing_threshold_kb` parameter?\n\nContext: # To explicitly disable vector indexing, set to `0`.

# If not set, the default value will be used.

indexing_threshold_kb: 20000



# Interval between forced flushes.

flush_interval_sec: 5



# Max number of threads, which can be used for optimization per collection.

# Note: Each optimization thread will also use `max_indexing_threads` for index building.

# So total number of threads used for optimization will be `max_optimization_threads * max_indexing_threads`
Using `memmap_threshold_kb` is not recommended in this case. It would require

the [optimizer](../../concepts/optimizer/) to constantly

transform in-memory segments into memmap segments on disk. This process is

slower, and the optimizer can be a bottleneck when ingesting a large amount of

data.



Read more about this in

[Configuring Memmap Storage](../../concepts/storage/#configuring-memmap-storage).



## Parallel upload into multiple shards
# Default value is 20,000, based on <https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md>.

# To disable vector indexing, set to `0`.

# Note: 1kB = 1 vector of size 256.

indexing_threshold_kb: 20000

```



In addition to the configuration file, you can also set optimizer parameters separately for each [collection](../collections).
The following command enables indexing for segments that have more than 10000 kB of vectors stored:





```http

PATCH /collections/{collection_name}

{

""optimizers_config"": {

""indexing_threshold"": 10000

}

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

-H 'Content-Type: application/json' \

--data-raw '{

""optimizers_config"": {

""indexing_threshold"": 10000

}

}'

```



```python

client.update_collection(
default_segment_number=5,

indexing_threshold=0,

),

quantization_config=models.BinaryQuantization(

binary=models.BinaryQuantizationConfig(always_ram=True),

),

)

```



#### What is happening in the OptimizerConfig? 



We're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded\n\nAnswer:",What is the significance of the `indexing_threshold_kb` parameter?,"The `indexing_threshold_kb` parameter specifies the maximum size, in Kilobytes, allowed for vectors for plain index. The default value for this parameter is set to 20000 KB. It is important to note that 1 KB is equivalent to 1 vector of size 256. This parameter plays a crucial role in determining the maximum size of vectors that can be used for indexing. If the `indexing_threshold_kb` parameter is not explicitly set, the default value will be used. Additionally, setting this parameter to `0` will explicitly disable vector indexing. This parameter directly impacts the efficiency and performance of the indexing process for the given vectors.","['# To explicitly disable vector indexing, set to `0`.\n\n    # If not set, the default value will be used.\n\n    indexing_threshold_kb: 20000\n\n\n\n    # Interval between forced flushes.\n\n    flush_interval_sec: 5\n\n\n\n    # Max number of threads, which can be used for optimization per collection.\n\n    # Note: Each optimization thread will also use `max_indexing_threads` for index building.\n\n    # So total number of threads used for optimization will be `max_optimization_threads * max_indexing_threads`', 'Using `memmap_threshold_kb` is not recommended in this case. It would require\n\nthe [optimizer](../../concepts/optimizer/) to constantly\n\ntransform in-memory segments into memmap segments on disk. This process is\n\nslower, and the optimizer can be a bottleneck when ingesting a large amount of\n\ndata.\n\n\n\nRead more about this in\n\n[Configuring Memmap Storage](../../concepts/storage/#configuring-memmap-storage).\n\n\n\n## Parallel upload into multiple shards', '# Default value is 20,000, based on <https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md>.\n\n    # To disable vector indexing, set to `0`.\n\n    # Note: 1kB = 1 vector of size 256.\n\n    indexing_threshold_kb: 20000\n\n```\n\n\n\nIn addition to the configuration file, you can also set optimizer parameters separately for each [collection](../collections).', 'The following command enables indexing for segments that have more than 10000 kB of vectors stored:\n\n\n\n\n\n```http\n\nPATCH /collections/{collection_name}\n\n{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 10000\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 10000\n\n    }\n\n  }\'\n\n```\n\n\n\n```python\n\nclient.update_collection(', ""default_segment_number=5,\n\n        indexing_threshold=0,\n\n    ),\n\n    quantization_config=models.BinaryQuantization(\n\n        binary=models.BinaryQuantizationConfig(always_ram=True),\n\n    ),\n\n)\n\n```\n\n\n\n#### What is happening in the OptimizerConfig? \n\n\n\nWe're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded""]","# To explicitly disable vector indexing, set to `0`.

    # If not set, the default value will be used.

    indexing_threshold_kb: 20000



    # Interval between forced flushes.

    flush_interval_sec: 5



    # Max number of threads, which can be used for optimization per collection.

    # Note: Each optimization thread will also use `max_indexing_threads` for index building.

    # So total number of threads used for optimization will be `max_optimization_threads * max_indexing_threads`
Using `memmap_threshold_kb` is not recommended in this case. It would require

the [optimizer](../../concepts/optimizer/) to constantly

transform in-memory segments into memmap segments on disk. This process is

slower, and the optimizer can be a bottleneck when ingesting a large amount of

data.



Read more about this in

[Configuring Memmap Storage](../../concepts/storage/#configuring-memmap-storage).



## Parallel upload into multiple shards
# Default value is 20,000, based on <https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md>.

    # To disable vector indexing, set to `0`.

    # Note: 1kB = 1 vector of size 256.

    indexing_threshold_kb: 20000

```



In addition to the configuration file, you can also set optimizer parameters separately for each [collection](../collections).
The following command enables indexing for segments that have more than 10000 kB of vectors stored:





```http

PATCH /collections/{collection_name}

{

    ""optimizers_config"": {

        ""indexing_threshold"": 10000

    }

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

  -H 'Content-Type: application/json' \

  --data-raw '{

    ""optimizers_config"": {

        ""indexing_threshold"": 10000

    }

  }'

```



```python

client.update_collection(
default_segment_number=5,

        indexing_threshold=0,

    ),

    quantization_config=models.BinaryQuantization(

        binary=models.BinaryQuantizationConfig(always_ram=True),

    ),

)

```



#### What is happening in the OptimizerConfig? 



We're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded",0.4731182795698925,0.4583333333333333,0.46560846560846564,0.1956521739130435,0.18947368421052632,0.1925133689839572,0.2903225806451613,0.28125,0.28571428571428575,0.45161290322580644,0.4375,0.4444444444444444,0.8890774250030518,0.6463710069656372,0.6414273381233215,0.6438896656036377,0.7354346197098494,0.8104456096577147,99,0.9705882352941176,0.3934078618339844
True,,4231.004000000001,bb4fa187,How can you optimize Qdrant for minimizing latency in search requests?,"To optimize Qdrant for minimizing latency in search requests, you can set up the system to use as many cores as possible for a single request. This can be achieved by setting the number of segments in the collection to be equal to the number of cores in the system. By doing this, each segment will be processed in parallel, leading to a faster final result. This approach allows for the efficient utilization of system resources and can significantly reduce the time taken from the moment a request is submitted to the moment a response is received. By optimizing for latency in this manner, you can enhance the overall speed and responsiveness of the search functionality in Qdrant.","To optimize Qdrant for minimizing latency in search requests, consider implementing the following strategies:

1. Utilize Multiple Cores: Set up Qdrant to use as many cores as possible for a single request by setting the number of segments in the collection to be equal to the number of cores in the system. This allows for parallel processing of each segment, resulting in faster retrieval of the final search results.

2. Disable Rescoring: In systems with high disk latency, the rescoring step may introduce bottlenecks. To improve search speed, consider disabling the `rescore` feature in the search parameters. This can be done by setting ""rescore"" to false in the search request.

3. Batch Search Feature: Take advantage of the new batch search feature in Qdrant, which allows handling multiple search requests efficiently with a single API call. This feature eliminates network overhead and enhances performance by processing multiple tasks simultaneously.

4. Built-in ARM Support: When performing searches, specify the shard or shards needed, and Qdrant will locate them without querying all machines in the cluster. This minimizes overhead and maximizes search performance by directing requests to the relevant nodes directly.

By implementing these strategies, you can optimize Qdrant for minimizing latency in search requests and improve the overall search performance of the system.","Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
In a system with high disk latency, the re-scoring step may become a bottleneck.



Consider disabling `rescore` to improve the search speed:



```http

POST /collections/{collection_name}/points/search

{

    ""params"": {

        ""quantization"": {

            ""rescore"": false

        }

    },

    ""vector"": [0.2, 0.1, 0.9, 0.7],

    ""limit"": 10

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.search(
Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows 

you to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way 

possible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead

or slow performance.



## Built-in ARM support
This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. 



### Common use cases
![sequential vs. concurrent flow](/articles_data/search-as-you-type/sayt_concurrency.png)



While this means more load for the Qdrant vector search engine, this is not the limiting factor. The relevant data is already in cache in many cases, so the overhead stays within acceptable bounds, and the maximum latency in case of prefix cache misses is measurably reduced.



The code is available on the [Qdrant github](https://github.com/qdrant/page-search)","['Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.\n\n\n\nTo prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{', 'In a system with high disk latency, the re-scoring step may become a bottleneck.\n\n\n\nConsider disabling `rescore` to improve the search speed:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{\n\n    ""params"": {\n\n        ""quantization"": {\n\n            ""rescore"": false\n\n        }\n\n    },\n\n    ""vector"": [0.2, 0.1, 0.9, 0.7],\n\n    ""limit"": 10\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.search(', 'Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows \n\nyou to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way \n\npossible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead\n\nor slow performance.\n\n\n\n## Built-in ARM support', 'This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. \n\n\n\n### Common use cases', '![sequential vs. concurrent flow](/articles_data/search-as-you-type/sayt_concurrency.png)\n\n\n\nWhile this means more load for the Qdrant vector search engine, this is not the limiting factor. The relevant data is already in cache in many cases, so the overhead stays within acceptable bounds, and the maximum latency in case of prefix cache misses is measurably reduced.\n\n\n\nThe code is available on the [Qdrant github](https://github.com/qdrant/page-search)']","Question: How can you optimize Qdrant for minimizing latency in search requests?\n\nContext: Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
In a system with high disk latency, the re-scoring step may become a bottleneck.



Consider disabling `rescore` to improve the search speed:



```http

POST /collections/{collection_name}/points/search

{

""params"": {

""quantization"": {

""rescore"": false

}

},

""vector"": [0.2, 0.1, 0.9, 0.7],

""limit"": 10

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.search(
Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows 

you to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way 

possible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead

or slow performance.



## Built-in ARM support
This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. 



### Common use cases
![sequential vs. concurrent flow](/articles_data/search-as-you-type/sayt_concurrency.png)



While this means more load for the Qdrant vector search engine, this is not the limiting factor. The relevant data is already in cache in many cases, so the overhead stays within acceptable bounds, and the maximum latency in case of prefix cache misses is measurably reduced.



The code is available on the [Qdrant github](https://github.com/qdrant/page-search)\n\nAnswer:",How can you optimize Qdrant for minimizing latency in search requests?,"To optimize Qdrant for minimizing latency in search requests, you can set up the system to use as many cores as possible for a single request. This can be achieved by setting the number of segments in the collection to be equal to the number of cores in the system. By doing this, each segment will be processed in parallel, leading to a faster final result. This approach allows for the efficient utilization of system resources and can significantly reduce the time taken from the moment a request is submitted to the moment a response is received. By optimizing for latency in this manner, you can enhance the overall speed and responsiveness of the search functionality in Qdrant.","['Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.\n\n\n\nTo prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{', 'In a system with high disk latency, the re-scoring step may become a bottleneck.\n\n\n\nConsider disabling `rescore` to improve the search speed:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{\n\n    ""params"": {\n\n        ""quantization"": {\n\n            ""rescore"": false\n\n        }\n\n    },\n\n    ""vector"": [0.2, 0.1, 0.9, 0.7],\n\n    ""limit"": 10\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.search(', 'Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows \n\nyou to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way \n\npossible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead\n\nor slow performance.\n\n\n\n## Built-in ARM support', 'This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. \n\n\n\n### Common use cases', '![sequential vs. concurrent flow](/articles_data/search-as-you-type/sayt_concurrency.png)\n\n\n\nWhile this means more load for the Qdrant vector search engine, this is not the limiting factor. The relevant data is already in cache in many cases, so the overhead stays within acceptable bounds, and the maximum latency in case of prefix cache misses is measurably reduced.\n\n\n\nThe code is available on the [Qdrant github](https://github.com/qdrant/page-search)']","Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
In a system with high disk latency, the re-scoring step may become a bottleneck.



Consider disabling `rescore` to improve the search speed:



```http

POST /collections/{collection_name}/points/search

{

    ""params"": {

        ""quantization"": {

            ""rescore"": false

        }

    },

    ""vector"": [0.2, 0.1, 0.9, 0.7],

    ""limit"": 10

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.search(
Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows 

you to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way 

possible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead

or slow performance.



## Built-in ARM support
This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. 



### Common use cases
![sequential vs. concurrent flow](/articles_data/search-as-you-type/sayt_concurrency.png)



While this means more load for the Qdrant vector search engine, this is not the limiting factor. The relevant data is already in cache in many cases, so the overhead stays within acceptable bounds, and the maximum latency in case of prefix cache misses is measurably reduced.



The code is available on the [Qdrant github](https://github.com/qdrant/page-search)",0.40512820512820513,0.7669902912621359,0.5302013422818792,0.211340206185567,0.4019607843137255,0.27702702702702703,0.2923076923076923,0.5533980582524272,0.3825503355704698,0.38974358974358975,0.7378640776699029,0.5100671140939598,0.9494694471359253,0.654338002204895,0.7394334673881531,0.6942880749702454,0.9815507195540704,0.9917367571033537,212,1.7966101694915255,0.5227393501536624
True,,3484.42,3b2532f3,How can Qdrant be configured to optimize throughput by utilizing multiple cores for parallel processing?,"To optimize throughput in Qdrant and utilize multiple cores for parallel processing, you can configure the system to use minimal number of segments, typically set to 2. By setting the default_segment_number to 2 in the optimizers_config section of the PUT request for creating a collection, you can instruct Qdrant to use a smaller number of segments. This approach benefits from the size of the index and reduces the overall number of vector comparisons needed to find the nearest neighbors. However, it is important to note that larger segments require more time to build the index. By adjusting this configuration parameter, you can strike a balance between maximizing throughput and optimizing the efficiency of vector search operations in Qdrant.","To optimize throughput by utilizing multiple cores for parallel processing in Qdrant, you can follow these steps:

1. Configure Qdrant to use a minimal number of segments, typically 2. This will allow Qdrant to leverage the benefits of using large segments, such as reducing the number of vector comparisons needed to find nearest neighbors. However, keep in mind that larger segments may take more time to build the index.

2. If you want to minimize latency for a single request, set the number of segments in the collection to be equal to the number of cores in the system. This will enable each segment to be processed in parallel, resulting in faster retrieval of the final result.

3. Consider using quantization techniques, such as scalar or product-based quantization, to further optimize resource usage and scale the infrastructure efficiently. Quantization helps in optimizing compute resources and can enhance the performance of Qdrant.

By implementing these configurations and optimizations, you can enhance Qdrant's throughput and efficiency by effectively utilizing multiple cores for parallel processing.",");

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,
Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  



## Common Use Cases:
led to some modest overhead in case of disk latency. The kernel may

stop a user thread trying to access a mapped region, which incurs some context

switching overhead plus the wait time until the disk IO is finished. Ultimately,

this works very well with the asynchronous nature of Qdrant's core.



One of the great optimizations Qdrant offers is quantization (either

[scalar](https://qdrant.tech/articles/scalar-quantization/) or 

[product](https://qdrant.tech/articles/product-quantization/)-based).
. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.","[');\n\n```\n\n\n\nTo prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.\n\nTo do that, you can configure qdrant to use minimal number of segments, which is usually 2.\n\nLarge segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,', 'Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.\n\n\n\nTo prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{', ""| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  \n\n\n\n## Common Use Cases:"", ""led to some modest overhead in case of disk latency. The kernel may\n\nstop a user thread trying to access a mapped region, which incurs some context\n\nswitching overhead plus the wait time until the disk IO is finished. Ultimately,\n\nthis works very well with the asynchronous nature of Qdrant's core.\n\n\n\nOne of the great optimizations Qdrant offers is quantization (either\n\n[scalar](https://qdrant.tech/articles/scalar-quantization/) or \n\n[product](https://qdrant.tech/articles/product-quantization/)-based)."", '. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.']","Question: How can Qdrant be configured to optimize throughput by utilizing multiple cores for parallel processing?\n\nContext: );

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,
Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  



## Common Use Cases:
led to some modest overhead in case of disk latency. The kernel may

stop a user thread trying to access a mapped region, which incurs some context

switching overhead plus the wait time until the disk IO is finished. Ultimately,

this works very well with the asynchronous nature of Qdrant's core.



One of the great optimizations Qdrant offers is quantization (either

[scalar](https://qdrant.tech/articles/scalar-quantization/) or 

[product](https://qdrant.tech/articles/product-quantization/)-based).
. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.\n\nAnswer:",How can Qdrant be configured to optimize throughput by utilizing multiple cores for parallel processing?,"To optimize throughput in Qdrant and utilize multiple cores for parallel processing, you can configure the system to use minimal number of segments, typically set to 2. By setting the default_segment_number to 2 in the optimizers_config section of the PUT request for creating a collection, you can instruct Qdrant to use a smaller number of segments. This approach benefits from the size of the index and reduces the overall number of vector comparisons needed to find the nearest neighbors. However, it is important to note that larger segments require more time to build the index. By adjusting this configuration parameter, you can strike a balance between maximizing throughput and optimizing the efficiency of vector search operations in Qdrant.","[');\n\n```\n\n\n\nTo prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.\n\nTo do that, you can configure qdrant to use minimal number of segments, which is usually 2.\n\nLarge segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,', 'Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.\n\n\n\nTo prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{', ""| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  \n\n\n\n## Common Use Cases:"", ""led to some modest overhead in case of disk latency. The kernel may\n\nstop a user thread trying to access a mapped region, which incurs some context\n\nswitching overhead plus the wait time until the disk IO is finished. Ultimately,\n\nthis works very well with the asynchronous nature of Qdrant's core.\n\n\n\nOne of the great optimizations Qdrant offers is quantization (either\n\n[scalar](https://qdrant.tech/articles/scalar-quantization/) or \n\n[product](https://qdrant.tech/articles/product-quantization/)-based)."", '. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.']",");

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,
Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  



## Common Use Cases:
led to some modest overhead in case of disk latency. The kernel may

stop a user thread trying to access a mapped region, which incurs some context

switching overhead plus the wait time until the disk IO is finished. Ultimately,

this works very well with the asynchronous nature of Qdrant's core.



One of the great optimizations Qdrant offers is quantization (either

[scalar](https://qdrant.tech/articles/scalar-quantization/) or 

[product](https://qdrant.tech/articles/product-quantization/)-based).
. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.",0.5,0.75,0.6,0.22981366459627328,0.34579439252336447,0.2761194029850746,0.30864197530864196,0.46296296296296297,0.37037037037037035,0.41975308641975306,0.6296296296296297,0.5037037037037038,0.8816419839859009,0.6565622091293335,0.6917786002159119,0.6737105250358582,0.9155559825303499,0.9715410006756429,173,1.4661016949152543,0.521455774253253
True,,3342.474,28f7b71c,How can one enable distributed deployment mode in Qdrant?,"To enable distributed deployment mode in Qdrant, one needs to enable the cluster mode in the configuration or set the ENV variable `QDRANT__CLUSTER__ENABLED=true`. This allows multiple Qdrant services to communicate with each other, distributing data across peers to extend storage capabilities and increase stability. In the configuration file, under the `cluster` section, set `enabled: true` to run Qdrant in distributed deployment mode. Additionally, configure the internal communication port between peers using the `p2p` section with the specified port number. Furthermore, adjust the parameters related to the distributed consensus algorithm as needed for optimal performance.","To enable distributed deployment mode in Qdrant, you can follow these steps:

1. Update the configuration file by setting `cluster.enabled` to `true`. Here is an example snippet from the configuration file:

```yaml
cluster:
  enabled: true
  p2p:
    port: 6335
  consensus:
    # Add your consensus configuration here if needed
```

2. Alternatively, you can set the environment variable `QDRANT__CLUSTER__ENABLED=true` to enable cluster mode.

By enabling distributed deployment mode, you allow multiple Qdrant services to communicate with each other, distributing data across peers to enhance storage capacity and improve stability. This mode is particularly beneficial for large enterprises dealing with extensive data volumes, ensuring performance scalability without compromising on capabilities. Additionally, Qdrant's distributed deployment mode operates effectively on bare metal infrastructure, providing users with control over data sovereignty and autonomous LLM regimes.

For more detailed information on custom sharding in Qdrant's distributed deployment mode, you can refer to the [user-defined sharding guide](https://qdrant.tech/documentation/guides/distributed_deployment/#user-defined-sharding).","To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.



```yaml

cluster:

  # Use `enabled: true` to run Qdrant in distributed deployment mode

  enabled: true

  # Configuration of the inter-cluster communication

  p2p:

    # Port for internal communication between peers

    port: 6335



  # Configuration related to distributed consensus algorithm

  consensus:
---

title: Distributed Deployment

weight: 100

aliases:

  - ../distributed_deployment

---



# Distributed deployment



Since version v0.8.0 Qdrant supports a distributed deployment mode.

In this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.
Qdrant’s distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model’s capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes
Qdrant's distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model's capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes
. This is called [custom sharding](https://qdrant.tech/documentation/guides/distributed_deployment/#user-defined-sharding).","['To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.\n\n\n\n```yaml\n\ncluster:\n\n  # Use `enabled: true` to run Qdrant in distributed deployment mode\n\n  enabled: true\n\n  # Configuration of the inter-cluster communication\n\n  p2p:\n\n    # Port for internal communication between peers\n\n    port: 6335\n\n\n\n  # Configuration related to distributed consensus algorithm\n\n  consensus:', '---\n\ntitle: Distributed Deployment\n\nweight: 100\n\naliases:\n\n  - ../distributed_deployment\n\n---\n\n\n\n# Distributed deployment\n\n\n\nSince version v0.8.0 Qdrant supports a distributed deployment mode.\n\nIn this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.', 'Qdrant’s distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model’s capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes', ""Qdrant's distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model's capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes"", '. This is called [custom sharding](https://qdrant.tech/documentation/guides/distributed_deployment/#user-defined-sharding).']","Question: How can one enable distributed deployment mode in Qdrant?\n\nContext: To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.



```yaml

cluster:

# Use `enabled: true` to run Qdrant in distributed deployment mode

enabled: true

# Configuration of the inter-cluster communication

p2p:

# Port for internal communication between peers

port: 6335



# Configuration related to distributed consensus algorithm

consensus:
---

title: Distributed Deployment

weight: 100

aliases:

- ../distributed_deployment

---



# Distributed deployment



Since version v0.8.0 Qdrant supports a distributed deployment mode.

In this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.
Qdrant’s distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model’s capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes
Qdrant's distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model's capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes
. This is called [custom sharding](https://qdrant.tech/documentation/guides/distributed_deployment/#user-defined-sharding).\n\nAnswer:",How can one enable distributed deployment mode in Qdrant?,"To enable distributed deployment mode in Qdrant, one needs to enable the cluster mode in the configuration or set the ENV variable `QDRANT__CLUSTER__ENABLED=true`. This allows multiple Qdrant services to communicate with each other, distributing data across peers to extend storage capabilities and increase stability. In the configuration file, under the `cluster` section, set `enabled: true` to run Qdrant in distributed deployment mode. Additionally, configure the internal communication port between peers using the `p2p` section with the specified port number. Furthermore, adjust the parameters related to the distributed consensus algorithm as needed for optimal performance.","['To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.\n\n\n\n```yaml\n\ncluster:\n\n  # Use `enabled: true` to run Qdrant in distributed deployment mode\n\n  enabled: true\n\n  # Configuration of the inter-cluster communication\n\n  p2p:\n\n    # Port for internal communication between peers\n\n    port: 6335\n\n\n\n  # Configuration related to distributed consensus algorithm\n\n  consensus:', '---\n\ntitle: Distributed Deployment\n\nweight: 100\n\naliases:\n\n  - ../distributed_deployment\n\n---\n\n\n\n# Distributed deployment\n\n\n\nSince version v0.8.0 Qdrant supports a distributed deployment mode.\n\nIn this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.', 'Qdrant’s distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model’s capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes', ""Qdrant's distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model's capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes"", '. This is called [custom sharding](https://qdrant.tech/documentation/guides/distributed_deployment/#user-defined-sharding).']","To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.



```yaml

cluster:

  # Use `enabled: true` to run Qdrant in distributed deployment mode

  enabled: true

  # Configuration of the inter-cluster communication

  p2p:

    # Port for internal communication between peers

    port: 6335



  # Configuration related to distributed consensus algorithm

  consensus:
---

title: Distributed Deployment

weight: 100

aliases:

  - ../distributed_deployment

---



# Distributed deployment



Since version v0.8.0 Qdrant supports a distributed deployment mode.

In this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.
Qdrant’s distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model’s capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes
Qdrant's distributed deployment mode plays a vital role in empowering large enterprises dealing with massive data volumes. It ensures that increasing data volumes do not hinder performance but rather enrich the model's capabilities, making scalability a seamless process. Moreover, Qdrant is well-suited for Pienso’s enterprise customers as it operates best on bare metal infrastructure, enabling them to maintain complete control over their data sovereignty and autonomous LLM regimes
. This is called [custom sharding](https://qdrant.tech/documentation/guides/distributed_deployment/#user-defined-sharding).",0.37735849056603776,0.6896551724137931,0.4878048780487806,0.1962025316455696,0.36046511627906974,0.2540983606557377,0.25157232704402516,0.45977011494252873,0.3252032520325203,0.3584905660377358,0.6551724137931034,0.46341463414634143,0.963408350944519,0.6652090549468994,0.7435584664344788,0.7022050619125366,0.9904298744901704,0.9014077169704251,152,1.6170212765957446,0.5897878719368745
True,,5386.079,08f729b2,What is the recommended approach for determining the number of shards when creating a collection using Qdrant?,"When creating a collection using Qdrant, it is recommended to set the number of shards to be a multiple of the number of nodes currently running in your cluster. For example, if you have 3 nodes in your cluster, setting the number of shards to 6 could be a good option. Shards are evenly distributed across all existing nodes when a collection is first created. It is important to note that Qdrant does not automatically rebalance shards if your cluster size or replication factor changes, as this operation can be expensive on large clusters. If scaling operations require moving shards, there are specific procedures outlined for this purpose, available as of version 0.9.0 of Qdrant.","When determining the number of shards for creating a collection using Qdrant, there are a few recommended approaches:

1. **Automatic Sharding**: If you do not specify the number of shards when creating a collection, Qdrant will automatically set the shard number to the total number of nodes in your cluster. This approach simplifies the process as it aligns with the cluster configuration.

2. **Custom Sharding**: You can manually set the number of shards based on your specific requirements. For example, if you want to parallelize the upload of a large dataset, creating multiple shards can help distribute the workload. A reasonable number of shards per machine is typically between 2 to 4.

3. **Sharding Method**: Qdrant also allows you to specify a custom sharding method when creating a collection. This can be useful if you have specific partitioning requirements or want to optimize the distribution of data across shards.

4. **Consistency and Performance**: Consider the trade-offs between consistency and performance when determining the number of shards. More shards can improve parallel processing and scalability but may also impact coordination overhead and resource utilization.

Overall, the recommended approach for determining the number of shards when creating a collection using Qdrant depends on your specific use case, data volume, and desired performance characteristics.","Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.



When you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 300,
.build())

            .setShardNumber(6)

            .build())

    .get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

	shardNumber: 6

);

```
.get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",

	// ... other collection parameters

	shardNumber: 1,

	shardingMethod: ShardingMethod.Custom

);

```
## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""shard_number"": 2

}

```



```python
import io.qdrant.client.grpc.Collections.ShardingMethod;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            // ... other collection parameters

            .setShardNumber(1)

            .setShardingMethod(ShardingMethod.Custom)

            .build())

    .get();

```



```csharp

using Qdrant.Client;","['Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.\n\n\n\nWhen you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 300,', '.build())\n\n            .setShardNumber(6)\n\n            .build())\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\n\tshardNumber: 6\n\n);\n\n```', '.get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\t// ... other collection parameters\n\n\tshardNumber: 1,\n\n\tshardingMethod: ShardingMethod.Custom\n\n);\n\n```', '## Parallel upload into multiple shards\n\n\n\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\n\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python', 'import io.qdrant.client.grpc.Collections.ShardingMethod;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            // ... other collection parameters\n\n            .setShardNumber(1)\n\n            .setShardingMethod(ShardingMethod.Custom)\n\n            .build())\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;']","Question: What is the recommended approach for determining the number of shards when creating a collection using Qdrant?\n\nContext: Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.



When you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 300,
.build())

.setShardNumber(6)

.build())

.get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

collectionName: ""{collection_name}"",

vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

shardNumber: 6

);

```
.get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

collectionName: ""{collection_name}"",

// ... other collection parameters

shardNumber: 1,

shardingMethod: ShardingMethod.Custom

);

```
## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""shard_number"": 2

}

```



```python
import io.qdrant.client.grpc.Collections.ShardingMethod;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createCollectionAsync(

CreateCollection.newBuilder()

.setCollectionName(""{collection_name}"")

// ... other collection parameters

.setShardNumber(1)

.setShardingMethod(ShardingMethod.Custom)

.build())

.get();

```



```csharp

using Qdrant.Client;\n\nAnswer:",What is the recommended approach for determining the number of shards when creating a collection using Qdrant?,"When creating a collection using Qdrant, it is recommended to set the number of shards to be a multiple of the number of nodes currently running in your cluster. For example, if you have 3 nodes in your cluster, setting the number of shards to 6 could be a good option. Shards are evenly distributed across all existing nodes when a collection is first created. It is important to note that Qdrant does not automatically rebalance shards if your cluster size or replication factor changes, as this operation can be expensive on large clusters. If scaling operations require moving shards, there are specific procedures outlined for this purpose, available as of version 0.9.0 of Qdrant.","['Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.\n\n\n\nWhen you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 300,', '.build())\n\n            .setShardNumber(6)\n\n            .build())\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\n\tshardNumber: 6\n\n);\n\n```', '.get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\t// ... other collection parameters\n\n\tshardNumber: 1,\n\n\tshardingMethod: ShardingMethod.Custom\n\n);\n\n```', '## Parallel upload into multiple shards\n\n\n\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\n\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python', 'import io.qdrant.client.grpc.Collections.ShardingMethod;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            // ... other collection parameters\n\n            .setShardNumber(1)\n\n            .setShardingMethod(ShardingMethod.Custom)\n\n            .build())\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;']","Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.



When you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 300,
.build())

            .setShardNumber(6)

            .build())

    .get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

	shardNumber: 6

);

```
.get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",

	// ... other collection parameters

	shardNumber: 1,

	shardingMethod: ShardingMethod.Custom

);

```
## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""shard_number"": 2

}

```



```python
import io.qdrant.client.grpc.Collections.ShardingMethod;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            // ... other collection parameters

            .setShardNumber(1)

            .setShardingMethod(ShardingMethod.Custom)

            .build())

    .get();

```



```csharp

using Qdrant.Client;",0.3473684210526316,0.6,0.44,0.1111111111111111,0.1926605504587156,0.14093959731543623,0.19473684210526315,0.33636363636363636,0.24666666666666665,0.30526315789473685,0.5272727272727272,0.38666666666666666,0.880725622177124,0.5823835730552673,0.6320841312408447,0.6062168478965759,0.6474977783758992,0.38581361472606657,211,1.8347826086956522,0.32278131693675416
True,,4274.002,413e559e,How is the discovery score computed in the context provided?,"In the context provided, the discovery score is computed using the formula: 

Discovery Score = sigmoid(s(v_t)) + Σ rank(v_i^+, v_i^-)

Here, s(v_t) represents the similarity function of the target vector, and v_i^+ and v_i^- represent the positive and negative examples, respectively. The sigmoid function is utilized to normalize the score within the range of 0 to 1. The sum of ranks is used to penalize vectors that are closer to negative examples than to positive ones. This means that the sum of individual ranks determines how many positive zones a point is in, while the closeness hierarchy serves as a secondary factor in the computation of the discovery score.","In the context provided, the discovery score is computed as follows:

1. For each pair of positive-negative examples $(v_i^+, v_i^-)$, the rank is calculated based on the similarity scores of the positive and negative examples. If the similarity score of the positive example is greater than or equal to the similarity score of the negative example, the rank is 1. Otherwise, the rank is -1.

2. The discovery score is then calculated by taking the sigmoid of the similarity score of the target vector $v_t$ and adding it to the sum of ranks for all positive-negative pairs in the context.

So, the discovery score formula is:

\[
\text{discovery score} = \text{sigmoid}(s(v_t)) + \sum \text{rank}(v_i^+, v_i^-)
\]

This formula combines the similarity of the target vector with the ranks of positive-negative pairs to generate the final discovery score.","$$

\text{rank}(v^+, v^-) = \begin{cases}

    1, &\quad s(v^+) \geq s(v^-) \\\\

    -1, &\quad s(v^+) < s(v^-)

\end{cases}

$$

where $v^+$ represents a positive example, $v^-$ represents a negative example, and $s(v)$ is the similarity score of a vector $v$ to the target vector. The discovery score is then computed as:

$$

 \text{discovery score} = \text{sigmoid}(s(v_t))+ \sum \text{rank}(v_i^+, v_i^-),

$$
The interface for providing context is similar to the recommendation API (ids or raw vectors). Still, in this case, they need to be provided in the form of positive-negative pairs.



Discovery API lets you do two new types of search:

- **Discovery search**: Uses the context (the pairs of positive-negative vectors) and a target to return the points more similar to the target, but constrained by the context.
client

    .discover(&DiscoverPoints {

        collection_name: ""{collection_name}"".to_string(),

        context: vec![

            ContextExamplePair {

                positive: Some(VectorExample {

                    example: Some(Example::Id(100.into())),

                }),

                negative: Some(VectorExample {

                    example: Some(Example::Id(718.into())),

                }),

            },

            ContextExamplePair {
The Discovery API can be used in two ways - either with or without the target point. The first case is called a **discovery search**, while the second is called a **context search**.



#### Discovery search



*Discovery search* is an operation that uses a target point to find the most relevant points in the collection, while performing the search in the preferred areas only. That is basically a search operation with more control over the search space.
. Let's keep this concepts of _target_ and _context_ in mind as we explore the new functionality of Qdrant: __Discovery search__.","['$$\n\n\\text{rank}(v^+, v^-) = \\begin{cases}\n\n    1, &\\quad s(v^+) \\geq s(v^-) \\\\\\\\\n\n    -1, &\\quad s(v^+) < s(v^-)\n\n\\end{cases}\n\n$$\n\nwhere $v^+$ represents a positive example, $v^-$ represents a negative example, and $s(v)$ is the similarity score of a vector $v$ to the target vector. The discovery score is then computed as:\n\n$$\n\n \\text{discovery score} = \\text{sigmoid}(s(v_t))+ \\sum \\text{rank}(v_i^+, v_i^-),\n\n$$', 'The interface for providing context is similar to the recommendation API (ids or raw vectors). Still, in this case, they need to be provided in the form of positive-negative pairs.\n\n\n\nDiscovery API lets you do two new types of search:\n\n- **Discovery search**: Uses the context (the pairs of positive-negative vectors) and a target to return the points more similar to the target, but constrained by the context.', 'client\n\n    .discover(&DiscoverPoints {\n\n        collection_name: ""{collection_name}"".to_string(),\n\n        context: vec![\n\n            ContextExamplePair {\n\n                positive: Some(VectorExample {\n\n                    example: Some(Example::Id(100.into())),\n\n                }),\n\n                negative: Some(VectorExample {\n\n                    example: Some(Example::Id(718.into())),\n\n                }),\n\n            },\n\n            ContextExamplePair {', 'The Discovery API can be used in two ways - either with or without the target point. The first case is called a **discovery search**, while the second is called a **context search**.\n\n\n\n#### Discovery search\n\n\n\n*Discovery search* is an operation that uses a target point to find the most relevant points in the collection, while performing the search in the preferred areas only. That is basically a search operation with more control over the search space.', "". Let's keep this concepts of _target_ and _context_ in mind as we explore the new functionality of Qdrant: __Discovery search__.""]","Question: How is the discovery score computed in the context provided?\n\nContext: $$

\text{rank}(v^+, v^-) = \begin{cases}

1, &\quad s(v^+) \geq s(v^-) \\\\

-1, &\quad s(v^+) < s(v^-)

\end{cases}

$$

where $v^+$ represents a positive example, $v^-$ represents a negative example, and $s(v)$ is the similarity score of a vector $v$ to the target vector. The discovery score is then computed as:

$$

\text{discovery score} = \text{sigmoid}(s(v_t))+ \sum \text{rank}(v_i^+, v_i^-),

$$
The interface for providing context is similar to the recommendation API (ids or raw vectors). Still, in this case, they need to be provided in the form of positive-negative pairs.



Discovery API lets you do two new types of search:

- **Discovery search**: Uses the context (the pairs of positive-negative vectors) and a target to return the points more similar to the target, but constrained by the context.
client

.discover(&DiscoverPoints {

collection_name: ""{collection_name}"".to_string(),

context: vec![

ContextExamplePair {

positive: Some(VectorExample {

example: Some(Example::Id(100.into())),

}),

negative: Some(VectorExample {

example: Some(Example::Id(718.into())),

}),

},

ContextExamplePair {
The Discovery API can be used in two ways - either with or without the target point. The first case is called a **discovery search**, while the second is called a **context search**.



#### Discovery search



*Discovery search* is an operation that uses a target point to find the most relevant points in the collection, while performing the search in the preferred areas only. That is basically a search operation with more control over the search space.
. Let's keep this concepts of _target_ and _context_ in mind as we explore the new functionality of Qdrant: __Discovery search__.\n\nAnswer:",How is the discovery score computed in the context provided?,"In the context provided, the discovery score is computed using the formula: 

Discovery Score = sigmoid(s(v_t)) + Σ rank(v_i^+, v_i^-)

Here, s(v_t) represents the similarity function of the target vector, and v_i^+ and v_i^- represent the positive and negative examples, respectively. The sigmoid function is utilized to normalize the score within the range of 0 to 1. The sum of ranks is used to penalize vectors that are closer to negative examples than to positive ones. This means that the sum of individual ranks determines how many positive zones a point is in, while the closeness hierarchy serves as a secondary factor in the computation of the discovery score.","['$$\n\n\\text{rank}(v^+, v^-) = \\begin{cases}\n\n    1, &\\quad s(v^+) \\geq s(v^-) \\\\\\\\\n\n    -1, &\\quad s(v^+) < s(v^-)\n\n\\end{cases}\n\n$$\n\nwhere $v^+$ represents a positive example, $v^-$ represents a negative example, and $s(v)$ is the similarity score of a vector $v$ to the target vector. The discovery score is then computed as:\n\n$$\n\n \\text{discovery score} = \\text{sigmoid}(s(v_t))+ \\sum \\text{rank}(v_i^+, v_i^-),\n\n$$', 'The interface for providing context is similar to the recommendation API (ids or raw vectors). Still, in this case, they need to be provided in the form of positive-negative pairs.\n\n\n\nDiscovery API lets you do two new types of search:\n\n- **Discovery search**: Uses the context (the pairs of positive-negative vectors) and a target to return the points more similar to the target, but constrained by the context.', 'client\n\n    .discover(&DiscoverPoints {\n\n        collection_name: ""{collection_name}"".to_string(),\n\n        context: vec![\n\n            ContextExamplePair {\n\n                positive: Some(VectorExample {\n\n                    example: Some(Example::Id(100.into())),\n\n                }),\n\n                negative: Some(VectorExample {\n\n                    example: Some(Example::Id(718.into())),\n\n                }),\n\n            },\n\n            ContextExamplePair {', 'The Discovery API can be used in two ways - either with or without the target point. The first case is called a **discovery search**, while the second is called a **context search**.\n\n\n\n#### Discovery search\n\n\n\n*Discovery search* is an operation that uses a target point to find the most relevant points in the collection, while performing the search in the preferred areas only. That is basically a search operation with more control over the search space.', "". Let's keep this concepts of _target_ and _context_ in mind as we explore the new functionality of Qdrant: __Discovery search__.""]","$$

\text{rank}(v^+, v^-) = \begin{cases}

    1, &\quad s(v^+) \geq s(v^-) \\\\

    -1, &\quad s(v^+) < s(v^-)

\end{cases}

$$

where $v^+$ represents a positive example, $v^-$ represents a negative example, and $s(v)$ is the similarity score of a vector $v$ to the target vector. The discovery score is then computed as:

$$

 \text{discovery score} = \text{sigmoid}(s(v_t))+ \sum \text{rank}(v_i^+, v_i^-),

$$
The interface for providing context is similar to the recommendation API (ids or raw vectors). Still, in this case, they need to be provided in the form of positive-negative pairs.



Discovery API lets you do two new types of search:

- **Discovery search**: Uses the context (the pairs of positive-negative vectors) and a target to return the points more similar to the target, but constrained by the context.
client

    .discover(&DiscoverPoints {

        collection_name: ""{collection_name}"".to_string(),

        context: vec![

            ContextExamplePair {

                positive: Some(VectorExample {

                    example: Some(Example::Id(100.into())),

                }),

                negative: Some(VectorExample {

                    example: Some(Example::Id(718.into())),

                }),

            },

            ContextExamplePair {
The Discovery API can be used in two ways - either with or without the target point. The first case is called a **discovery search**, while the second is called a **context search**.



#### Discovery search



*Discovery search* is an operation that uses a target point to find the most relevant points in the collection, while performing the search in the preferred areas only. That is basically a search operation with more control over the search space.
. Let's keep this concepts of _target_ and _context_ in mind as we explore the new functionality of Qdrant: __Discovery search__.",0.49206349206349204,0.62,0.5486725663716814,0.216,0.2727272727272727,0.24107142857142858,0.2698412698412698,0.34,0.30088495575221236,0.4126984126984127,0.52,0.46017699115044247,0.9418693780899048,0.6187829971313477,0.6224616765975952,0.6206169128417969,0.9555186487268656,0.9483269496510426,141,1.2702702702702702,0.5582944361687876
True,,4292.287,efe4e5fb,How does Qdrant optimize storage at the segment level?,"Qdrant optimizes storage at the segment level by applying changes in batches rather than individually. When optimization is needed, the segment to be optimized remains readable during the rebuild process. This is achieved by wrapping the segment into a proxy that handles data changes transparently. Changed data is placed in a copy-on-write segment, prioritizing retrieval and subsequent updates. This approach ensures efficiency in storage optimization within Qdrant.","Qdrant optimizes storage at the segment level through various techniques such as creating temporary segments, utilizing copy-on-write segments during optimization, and merging small segments into larger ones using the Merge Optimizer when too many segments are created. These strategies help balance the trade-off between storage efficiency and search performance.

Additionally, starting from version 1.1.0, Qdrant introduces the mechanism of Scalar Quantization for memory optimization by compressing embeddings. Scalar quantization is a data compression technique that converts floating-point values, resulting in improved memory usage and performance.

To optimize throughput, Qdrant can be configured to use multiple cores for processing requests in parallel. Configuring Qdrant with a minimal number of segments, typically 2, can help maximize performance. While large segments offer benefits in terms of index size and reduced vector comparisons for finding nearest neighbors, they require more time to build the index.

Overall, Qdrant's storage efficiency delivers cost savings on hardware while maintaining a responsive system, even with extensive data sets. An independent benchmark stress test conducted by Pienso demonstrated that Qdrant efficiently stored 128 million documents, consuming only 20.4GB of storage and 1.25GB of memory.","The service may require the creation of temporary segments.

Such segments, for example, are created as copy-on-write segments during optimization itself.



It is also essential to have at least one small segment that Qdrant will use to store frequently updated data.

On the other hand, too many small segments lead to suboptimal search performance.



There is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.
```



While Qdrant offers various options to store some parts of the data on disk, starting 

from version 1.1.0, you can also optimize your memory by compressing the embeddings. 

We've implemented the mechanism of **Scalar Quantization**! It turns out to have not 

only a positive impact on memory but also on the performance. 



## Scalar Quantization



Scalar quantization is a data compression technique that converts floating point values
);

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,
Qdrant’s storage efficiency delivers cost savings on hardware while ensuring a responsive system even with extensive data sets. In an independent benchmark stress test, Pienso discovered that Qdrant could efficiently store 128 million documents, consuming a mere 20.4GB of storage and only 1.25GB of memory
Qdrant's storage efficiency delivers cost savings on hardware while ensuring a responsive system even with extensive data sets. In an independent benchmark stress test, Pienso discovered that Qdrant could efficiently store 128 million documents, consuming a mere 20.4GB of storage and only 1.25GB of memory","['The service may require the creation of temporary segments.\n\nSuch segments, for example, are created as copy-on-write segments during optimization itself.\n\n\n\nIt is also essential to have at least one small segment that Qdrant will use to store frequently updated data.\n\nOn the other hand, too many small segments lead to suboptimal search performance.\n\n\n\nThere is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.', ""```\n\n\n\nWhile Qdrant offers various options to store some parts of the data on disk, starting \n\nfrom version 1.1.0, you can also optimize your memory by compressing the embeddings. \n\nWe've implemented the mechanism of **Scalar Quantization**! It turns out to have not \n\nonly a positive impact on memory but also on the performance. \n\n\n\n## Scalar Quantization\n\n\n\nScalar quantization is a data compression technique that converts floating point values"", ');\n\n```\n\n\n\nTo prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.\n\nTo do that, you can configure qdrant to use minimal number of segments, which is usually 2.\n\nLarge segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,', 'Qdrant’s storage efficiency delivers cost savings on hardware while ensuring a responsive system even with extensive data sets. In an independent benchmark stress test, Pienso discovered that Qdrant could efficiently store 128 million documents, consuming a mere 20.4GB of storage and only 1.25GB of memory', ""Qdrant's storage efficiency delivers cost savings on hardware while ensuring a responsive system even with extensive data sets. In an independent benchmark stress test, Pienso discovered that Qdrant could efficiently store 128 million documents, consuming a mere 20.4GB of storage and only 1.25GB of memory""]","Question: How does Qdrant optimize storage at the segment level?\n\nContext: The service may require the creation of temporary segments.

Such segments, for example, are created as copy-on-write segments during optimization itself.



It is also essential to have at least one small segment that Qdrant will use to store frequently updated data.

On the other hand, too many small segments lead to suboptimal search performance.



There is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.
```



While Qdrant offers various options to store some parts of the data on disk, starting 

from version 1.1.0, you can also optimize your memory by compressing the embeddings. 

We've implemented the mechanism of **Scalar Quantization**! It turns out to have not 

only a positive impact on memory but also on the performance. 



## Scalar Quantization



Scalar quantization is a data compression technique that converts floating point values
);

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,
Qdrant’s storage efficiency delivers cost savings on hardware while ensuring a responsive system even with extensive data sets. In an independent benchmark stress test, Pienso discovered that Qdrant could efficiently store 128 million documents, consuming a mere 20.4GB of storage and only 1.25GB of memory
Qdrant's storage efficiency delivers cost savings on hardware while ensuring a responsive system even with extensive data sets. In an independent benchmark stress test, Pienso discovered that Qdrant could efficiently store 128 million documents, consuming a mere 20.4GB of storage and only 1.25GB of memory\n\nAnswer:",How does Qdrant optimize storage at the segment level?,"Qdrant optimizes storage at the segment level by applying changes in batches rather than individually. When optimization is needed, the segment to be optimized remains readable during the rebuild process. This is achieved by wrapping the segment into a proxy that handles data changes transparently. Changed data is placed in a copy-on-write segment, prioritizing retrieval and subsequent updates. This approach ensures efficiency in storage optimization within Qdrant.","['The service may require the creation of temporary segments.\n\nSuch segments, for example, are created as copy-on-write segments during optimization itself.\n\n\n\nIt is also essential to have at least one small segment that Qdrant will use to store frequently updated data.\n\nOn the other hand, too many small segments lead to suboptimal search performance.\n\n\n\nThere is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.', ""```\n\n\n\nWhile Qdrant offers various options to store some parts of the data on disk, starting \n\nfrom version 1.1.0, you can also optimize your memory by compressing the embeddings. \n\nWe've implemented the mechanism of **Scalar Quantization**! It turns out to have not \n\nonly a positive impact on memory but also on the performance. \n\n\n\n## Scalar Quantization\n\n\n\nScalar quantization is a data compression technique that converts floating point values"", ');\n\n```\n\n\n\nTo prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.\n\nTo do that, you can configure qdrant to use minimal number of segments, which is usually 2.\n\nLarge segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,', 'Qdrant’s storage efficiency delivers cost savings on hardware while ensuring a responsive system even with extensive data sets. In an independent benchmark stress test, Pienso discovered that Qdrant could efficiently store 128 million documents, consuming a mere 20.4GB of storage and only 1.25GB of memory', ""Qdrant's storage efficiency delivers cost savings on hardware while ensuring a responsive system even with extensive data sets. In an independent benchmark stress test, Pienso discovered that Qdrant could efficiently store 128 million documents, consuming a mere 20.4GB of storage and only 1.25GB of memory""]","The service may require the creation of temporary segments.

Such segments, for example, are created as copy-on-write segments during optimization itself.



It is also essential to have at least one small segment that Qdrant will use to store frequently updated data.

On the other hand, too many small segments lead to suboptimal search performance.



There is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.
```



While Qdrant offers various options to store some parts of the data on disk, starting 

from version 1.1.0, you can also optimize your memory by compressing the embeddings. 

We've implemented the mechanism of **Scalar Quantization**! It turns out to have not 

only a positive impact on memory but also on the performance. 



## Scalar Quantization



Scalar quantization is a data compression technique that converts floating point values
);

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,
Qdrant’s storage efficiency delivers cost savings on hardware while ensuring a responsive system even with extensive data sets. In an independent benchmark stress test, Pienso discovered that Qdrant could efficiently store 128 million documents, consuming a mere 20.4GB of storage and only 1.25GB of memory
Qdrant's storage efficiency delivers cost savings on hardware while ensuring a responsive system even with extensive data sets. In an independent benchmark stress test, Pienso discovered that Qdrant could efficiently store 128 million documents, consuming a mere 20.4GB of storage and only 1.25GB of memory",0.1837837837837838,0.5396825396825397,0.2741935483870968,0.04891304347826087,0.14516129032258066,0.07317073170731708,0.0918918918918919,0.2698412698412698,0.1370967741935484,0.16756756756756758,0.49206349206349204,0.25,0.8010402917861938,0.5629812479019165,0.6518287658691406,0.6041560173034668,0.9860120198702336,0.6323150955140591,186,2.7761194029850746,0.23721083586592895
True,,1845.96,9f668eb5,What is the purpose of the Vacuum Optimizer in the Qdrant database system?,"The Vacuum Optimizer in the Qdrant database system is utilized to address the issue of accumulated deleted records within segments. When records are marked as deleted instead of being immediately removed, they can accumulate over time, occupying memory and slowing down the system. The Vacuum Optimizer is triggered when a segment has accumulated a significant number of deleted records, as defined by the criteria set in the configuration file. This optimizer helps in optimizing segments by removing these accumulated deleted records, thereby improving system performance and efficiency. The configuration file specifies parameters such as the minimal fraction of deleted vectors and the minimal number of vectors in a segment required to trigger the Vacuum Optimizer. By running the Vacuum Optimizer, the system can effectively manage and optimize segments to prevent performance degradation due to accumulated deleted records.","The purpose of the Vacuum Optimizer in the Qdrant database system is to address the accumulation of deleted records within segments. As Qdrant marks records as deleted instead of immediately deleting them, over time, these deleted records can accumulate, occupy memory, and slow down the system. The Vacuum Optimizer is used to manage this issue by cleaning up and optimizing segments that have accumulated too many deleted records. This optimization process helps maintain system performance and efficiency by ensuring that unnecessary data is properly managed and does not negatively impact the system's operations.","Like many other databases, Qdrant does not delete entries immediately after a query.

Instead, it marks records as deleted and ignores them for future queries.



This strategy allows us to minimize disk access - one of the slowest operations.

However, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.



To avoid these adverse effects, Vacuum Optimizer is used.

It is used if the segment has accumulated too many deleted records.
---

title: Optimizer

weight: 70

aliases:

  - ../optimizer

---



# Optimizer



It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.



Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).
Systems like Qdrant exemplify scalability in vector databases. It leverages Rust's efficiency in **memory management** and **performance**, which allows handling of large-scale data with optimized resource usage.





### Efficient Query Processing
Qdrant is designed to handle large-scale datasets with billions of data points. Its architecture employs techniques like binary and scalar quantization for efficient storage and retrieval. When you inject FastEmbed’s CPU-first design and lightweight nature into this equation, you end up with a system that can scale seamlessly while maintaining low latency.



## Summary
Conversely, the only up-front investment with vector databases is the indexing (which requires more compute). After this step, everything else is a breeze. Once setup, Qdrant easily scales via [features like Multitenancy and Sharding](https://qdrant.tech/articles/multitenancy/). This lets you scale up your reliance on the vector retrieval process and minimize your use of the compute-heavy LLMs. As an optimization  measure, Qdrant is irreplaceable. 



Julien Simon from HuggingFace says it best:","['Like many other databases, Qdrant does not delete entries immediately after a query.\n\nInstead, it marks records as deleted and ignores them for future queries.\n\n\n\nThis strategy allows us to minimize disk access - one of the slowest operations.\n\nHowever, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.\n\n\n\nTo avoid these adverse effects, Vacuum Optimizer is used.\n\nIt is used if the segment has accumulated too many deleted records.', '---\n\ntitle: Optimizer\n\nweight: 70\n\naliases:\n\n  - ../optimizer\n\n---\n\n\n\n# Optimizer\n\n\n\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\n\n\n\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).', ""Systems like Qdrant exemplify scalability in vector databases. It leverages Rust's efficiency in **memory management** and **performance**, which allows handling of large-scale data with optimized resource usage.\n\n\n\n\n\n### Efficient Query Processing"", 'Qdrant is designed to handle large-scale datasets with billions of data points. Its architecture employs techniques like binary and scalar quantization for efficient storage and retrieval. When you inject FastEmbed’s CPU-first design and lightweight nature into this equation, you end up with a system that can scale seamlessly while maintaining low latency.\n\n\n\n## Summary', 'Conversely, the only up-front investment with vector databases is the indexing (which requires more compute). After this step, everything else is a breeze. Once setup, Qdrant easily scales via [features like Multitenancy and Sharding](https://qdrant.tech/articles/multitenancy/). This lets you scale up your reliance on the vector retrieval process and minimize your use of the compute-heavy LLMs. As an optimization  measure, Qdrant is irreplaceable. \n\n\n\nJulien Simon from HuggingFace says it best:']","Question: What is the purpose of the Vacuum Optimizer in the Qdrant database system?\n\nContext: Like many other databases, Qdrant does not delete entries immediately after a query.

Instead, it marks records as deleted and ignores them for future queries.



This strategy allows us to minimize disk access - one of the slowest operations.

However, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.



To avoid these adverse effects, Vacuum Optimizer is used.

It is used if the segment has accumulated too many deleted records.
---

title: Optimizer

weight: 70

aliases:

- ../optimizer

---



# Optimizer



It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.



Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).
Systems like Qdrant exemplify scalability in vector databases. It leverages Rust's efficiency in **memory management** and **performance**, which allows handling of large-scale data with optimized resource usage.





### Efficient Query Processing
Qdrant is designed to handle large-scale datasets with billions of data points. Its architecture employs techniques like binary and scalar quantization for efficient storage and retrieval. When you inject FastEmbed’s CPU-first design and lightweight nature into this equation, you end up with a system that can scale seamlessly while maintaining low latency.



## Summary
Conversely, the only up-front investment with vector databases is the indexing (which requires more compute). After this step, everything else is a breeze. Once setup, Qdrant easily scales via [features like Multitenancy and Sharding](https://qdrant.tech/articles/multitenancy/). This lets you scale up your reliance on the vector retrieval process and minimize your use of the compute-heavy LLMs. As an optimization  measure, Qdrant is irreplaceable. 



Julien Simon from HuggingFace says it best:\n\nAnswer:",What is the purpose of the Vacuum Optimizer in the Qdrant database system?,"The Vacuum Optimizer in the Qdrant database system is utilized to address the issue of accumulated deleted records within segments. When records are marked as deleted instead of being immediately removed, they can accumulate over time, occupying memory and slowing down the system. The Vacuum Optimizer is triggered when a segment has accumulated a significant number of deleted records, as defined by the criteria set in the configuration file. This optimizer helps in optimizing segments by removing these accumulated deleted records, thereby improving system performance and efficiency. The configuration file specifies parameters such as the minimal fraction of deleted vectors and the minimal number of vectors in a segment required to trigger the Vacuum Optimizer. By running the Vacuum Optimizer, the system can effectively manage and optimize segments to prevent performance degradation due to accumulated deleted records.","['Like many other databases, Qdrant does not delete entries immediately after a query.\n\nInstead, it marks records as deleted and ignores them for future queries.\n\n\n\nThis strategy allows us to minimize disk access - one of the slowest operations.\n\nHowever, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.\n\n\n\nTo avoid these adverse effects, Vacuum Optimizer is used.\n\nIt is used if the segment has accumulated too many deleted records.', '---\n\ntitle: Optimizer\n\nweight: 70\n\naliases:\n\n  - ../optimizer\n\n---\n\n\n\n# Optimizer\n\n\n\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\n\n\n\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).', ""Systems like Qdrant exemplify scalability in vector databases. It leverages Rust's efficiency in **memory management** and **performance**, which allows handling of large-scale data with optimized resource usage.\n\n\n\n\n\n### Efficient Query Processing"", 'Qdrant is designed to handle large-scale datasets with billions of data points. Its architecture employs techniques like binary and scalar quantization for efficient storage and retrieval. When you inject FastEmbed’s CPU-first design and lightweight nature into this equation, you end up with a system that can scale seamlessly while maintaining low latency.\n\n\n\n## Summary', 'Conversely, the only up-front investment with vector databases is the indexing (which requires more compute). After this step, everything else is a breeze. Once setup, Qdrant easily scales via [features like Multitenancy and Sharding](https://qdrant.tech/articles/multitenancy/). This lets you scale up your reliance on the vector retrieval process and minimize your use of the compute-heavy LLMs. As an optimization  measure, Qdrant is irreplaceable. \n\n\n\nJulien Simon from HuggingFace says it best:']","Like many other databases, Qdrant does not delete entries immediately after a query.

Instead, it marks records as deleted and ignores them for future queries.



This strategy allows us to minimize disk access - one of the slowest operations.

However, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.



To avoid these adverse effects, Vacuum Optimizer is used.

It is used if the segment has accumulated too many deleted records.
---

title: Optimizer

weight: 70

aliases:

  - ../optimizer

---



# Optimizer



It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.



Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).
Systems like Qdrant exemplify scalability in vector databases. It leverages Rust's efficiency in **memory management** and **performance**, which allows handling of large-scale data with optimized resource usage.





### Efficient Query Processing
Qdrant is designed to handle large-scale datasets with billions of data points. Its architecture employs techniques like binary and scalar quantization for efficient storage and retrieval. When you inject FastEmbed’s CPU-first design and lightweight nature into this equation, you end up with a system that can scale seamlessly while maintaining low latency.



## Summary
Conversely, the only up-front investment with vector databases is the indexing (which requires more compute). After this step, everything else is a breeze. Once setup, Qdrant easily scales via [features like Multitenancy and Sharding](https://qdrant.tech/articles/multitenancy/). This lets you scale up your reliance on the vector retrieval process and minimize your use of the compute-heavy LLMs. As an optimization  measure, Qdrant is irreplaceable. 



Julien Simon from HuggingFace says it best:",0.7011494252873564,0.5041322314049587,0.5865384615384615,0.3953488372093023,0.2833333333333333,0.3300970873786408,0.5172413793103449,0.371900826446281,0.43269230769230765,0.6436781609195402,0.4628099173553719,0.5384615384615385,0.9589911699295044,0.7498488426208496,0.7075067758560181,0.7280626893043518,0.9891512467584107,0.8459971414364796,94,0.6861313868613139,0.4866169672028085
True,,1552.77,d1735d2c,What is the purpose of the `payload` field in the data points?,"The `payload` field in the data points being upserted using the QdrantClient serves as a container for additional metadata or information related to the vector data being stored. It allows users to associate supplementary details with each data point, such as the city name, price, or any other custom attributes that provide context or additional insights into the vector data. This metadata can be used for filtering, searching, or categorizing the vectors during retrieval or query operations, enabling more efficient and targeted data analysis and retrieval processes. In the examples given, the `payload` field includes information like city names, prices, and other relevant details specific to each data point, enhancing the overall utility and relevance of the stored vector data.","The purpose of the `payload` field in the data points is to store additional information that is associated with the vectors. This additional information can be used for various purposes such as filtering, sorting, or retrieving specific details related to the vectors. The payload allows users to store custom properties or metadata along with the vectors, making it easier to work with and retrieve relevant information when needed.","## Payload



A [Payload](/documentation/concepts/payload/) describes information that you can store with vectors.



## Points



[Points](/documentation/concepts/points/) are a record which consists of a vector and an optional payload. 



## Search



[Search](/documentation/concepts/search/) describes _similarity search_, which set up related objects close to each other in vector space.



## Explore
```http

POST /collections/{collection_name}/points/payload

{

    ""payload"": {

        ""property1"": ""string"",

        ""property2"": ""string""

    },

    ""filter"": {

        ""must"": [

            {

                ""key"": ""color"",

                ""match"": {

                    ""value"": ""red""

                }

            }

        ]

    }

}

```



```python

client.set_payload(

    collection_name=""{collection_name}"",

    payload={

        ""property1"": ""string"",
```http

PUT /collections/{collection_name}/points

{

    ""points"": [

        {

            ""id"": 1,

            ""vector"": [0.05, 0.61, 0.76, 0.74],

            ""payload"": {""city"": ""Berlin"", ""price"": 1.99}

        },

        {

            ""id"": 2,

            ""vector"": [0.19, 0.81, 0.75, 0.11],

            ""payload"": {""city"": [""Berlin"", ""London""], ""price"": 1.99}

        },

        {

            ""id"": 3,

            ""vector"": [0.36, 0.55, 0.47, 0.94],
""payloads"": [

            {""color"": ""red""},

            {""color"": ""green""},

            {""color"": ""blue""}

        ],

        ""vectors"": [

            [0.9, 0.1, 0.1],

            [0.1, 0.9, 0.1],

            [0.1, 0.1, 0.9]

        ]

    }

}

```



```python

client.upsert(

    collection_name=""{collection_name}"",

    points=models.Batch(

        ids=[1, 2, 3],

        payloads=[

            {""color"": ""red""},

            {""color"": ""green""},

            {""color"": ""blue""},
""with_payload"": [""title"", ""text""],



        // Options for specifying what to bring from the vector(s) 

        // of the looked up point, true by default

        ""with_vectors: false

    }

}

```



```python

client.search_groups(

    collection_name=""chunks"",

    # Same as in the regular search() API

    query_vector=[1.1],

    # Grouping parameters

    group_by=""document_id"",  # Path of the field to group by

    limit=2,  # Max amount of groups","['## Payload\n\n\n\nA [Payload](/documentation/concepts/payload/) describes information that you can store with vectors.\n\n\n\n## Points\n\n\n\n[Points](/documentation/concepts/points/) are a record which consists of a vector and an optional payload. \n\n\n\n## Search\n\n\n\n[Search](/documentation/concepts/search/) describes _similarity search_, which set up related objects close to each other in vector space.\n\n\n\n## Explore', '```http\n\nPOST /collections/{collection_name}/points/payload\n\n{\n\n    ""payload"": {\n\n        ""property1"": ""string"",\n\n        ""property2"": ""string""\n\n    },\n\n    ""filter"": {\n\n        ""must"": [\n\n            {\n\n                ""key"": ""color"",\n\n                ""match"": {\n\n                    ""value"": ""red""\n\n                }\n\n            }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.set_payload(\n\n    collection_name=""{collection_name}"",\n\n    payload={\n\n        ""property1"": ""string"",', '```http\n\nPUT /collections/{collection_name}/points\n\n{\n\n    ""points"": [\n\n        {\n\n            ""id"": 1,\n\n            ""vector"": [0.05, 0.61, 0.76, 0.74],\n\n            ""payload"": {""city"": ""Berlin"", ""price"": 1.99}\n\n        },\n\n        {\n\n            ""id"": 2,\n\n            ""vector"": [0.19, 0.81, 0.75, 0.11],\n\n            ""payload"": {""city"": [""Berlin"", ""London""], ""price"": 1.99}\n\n        },\n\n        {\n\n            ""id"": 3,\n\n            ""vector"": [0.36, 0.55, 0.47, 0.94],', '""payloads"": [\n\n            {""color"": ""red""},\n\n            {""color"": ""green""},\n\n            {""color"": ""blue""}\n\n        ],\n\n        ""vectors"": [\n\n            [0.9, 0.1, 0.1],\n\n            [0.1, 0.9, 0.1],\n\n            [0.1, 0.1, 0.9]\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.upsert(\n\n    collection_name=""{collection_name}"",\n\n    points=models.Batch(\n\n        ids=[1, 2, 3],\n\n        payloads=[\n\n            {""color"": ""red""},\n\n            {""color"": ""green""},\n\n            {""color"": ""blue""},', '""with_payload"": [""title"", ""text""],\n\n\n\n        // Options for specifying what to bring from the vector(s) \n\n        // of the looked up point, true by default\n\n        ""with_vectors: false\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.search_groups(\n\n    collection_name=""chunks"",\n\n    # Same as in the regular search() API\n\n    query_vector=[1.1],\n\n    # Grouping parameters\n\n    group_by=""document_id"",  # Path of the field to group by\n\n    limit=2,  # Max amount of groups']","Question: What is the purpose of the `payload` field in the data points?\n\nContext: ## Payload



A [Payload](/documentation/concepts/payload/) describes information that you can store with vectors.



## Points



[Points](/documentation/concepts/points/) are a record which consists of a vector and an optional payload. 



## Search



[Search](/documentation/concepts/search/) describes _similarity search_, which set up related objects close to each other in vector space.



## Explore
```http

POST /collections/{collection_name}/points/payload

{

""payload"": {

""property1"": ""string"",

""property2"": ""string""

},

""filter"": {

""must"": [

{

""key"": ""color"",

""match"": {

""value"": ""red""

}

}

]

}

}

```



```python

client.set_payload(

collection_name=""{collection_name}"",

payload={

""property1"": ""string"",
```http

PUT /collections/{collection_name}/points

{

""points"": [

{

""id"": 1,

""vector"": [0.05, 0.61, 0.76, 0.74],

""payload"": {""city"": ""Berlin"", ""price"": 1.99}

},

{

""id"": 2,

""vector"": [0.19, 0.81, 0.75, 0.11],

""payload"": {""city"": [""Berlin"", ""London""], ""price"": 1.99}

},

{

""id"": 3,

""vector"": [0.36, 0.55, 0.47, 0.94],
""payloads"": [

{""color"": ""red""},

{""color"": ""green""},

{""color"": ""blue""}

],

""vectors"": [

[0.9, 0.1, 0.1],

[0.1, 0.9, 0.1],

[0.1, 0.1, 0.9]

]

}

}

```



```python

client.upsert(

collection_name=""{collection_name}"",

points=models.Batch(

ids=[1, 2, 3],

payloads=[

{""color"": ""red""},

{""color"": ""green""},

{""color"": ""blue""},
""with_payload"": [""title"", ""text""],



// Options for specifying what to bring from the vector(s) 

// of the looked up point, true by default

""with_vectors: false

}

}

```



```python

client.search_groups(

collection_name=""chunks"",

# Same as in the regular search() API

query_vector=[1.1],

# Grouping parameters

group_by=""document_id"",  # Path of the field to group by

limit=2,  # Max amount of groups\n\nAnswer:",What is the purpose of the `payload` field in the data points?,"The `payload` field in the data points being upserted using the QdrantClient serves as a container for additional metadata or information related to the vector data being stored. It allows users to associate supplementary details with each data point, such as the city name, price, or any other custom attributes that provide context or additional insights into the vector data. This metadata can be used for filtering, searching, or categorizing the vectors during retrieval or query operations, enabling more efficient and targeted data analysis and retrieval processes. In the examples given, the `payload` field includes information like city names, prices, and other relevant details specific to each data point, enhancing the overall utility and relevance of the stored vector data.","['## Payload\n\n\n\nA [Payload](/documentation/concepts/payload/) describes information that you can store with vectors.\n\n\n\n## Points\n\n\n\n[Points](/documentation/concepts/points/) are a record which consists of a vector and an optional payload. \n\n\n\n## Search\n\n\n\n[Search](/documentation/concepts/search/) describes _similarity search_, which set up related objects close to each other in vector space.\n\n\n\n## Explore', '```http\n\nPOST /collections/{collection_name}/points/payload\n\n{\n\n    ""payload"": {\n\n        ""property1"": ""string"",\n\n        ""property2"": ""string""\n\n    },\n\n    ""filter"": {\n\n        ""must"": [\n\n            {\n\n                ""key"": ""color"",\n\n                ""match"": {\n\n                    ""value"": ""red""\n\n                }\n\n            }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.set_payload(\n\n    collection_name=""{collection_name}"",\n\n    payload={\n\n        ""property1"": ""string"",', '```http\n\nPUT /collections/{collection_name}/points\n\n{\n\n    ""points"": [\n\n        {\n\n            ""id"": 1,\n\n            ""vector"": [0.05, 0.61, 0.76, 0.74],\n\n            ""payload"": {""city"": ""Berlin"", ""price"": 1.99}\n\n        },\n\n        {\n\n            ""id"": 2,\n\n            ""vector"": [0.19, 0.81, 0.75, 0.11],\n\n            ""payload"": {""city"": [""Berlin"", ""London""], ""price"": 1.99}\n\n        },\n\n        {\n\n            ""id"": 3,\n\n            ""vector"": [0.36, 0.55, 0.47, 0.94],', '""payloads"": [\n\n            {""color"": ""red""},\n\n            {""color"": ""green""},\n\n            {""color"": ""blue""}\n\n        ],\n\n        ""vectors"": [\n\n            [0.9, 0.1, 0.1],\n\n            [0.1, 0.9, 0.1],\n\n            [0.1, 0.1, 0.9]\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.upsert(\n\n    collection_name=""{collection_name}"",\n\n    points=models.Batch(\n\n        ids=[1, 2, 3],\n\n        payloads=[\n\n            {""color"": ""red""},\n\n            {""color"": ""green""},\n\n            {""color"": ""blue""},', '""with_payload"": [""title"", ""text""],\n\n\n\n        // Options for specifying what to bring from the vector(s) \n\n        // of the looked up point, true by default\n\n        ""with_vectors: false\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.search_groups(\n\n    collection_name=""chunks"",\n\n    # Same as in the regular search() API\n\n    query_vector=[1.1],\n\n    # Grouping parameters\n\n    group_by=""document_id"",  # Path of the field to group by\n\n    limit=2,  # Max amount of groups']","## Payload



A [Payload](/documentation/concepts/payload/) describes information that you can store with vectors.



## Points



[Points](/documentation/concepts/points/) are a record which consists of a vector and an optional payload. 



## Search



[Search](/documentation/concepts/search/) describes _similarity search_, which set up related objects close to each other in vector space.



## Explore
```http

POST /collections/{collection_name}/points/payload

{

    ""payload"": {

        ""property1"": ""string"",

        ""property2"": ""string""

    },

    ""filter"": {

        ""must"": [

            {

                ""key"": ""color"",

                ""match"": {

                    ""value"": ""red""

                }

            }

        ]

    }

}

```



```python

client.set_payload(

    collection_name=""{collection_name}"",

    payload={

        ""property1"": ""string"",
```http

PUT /collections/{collection_name}/points

{

    ""points"": [

        {

            ""id"": 1,

            ""vector"": [0.05, 0.61, 0.76, 0.74],

            ""payload"": {""city"": ""Berlin"", ""price"": 1.99}

        },

        {

            ""id"": 2,

            ""vector"": [0.19, 0.81, 0.75, 0.11],

            ""payload"": {""city"": [""Berlin"", ""London""], ""price"": 1.99}

        },

        {

            ""id"": 3,

            ""vector"": [0.36, 0.55, 0.47, 0.94],
""payloads"": [

            {""color"": ""red""},

            {""color"": ""green""},

            {""color"": ""blue""}

        ],

        ""vectors"": [

            [0.9, 0.1, 0.1],

            [0.1, 0.9, 0.1],

            [0.1, 0.1, 0.9]

        ]

    }

}

```



```python

client.upsert(

    collection_name=""{collection_name}"",

    points=models.Batch(

        ids=[1, 2, 3],

        payloads=[

            {""color"": ""red""},

            {""color"": ""green""},

            {""color"": ""blue""},
""with_payload"": [""title"", ""text""],



        // Options for specifying what to bring from the vector(s) 

        // of the looked up point, true by default

        ""with_vectors: false

    }

}

```



```python

client.search_groups(

    collection_name=""chunks"",

    # Same as in the regular search() API

    query_vector=[1.1],

    # Grouping parameters

    group_by=""document_id"",  # Path of the field to group by

    limit=2,  # Max amount of groups",0.7213114754098361,0.4074074074074074,0.5207100591715976,0.21666666666666667,0.12149532710280374,0.15568862275449102,0.36065573770491804,0.2037037037037037,0.2603550295857988,0.6065573770491803,0.3425925925925926,0.43786982248520717,0.7488260865211487,0.7338770627975464,0.6737660765647888,0.7025381326675415,0.7338763102889061,0.5250671431422234,68,0.5666666666666667,0.6355585208637463
True,,2536.298,b8f57991,What is the purpose of shard replication in Qdrant?,"Shard replication in Qdrant increases the reliability of the cluster by keeping several copies of a shard spread across the cluster. This ensures the availability of the data in case of node failures, except if all replicas are lost.","The purpose of shard replication in Qdrant is to increase the reliability of the cluster by maintaining several copies of a shard spread across the cluster. Replicating shards ensures data consistency and availability in case of node failures or data loss. By replicating shards, Qdrant can transfer index and quantization data from the source shard to the target shard, eliminating the need for re-optimization and ensuring immediate availability of the data on the target node. This process helps maintain performance levels and consistency across the cluster, especially for large shards where performance improvements can be significant. However, it is important to consider the storage space requirements when setting the replication factor to avoid any issues related to storage capacity.","Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.

If there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.



### Recover from a URL or local file



*Available as of v0.11.3*
ordering guarantees can be `strong`[^ordered], required for some applications.



The `stream_records` method is currently used as default. This may change in the

future.



## Replication



*Available as of v0.11.0*



Qdrant allows you to replicate shards between nodes in the cluster.



Shard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.
order to the target shard to bring it into the same state as the source. There

are two important benefits: 1. It transfers index and quantization data, so that

the shard does not have to be optimized again on the target node, making them

immediately available. This way, Qdrant ensures that there will be no

degradation in performance at the end of the transfer. Especially on large

shards, this can give a huge performance improvement. 2. The consistency and
Since a replication factor of ""2"" would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.



### Creating new shard replicas



It is possible to create or delete replicas manually on an existing collection using the [Update collection cluster setup API](https://qdrant.github.io/qdrant/redoc/index.html?v=v0.11.0#tag/cluster/operation/update_collection_cluster).
This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. 



### Common use cases","['Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.\n\nIf there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.\n\n\n\n### Recover from a URL or local file\n\n\n\n*Available as of v0.11.3*', 'ordering guarantees can be `strong`[^ordered], required for some applications.\n\n\n\nThe `stream_records` method is currently used as default. This may change in the\n\nfuture.\n\n\n\n## Replication\n\n\n\n*Available as of v0.11.0*\n\n\n\nQdrant allows you to replicate shards between nodes in the cluster.\n\n\n\nShard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.', 'order to the target shard to bring it into the same state as the source. There\n\nare two important benefits: 1. It transfers index and quantization data, so that\n\nthe shard does not have to be optimized again on the target node, making them\n\nimmediately available. This way, Qdrant ensures that there will be no\n\ndegradation in performance at the end of the transfer. Especially on large\n\nshards, this can give a huge performance improvement. 2. The consistency and', 'Since a replication factor of ""2"" would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.\n\n\n\n### Creating new shard replicas\n\n\n\nIt is possible to create or delete replicas manually on an existing collection using the [Update collection cluster setup API](https://qdrant.github.io/qdrant/redoc/index.html?v=v0.11.0#tag/cluster/operation/update_collection_cluster).', 'This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. \n\n\n\n### Common use cases']","Question: What is the purpose of shard replication in Qdrant?\n\nContext: Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.

If there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.



### Recover from a URL or local file



*Available as of v0.11.3*
ordering guarantees can be `strong`[^ordered], required for some applications.



The `stream_records` method is currently used as default. This may change in the

future.



## Replication



*Available as of v0.11.0*



Qdrant allows you to replicate shards between nodes in the cluster.



Shard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.
order to the target shard to bring it into the same state as the source. There

are two important benefits: 1. It transfers index and quantization data, so that

the shard does not have to be optimized again on the target node, making them

immediately available. This way, Qdrant ensures that there will be no

degradation in performance at the end of the transfer. Especially on large

shards, this can give a huge performance improvement. 2. The consistency and
Since a replication factor of ""2"" would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.



### Creating new shard replicas



It is possible to create or delete replicas manually on an existing collection using the [Update collection cluster setup API](https://qdrant.github.io/qdrant/redoc/index.html?v=v0.11.0#tag/cluster/operation/update_collection_cluster).
This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. 



### Common use cases\n\nAnswer:",What is the purpose of shard replication in Qdrant?,"Shard replication in Qdrant increases the reliability of the cluster by keeping several copies of a shard spread across the cluster. This ensures the availability of the data in case of node failures, except if all replicas are lost.","['Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.\n\nIf there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.\n\n\n\n### Recover from a URL or local file\n\n\n\n*Available as of v0.11.3*', 'ordering guarantees can be `strong`[^ordered], required for some applications.\n\n\n\nThe `stream_records` method is currently used as default. This may change in the\n\nfuture.\n\n\n\n## Replication\n\n\n\n*Available as of v0.11.0*\n\n\n\nQdrant allows you to replicate shards between nodes in the cluster.\n\n\n\nShard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.', 'order to the target shard to bring it into the same state as the source. There\n\nare two important benefits: 1. It transfers index and quantization data, so that\n\nthe shard does not have to be optimized again on the target node, making them\n\nimmediately available. This way, Qdrant ensures that there will be no\n\ndegradation in performance at the end of the transfer. Especially on large\n\nshards, this can give a huge performance improvement. 2. The consistency and', 'Since a replication factor of ""2"" would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.\n\n\n\n### Creating new shard replicas\n\n\n\nIt is possible to create or delete replicas manually on an existing collection using the [Update collection cluster setup API](https://qdrant.github.io/qdrant/redoc/index.html?v=v0.11.0#tag/cluster/operation/update_collection_cluster).', 'This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. \n\n\n\n### Common use cases']","Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.

If there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.



### Recover from a URL or local file



*Available as of v0.11.3*
ordering guarantees can be `strong`[^ordered], required for some applications.



The `stream_records` method is currently used as default. This may change in the

future.



## Replication



*Available as of v0.11.0*



Qdrant allows you to replicate shards between nodes in the cluster.



Shard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.
order to the target shard to bring it into the same state as the source. There

are two important benefits: 1. It transfers index and quantization data, so that

the shard does not have to be optimized again on the target node, making them

immediately available. This way, Qdrant ensures that there will be no

degradation in performance at the end of the transfer. Especially on large

shards, this can give a huge performance improvement. 2. The consistency and
Since a replication factor of ""2"" would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.



### Creating new shard replicas



It is possible to create or delete replicas manually on an existing collection using the [Update collection cluster setup API](https://qdrant.github.io/qdrant/redoc/index.html?v=v0.11.0#tag/cluster/operation/update_collection_cluster).
This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. 



### Common use cases",0.24299065420560748,0.7878787878787878,0.3714285714285714,0.1792452830188679,0.59375,0.2753623188405797,0.21495327102803738,0.696969696969697,0.32857142857142857,0.24299065420560748,0.7878787878787878,0.3714285714285714,0.8586634397506714,0.645196795463562,0.7946925163269043,0.712183952331543,0.9762140597071266,0.6307284177746624,119,3.051282051282051,0.3739991344575818
True,,5444.9,c1bc53db,What are the steps to set up product quantization in QdrantClient?,"To set up product quantization in QdrantClient, you need to specify the quantization parameters in the quantization_config section of the collection configuration. The compression ratio can be set to x16 and the always_ram parameter can be set to true to store quantized vectors in RAM. The vectors_config section should also be specified with the desired vector size and distance metric. Finally, you can use the create_collection method of the QdrantClient to create the collection with the specified configurations.","To set up product quantization in QdrantClient, you can follow these steps:

1. Import the necessary classes and packages related to product quantization:

```java
import io.qdrant.client.grpc.Collections.ProductQuantization;
import io.qdrant.client.grpc.Collections.QuantizationConfig;
import io.qdrant.client.grpc.Collections.VectorParams;
import io.qdrant.client.grpc.Collections.VectorsConfig;
```

2. Initialize the QdrantClient and connect to the Qdrant server:

```java
QdrantClient client = new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());
```

3. Create a collection with the specified collection name and vectors configuration including product quantization settings:

```java
client.createCollectionAsync(
    CreateCollection.newBuilder()
        .setCollectionName(""{collection_name}"")
        .setVectorsConfig(
            VectorsConfig.newBuilder()
                .setQuantizationConfig(
                    QuantizationConfig.newBuilder()
                        .setProductQuantization(
                            ProductQuantization.newBuilder()
                                // Add product quantization configuration here
                        )
                )
        )
);
```

This will set up product quantization for the specified collection in QdrantClient.","import io.qdrant.client.grpc.Collections.ProductQuantization;

import io.qdrant.client.grpc.Collections.QuantizationConfig;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(
use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, CompressionRatio,

        CreateCollection, Distance, ProductQuantization, QuantizationConfig, VectorParams,

        VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".to_string(),

        vectors_config: Some(VectorsConfig {
quantization_config=models.BinaryQuantization(

        binary=models.BinaryQuantizationConfig(

            always_ram=True,

        ),

    ),

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createCollection(""{collection_name}"", {

  vectors: {

    size: 1536,

    distance: ""Cosine"",

  },

  quantization_config: {

    binary: {

      always_ram: true,

    },

  },

});

```
use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,

        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,

        VectorParams, VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".to_string(),
use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,

        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,

        VectorParams, VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".to_string(),","['import io.qdrant.client.grpc.Collections.ProductQuantization;\n\nimport io.qdrant.client.grpc.Collections.QuantizationConfig;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(', 'use qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, CompressionRatio,\n\n        CreateCollection, Distance, ProductQuantization, QuantizationConfig, VectorParams,\n\n        VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".to_string(),\n\n        vectors_config: Some(VectorsConfig {', 'quantization_config=models.BinaryQuantization(\n\n        binary=models.BinaryQuantizationConfig(\n\n            always_ram=True,\n\n        ),\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createCollection(""{collection_name}"", {\n\n  vectors: {\n\n    size: 1536,\n\n    distance: ""Cosine"",\n\n  },\n\n  quantization_config: {\n\n    binary: {\n\n      always_ram: true,\n\n    },\n\n  },\n\n});\n\n```', 'use qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,\n\n        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,\n\n        VectorParams, VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".to_string(),', 'use qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,\n\n        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,\n\n        VectorParams, VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".to_string(),']","Question: What are the steps to set up product quantization in QdrantClient?\n\nContext: import io.qdrant.client.grpc.Collections.ProductQuantization;

import io.qdrant.client.grpc.Collections.QuantizationConfig;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createCollectionAsync(

CreateCollection.newBuilder()

.setCollectionName(""{collection_name}"")

.setVectorsConfig(
use qdrant_client::{

client::QdrantClient,

qdrant::{

quantization_config::Quantization, vectors_config::Config, CompressionRatio,

CreateCollection, Distance, ProductQuantization, QuantizationConfig, VectorParams,

VectorsConfig,

},

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

.create_collection(&CreateCollection {

collection_name: ""{collection_name}"".to_string(),

vectors_config: Some(VectorsConfig {
quantization_config=models.BinaryQuantization(

binary=models.BinaryQuantizationConfig(

always_ram=True,

),

),

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createCollection(""{collection_name}"", {

vectors: {

size: 1536,

distance: ""Cosine"",

},

quantization_config: {

binary: {

always_ram: true,

},

},

});

```
use qdrant_client::{

client::QdrantClient,

qdrant::{

quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,

OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,

VectorParams, VectorsConfig,

},

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

.create_collection(&CreateCollection {

collection_name: ""{collection_name}"".to_string(),
use qdrant_client::{

client::QdrantClient,

qdrant::{

quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,

OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,

VectorParams, VectorsConfig,

},

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

.create_collection(&CreateCollection {

collection_name: ""{collection_name}"".to_string(),\n\nAnswer:",What are the steps to set up product quantization in QdrantClient?,"To set up product quantization in QdrantClient, you need to specify the quantization parameters in the quantization_config section of the collection configuration. The compression ratio can be set to x16 and the always_ram parameter can be set to true to store quantized vectors in RAM. The vectors_config section should also be specified with the desired vector size and distance metric. Finally, you can use the create_collection method of the QdrantClient to create the collection with the specified configurations.","['import io.qdrant.client.grpc.Collections.ProductQuantization;\n\nimport io.qdrant.client.grpc.Collections.QuantizationConfig;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(', 'use qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, CompressionRatio,\n\n        CreateCollection, Distance, ProductQuantization, QuantizationConfig, VectorParams,\n\n        VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".to_string(),\n\n        vectors_config: Some(VectorsConfig {', 'quantization_config=models.BinaryQuantization(\n\n        binary=models.BinaryQuantizationConfig(\n\n            always_ram=True,\n\n        ),\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createCollection(""{collection_name}"", {\n\n  vectors: {\n\n    size: 1536,\n\n    distance: ""Cosine"",\n\n  },\n\n  quantization_config: {\n\n    binary: {\n\n      always_ram: true,\n\n    },\n\n  },\n\n});\n\n```', 'use qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,\n\n        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,\n\n        VectorParams, VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".to_string(),', 'use qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,\n\n        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,\n\n        VectorParams, VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".to_string(),']","import io.qdrant.client.grpc.Collections.ProductQuantization;

import io.qdrant.client.grpc.Collections.QuantizationConfig;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(
use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, CompressionRatio,

        CreateCollection, Distance, ProductQuantization, QuantizationConfig, VectorParams,

        VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".to_string(),

        vectors_config: Some(VectorsConfig {
quantization_config=models.BinaryQuantization(

        binary=models.BinaryQuantizationConfig(

            always_ram=True,

        ),

    ),

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createCollection(""{collection_name}"", {

  vectors: {

    size: 1536,

    distance: ""Cosine"",

  },

  quantization_config: {

    binary: {

      always_ram: true,

    },

  },

});

```
use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,

        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,

        VectorParams, VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".to_string(),
use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,

        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,

        VectorParams, VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".to_string(),",0.2564102564102564,0.4225352112676056,0.3191489361702127,0.09482758620689655,0.15714285714285714,0.1182795698924731,0.17094017094017094,0.28169014084507044,0.2127659574468085,0.23076923076923078,0.38028169014084506,0.2872340425531915,0.8596932888031006,0.5761158466339111,0.6144658923149109,0.594673216342926,0.9401804208755493,0.8325751964002848,96,1.2307692307692308,0.5535008584907751
True,,2709.787,348410b6,What are the different write ordering options available in Qdrant?,"Qdrant provides three different write ordering options: weak, medium, and strong. Weak ordering does not provide any additional guarantees and allows write operations to be freely reordered. Medium ordering serializes write operations through a dynamically elected leader, which may cause minor inconsistencies in case of leader change. Strong ordering serializes write operations through the permanent leader, providing strong consistency but may result in write operations being unavailable if the leader is down.","The different write ordering options available in Qdrant are:

1. **Sequential Write Ordering:** This option ensures that write operations are executed in the same order on all replicas. It routes the operation to the leader replica of the shard and waits for the response before responding to the client. This helps avoid data inconsistency in case of concurrent updates of the same documents.

2. **Batch Write:** Qdrant supports batch writing with a specified batch size. This allows multiple write operations to be grouped together and executed efficiently.

3. **Write Consistency Factor:** This parameter can be set to specify the level of write consistency required. It determines the number of replicas that must acknowledge the write operation before it is considered successful.

By using these write ordering options, developers can ensure data consistency and reliability in Qdrant databases.","- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
location=""http://localhost:6333"",

            collection_name=""test"",

        ),

        write_config=QdrantWriteConfig(batch_size=80),

    )



if __name__ == ""__main__"":

    writer = get_writer()

    runner = LocalRunner(

        processor_config=ProcessorConfig(

            verbose=True,

            output_dir=""local-output-to-qdrant"",

            num_processes=2,

        ),

        connector_config=SimpleLocalConfig(
""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

    shard_number=6,

    replication_factor=2,

    write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
Optionally, you can disable rescoring with search `params`, which will reduce the number of disk reads even further, but potentially slightly decrease the precision.



```http

POST /collections/{collection_name}/points/search

{

    ""params"": {

        ""quantization"": {

            ""rescore"": false

        }

    },

    ""vector"": [0.2, 0.1, 0.9, 0.7],

    ""limit"": 10

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models","['- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", 'location=""http://localhost:6333"",\n\n            collection_name=""test"",\n\n        ),\n\n        write_config=QdrantWriteConfig(batch_size=80),\n\n    )\n\n\n\nif __name__ == ""__main__"":\n\n    writer = get_writer()\n\n    runner = LocalRunner(\n\n        processor_config=ProcessorConfig(\n\n            verbose=True,\n\n            output_dir=""local-output-to-qdrant"",\n\n            num_processes=2,\n\n        ),\n\n        connector_config=SimpleLocalConfig(', '""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n\n    shard_number=6,\n\n    replication_factor=2,\n\n    write_consistency_factor=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', 'Optionally, you can disable rescoring with search `params`, which will reduce the number of disk reads even further, but potentially slightly decrease the precision.\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{\n\n    ""params"": {\n\n        ""quantization"": {\n\n            ""rescore"": false\n\n        }\n\n    },\n\n    ""vector"": [0.2, 0.1, 0.9, 0.7],\n\n    ""limit"": 10\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models']","Question: What are the different write ordering options available in Qdrant?\n\nContext: - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
location=""http://localhost:6333"",

collection_name=""test"",

),

write_config=QdrantWriteConfig(batch_size=80),

)



if __name__ == ""__main__"":

writer = get_writer()

runner = LocalRunner(

processor_config=ProcessorConfig(

verbose=True,

output_dir=""local-output-to-qdrant"",

num_processes=2,

),

connector_config=SimpleLocalConfig(
""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

shard_number=6,

replication_factor=2,

write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
Optionally, you can disable rescoring with search `params`, which will reduce the number of disk reads even further, but potentially slightly decrease the precision.



```http

POST /collections/{collection_name}/points/search

{

""params"": {

""quantization"": {

""rescore"": false

}

},

""vector"": [0.2, 0.1, 0.9, 0.7],

""limit"": 10

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models\n\nAnswer:",What are the different write ordering options available in Qdrant?,"Qdrant provides three different write ordering options: weak, medium, and strong. Weak ordering does not provide any additional guarantees and allows write operations to be freely reordered. Medium ordering serializes write operations through a dynamically elected leader, which may cause minor inconsistencies in case of leader change. Strong ordering serializes write operations through the permanent leader, providing strong consistency but may result in write operations being unavailable if the leader is down.","['- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", 'location=""http://localhost:6333"",\n\n            collection_name=""test"",\n\n        ),\n\n        write_config=QdrantWriteConfig(batch_size=80),\n\n    )\n\n\n\nif __name__ == ""__main__"":\n\n    writer = get_writer()\n\n    runner = LocalRunner(\n\n        processor_config=ProcessorConfig(\n\n            verbose=True,\n\n            output_dir=""local-output-to-qdrant"",\n\n            num_processes=2,\n\n        ),\n\n        connector_config=SimpleLocalConfig(', '""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n\n    shard_number=6,\n\n    replication_factor=2,\n\n    write_consistency_factor=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', 'Optionally, you can disable rescoring with search `params`, which will reduce the number of disk reads even further, but potentially slightly decrease the precision.\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{\n\n    ""params"": {\n\n        ""quantization"": {\n\n            ""rescore"": false\n\n        }\n\n    },\n\n    ""vector"": [0.2, 0.1, 0.9, 0.7],\n\n    ""limit"": 10\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models']","- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
location=""http://localhost:6333"",

            collection_name=""test"",

        ),

        write_config=QdrantWriteConfig(batch_size=80),

    )



if __name__ == ""__main__"":

    writer = get_writer()

    runner = LocalRunner(

        processor_config=ProcessorConfig(

            verbose=True,

            output_dir=""local-output-to-qdrant"",

            num_processes=2,

        ),

        connector_config=SimpleLocalConfig(
""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

    shard_number=6,

    replication_factor=2,

    write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
Optionally, you can disable rescoring with search `params`, which will reduce the number of disk reads even further, but potentially slightly decrease the precision.



```http

POST /collections/{collection_name}/points/search

{

    ""params"": {

        ""quantization"": {

            ""rescore"": false

        }

    },

    ""vector"": [0.2, 0.1, 0.9, 0.7],

    ""limit"": 10

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models",0.24,0.43478260869565216,0.30927835051546393,0.08870967741935484,0.16176470588235295,0.11458333333333333,0.16,0.2898550724637681,0.2061855670103093,0.232,0.42028985507246375,0.29896907216494845,0.8288478255271912,0.5656962990760803,0.5831602215766907,0.5742955207824707,0.7449243089014834,0.5050804018974304,137,1.9027777777777777,0.5447012202944747
True,,2963.328,0d468f45,How can Qdrant be optimized for low memory footprint and high speed search?,"To optimize Qdrant for low memory footprint and high speed search, you can use vector quantization. This involves converting vectors into a more compact representation that can be stored in memory and used for search. By using smaller vectors, you can cache more in RAM and reduce the number of disk reads. To configure in-memory quantization with on-disk original vectors, you need to create a collection with the specified configuration.","To optimize Qdrant for low memory footprint and high-speed search, here are some strategies you can consider:

1. **Scalar Quantization**: Reduce the memory footprint by converting vectors to `uint8` for faster comparison using SIMD CPU instructions. This can significantly speed up the search process while reducing memory consumption by a factor of 4.

2. **SSD Storage**: Utilize local SSDs for storage to increase search speed. With a production-grade disk, the search speed can be further improved. Configurations that offer high IOPS (Input/Output Operations Per Second) can help serve large datasets with low search latency in Qdrant.

3. **Sharding**: Implement sharding to distribute data across multiple machines in your cluster. This allows Qdrant to optimize search queries by specifying which shard to search, minimizing overhead, and maximizing performance. By knowing where to find the data, Qdrant can avoid querying all machines in the cluster for results.

By combining these strategies and tuning Qdrant based on your specific needs, you can achieve a balance between low memory consumption and high-speed search performance.","---

title: Optimize Resources

weight: 11

aliases:

  - ../tutorials/optimize

---



# Optimize Qdrant



Different use cases have different requirements for balancing between memory, speed, and precision.

Qdrant is designed to be flexible and customizable so you can tune it to your needs.



![Trafeoff](/docs/tradeoff.png)



Let's look deeper into each of those possible optimization scenarios.



## Prefer low memory footprint with high speed search
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
With a local SSD, we were able to increase the search speed by 10x!



With the production-grade disk, the search speed could be even higher. 

Some configurations of the SSDs can reach 1M IOPS and more.



Which might be an interesting option to serve large datasets with low search latency in Qdrant.





## Conclusion



In this article, we showed that Qdrant have flexibility in terms of RAM usage and can be used to serve large datasets.
It provides configurable trade-offs between RAM usage and search speed.



We are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.

Please feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!
This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. 



### Common use cases","[""---\n\ntitle: Optimize Resources\n\nweight: 11\n\naliases:\n\n  - ../tutorials/optimize\n\n---\n\n\n\n# Optimize Qdrant\n\n\n\nDifferent use cases have different requirements for balancing between memory, speed, and precision.\n\nQdrant is designed to be flexible and customizable so you can tune it to your needs.\n\n\n\n![Trafeoff](/docs/tradeoff.png)\n\n\n\nLet's look deeper into each of those possible optimization scenarios.\n\n\n\n## Prefer low memory footprint with high speed search"", 'Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.\n\n\n\nIn addition to reducing the memory footprint, scalar quantization also speeds up the search process.\n\nQdrant uses a special SIMD CPU instruction to perform fast vector comparison.\n\nThis instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.', 'With a local SSD, we were able to increase the search speed by 10x!\n\n\n\nWith the production-grade disk, the search speed could be even higher. \n\nSome configurations of the SSDs can reach 1M IOPS and more.\n\n\n\nWhich might be an interesting option to serve large datasets with low search latency in Qdrant.\n\n\n\n\n\n## Conclusion\n\n\n\nIn this article, we showed that Qdrant have flexibility in terms of RAM usage and can be used to serve large datasets.', 'It provides configurable trade-offs between RAM usage and search speed.\n\n\n\nWe are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.\n\nPlease feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!', 'This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. \n\n\n\n### Common use cases']","Question: How can Qdrant be optimized for low memory footprint and high speed search?\n\nContext: ---

title: Optimize Resources

weight: 11

aliases:

- ../tutorials/optimize

---



# Optimize Qdrant



Different use cases have different requirements for balancing between memory, speed, and precision.

Qdrant is designed to be flexible and customizable so you can tune it to your needs.



![Trafeoff](/docs/tradeoff.png)



Let's look deeper into each of those possible optimization scenarios.



## Prefer low memory footprint with high speed search
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
With a local SSD, we were able to increase the search speed by 10x!



With the production-grade disk, the search speed could be even higher. 

Some configurations of the SSDs can reach 1M IOPS and more.



Which might be an interesting option to serve large datasets with low search latency in Qdrant.





## Conclusion



In this article, we showed that Qdrant have flexibility in terms of RAM usage and can be used to serve large datasets.
It provides configurable trade-offs between RAM usage and search speed.



We are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.

Please feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!
This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. 



### Common use cases\n\nAnswer:",How can Qdrant be optimized for low memory footprint and high speed search?,"To optimize Qdrant for low memory footprint and high speed search, you can use vector quantization. This involves converting vectors into a more compact representation that can be stored in memory and used for search. By using smaller vectors, you can cache more in RAM and reduce the number of disk reads. To configure in-memory quantization with on-disk original vectors, you need to create a collection with the specified configuration.","[""---\n\ntitle: Optimize Resources\n\nweight: 11\n\naliases:\n\n  - ../tutorials/optimize\n\n---\n\n\n\n# Optimize Qdrant\n\n\n\nDifferent use cases have different requirements for balancing between memory, speed, and precision.\n\nQdrant is designed to be flexible and customizable so you can tune it to your needs.\n\n\n\n![Trafeoff](/docs/tradeoff.png)\n\n\n\nLet's look deeper into each of those possible optimization scenarios.\n\n\n\n## Prefer low memory footprint with high speed search"", 'Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.\n\n\n\nIn addition to reducing the memory footprint, scalar quantization also speeds up the search process.\n\nQdrant uses a special SIMD CPU instruction to perform fast vector comparison.\n\nThis instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.', 'With a local SSD, we were able to increase the search speed by 10x!\n\n\n\nWith the production-grade disk, the search speed could be even higher. \n\nSome configurations of the SSDs can reach 1M IOPS and more.\n\n\n\nWhich might be an interesting option to serve large datasets with low search latency in Qdrant.\n\n\n\n\n\n## Conclusion\n\n\n\nIn this article, we showed that Qdrant have flexibility in terms of RAM usage and can be used to serve large datasets.', 'It provides configurable trade-offs between RAM usage and search speed.\n\n\n\nWe are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.\n\nPlease feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!', 'This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. \n\n\n\n### Common use cases']","---

title: Optimize Resources

weight: 11

aliases:

  - ../tutorials/optimize

---



# Optimize Qdrant



Different use cases have different requirements for balancing between memory, speed, and precision.

Qdrant is designed to be flexible and customizable so you can tune it to your needs.



![Trafeoff](/docs/tradeoff.png)



Let's look deeper into each of those possible optimization scenarios.



## Prefer low memory footprint with high speed search
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
With a local SSD, we were able to increase the search speed by 10x!



With the production-grade disk, the search speed could be even higher. 

Some configurations of the SSDs can reach 1M IOPS and more.



Which might be an interesting option to serve large datasets with low search latency in Qdrant.





## Conclusion



In this article, we showed that Qdrant have flexibility in terms of RAM usage and can be used to serve large datasets.
It provides configurable trade-offs between RAM usage and search speed.



We are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.

Please feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!
This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. 



### Common use cases",0.26506024096385544,0.6470588235294118,0.37606837606837606,0.08484848484848485,0.208955223880597,0.12068965517241381,0.16265060240963855,0.39705882352941174,0.23076923076923075,0.24096385542168675,0.5882352941176471,0.34188034188034183,0.924189031124115,0.5753461122512817,0.6389017701148987,0.6054606437683105,0.7500068232475314,0.8788410782581195,170,2.4285714285714284,0.4934941623511575
True,,5830.031,6dcc2f4a,How does Qdrant optimize memory and search speed for sparse vectors?,"Qdrant optimizes memory and search speed for sparse vectors by utilizing an inverted index structure to store vectors for each non-zero dimension. This approach allows Qdrant to efficiently represent sparse vectors, which are characterized by a high proportion of zeroes. By only storing information about non-zero dimensions, Qdrant reduces the memory footprint required to store sparse vectors and also speeds up search operations by focusing only on relevant dimensions during indexing and querying processes. This optimization ensures that Qdrant can handle sparse vectors effectively while maintaining efficient memory usage and search performance.","Qdrant optimizes memory and search speed for sparse vectors through the use of efficient indexing and specialized techniques such as scalar quantization.

1. **Support for Sparse Vectors:** Qdrant can handle sparse vectors, which have a high proportion of zeros. Instead of converting them to dense vectors, Qdrant stores vectors in an inverted index structure for each non-zero dimension. This approach optimizes memory usage by only storing information about non-zero values.

2. **Scalar Quantization:** Qdrant utilizes scalar quantization to further reduce the memory footprint of vectors. By converting vector values to `uint8`, the amount of memory needed to store a vector is reduced by a factor of 4. This reduction in memory usage not only saves space but also speeds up the search process.

3. **SIMD CPU Instruction:** Qdrant leverages a special SIMD CPU instruction that works with 8-bit integers for fast vector comparison. This instruction allows Qdrant to perform comparisons quickly, especially after applying scalar quantization.

4. **Future Enhancements:** The Qdrant team is continuously working on improving memory usage and search speed. They have plans to introduce Product Quantization in the future, which will offer even more memory savings and speed enhancements.

By incorporating these techniques and enhancements, Qdrant aims to make billion-scale vector search more affordable while maintaining efficiency in both memory usage and search speed.","performance.



## Sparse Vector Index



*Available as of v1.7.0*



### Key Features of Sparse Vector Index

- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.

- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.



### Search Mechanism

- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.
- **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.





#### Memory and speed tuning



In this section, we will discuss how to tune the memory and speed of the search process with quantization.



There are 3 possible modes to place storage of vectors within the qdrant collection:
Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
Our plan for the current [open-source roadmap](https://github.com/qdrant/qdrant/blob/master/docs/roadmap/README.md) is to make billion-scale vector search affordable. Our recent release of the [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/) improves both memory usage (x4) as well as speed (x2). Upcoming [Product Quantization](https://www.irisa.fr/texmex/people/jegou/papers/jegou_searching_with_quantization.pdf) will introduce even another option with more memory saving. Stay tuned","['performance.\n\n\n\n## Sparse Vector Index\n\n\n\n*Available as of v1.7.0*\n\n\n\n### Key Features of Sparse Vector Index\n\n- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.\n\n- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.\n\n\n\n### Search Mechanism\n\n- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.', '- **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.\n\n\n\n\n\n#### Memory and speed tuning\n\n\n\nIn this section, we will discuss how to tune the memory and speed of the search process with quantization.\n\n\n\nThere are 3 possible modes to place storage of vectors within the qdrant collection:', 'Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.', 'Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.\n\n\n\nIn addition to reducing the memory footprint, scalar quantization also speeds up the search process.\n\nQdrant uses a special SIMD CPU instruction to perform fast vector comparison.\n\nThis instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.', 'Our plan for the current [open-source roadmap](https://github.com/qdrant/qdrant/blob/master/docs/roadmap/README.md) is to make billion-scale vector search affordable. Our recent release of the [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/) improves both memory usage (x4) as well as speed (x2). Upcoming [Product Quantization](https://www.irisa.fr/texmex/people/jegou/papers/jegou_searching_with_quantization.pdf) will introduce even another option with more memory saving. Stay tuned']","Question: How does Qdrant optimize memory and search speed for sparse vectors?\n\nContext: performance.



## Sparse Vector Index



*Available as of v1.7.0*



### Key Features of Sparse Vector Index

- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.

- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.



### Search Mechanism

- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.
- **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.





#### Memory and speed tuning



In this section, we will discuss how to tune the memory and speed of the search process with quantization.



There are 3 possible modes to place storage of vectors within the qdrant collection:
Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
Our plan for the current [open-source roadmap](https://github.com/qdrant/qdrant/blob/master/docs/roadmap/README.md) is to make billion-scale vector search affordable. Our recent release of the [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/) improves both memory usage (x4) as well as speed (x2). Upcoming [Product Quantization](https://www.irisa.fr/texmex/people/jegou/papers/jegou_searching_with_quantization.pdf) will introduce even another option with more memory saving. Stay tuned\n\nAnswer:",How does Qdrant optimize memory and search speed for sparse vectors?,"Qdrant optimizes memory and search speed for sparse vectors by utilizing an inverted index structure to store vectors for each non-zero dimension. This approach allows Qdrant to efficiently represent sparse vectors, which are characterized by a high proportion of zeroes. By only storing information about non-zero dimensions, Qdrant reduces the memory footprint required to store sparse vectors and also speeds up search operations by focusing only on relevant dimensions during indexing and querying processes. This optimization ensures that Qdrant can handle sparse vectors effectively while maintaining efficient memory usage and search performance.","['performance.\n\n\n\n## Sparse Vector Index\n\n\n\n*Available as of v1.7.0*\n\n\n\n### Key Features of Sparse Vector Index\n\n- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.\n\n- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.\n\n\n\n### Search Mechanism\n\n- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.', '- **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.\n\n\n\n\n\n#### Memory and speed tuning\n\n\n\nIn this section, we will discuss how to tune the memory and speed of the search process with quantization.\n\n\n\nThere are 3 possible modes to place storage of vectors within the qdrant collection:', 'Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.', 'Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.\n\n\n\nIn addition to reducing the memory footprint, scalar quantization also speeds up the search process.\n\nQdrant uses a special SIMD CPU instruction to perform fast vector comparison.\n\nThis instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.', 'Our plan for the current [open-source roadmap](https://github.com/qdrant/qdrant/blob/master/docs/roadmap/README.md) is to make billion-scale vector search affordable. Our recent release of the [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/) improves both memory usage (x4) as well as speed (x2). Upcoming [Product Quantization](https://www.irisa.fr/texmex/people/jegou/papers/jegou_searching_with_quantization.pdf) will introduce even another option with more memory saving. Stay tuned']","performance.



## Sparse Vector Index



*Available as of v1.7.0*



### Key Features of Sparse Vector Index

- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.

- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.



### Search Mechanism

- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.
- **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.





#### Memory and speed tuning



In this section, we will discuss how to tune the memory and speed of the search process with quantization.



There are 3 possible modes to place storage of vectors within the qdrant collection:
Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
Our plan for the current [open-source roadmap](https://github.com/qdrant/qdrant/blob/master/docs/roadmap/README.md) is to make billion-scale vector search affordable. Our recent release of the [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/) improves both memory usage (x4) as well as speed (x2). Upcoming [Product Quantization](https://www.irisa.fr/texmex/people/jegou/papers/jegou_searching_with_quantization.pdf) will introduce even another option with more memory saving. Stay tuned",0.36666666666666664,0.8461538461538461,0.5116279069767441,0.22488038277511962,0.5222222222222223,0.31438127090301005,0.23333333333333334,0.5384615384615384,0.32558139534883723,0.3523809523809524,0.8131868131868132,0.4916943521594684,0.9072453379631042,0.6558045744895935,0.780479907989502,0.7127310633659363,0.6618651728145778,0.9440691056661308,217,2.358695652173913,0.3837422846544711
True,,1981.161,f8c5cb6d,How does Qdrant address the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters?,"Qdrant addresses the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters by extending the HNSW graph with additional edges based on the stored payload values. This extension allows for more efficient searching of nearby vectors using the HNSW index while applying filters during the search in the graph. By adding extra edges, Qdrant minimizes the overhead on condition checks since the conditions only need to be calculated for a small fraction of the points involved in the search. This approach improves the performance of the HNSW index in cases where the traditional methods of weak filters or complete rescore are not optimal, providing a more balanced solution for scenarios that fall in between.","Qdrant addresses the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters by allowing users to experiment with multiple values of the parameters M and EF in the indexing algorithm. This flexibility enables users to fine-tune the algorithm according to their specific use cases, optimizing performance for scenarios that fall between weak and stringent filters. Additionally, Qdrant's unique filtering capabilities and modifications to the HNSW algorithm contribute to high performance, minimal latency, and accurate results in large-scale, high-dimensional operations.","Read more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.
Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements
2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.

3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.

4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.

5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.
All right, keep going. I like it.



Rishabh Bhardwaj:

Yeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.



Demetrios:

Right.



Rishabh Bhardwaj:
.tech/articles/filtrable-hnsw/) and unique [filtering](https://qdrant.tech/documentation/concepts/filtering/) capabilities consistently lead to [highest RPS](https://qdrant.tech/benchmarks/), minimal latency, and high control with accuracy when running large-scale, high-dimensional operations.","['Read more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.', 'Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements', '2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.\n\n3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.\n\n4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.\n\n5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.', 'All right, keep going. I like it.\n\n\n\nRishabh Bhardwaj:\n\nYeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.\n\n\n\nDemetrios:\n\nRight.\n\n\n\nRishabh Bhardwaj:', '.tech/articles/filtrable-hnsw/) and unique [filtering](https://qdrant.tech/documentation/concepts/filtering/) capabilities consistently lead to [highest RPS](https://qdrant.tech/benchmarks/), minimal latency, and high control with accuracy when running large-scale, high-dimensional operations.']","Question: How does Qdrant address the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters?\n\nContext: Read more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.
Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements
2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.

3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.

4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.

5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.
All right, keep going. I like it.



Rishabh Bhardwaj:

Yeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.



Demetrios:

Right.



Rishabh Bhardwaj:
.tech/articles/filtrable-hnsw/) and unique [filtering](https://qdrant.tech/documentation/concepts/filtering/) capabilities consistently lead to [highest RPS](https://qdrant.tech/benchmarks/), minimal latency, and high control with accuracy when running large-scale, high-dimensional operations.\n\nAnswer:",How does Qdrant address the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters?,"Qdrant addresses the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters by extending the HNSW graph with additional edges based on the stored payload values. This extension allows for more efficient searching of nearby vectors using the HNSW index while applying filters during the search in the graph. By adding extra edges, Qdrant minimizes the overhead on condition checks since the conditions only need to be calculated for a small fraction of the points involved in the search. This approach improves the performance of the HNSW index in cases where the traditional methods of weak filters or complete rescore are not optimal, providing a more balanced solution for scenarios that fall in between.","['Read more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.', 'Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements', '2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.\n\n3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.\n\n4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.\n\n5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.', 'All right, keep going. I like it.\n\n\n\nRishabh Bhardwaj:\n\nYeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.\n\n\n\nDemetrios:\n\nRight.\n\n\n\nRishabh Bhardwaj:', '.tech/articles/filtrable-hnsw/) and unique [filtering](https://qdrant.tech/documentation/concepts/filtering/) capabilities consistently lead to [highest RPS](https://qdrant.tech/benchmarks/), minimal latency, and high control with accuracy when running large-scale, high-dimensional operations.']","Read more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.
Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements
2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.

3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.

4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.

5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.
All right, keep going. I like it.



Rishabh Bhardwaj:

Yeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.



Demetrios:

Right.



Rishabh Bhardwaj:
.tech/articles/filtrable-hnsw/) and unique [filtering](https://qdrant.tech/documentation/concepts/filtering/) capabilities consistently lead to [highest RPS](https://qdrant.tech/benchmarks/), minimal latency, and high control with accuracy when running large-scale, high-dimensional operations.",0.5176470588235295,0.41509433962264153,0.46073298429319376,0.25,0.2,0.22222222222222224,0.36470588235294116,0.29245283018867924,0.3246073298429319,0.4823529411764706,0.3867924528301887,0.42931937172774864,0.8385659456253052,0.6883314251899719,0.6703416109085083,0.6792173981666565,0.9773778031812981,0.8409516985993832,89,0.7235772357723578,0.5117696918476222
True,,4022.2090000000003,2be5ae43,How is metric learning utilized in addressing the challenge of detecting anomalies in coffee beans?,"Metric learning is utilized in addressing the challenge of detecting anomalies in coffee beans by encoding images in an n-dimensional vector space and using learned similarities to label images during the inference process. This approach involves representing the images in a vector space where similar images are closer together in the space. By doing so, the model can effectively classify and detect anomalies in coffee beans based on the similarities learned during training. The KNN (K-Nearest Neighbors) classification method is commonly used in this approach as it simplifies the process of determining the class label of an image based on the labels of its nearest neighbors in the vector space. This method allows for the adaptation to new types of defects and changing shooting conditions, making the model more robust and accurate in detecting anomalies in coffee beans.","In the context of detecting anomalies in coffee beans, metric learning is utilized in the following ways:

1. **Utilizing Unlabeled Data**: Metric learning allows for leveraging unlabeled data, which is beneficial as labeling data can be time-consuming and expensive. By learning a suitable metric space, anomalies can be detected using patterns in the data without the need for explicit labels.

2. **Adapting to Changing Requirements**: The relevant metrics, such as precision or recall, can be adjusted according to changing requirements during the inference phase without the need for re-training the model. This flexibility is crucial in detecting anomalies in coffee beans, where new types of defects may emerge, and shooting conditions may vary over time.

3. **Dynamic Updating of Classifier**: Queries labeled with a high score can be dynamically added to the KNN (K-Nearest Neighbors) classifier on the fly as new data points are collected. This dynamic updating mechanism ensures that the anomaly detection model remains up-to-date with the latest data and can adapt to new anomalies in coffee bean quality.

By utilizing metric learning in the anomaly detection process for coffee beans, it becomes possible to improve the efficiency of anomaly detection while reducing the reliance on extensive labeled data sets.","---

title: Metric Learning for Anomaly Detection

short_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""

description: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.

social_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg

preview_dir: /articles_data/detecting-coffee-anomalies/preview
{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}



We should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it's own.

In the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.



Let's find out how metric learning might help to address this challenge.



## Metric Learning Approach
{{< figure src=/articles_data/detecting-coffee-anomalies/anomalies_detection.png caption=""Production deployment"" >}}



This approach has the following advantages:



* We can benefit from unlabeled data, considering labeling is time-consuming and expensive.

* The relevant metric, e.g., precision or recall, can be tuned according to changing requirements during the inference without re-training.

* Queries labeled with a high score can be added to the KNN classifier on the fly as new data points.
We will describe our approach in the further articles.



This time it converged smoothly, and our evaluation metrics also improved considerably to match the supervised classification approach.



{{< figure src=/articles_data/detecting-coffee-anomalies/ae_report_knn.png caption=""Metrics for the autoencoder model with KNN classifier"" >}}



{{< figure src=/articles_data/detecting-coffee-anomalies/ft_report_knn.png caption=""Metrics for the finetuned model with KNN classifier"" >}}
---

title: Anomalies Detection

icon: bot

tabid: anomalies

image: /content/images/solutions/anomaly_detection.svg

image_caption: Automated FAQ

default_link: /articles/detecting-coffee-anomalies/

default_link_name: 

weight: 60

draft: false

short_description: |

    Anomaly detection is one of the non-obvious applications of Similarity Learning.

    However, it has a number of properties that make it an excellent way to [approach anomaly detection](/articles/detecting-coffee-anomalies/).","['---\n\ntitle: Metric Learning for Anomaly Detection\n\nshort_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""\n\ndescription: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.\n\nsocial_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg\n\npreview_dir: /articles_data/detecting-coffee-anomalies/preview', '{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}\n\n\n\nWe should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it\'s own.\n\nIn the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.\n\n\n\nLet\'s find out how metric learning might help to address this challenge.\n\n\n\n## Metric Learning Approach', '{{< figure src=/articles_data/detecting-coffee-anomalies/anomalies_detection.png caption=""Production deployment"" >}}\n\n\n\nThis approach has the following advantages:\n\n\n\n* We can benefit from unlabeled data, considering labeling is time-consuming and expensive.\n\n* The relevant metric, e.g., precision or recall, can be tuned according to changing requirements during the inference without re-training.\n\n* Queries labeled with a high score can be added to the KNN classifier on the fly as new data points.', 'We will describe our approach in the further articles.\n\n\n\nThis time it converged smoothly, and our evaluation metrics also improved considerably to match the supervised classification approach.\n\n\n\n{{< figure src=/articles_data/detecting-coffee-anomalies/ae_report_knn.png caption=""Metrics for the autoencoder model with KNN classifier"" >}}\n\n\n\n{{< figure src=/articles_data/detecting-coffee-anomalies/ft_report_knn.png caption=""Metrics for the finetuned model with KNN classifier"" >}}', '---\n\ntitle: Anomalies Detection\n\nicon: bot\n\ntabid: anomalies\n\nimage: /content/images/solutions/anomaly_detection.svg\n\nimage_caption: Automated FAQ\n\ndefault_link: /articles/detecting-coffee-anomalies/\n\ndefault_link_name: \n\nweight: 60\n\ndraft: false\n\nshort_description: |\n\n    Anomaly detection is one of the non-obvious applications of Similarity Learning.\n\n    However, it has a number of properties that make it an excellent way to [approach anomaly detection](/articles/detecting-coffee-anomalies/).']","Question: How is metric learning utilized in addressing the challenge of detecting anomalies in coffee beans?\n\nContext: ---

title: Metric Learning for Anomaly Detection

short_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""

description: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.

social_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg

preview_dir: /articles_data/detecting-coffee-anomalies/preview
{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}



We should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it's own.

In the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.



Let's find out how metric learning might help to address this challenge.



## Metric Learning Approach
{{< figure src=/articles_data/detecting-coffee-anomalies/anomalies_detection.png caption=""Production deployment"" >}}



This approach has the following advantages:



* We can benefit from unlabeled data, considering labeling is time-consuming and expensive.

* The relevant metric, e.g., precision or recall, can be tuned according to changing requirements during the inference without re-training.

* Queries labeled with a high score can be added to the KNN classifier on the fly as new data points.
We will describe our approach in the further articles.



This time it converged smoothly, and our evaluation metrics also improved considerably to match the supervised classification approach.



{{< figure src=/articles_data/detecting-coffee-anomalies/ae_report_knn.png caption=""Metrics for the autoencoder model with KNN classifier"" >}}



{{< figure src=/articles_data/detecting-coffee-anomalies/ft_report_knn.png caption=""Metrics for the finetuned model with KNN classifier"" >}}
---

title: Anomalies Detection

icon: bot

tabid: anomalies

image: /content/images/solutions/anomaly_detection.svg

image_caption: Automated FAQ

default_link: /articles/detecting-coffee-anomalies/

default_link_name: 

weight: 60

draft: false

short_description: |

Anomaly detection is one of the non-obvious applications of Similarity Learning.

However, it has a number of properties that make it an excellent way to [approach anomaly detection](/articles/detecting-coffee-anomalies/).\n\nAnswer:",How is metric learning utilized in addressing the challenge of detecting anomalies in coffee beans?,"Metric learning is utilized in addressing the challenge of detecting anomalies in coffee beans by encoding images in an n-dimensional vector space and using learned similarities to label images during the inference process. This approach involves representing the images in a vector space where similar images are closer together in the space. By doing so, the model can effectively classify and detect anomalies in coffee beans based on the similarities learned during training. The KNN (K-Nearest Neighbors) classification method is commonly used in this approach as it simplifies the process of determining the class label of an image based on the labels of its nearest neighbors in the vector space. This method allows for the adaptation to new types of defects and changing shooting conditions, making the model more robust and accurate in detecting anomalies in coffee beans.","['---\n\ntitle: Metric Learning for Anomaly Detection\n\nshort_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""\n\ndescription: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.\n\nsocial_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg\n\npreview_dir: /articles_data/detecting-coffee-anomalies/preview', '{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}\n\n\n\nWe should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it\'s own.\n\nIn the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.\n\n\n\nLet\'s find out how metric learning might help to address this challenge.\n\n\n\n## Metric Learning Approach', '{{< figure src=/articles_data/detecting-coffee-anomalies/anomalies_detection.png caption=""Production deployment"" >}}\n\n\n\nThis approach has the following advantages:\n\n\n\n* We can benefit from unlabeled data, considering labeling is time-consuming and expensive.\n\n* The relevant metric, e.g., precision or recall, can be tuned according to changing requirements during the inference without re-training.\n\n* Queries labeled with a high score can be added to the KNN classifier on the fly as new data points.', 'We will describe our approach in the further articles.\n\n\n\nThis time it converged smoothly, and our evaluation metrics also improved considerably to match the supervised classification approach.\n\n\n\n{{< figure src=/articles_data/detecting-coffee-anomalies/ae_report_knn.png caption=""Metrics for the autoencoder model with KNN classifier"" >}}\n\n\n\n{{< figure src=/articles_data/detecting-coffee-anomalies/ft_report_knn.png caption=""Metrics for the finetuned model with KNN classifier"" >}}', '---\n\ntitle: Anomalies Detection\n\nicon: bot\n\ntabid: anomalies\n\nimage: /content/images/solutions/anomaly_detection.svg\n\nimage_caption: Automated FAQ\n\ndefault_link: /articles/detecting-coffee-anomalies/\n\ndefault_link_name: \n\nweight: 60\n\ndraft: false\n\nshort_description: |\n\n    Anomaly detection is one of the non-obvious applications of Similarity Learning.\n\n    However, it has a number of properties that make it an excellent way to [approach anomaly detection](/articles/detecting-coffee-anomalies/).']","---

title: Metric Learning for Anomaly Detection

short_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""

description: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.

social_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg

preview_dir: /articles_data/detecting-coffee-anomalies/preview
{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}



We should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it's own.

In the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.



Let's find out how metric learning might help to address this challenge.



## Metric Learning Approach
{{< figure src=/articles_data/detecting-coffee-anomalies/anomalies_detection.png caption=""Production deployment"" >}}



This approach has the following advantages:



* We can benefit from unlabeled data, considering labeling is time-consuming and expensive.

* The relevant metric, e.g., precision or recall, can be tuned according to changing requirements during the inference without re-training.

* Queries labeled with a high score can be added to the KNN classifier on the fly as new data points.
We will describe our approach in the further articles.



This time it converged smoothly, and our evaluation metrics also improved considerably to match the supervised classification approach.



{{< figure src=/articles_data/detecting-coffee-anomalies/ae_report_knn.png caption=""Metrics for the autoencoder model with KNN classifier"" >}}



{{< figure src=/articles_data/detecting-coffee-anomalies/ft_report_knn.png caption=""Metrics for the finetuned model with KNN classifier"" >}}
---

title: Anomalies Detection

icon: bot

tabid: anomalies

image: /content/images/solutions/anomaly_detection.svg

image_caption: Automated FAQ

default_link: /articles/detecting-coffee-anomalies/

default_link_name: 

weight: 60

draft: false

short_description: |

    Anomaly detection is one of the non-obvious applications of Similarity Learning.

    However, it has a number of properties that make it an excellent way to [approach anomaly detection](/articles/detecting-coffee-anomalies/).",0.37894736842105264,0.5806451612903226,0.45859872611464975,0.14814814814814814,0.22764227642276422,0.17948717948717946,0.1736842105263158,0.2661290322580645,0.21019108280254778,0.3526315789473684,0.5403225806451613,0.42675159235668786,0.7384087443351746,0.610741376876831,0.6517077088356018,0.6305598616600037,0.7863716038293205,0.8015568141825498,202,1.463768115942029,0.5735324855381473
True,,3944.241,4018ac95,How can one reproduce the benchmark for Open Source vector databases?,"To reproduce the benchmark for Open Source vector databases, one can access the source code available on Github at https://github.com/qdrant/vector-db-benchmark. The repository contains a `README.md` file that provides detailed instructions on how to run the benchmark for a specific engine. By following the guidelines outlined in the README file, individuals can easily replicate the benchmarks conducted on the Open Source vector databases. Additionally, the document emphasizes the importance of transparency in the benchmarking process and invites contributions from individuals who may be able to provide insights, identify misconfigurations, or suggest improvements. Interested parties can contribute to the benchmarking efforts by accessing the benchmark repository at https://github.com/qdrant/vector-db-benchmark.","To reproduce the benchmark for Open Source vector databases, you can follow these steps:

1. **Access the Source Code:** The source code for the benchmark is available on [Github](https://github.com/qdrant/vector-db-benchmark).

2. **Review the README File:** The repository includes a `README.md` file that describes the process of running the benchmark for a specific engine. Make sure to read and understand the instructions provided in the README file.

3. **Run the Benchmark:** Follow the instructions in the README file to run the benchmark for the specific engine you are interested in testing. This process may involve setting up configurations, running tests, and analyzing the results.

4. **Contribute and Improve:** If you are interested in contributing to the benchmarking project or improving the performance of vector databases, you can actively participate in the development process. Contributions are welcome, and your involvement can help enhance the accuracy and reliability of the benchmarks.

5. **Experiment and Test:** Feel free to experiment with different configurations, test various combinations, and explore ways to optimize the benchmarking process. By actively engaging with the benchmarking framework, you can contribute to the continuous improvement of performance evaluations for vector databases.

By following these steps, you can reproduce the benchmark for Open Source vector databases and contribute to the ongoing efforts to enhance the performance of vector search engines and databases.","That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.



This is not the final list, and we’ll continue benchmarking as many different engines as possible.



## How to reproduce the benchmark?



The source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.
If you're interested in testing the benchmark yourself or want to contribute to its development, head over to our [benchmark repository](https://github.com/qdrant/vector-db-benchmark). We appreciate your support and involvement in improving the performance of vector databases.
However, we tried our best, kept scrolling the docs up and down, experimented with combinations of different configurations, and gave all of them an equal chance to stand out. If you believe you can do it better than us, our **benchmarks are fully [open-sourced](https://github.com/qdrant/vector-db-benchmark), and contributions are welcome**!





## What do we measure?



There are several factors considered while deciding on which database to use.
---

title: Vector Database Benchmarks

description: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.

keywords:

  - vector databases comparative benchmark

  - ANN Benchmark

  - Qdrant vs Milvus

  - Qdrant vs Weaviate

  - Qdrant vs Redis

  - Qdrant vs ElasticSearch

  - benchmark

  - performance

  - latency

  - RPS

  - comparison

  - vector search

  - embedding

preview_image: /benchmarks/benchmark-1.png
---

draft: false

id: 2

title: How vector search should be benchmarked?

weight: 1

---



# Benchmarking Vector Databases","['That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.\n\n\n\nThis is not the final list, and we’ll continue benchmarking as many different engines as possible.\n\n\n\n## How to reproduce the benchmark?\n\n\n\nThe source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.', ""If you're interested in testing the benchmark yourself or want to contribute to its development, head over to our [benchmark repository](https://github.com/qdrant/vector-db-benchmark). We appreciate your support and involvement in improving the performance of vector databases."", 'However, we tried our best, kept scrolling the docs up and down, experimented with combinations of different configurations, and gave all of them an equal chance to stand out. If you believe you can do it better than us, our **benchmarks are fully [open-sourced](https://github.com/qdrant/vector-db-benchmark), and contributions are welcome**!\n\n\n\n\n\n## What do we measure?\n\n\n\nThere are several factors considered while deciding on which database to use.', '---\n\ntitle: Vector Database Benchmarks\n\ndescription: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.\n\nkeywords:\n\n  - vector databases comparative benchmark\n\n  - ANN Benchmark\n\n  - Qdrant vs Milvus\n\n  - Qdrant vs Weaviate\n\n  - Qdrant vs Redis\n\n  - Qdrant vs ElasticSearch\n\n  - benchmark\n\n  - performance\n\n  - latency\n\n  - RPS\n\n  - comparison\n\n  - vector search\n\n  - embedding\n\npreview_image: /benchmarks/benchmark-1.png', '---\n\ndraft: false\n\nid: 2\n\ntitle: How vector search should be benchmarked?\n\nweight: 1\n\n---\n\n\n\n# Benchmarking Vector Databases']","Question: How can one reproduce the benchmark for Open Source vector databases?\n\nContext: That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.



This is not the final list, and we’ll continue benchmarking as many different engines as possible.



## How to reproduce the benchmark?



The source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.
If you're interested in testing the benchmark yourself or want to contribute to its development, head over to our [benchmark repository](https://github.com/qdrant/vector-db-benchmark). We appreciate your support and involvement in improving the performance of vector databases.
However, we tried our best, kept scrolling the docs up and down, experimented with combinations of different configurations, and gave all of them an equal chance to stand out. If you believe you can do it better than us, our **benchmarks are fully [open-sourced](https://github.com/qdrant/vector-db-benchmark), and contributions are welcome**!





## What do we measure?



There are several factors considered while deciding on which database to use.
---

title: Vector Database Benchmarks

description: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.

keywords:

- vector databases comparative benchmark

- ANN Benchmark

- Qdrant vs Milvus

- Qdrant vs Weaviate

- Qdrant vs Redis

- Qdrant vs ElasticSearch

- benchmark

- performance

- latency

- RPS

- comparison

- vector search

- embedding

preview_image: /benchmarks/benchmark-1.png
---

draft: false

id: 2

title: How vector search should be benchmarked?

weight: 1

---



# Benchmarking Vector Databases\n\nAnswer:",How can one reproduce the benchmark for Open Source vector databases?,"To reproduce the benchmark for Open Source vector databases, one can access the source code available on Github at https://github.com/qdrant/vector-db-benchmark. The repository contains a `README.md` file that provides detailed instructions on how to run the benchmark for a specific engine. By following the guidelines outlined in the README file, individuals can easily replicate the benchmarks conducted on the Open Source vector databases. Additionally, the document emphasizes the importance of transparency in the benchmarking process and invites contributions from individuals who may be able to provide insights, identify misconfigurations, or suggest improvements. Interested parties can contribute to the benchmarking efforts by accessing the benchmark repository at https://github.com/qdrant/vector-db-benchmark.","['That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.\n\n\n\nThis is not the final list, and we’ll continue benchmarking as many different engines as possible.\n\n\n\n## How to reproduce the benchmark?\n\n\n\nThe source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.', ""If you're interested in testing the benchmark yourself or want to contribute to its development, head over to our [benchmark repository](https://github.com/qdrant/vector-db-benchmark). We appreciate your support and involvement in improving the performance of vector databases."", 'However, we tried our best, kept scrolling the docs up and down, experimented with combinations of different configurations, and gave all of them an equal chance to stand out. If you believe you can do it better than us, our **benchmarks are fully [open-sourced](https://github.com/qdrant/vector-db-benchmark), and contributions are welcome**!\n\n\n\n\n\n## What do we measure?\n\n\n\nThere are several factors considered while deciding on which database to use.', '---\n\ntitle: Vector Database Benchmarks\n\ndescription: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.\n\nkeywords:\n\n  - vector databases comparative benchmark\n\n  - ANN Benchmark\n\n  - Qdrant vs Milvus\n\n  - Qdrant vs Weaviate\n\n  - Qdrant vs Redis\n\n  - Qdrant vs ElasticSearch\n\n  - benchmark\n\n  - performance\n\n  - latency\n\n  - RPS\n\n  - comparison\n\n  - vector search\n\n  - embedding\n\npreview_image: /benchmarks/benchmark-1.png', '---\n\ndraft: false\n\nid: 2\n\ntitle: How vector search should be benchmarked?\n\nweight: 1\n\n---\n\n\n\n# Benchmarking Vector Databases']","That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.



This is not the final list, and we’ll continue benchmarking as many different engines as possible.



## How to reproduce the benchmark?



The source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.
If you're interested in testing the benchmark yourself or want to contribute to its development, head over to our [benchmark repository](https://github.com/qdrant/vector-db-benchmark). We appreciate your support and involvement in improving the performance of vector databases.
However, we tried our best, kept scrolling the docs up and down, experimented with combinations of different configurations, and gave all of them an equal chance to stand out. If you believe you can do it better than us, our **benchmarks are fully [open-sourced](https://github.com/qdrant/vector-db-benchmark), and contributions are welcome**!





## What do we measure?



There are several factors considered while deciding on which database to use.
---

title: Vector Database Benchmarks

description: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.

keywords:

  - vector databases comparative benchmark

  - ANN Benchmark

  - Qdrant vs Milvus

  - Qdrant vs Weaviate

  - Qdrant vs Redis

  - Qdrant vs ElasticSearch

  - benchmark

  - performance

  - latency

  - RPS

  - comparison

  - vector search

  - embedding

preview_image: /benchmarks/benchmark-1.png
---

draft: false

id: 2

title: How vector search should be benchmarked?

weight: 1

---



# Benchmarking Vector Databases",0.3333333333333333,0.6346153846153846,0.4370860927152318,0.17766497461928935,0.33980582524271846,0.23333333333333334,0.26262626262626265,0.5,0.34437086092715236,0.3181818181818182,0.6057692307692307,0.4172185430463576,0.9327653646469116,0.6596009731292725,0.7060518860816956,0.6820364594459534,0.673567791415068,0.9917265790863894,221,2.0462962962962963,0.6202044865009908
True,,2869.691,9e3773f3,What is the purpose of the `ordering` parameter in Qdrant and when should it be used?,"The `ordering` parameter in Qdrant is used with update and delete operations to ensure that the operations are executed in the same order on all replicas. When this option is enabled, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This is beneficial in preventing data inconsistency that may arise from concurrent updates of the same documents. The `ordering` parameter is particularly recommended when read operations are more frequent than updates and when search performance is critical. By enforcing a specific order of operations across replicas, the `ordering` parameter helps maintain data consistency and integrity in distributed environments.","The purpose of the `ordering` parameter in Qdrant is to ensure that update and delete operations are executed in the same order on all replicas. When using the `ordering` parameter with operations, Qdrant routes the operation to the leader replica of the shard and waits for the response before responding to the client. This helps to avoid data inconsistency in cases of concurrent updates of the same documents.

The `ordering` parameter should be used when maintaining data consistency across replicas is crucial, especially in scenarios where concurrent updates to the same documents are expected. By enforcing a strict order of operations, the `ordering` parameter helps prevent conflicts and ensures that all replicas have the same updated data at any given time.","- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
Payload = { [""city""] = ""blue"" }

		}

	},

	ordering: WriteOrderingType.Strong

);

```



## Listener mode



<aside role=""alert"">This is an experimental feature, its behavior may change in the future.</aside>



In some cases it might be useful to have a Qdrant node that only accumulates data and does not participate in search operations.

There are several scenarios where this can be useful:
WITH ENGINE = ""qdrant"",

PARAMETERS = {

    ""location"": "":memory:"",

    ""collection_config"": {

        ""size"": 386,

        ""distance"": ""Cosine""

    }

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.","['- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', 'Payload = { [""city""] = ""blue"" }\n\n\t\t}\n\n\t},\n\n\tordering: WriteOrderingType.Strong\n\n);\n\n```\n\n\n\n## Listener mode\n\n\n\n<aside role=""alert"">This is an experimental feature, its behavior may change in the future.</aside>\n\n\n\nIn some cases it might be useful to have a Qdrant node that only accumulates data and does not participate in search operations.\n\nThere are several scenarios where this can be useful:', 'WITH ENGINE = ""qdrant"",\n\nPARAMETERS = {\n\n    ""location"": "":memory:"",\n\n    ""collection_config"": {\n\n        ""size"": 386,\n\n        ""distance"": ""Cosine""\n\n    }\n\n}\n\n```\n\n\n\nThe available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).\n\n\n\n## Creating a new table', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", '. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.']","Question: What is the purpose of the `ordering` parameter in Qdrant and when should it be used?\n\nContext: - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
Payload = { [""city""] = ""blue"" }

}

},

ordering: WriteOrderingType.Strong

);

```



## Listener mode



<aside role=""alert"">This is an experimental feature, its behavior may change in the future.</aside>



In some cases it might be useful to have a Qdrant node that only accumulates data and does not participate in search operations.

There are several scenarios where this can be useful:
WITH ENGINE = ""qdrant"",

PARAMETERS = {

""location"": "":memory:"",

""collection_config"": {

""size"": 386,

""distance"": ""Cosine""

}

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.\n\nAnswer:",What is the purpose of the `ordering` parameter in Qdrant and when should it be used?,"The `ordering` parameter in Qdrant is used with update and delete operations to ensure that the operations are executed in the same order on all replicas. When this option is enabled, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This is beneficial in preventing data inconsistency that may arise from concurrent updates of the same documents. The `ordering` parameter is particularly recommended when read operations are more frequent than updates and when search performance is critical. By enforcing a specific order of operations across replicas, the `ordering` parameter helps maintain data consistency and integrity in distributed environments.","['- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', 'Payload = { [""city""] = ""blue"" }\n\n\t\t}\n\n\t},\n\n\tordering: WriteOrderingType.Strong\n\n);\n\n```\n\n\n\n## Listener mode\n\n\n\n<aside role=""alert"">This is an experimental feature, its behavior may change in the future.</aside>\n\n\n\nIn some cases it might be useful to have a Qdrant node that only accumulates data and does not participate in search operations.\n\nThere are several scenarios where this can be useful:', 'WITH ENGINE = ""qdrant"",\n\nPARAMETERS = {\n\n    ""location"": "":memory:"",\n\n    ""collection_config"": {\n\n        ""size"": 386,\n\n        ""distance"": ""Cosine""\n\n    }\n\n}\n\n```\n\n\n\nThe available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).\n\n\n\n## Creating a new table', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", '. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.']","- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
Payload = { [""city""] = ""blue"" }

		}

	},

	ordering: WriteOrderingType.Strong

);

```



## Listener mode



<aside role=""alert"">This is an experimental feature, its behavior may change in the future.</aside>



In some cases it might be useful to have a Qdrant node that only accumulates data and does not participate in search operations.

There are several scenarios where this can be useful:
WITH ENGINE = ""qdrant"",

PARAMETERS = {

    ""location"": "":memory:"",

    ""collection_config"": {

        ""size"": 386,

        ""distance"": ""Cosine""

    }

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.",0.6792452830188679,0.7272727272727273,0.7024390243902439,0.45714285714285713,0.4897959183673469,0.4729064039408867,0.5188679245283019,0.5555555555555556,0.5365853658536586,0.6415094339622641,0.6868686868686869,0.6634146341463415,0.9552024602890015,0.7770710587501526,0.7801331877708435,0.7785991430282593,0.9962016836856492,0.815013277399703,121,1.09009009009009,0.5268187598825261
True,,5326.782999999999,231df55a,How can you enable API key based authentication in Qdrant instances for security purposes?,"To enable API key based authentication in Qdrant instances for security purposes, you must specify a key in the configuration. This can be done by adding the following code snippet to your configuration file:

```yaml
service:
  # Set an api-key.
  # If set, all requests must include a header with the api-key.
  # example header: `api-key: <API-KEY>`
  #
  # If you enable this you should also enable TLS.
  # (Either above or via an external service like nginx.)
  # Sending an api-key over an unencrypted channel is insecure.
  api_key: your_secret_api_key_here
```

By adding this configuration, all requests to your Qdrant instance must include a header with the specified API key. This simple form of client authentication helps secure your instance and is available starting from version 1.2.0","To enable API key based authentication in Qdrant instances for security purposes, you can follow these steps:

1. Specify the API key in the configuration file:

```yaml
service:
  api_key: your_secret_api_key_here
```

2. Alternatively, you can set the API key using an environment variable:

```bash
export QDRANT__SERVICE__API_KEY=your_secret_api_key_here
```

3. Ensure that TLS is enabled to prevent leaking the API key over an unencrypted connection.

4. When making requests to your Qdrant instance, include the API key in the request header. For example, using curl:

```bash
curl -X GET https://localhost:6333 --header 'api-key: your_secret_api_key_here'
```

5. When using official Qdrant clients for Python, Go, Rust, .NET, or Java, you can pass the API key as a parameter. For example, in Python:

```python
from qdrant_client import QdrantClient

client = QdrantClient(
    url=""https://localhost"",
    port=6333,
    api_key=""your_secret_api_key_here""
)
```

By following these steps, you can enable API key based authentication in your Qdrant instance, enhancing its security.","This can be used to secure your instance.



To enable API key based authentication in your own Qdrant instance you must

specify a key in the configuration:



```yaml

service:

  # Set an api-key.

  # If set, all requests must include a header with the api-key.

  # example header: `api-key: <API-KEY>`

  #

  # If you enable this you should also enable TLS.

  # (Either above or via an external service like nginx.)

  # Sending an api-key over an unencrypted channel is insecure.
api_key: your_secret_api_key_here

```



Or alternatively, you can use the environment variable:



```bash

export QDRANT__SERVICE__API_KEY=your_secret_api_key_here

```



<aside role=""alert""><a href=""#tls"">TLS</a> must be used to prevent leaking the API key over an unencrypted connection.</aside>



For using API key based authentication in Qdrant cloud see the cloud

[Authentication](https://qdrant.tech/documentation/cloud/authentication)

section.
## Authenticate via SDK



Now that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.

Our official Qdrant clients for Python, TypeScript, Go, Rust, .NET and Java all support the API key parameter. 



```bash

curl \

  -X GET https://xyz-example.eu-central.aws.cloud.qdrant.io:6333 \

  --header 'api-key: <provide-your-own-key>'



# Alternatively, you can use the `Authorization` header with the `Bearer` prefix

curl \
section.



The API key then needs to be present in all REST or gRPC requests to your instance.

All official Qdrant clients for Python, Go, Rust, .NET and Java support the API key parameter.



<!---

Examples with clients

-->



```bash

curl \

  -X GET https://localhost:6333 \

  --header 'api-key: your_secret_api_key_here'

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(

    url=""https://localhost"",

    port=6333,

    api_key=""your_secret_api_key_here"",

)

```
```

> **Note:** The API key needs to be present in the request header every time you make a request via Rest or gRPC interface.



## Step 3: Authenticate via SDK



Now that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.

Our official Qdrant clients for Python, TypeScript, Go, Rust, and .NET all support the API key parameter. 



```python

from qdrant_client import QdrantClient



qdrant_client = QdrantClient(","['This can be used to secure your instance.\n\n\n\nTo enable API key based authentication in your own Qdrant instance you must\n\nspecify a key in the configuration:\n\n\n\n```yaml\n\nservice:\n\n  # Set an api-key.\n\n  # If set, all requests must include a header with the api-key.\n\n  # example header: `api-key: <API-KEY>`\n\n  #\n\n  # If you enable this you should also enable TLS.\n\n  # (Either above or via an external service like nginx.)\n\n  # Sending an api-key over an unencrypted channel is insecure.', 'api_key: your_secret_api_key_here\n\n```\n\n\n\nOr alternatively, you can use the environment variable:\n\n\n\n```bash\n\nexport QDRANT__SERVICE__API_KEY=your_secret_api_key_here\n\n```\n\n\n\n<aside role=""alert""><a href=""#tls"">TLS</a> must be used to prevent leaking the API key over an unencrypted connection.</aside>\n\n\n\nFor using API key based authentication in Qdrant cloud see the cloud\n\n[Authentication](https://qdrant.tech/documentation/cloud/authentication)\n\nsection.', ""## Authenticate via SDK\n\n\n\nNow that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.\n\nOur official Qdrant clients for Python, TypeScript, Go, Rust, .NET and Java all support the API key parameter. \n\n\n\n```bash\n\ncurl \\\n\n  -X GET https://xyz-example.eu-central.aws.cloud.qdrant.io:6333 \\\n\n  --header 'api-key: <provide-your-own-key>'\n\n\n\n# Alternatively, you can use the `Authorization` header with the `Bearer` prefix\n\ncurl \\"", 'section.\n\n\n\nThe API key then needs to be present in all REST or gRPC requests to your instance.\n\nAll official Qdrant clients for Python, Go, Rust, .NET and Java support the API key parameter.\n\n\n\n<!---\n\nExamples with clients\n\n-->\n\n\n\n```bash\n\ncurl \\\n\n  -X GET https://localhost:6333 \\\n\n  --header \'api-key: your_secret_api_key_here\'\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(\n\n    url=""https://localhost"",\n\n    port=6333,\n\n    api_key=""your_secret_api_key_here"",\n\n)\n\n```', '```\n\n> **Note:** The API key needs to be present in the request header every time you make a request via Rest or gRPC interface.\n\n\n\n## Step 3: Authenticate via SDK\n\n\n\nNow that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.\n\nOur official Qdrant clients for Python, TypeScript, Go, Rust, and .NET all support the API key parameter. \n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nqdrant_client = QdrantClient(']","Question: How can you enable API key based authentication in Qdrant instances for security purposes?\n\nContext: This can be used to secure your instance.



To enable API key based authentication in your own Qdrant instance you must

specify a key in the configuration:



```yaml

service:

# Set an api-key.

# If set, all requests must include a header with the api-key.

# example header: `api-key: <API-KEY>`

#

# If you enable this you should also enable TLS.

# (Either above or via an external service like nginx.)

# Sending an api-key over an unencrypted channel is insecure.
api_key: your_secret_api_key_here

```



Or alternatively, you can use the environment variable:



```bash

export QDRANT__SERVICE__API_KEY=your_secret_api_key_here

```



<aside role=""alert""><a href=""#tls"">TLS</a> must be used to prevent leaking the API key over an unencrypted connection.</aside>



For using API key based authentication in Qdrant cloud see the cloud

[Authentication](https://qdrant.tech/documentation/cloud/authentication)

section.
## Authenticate via SDK



Now that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.

Our official Qdrant clients for Python, TypeScript, Go, Rust, .NET and Java all support the API key parameter. 



```bash

curl \

-X GET https://xyz-example.eu-central.aws.cloud.qdrant.io:6333 \

--header 'api-key: <provide-your-own-key>'



# Alternatively, you can use the `Authorization` header with the `Bearer` prefix

curl \
section.



The API key then needs to be present in all REST or gRPC requests to your instance.

All official Qdrant clients for Python, Go, Rust, .NET and Java support the API key parameter.



<!---

Examples with clients

-->



```bash

curl \

-X GET https://localhost:6333 \

--header 'api-key: your_secret_api_key_here'

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(

url=""https://localhost"",

port=6333,

api_key=""your_secret_api_key_here"",

)

```
```

> **Note:** The API key needs to be present in the request header every time you make a request via Rest or gRPC interface.



## Step 3: Authenticate via SDK



Now that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.

Our official Qdrant clients for Python, TypeScript, Go, Rust, and .NET all support the API key parameter. 



```python

from qdrant_client import QdrantClient



qdrant_client = QdrantClient(\n\nAnswer:",How can you enable API key based authentication in Qdrant instances for security purposes?,"To enable API key based authentication in Qdrant instances for security purposes, you must specify a key in the configuration. This can be done by adding the following code snippet to your configuration file:

```yaml
service:
  # Set an api-key.
  # If set, all requests must include a header with the api-key.
  # example header: `api-key: <API-KEY>`
  #
  # If you enable this you should also enable TLS.
  # (Either above or via an external service like nginx.)
  # Sending an api-key over an unencrypted channel is insecure.
  api_key: your_secret_api_key_here
```

By adding this configuration, all requests to your Qdrant instance must include a header with the specified API key. This simple form of client authentication helps secure your instance and is available starting from version 1.2.0","['This can be used to secure your instance.\n\n\n\nTo enable API key based authentication in your own Qdrant instance you must\n\nspecify a key in the configuration:\n\n\n\n```yaml\n\nservice:\n\n  # Set an api-key.\n\n  # If set, all requests must include a header with the api-key.\n\n  # example header: `api-key: <API-KEY>`\n\n  #\n\n  # If you enable this you should also enable TLS.\n\n  # (Either above or via an external service like nginx.)\n\n  # Sending an api-key over an unencrypted channel is insecure.', 'api_key: your_secret_api_key_here\n\n```\n\n\n\nOr alternatively, you can use the environment variable:\n\n\n\n```bash\n\nexport QDRANT__SERVICE__API_KEY=your_secret_api_key_here\n\n```\n\n\n\n<aside role=""alert""><a href=""#tls"">TLS</a> must be used to prevent leaking the API key over an unencrypted connection.</aside>\n\n\n\nFor using API key based authentication in Qdrant cloud see the cloud\n\n[Authentication](https://qdrant.tech/documentation/cloud/authentication)\n\nsection.', ""## Authenticate via SDK\n\n\n\nNow that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.\n\nOur official Qdrant clients for Python, TypeScript, Go, Rust, .NET and Java all support the API key parameter. \n\n\n\n```bash\n\ncurl \\\n\n  -X GET https://xyz-example.eu-central.aws.cloud.qdrant.io:6333 \\\n\n  --header 'api-key: <provide-your-own-key>'\n\n\n\n# Alternatively, you can use the `Authorization` header with the `Bearer` prefix\n\ncurl \\"", 'section.\n\n\n\nThe API key then needs to be present in all REST or gRPC requests to your instance.\n\nAll official Qdrant clients for Python, Go, Rust, .NET and Java support the API key parameter.\n\n\n\n<!---\n\nExamples with clients\n\n-->\n\n\n\n```bash\n\ncurl \\\n\n  -X GET https://localhost:6333 \\\n\n  --header \'api-key: your_secret_api_key_here\'\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(\n\n    url=""https://localhost"",\n\n    port=6333,\n\n    api_key=""your_secret_api_key_here"",\n\n)\n\n```', '```\n\n> **Note:** The API key needs to be present in the request header every time you make a request via Rest or gRPC interface.\n\n\n\n## Step 3: Authenticate via SDK\n\n\n\nNow that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.\n\nOur official Qdrant clients for Python, TypeScript, Go, Rust, and .NET all support the API key parameter. \n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nqdrant_client = QdrantClient(']","This can be used to secure your instance.



To enable API key based authentication in your own Qdrant instance you must

specify a key in the configuration:



```yaml

service:

  # Set an api-key.

  # If set, all requests must include a header with the api-key.

  # example header: `api-key: <API-KEY>`

  #

  # If you enable this you should also enable TLS.

  # (Either above or via an external service like nginx.)

  # Sending an api-key over an unencrypted channel is insecure.
api_key: your_secret_api_key_here

```



Or alternatively, you can use the environment variable:



```bash

export QDRANT__SERVICE__API_KEY=your_secret_api_key_here

```



<aside role=""alert""><a href=""#tls"">TLS</a> must be used to prevent leaking the API key over an unencrypted connection.</aside>



For using API key based authentication in Qdrant cloud see the cloud

[Authentication](https://qdrant.tech/documentation/cloud/authentication)

section.
## Authenticate via SDK



Now that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.

Our official Qdrant clients for Python, TypeScript, Go, Rust, .NET and Java all support the API key parameter. 



```bash

curl \

  -X GET https://xyz-example.eu-central.aws.cloud.qdrant.io:6333 \

  --header 'api-key: <provide-your-own-key>'



# Alternatively, you can use the `Authorization` header with the `Bearer` prefix

curl \
section.



The API key then needs to be present in all REST or gRPC requests to your instance.

All official Qdrant clients for Python, Go, Rust, .NET and Java support the API key parameter.



<!---

Examples with clients

-->



```bash

curl \

  -X GET https://localhost:6333 \

  --header 'api-key: your_secret_api_key_here'

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(

    url=""https://localhost"",

    port=6333,

    api_key=""your_secret_api_key_here"",

)

```
```

> **Note:** The API key needs to be present in the request header every time you make a request via Rest or gRPC interface.



## Step 3: Authenticate via SDK



Now that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.

Our official Qdrant clients for Python, TypeScript, Go, Rust, and .NET all support the API key parameter. 



```python

from qdrant_client import QdrantClient



qdrant_client = QdrantClient(",0.44785276073619634,0.6083333333333333,0.5159010600706714,0.24074074074074073,0.3277310924369748,0.2775800711743772,0.3006134969325153,0.4083333333333333,0.3462897526501767,0.4110429447852761,0.5583333333333333,0.47349823321554774,0.9624468088150024,0.6454376578330994,0.6767098903656006,0.6607038974761963,0.8415429224260151,0.9613953361986205,147,1.2352941176470589,0.7710284962091639
