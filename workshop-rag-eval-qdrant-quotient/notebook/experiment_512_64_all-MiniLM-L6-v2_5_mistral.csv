input_text,answer,documents,context
What is vaccum optimizer ?,"The Vacuum Optimizer is a feature in the Qdrant database designed to manage and improve system performance by removing obsolete data. In Qdrant, when records are deleted, they are not immediately removed from the database. Instead, they are marked as deleted and subsequently ignored in future queries. This approach helps minimize disk access but results in accumulated deleted records that can occupy valuable memory and degrade performance over time.
The Vacuum Optimizer addresses this issue by systematically clearing out these marked-as-deleted records once they reach a certain threshold. This threshold is configurable and includes conditions such as the minimum fraction of deleted vectors in a segment and the minimum number of vectors present in a segment. ","['$$\n\nwhere $s(v)$ is the similarity function, $v_t$ is the target vector, and again $v_i^+$ and $v_i^-$ are the positive and negative examples, respectively. The sigmoid function is used to normalize the score between 0 and 1 and the sum of ranks is used to penalize vectors that are closer to the negative examples than to the positive ones. In other words, the sum of individual ranks determines how many positive zones a point is in, while the closeness hierarchy comes second.\n\n\n\nExample:\n\n\n\n```http', '$$\n\nwhere $s(v)$ is the similarity function, $v_t$ is the target vector, and again $v_i^+$ and $v_i^-$ are the positive and negative examples, respectively. The sigmoid function is used to normalize the score between 0 and 1 and the sum of ranks is used to penalize vectors that are closer to the negative examples than to the positive ones. In other words, the sum of individual ranks determines how many positive zones a point is in, while the closeness hierarchy comes second.\n\n\n\nExample:\n\n\n\n```http', '![Context search](/docs/context-search.png)\n\n\n\nWe can directly associate the score function to a loss function, where 0.0 is the maximum score a point can have, which means it is only in positive areas. As soon as a point exists closer to a negative example, its loss will simply be the difference of the positive and negative similarities.\n\n\n\n$$\n\n\\text{context score} = \\sum \\min(s(v^+_i) - s(v^-_i), 0.0)\n\n$$', '![Context search](/docs/context-search.png)\n\n\n\nWe can directly associate the score function to a loss function, where 0.0 is the maximum score a point can have, which means it is only in positive areas. As soon as a point exists closer to a negative example, its loss will simply be the difference of the positive and negative similarities.\n\n\n\n$$\n\n\\text{context score} = \\sum \\min(s(v^+_i) - s(v^-_i), 0.0)\n\n$$', 'potential future developments of this technique.\n\npreview_image: /blog/from_cms/andrey-vasnetsov-cropped.png\n\ndate: 2024-01-09T10:30:10.952Z\n\nauthor: Demetrios Brinkmann\n\nfeatured: false\n\ntags:\n\n  - Vector Space Talks\n\n  - Binary Quantization\n\n  - Qdrant\n\n---\n\n\n\n> *""Everything changed when we actually tried binary quantization with OpenAI model.”*\\\n\n> -- Andrey Vasnetsov']","$$

where $s(v)$ is the similarity function, $v_t$ is the target vector, and again $v_i^+$ and $v_i^-$ are the positive and negative examples, respectively. The sigmoid function is used to normalize the score between 0 and 1 and the sum of ranks is used to penalize vectors that are closer to the negative examples than to the positive ones. In other words, the sum of individual ranks determines how many positive zones a point is in, while the closeness hierarchy comes second.



Example:



```http
$$

where $s(v)$ is the similarity function, $v_t$ is the target vector, and again $v_i^+$ and $v_i^-$ are the positive and negative examples, respectively. The sigmoid function is used to normalize the score between 0 and 1 and the sum of ranks is used to penalize vectors that are closer to the negative examples than to the positive ones. In other words, the sum of individual ranks determines how many positive zones a point is in, while the closeness hierarchy comes second.



Example:



```http
![Context search](/docs/context-search.png)



We can directly associate the score function to a loss function, where 0.0 is the maximum score a point can have, which means it is only in positive areas. As soon as a point exists closer to a negative example, its loss will simply be the difference of the positive and negative similarities.



$$

\text{context score} = \sum \min(s(v^+_i) - s(v^-_i), 0.0)

$$
![Context search](/docs/context-search.png)



We can directly associate the score function to a loss function, where 0.0 is the maximum score a point can have, which means it is only in positive areas. As soon as a point exists closer to a negative example, its loss will simply be the difference of the positive and negative similarities.



$$

\text{context score} = \sum \min(s(v^+_i) - s(v^-_i), 0.0)

$$
potential future developments of this technique.

preview_image: /blog/from_cms/andrey-vasnetsov-cropped.png

date: 2024-01-09T10:30:10.952Z

author: Demetrios Brinkmann

featured: false

tags:

  - Vector Space Talks

  - Binary Quantization

  - Qdrant

---



> *""Everything changed when we actually tried binary quantization with OpenAI model.”*\

> -- Andrey Vasnetsov"
Tell me about ‘always_ram’ parameter?,"In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times.","['}\n\n);\n\n```\n\n\n\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\n\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\n\n\n\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\n\n\n\n### Setting up Product Quantization', '}\n\n);\n\n```\n\n\n\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\n\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\n\n\n\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\n\n\n\n### Setting up Product Quantization', '- **All in RAM** - all vector, original and quantized, are loaded and kept in RAM. This is the fastest mode, but requires a lot of RAM. Enabled by default.\n\n\n\n- **Original on Disk, quantized in RAM** - this is a hybrid mode, allows to obtain a good balance between speed and memory usage. Recommended scenario if you are aiming to shrink the memory footprint while keeping the search speed.\n\n\n\nThis mode is enabled by setting `always_ram` to `true` in the quantization config while using memmap storage:', '- **All in RAM** - all vector, original and quantized, are loaded and kept in RAM. This is the fastest mode, but requires a lot of RAM. Enabled by default.\n\n\n\n- **Original on Disk, quantized in RAM** - this is a hybrid mode, allows to obtain a good balance between speed and memory usage. Recommended scenario if you are aiming to shrink the memory footprint while keeping the search speed.\n\n\n\nThis mode is enabled by setting `always_ram` to `true` in the quantization config while using memmap storage:', '**In-memory storage** - Stores all vectors in RAM, has the highest speed since disk access is required only for persistence.\n\n\n\n**Memmap storage** - Creates a virtual address space associated with the file on disk. [Wiki](https://en.wikipedia.org/wiki/Memory-mapped_file).\n\nMmapped files are not directly loaded into RAM. Instead, they use page cache to access the contents of the file.\n\nThis scheme allows flexible use of available memory. With sufficient RAM, it is almost as fast as in-memory storage.']","}

);

```



`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.

However, in some setups you might want to keep quantized vectors in RAM to speed up the search process.



In this case, you can set `always_ram` to `true` to store quantized vectors in RAM.



### Setting up Product Quantization
}

);

```



`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.

However, in some setups you might want to keep quantized vectors in RAM to speed up the search process.



In this case, you can set `always_ram` to `true` to store quantized vectors in RAM.



### Setting up Product Quantization
- **All in RAM** - all vector, original and quantized, are loaded and kept in RAM. This is the fastest mode, but requires a lot of RAM. Enabled by default.



- **Original on Disk, quantized in RAM** - this is a hybrid mode, allows to obtain a good balance between speed and memory usage. Recommended scenario if you are aiming to shrink the memory footprint while keeping the search speed.



This mode is enabled by setting `always_ram` to `true` in the quantization config while using memmap storage:
- **All in RAM** - all vector, original and quantized, are loaded and kept in RAM. This is the fastest mode, but requires a lot of RAM. Enabled by default.



- **Original on Disk, quantized in RAM** - this is a hybrid mode, allows to obtain a good balance between speed and memory usage. Recommended scenario if you are aiming to shrink the memory footprint while keeping the search speed.



This mode is enabled by setting `always_ram` to `true` in the quantization config while using memmap storage:
**In-memory storage** - Stores all vectors in RAM, has the highest speed since disk access is required only for persistence.



**Memmap storage** - Creates a virtual address space associated with the file on disk. [Wiki](https://en.wikipedia.org/wiki/Memory-mapped_file).

Mmapped files are not directly loaded into RAM. Instead, they use page cache to access the contents of the file.

This scheme allows flexible use of available memory. With sufficient RAM, it is almost as fast as in-memory storage."
What is difference between scalar and product quantization?,"While both methods aim to reduce the memory footprint and potentially speed up operations, scalar quantization offers faster processing with SIMD-friendly operations and minimal accuracy loss, making it suitable for scenarios where speed is critical. Product quantization achieves higher compression rates and is better suited for handling very high-dimensional vectors where memory space is more of a concern than computation speed.","['But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\n\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\n\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n## How to choose the right quantization method', 'But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\n\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\n\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n## How to choose the right quantization method', '. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.', '. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.', '<td>474 s</td>\n\n      </tr>\n\n   </tbody>\n\n</table>\n\n\n\nIt turns out that in some cases, Product Quantization may not only reduce the memory usage, \n\nbut also the search time.\n\n\n\n## Good practices\n\n\n\nCompared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\n\n\n\nProduct Quantization tends to be favored in certain specific scenarios:']","But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method
But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method
. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
<td>474 s</td>

      </tr>

   </tbody>

</table>



It turns out that in some cases, Product Quantization may not only reduce the memory usage, 

but also the search time.



## Good practices



Compared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.



Product Quantization tends to be favored in certain specific scenarios:"
What is ‘best_score’ strategy?,"The `best_score` strategy is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided.","['This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.\n\n\n\n### Best score strategy\n\n\n\n*Available as of v1.6.0*\n\n\n\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.', 'This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.\n\n\n\n### Best score strategy\n\n\n\n*Available as of v1.6.0*\n\n\n\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.', '* Score is always in descending order (larger is better), regardless of the metric used.\n\n* Best possible score is `0.0`, and it is normal that many points get this score.\n\n\n\n</aside>', '* Score is always in descending order (larger is better), regardless of the metric used.\n\n* Best possible score is `0.0`, and it is normal that many points get this score.\n\n\n\n</aside>', '. But if it is on the negative side, it will be assigned a more and more negative score the further it gets.']","This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
* Score is always in descending order (larger is better), regardless of the metric used.

* Best possible score is `0.0`, and it is normal that many points get this score.



</aside>
* Score is always in descending order (larger is better), regardless of the metric used.

* Best possible score is `0.0`, and it is normal that many points get this score.



</aside>
. But if it is on the negative side, it will be assigned a more and more negative score the further it gets."
How does oversampling helps?,"Defines how many extra vectors should be pre-selected using quantized index, and then re-scored using original vectors. For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring. Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.","['oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.', 'oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.', 'These two parameters are how you are going to balance speed versus accuracy. The larger the size of your oversample, the more items you need to read from disk and the more elements you have to search with the relatively slower full vector index. On the other hand, doing this will produce more accurate results.', 'These two parameters are how you are going to balance speed versus accuracy. The larger the size of your oversample, the more items you need to read from disk and the more elements you have to search with the relatively slower full vector index. On the other hand, doing this will produce more accurate results.', 'Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user']","oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
These two parameters are how you are going to balance speed versus accuracy. The larger the size of your oversample, the more items you need to read from disk and the more elements you have to search with the relatively slower full vector index. On the other hand, doing this will produce more accurate results.
These two parameters are how you are going to balance speed versus accuracy. The larger the size of your oversample, the more items you need to read from disk and the more elements you have to search with the relatively slower full vector index. On the other hand, doing this will produce more accurate results.
Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user"
What is the purpose of ‘CreatePayloadIndexAsync’?,CreatePayloadIndexAsync is a method in the Qdrant library that enables the creation of a payload index in Qdrant. A payload index is a data structure designed to store supplemental information about the data stored in Qdrant. This method requires a Qdrant instance and a payload index name as input parameters.,"['.', '.', '.', '.', '.']",".
.
.
.
."
What is the purpose of ef_construct in HNSW ?,"In HNSW algorithm the ef_construct parameter is the number of neighbours to consider during the index building. The larger the value, the higher the precision, but the longer the indexing time. The default values of this parameters 100","['Parameter `limit` (or its alias - `top`) specifies the amount of most similar results we would like to retrieve.\n\n\n\nValues under the key `params` specify custom parameters for the search.\n\nCurrently, it could be:\n\n\n\n* `hnsw_ef` - value that specifies `ef` parameter of the HNSW algorithm.\n\n* `exact` - option to not use the approximate search (ANN). If set to true, the search may run for a long as it performs a full scan to retrieve exact results.', 'Parameter `limit` (or its alias - `top`) specifies the amount of most similar results we would like to retrieve.\n\n\n\nValues under the key `params` specify custom parameters for the search.\n\nCurrently, it could be:\n\n\n\n* `hnsw_ef` - value that specifies `ef` parameter of the HNSW algorithm.\n\n* `exact` - option to not use the approximate search (ANN). If set to true, the search may run for a long as it performs a full scan to retrieve exact results.', ""The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \n\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\n\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and"", ""The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \n\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\n\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and"", '- `exact` - if set to `true`, will perform exact search, which will be slower, but more accurate. You can use it to compare results of the search with different `hnsw_ef` values versus the ground truth.\n\n\n\n## Latency vs Throughput\n\n\n\n- There are two main approaches to measure the speed of search:\n\n  - latency of the request - the time from the moment request is submitted to the moment a response is received\n\n  - throughput - the number of requests per second the system can handle']","Parameter `limit` (or its alias - `top`) specifies the amount of most similar results we would like to retrieve.



Values under the key `params` specify custom parameters for the search.

Currently, it could be:



* `hnsw_ef` - value that specifies `ef` parameter of the HNSW algorithm.

* `exact` - option to not use the approximate search (ANN). If set to true, the search may run for a long as it performs a full scan to retrieve exact results.
Parameter `limit` (or its alias - `top`) specifies the amount of most similar results we would like to retrieve.



Values under the key `params` specify custom parameters for the search.

Currently, it could be:



* `hnsw_ef` - value that specifies `ef` parameter of the HNSW algorithm.

* `exact` - option to not use the approximate search (ANN). If set to true, the search may run for a long as it performs a full scan to retrieve exact results.
The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of 

neighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.

The default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and
The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of 

neighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.

The default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and
- `exact` - if set to `true`, will perform exact search, which will be slower, but more accurate. You can use it to compare results of the search with different `hnsw_ef` values versus the ground truth.



## Latency vs Throughput



- There are two main approaches to measure the speed of search:

  - latency of the request - the time from the moment request is submitted to the moment a response is received

  - throughput - the number of requests per second the system can handle"
How do you use ‘ordering’ parameter?,"Write ordering can be specified for any write request to serialize it through a single “leader” node, which ensures that all write operations (issued with the same ordering) are performed and observed sequentially. It is of 3 types weak , medium and strong and is used in python with additional param ordering=models.WriteOrdering.STRONG to upsert request.","['.', '.', '.', '.', '.']",".
.
.
.
."
What is significance of ‘on_disk_payload’ setting?,"The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes.","['Read more about the payload storage in the [Storage](../../concepts/storage/#payload-storage) section.\n\n\n\n\n\n## Storage focused configuration\n\n\n\nIf your priority is to serve large amount of vectors with an average search latency, it is recommended to configure [mmap storage](../../concepts/storage/#configuring-memmap-storage).\n\nIn this case vectors will be stored on the disc in memory-mapped files, and only the most frequently used vectors will be kept in RAM.', 'Read more about the payload storage in the [Storage](../../concepts/storage/#payload-storage) section.\n\n\n\n\n\n## Storage focused configuration\n\n\n\nIf your priority is to serve large amount of vectors with an average search latency, it is recommended to configure [mmap storage](../../concepts/storage/#configuring-memmap-storage).\n\nIn this case vectors will be stored on the disc in memory-mapped files, and only the most frequently used vectors will be kept in RAM.', ""temp_path: null\n\n\n\n  # If true - point's payload will not be stored in memory.\n\n  # It will be read from the disk every time it is requested.\n\n  # This setting saves RAM by (slightly) increasing the response time.\n\n  # Note: those payload values that are involved in filtering and are indexed - remain in RAM.\n\n  on_disk_payload: true\n\n\n\n  # Maximum number of concurrent updates to shard replicas\n\n  # If `null` - maximum concurrency is used.\n\n  update_concurrency: null"", ""temp_path: null\n\n\n\n  # If true - point's payload will not be stored in memory.\n\n  # It will be read from the disk every time it is requested.\n\n  # This setting saves RAM by (slightly) increasing the response time.\n\n  # Note: those payload values that are involved in filtering and are indexed - remain in RAM.\n\n  on_disk_payload: true\n\n\n\n  # Maximum number of concurrent updates to shard replicas\n\n  # If `null` - maximum concurrency is used.\n\n  update_concurrency: null"", '```text\n\nmemory_size = number_of_vectors * vector_dimension * 4 bytes * 1.5\n\n```\n\n\n\nExtra 50% is needed for metadata (indexes, point versions, etc.) as well as for temporary segments constructed during the optimization process.\n\n\n\nIf you need to have payloads along with the vectors, it is recommended to store it on the disc, and only keep [indexed fields](../../concepts/indexing/#payload-index) in RAM.']","Read more about the payload storage in the [Storage](../../concepts/storage/#payload-storage) section.





## Storage focused configuration



If your priority is to serve large amount of vectors with an average search latency, it is recommended to configure [mmap storage](../../concepts/storage/#configuring-memmap-storage).

In this case vectors will be stored on the disc in memory-mapped files, and only the most frequently used vectors will be kept in RAM.
Read more about the payload storage in the [Storage](../../concepts/storage/#payload-storage) section.





## Storage focused configuration



If your priority is to serve large amount of vectors with an average search latency, it is recommended to configure [mmap storage](../../concepts/storage/#configuring-memmap-storage).

In this case vectors will be stored on the disc in memory-mapped files, and only the most frequently used vectors will be kept in RAM.
temp_path: null



  # If true - point's payload will not be stored in memory.

  # It will be read from the disk every time it is requested.

  # This setting saves RAM by (slightly) increasing the response time.

  # Note: those payload values that are involved in filtering and are indexed - remain in RAM.

  on_disk_payload: true



  # Maximum number of concurrent updates to shard replicas

  # If `null` - maximum concurrency is used.

  update_concurrency: null
temp_path: null



  # If true - point's payload will not be stored in memory.

  # It will be read from the disk every time it is requested.

  # This setting saves RAM by (slightly) increasing the response time.

  # Note: those payload values that are involved in filtering and are indexed - remain in RAM.

  on_disk_payload: true



  # Maximum number of concurrent updates to shard replicas

  # If `null` - maximum concurrency is used.

  update_concurrency: null
```text

memory_size = number_of_vectors * vector_dimension * 4 bytes * 1.5

```



Extra 50% is needed for metadata (indexes, point versions, etc.) as well as for temporary segments constructed during the optimization process.



If you need to have payloads along with the vectors, it is recommended to store it on the disc, and only keep [indexed fields](../../concepts/indexing/#payload-index) in RAM."
What is the impact of ‘write_consistency_factor’ ?,"The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully.","['- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.', '- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.', '### Write consistency factor\n\n\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\n\nIt can be configured at the collection\'s creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python', '### Write consistency factor\n\n\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\n\nIt can be configured at the collection\'s creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python', 'limit: 3,\n\n\treadConsistency: new ReadConsistency { Type = ReadConsistencyType.Majority }\n\n);\n\n```\n\n\n\n### Write ordering\n\n\n\nWrite `ordering` can be specified for any write request to serialize it through a single ""leader"" node,\n\nwhich ensures that all write operations (issued with the same `ordering`) are performed and observed\n\nsequentially.\n\n\n\n- `weak` _(default)_ ordering does not provide any additional guarantees, so write operations can be freely reordered.']","- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

    ""write_consistency_factor"": 2,

}

```



```python
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

    ""write_consistency_factor"": 2,

}

```



```python
limit: 3,

	readConsistency: new ReadConsistency { Type = ReadConsistencyType.Majority }

);

```



### Write ordering



Write `ordering` can be specified for any write request to serialize it through a single ""leader"" node,

which ensures that all write operations (issued with the same `ordering`) are performed and observed

sequentially.



- `weak` _(default)_ ordering does not provide any additional guarantees, so write operations can be freely reordered."
What is the purpose of oversampling in Qdrant search process?,Oversampling in Qdrant search process defines how many extra vectors should be pre-selected using quantized index and then re-scored using original vectors to improve search quality.,"['### Oversampling and Rescoring\n\n\n\nA distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.\n\nThis enables the best combination of speed, accuracy, and RAM usage.\n\n\n\nQdrant stores the original vectors, so it is possible to rescore the top-k results with\n\nthe original vectors after doing the neighbours search in quantized space. That obviously', '### Oversampling and Rescoring\n\n\n\nA distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.\n\nThis enables the best combination of speed, accuracy, and RAM usage.\n\n\n\nQdrant stores the original vectors, so it is possible to rescore the top-k results with\n\nthe original vectors after doing the neighbours search in quantized space. That obviously', ""Right? So unfortunately Qdrant architecture is not designed and not expecting this type of behavior for several reasons. First of all, removing of the original vectors will compromise some features like oversampling, like segment building. And actually removing of those original vectors will only be compatible with some types of quantization for example, it won't be compatible with scalar quantization because in this case we won't be able to rebuild index to do maintenance of the system"", ""Right? So unfortunately Qdrant architecture is not designed and not expecting this type of behavior for several reasons. First of all, removing of the original vectors will compromise some features like oversampling, like segment building. And actually removing of those original vectors will only be compatible with some types of quantization for example, it won't be compatible with scalar quantization because in this case we won't be able to rebuild index to do maintenance of the system"", 'We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.\n\n\n\nIn order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.\n\n\n\nAdditionally, Qdrant uses a bunch of internal heuristics to optimize the performance.\n\nTo better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.\n\nWith this information, we can make Qdrant faster for everyone.']","### Oversampling and Rescoring



A distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.

This enables the best combination of speed, accuracy, and RAM usage.



Qdrant stores the original vectors, so it is possible to rescore the top-k results with

the original vectors after doing the neighbours search in quantized space. That obviously
### Oversampling and Rescoring



A distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.

This enables the best combination of speed, accuracy, and RAM usage.



Qdrant stores the original vectors, so it is possible to rescore the top-k results with

the original vectors after doing the neighbours search in quantized space. That obviously
Right? So unfortunately Qdrant architecture is not designed and not expecting this type of behavior for several reasons. First of all, removing of the original vectors will compromise some features like oversampling, like segment building. And actually removing of those original vectors will only be compatible with some types of quantization for example, it won't be compatible with scalar quantization because in this case we won't be able to rebuild index to do maintenance of the system
Right? So unfortunately Qdrant architecture is not designed and not expecting this type of behavior for several reasons. First of all, removing of the original vectors will compromise some features like oversampling, like segment building. And actually removing of those original vectors will only be compatible with some types of quantization for example, it won't be compatible with scalar quantization because in this case we won't be able to rebuild index to do maintenance of the system
We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.



In order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.



Additionally, Qdrant uses a bunch of internal heuristics to optimize the performance.

To better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.

With this information, we can make Qdrant faster for everyone."
How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?,"Qdrant uses a different approach that does not require pre- or post-filtering, effectively addressing the accuracy problem while maintaining search efficiency.","['HNSW is chosen for several reasons.\n\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\n\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\n\n\n\n*Available as of v1.1.1*\n\n\n\nThe HNSW parameters can also be configured on a collection and named vector\n\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\n\nperformance.', 'HNSW is chosen for several reasons.\n\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\n\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\n\n\n\n*Available as of v1.1.1*\n\n\n\nThe HNSW parameters can also be configured on a collection and named vector\n\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\n\nperformance.', ""All right, cool. Well, that sets the scene for us. Now, I feel like you brought some slides along. Feel free to share those whenever you want. I'm going to fire away the first question and ask about this. I'm going to go straight into Qdrant questions and ask you to elaborate on how the unique modification of Qdrant of the HNSW algorithm benefits your solution"", ""All right, cool. Well, that sets the scene for us. Now, I feel like you brought some slides along. Feel free to share those whenever you want. I'm going to fire away the first question and ask about this. I'm going to go straight into Qdrant questions and ask you to elaborate on how the unique modification of Qdrant of the HNSW algorithm benefits your solution"", 'All right, keep going. I like it.\n\n\n\nRishabh Bhardwaj:\n\nYeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.\n\n\n\nDemetrios:\n\nRight.\n\n\n\nRishabh Bhardwaj:']","HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
All right, cool. Well, that sets the scene for us. Now, I feel like you brought some slides along. Feel free to share those whenever you want. I'm going to fire away the first question and ask about this. I'm going to go straight into Qdrant questions and ask you to elaborate on how the unique modification of Qdrant of the HNSW algorithm benefits your solution
All right, cool. Well, that sets the scene for us. Now, I feel like you brought some slides along. Feel free to share those whenever you want. I'm going to fire away the first question and ask about this. I'm going to go straight into Qdrant questions and ask you to elaborate on how the unique modification of Qdrant of the HNSW algorithm benefits your solution
All right, keep going. I like it.



Rishabh Bhardwaj:

Yeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.



Demetrios:

Right.



Rishabh Bhardwaj:"
What is the difference between regular and neural search?,"Regular full-text search involves searching for keywords within a document, while neural search considers the real meaning of the query and documents, allowing for more accurate results.","['In this tutorial we are going to find answers to these questions:\n\n\n\n* What is the difference between regular and neural search?\n\n* What neural networks could be used for search?\n\n* In what tasks is neural network search useful?\n\n* How to build and deploy own neural search service step-by-step?\n\n\n\n**What is neural search?**', 'In this tutorial we are going to find answers to these questions:\n\n\n\n* What is the difference between regular and neural search?\n\n* What neural networks could be used for search?\n\n* In what tasks is neural network search useful?\n\n* How to build and deploy own neural search service step-by-step?\n\n\n\n**What is neural search?**', ""These days, search technology is the heart of a variety of applications.\n\nFrom web-pages search to product recommendations.\n\nFor many years, this technology didn't get much change until neural networks came into play.\n\n\n\nIn this tutorial we are going to find answers to these questions:\n\n\n\n* What is the difference between regular and neural search?\n\n* What neural networks could be used for search?\n\n* In what tasks is neural network search useful?"", ""These days, search technology is the heart of a variety of applications.\n\nFrom web-pages search to product recommendations.\n\nFor many years, this technology didn't get much change until neural networks came into play.\n\n\n\nIn this tutorial we are going to find answers to these questions:\n\n\n\n* What is the difference between regular and neural search?\n\n* What neural networks could be used for search?\n\n* In what tasks is neural network search useful?"", '**What is neural search?**\n\n\n\nA regular full-text search, such as Google’s, consists of searching for keywords inside a document. For this reason, the algorithm can not take into account the real meaning of the query and documents. Many documents that might be of interest to the user are not found because they use different wording.']","In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



**What is neural search?**
In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



**What is neural search?**
These days, search technology is the heart of a variety of applications.

From web-pages search to product recommendations.

For many years, this technology didn't get much change until neural networks came into play.



In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?
These days, search technology is the heart of a variety of applications.

From web-pages search to product recommendations.

For many years, this technology didn't get much change until neural networks came into play.



In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?
**What is neural search?**



A regular full-text search, such as Google’s, consists of searching for keywords inside a document. For this reason, the algorithm can not take into account the real meaning of the query and documents. Many documents that might be of interest to the user are not found because they use different wording."
How can I use Qdrant as a vector store in Langchain Go?,"You can use Qdrant as a vector store in Langchain Go by installing the `langchain-go` project dependency and customizing the values for your configuration, such as the Qdrant REST URL and collection name.","['might also be found in the [LangChain documentation](https://python.langchain.com/docs/integrations/vectorstores/qdrant).', 'might also be found in the [LangChain documentation](https://python.langchain.com/docs/integrations/vectorstores/qdrant).', 'qdrant.WithEmbedder(e),\n\n )\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n```\n\n\n\n## Further Reading\n\n\n\n- You can find usage examples of Langchain Go [here](https://github.com/tmc/langchaingo/tree/main/examples).', 'qdrant.WithEmbedder(e),\n\n )\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n```\n\n\n\n## Further Reading\n\n\n\n- You can find usage examples of Langchain Go [here](https://github.com/tmc/langchaingo/tree/main/examples).', '---\n\ntitle: Langchain Go\n\nweight: 120\n\n---\n\n\n\n# Langchain Go\n\n\n\n[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.\n\n\n\nYou can use Qdrant as a vector store in Langchain Go.\n\n\n\n## Setup\n\n\n\nInstall the `langchain-go` project dependency\n\n\n\n```bash\n\ngo get -u github.com/tmc/langchaingo\n\n```\n\n\n\n## Usage\n\n\n\nBefore you use the following code sample, customize the following values for your configuration:']","might also be found in the [LangChain documentation](https://python.langchain.com/docs/integrations/vectorstores/qdrant).
might also be found in the [LangChain documentation](https://python.langchain.com/docs/integrations/vectorstores/qdrant).
qdrant.WithEmbedder(e),

 )

 if err != nil {

  log.Fatal(err)

 }

```



## Further Reading



- You can find usage examples of Langchain Go [here](https://github.com/tmc/langchaingo/tree/main/examples).
qdrant.WithEmbedder(e),

 )

 if err != nil {

  log.Fatal(err)

 }

```



## Further Reading



- You can find usage examples of Langchain Go [here](https://github.com/tmc/langchaingo/tree/main/examples).
---

title: Langchain Go

weight: 120

---



# Langchain Go



[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.



You can use Qdrant as a vector store in Langchain Go.



## Setup



Install the `langchain-go` project dependency



```bash

go get -u github.com/tmc/langchaingo

```



## Usage



Before you use the following code sample, customize the following values for your configuration:"
How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?,Dust leveraged the control of the MMAP payload threshold and Scalar Quantization in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively.,"['compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP\n\npayload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage\n\nthe balance between storing vectors on disk and keeping quantized vectors in RAM,\n\nmore effectively. “This allowed us to scale smoothly from there,” Polu says.', 'compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP\n\npayload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage\n\nthe balance between storing vectors on disk and keeping quantized vectors in RAM,\n\nmore effectively. “This allowed us to scale smoothly from there,” Polu says.', 'Dust was able to scale its application with Qdrant while maintaining low latency\n\nacross hundreds of thousands of collections with retrieval only taking\n\nmilliseconds, as well as maintaining high accuracy. Additionally, Polu highlights\n\nthe efficiency gains Dust was able to unlock with Qdrant: ""We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as\n\nwe don’t have to run lots of nodes in parallel. While being memory-bound, we were', 'Dust was able to scale its application with Qdrant while maintaining low latency\n\nacross hundreds of thousands of collections with retrieval only taking\n\nmilliseconds, as well as maintaining high accuracy. Additionally, Polu highlights\n\nthe efficiency gains Dust was able to unlock with Qdrant: ""We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as\n\nwe don’t have to run lots of nodes in parallel. While being memory-bound, we were', 'library support.\n\n\n\nWhen building their solution with Qdrant, Dust took a two step approach:\n\n\n\n1. **Get started quickly:** Initially, Dust wanted to get started quickly and opted for\n\n[Qdrant Cloud](https://qdrant.to/cloud), Qdrant’s managed solution, to reduce the administrative load on\n\nDust’s end. In addition, they created clusters and deployed them on Google\n\nCloud since Dust wanted to have those run directly in their existing Google']","compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP

payload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage

the balance between storing vectors on disk and keeping quantized vectors in RAM,

more effectively. “This allowed us to scale smoothly from there,” Polu says.
compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP

payload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage

the balance between storing vectors on disk and keeping quantized vectors in RAM,

more effectively. “This allowed us to scale smoothly from there,” Polu says.
Dust was able to scale its application with Qdrant while maintaining low latency

across hundreds of thousands of collections with retrieval only taking

milliseconds, as well as maintaining high accuracy. Additionally, Polu highlights

the efficiency gains Dust was able to unlock with Qdrant: ""We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as

we don’t have to run lots of nodes in parallel. While being memory-bound, we were
Dust was able to scale its application with Qdrant while maintaining low latency

across hundreds of thousands of collections with retrieval only taking

milliseconds, as well as maintaining high accuracy. Additionally, Polu highlights

the efficiency gains Dust was able to unlock with Qdrant: ""We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as

we don’t have to run lots of nodes in parallel. While being memory-bound, we were
library support.



When building their solution with Qdrant, Dust took a two step approach:



1. **Get started quickly:** Initially, Dust wanted to get started quickly and opted for

[Qdrant Cloud](https://qdrant.to/cloud), Qdrant’s managed solution, to reduce the administrative load on

Dust’s end. In addition, they created clusters and deployed them on Google

Cloud since Dust wanted to have those run directly in their existing Google"
Why do we still need keyword search?,"Keyword search is still useful in cases of out-of-domain search, where words are just words regardless of their meaning.","[""preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has\n\nsome clear advantages in the following scenarios:\n\n\n\n- 🌍 Multi-lingual & multi-modal search\n\n- 🤔 For short texts with typos and ambiguous content-dependent meanings\n\n- 👨\u200d🔬 Specialized domains with tuned encoder models\n\n- 📄 Document-as-a-Query similarity search\n\n\n\nIt doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method\n\nmight be useful:"", ""preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has\n\nsome clear advantages in the following scenarios:\n\n\n\n- 🌍 Multi-lingual & multi-modal search\n\n- 🤔 For short texts with typos and ambiguous content-dependent meanings\n\n- 👨\u200d🔬 Specialized domains with tuned encoder models\n\n- 📄 Document-as-a-Query similarity search\n\n\n\nIt doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method\n\nmight be useful:"", '. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.', '. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.', 'search quality.\n\n\n\n### Quality metrics\n\n\n\nThere are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), \n\nare based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),']","preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has

some clear advantages in the following scenarios:



- 🌍 Multi-lingual & multi-modal search

- 🤔 For short texts with typos and ambiguous content-dependent meanings

- 👨‍🔬 Specialized domains with tuned encoder models

- 📄 Document-as-a-Query similarity search



It doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method

might be useful:
preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has

some clear advantages in the following scenarios:



- 🌍 Multi-lingual & multi-modal search

- 🤔 For short texts with typos and ambiguous content-dependent meanings

- 👨‍🔬 Specialized domains with tuned encoder models

- 📄 Document-as-a-Query similarity search



It doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method

might be useful:
. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.
. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.
search quality.



### Quality metrics



There are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), 

are based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),"
What principles did Qdrant follow while designing benchmarks for vector search engines?,"Qdrant followed the principles of doing comparative benchmarks focusing on relative numbers rather than absolute numbers, and using affordable hardware for easy result reproduction.","['. In this article, we will compare how Qdrant performs against the other vector search engines.', '. In this article, we will compare how Qdrant performs against the other vector search engines.', ""tags: # Change this, related by tags posts will be shown on the blog page\n\n  - qdrant\n\n  - benchmarks\n\n  - performance\n\n---\n\n\n\nIt's time for an update to Qdrant's benchmarks! \n\n\n\nWe've compared how Qdrant performs against the other vector search engines to give you a thorough performance analysis. Let's get into what's new and what remains the same in our approach. \n\n\n\n### What's Changed?\n\n\n\n#### All engines have improved"", ""tags: # Change this, related by tags posts will be shown on the blog page\n\n  - qdrant\n\n  - benchmarks\n\n  - performance\n\n---\n\n\n\nIt's time for an update to Qdrant's benchmarks! \n\n\n\nWe've compared how Qdrant performs against the other vector search engines to give you a thorough performance analysis. Let's get into what's new and what remains the same in our approach. \n\n\n\n### What's Changed?\n\n\n\n#### All engines have improved"", ""All of those engines might be easily used in combination with the vector search offered by Qdrant. But the \n\nexact way how to combine the results of both algorithms to achieve the best search precision might be still \n\nunclear. So we need to understand how to do it effectively. We will be using reference datasets to benchmark \n\nthe search quality.\n\n\n\n## Why not linear combination?\n\n\n\nIt's often proposed to use full-text and vector search scores to form a linear combination formula to rerank""]",". In this article, we will compare how Qdrant performs against the other vector search engines.
. In this article, we will compare how Qdrant performs against the other vector search engines.
tags: # Change this, related by tags posts will be shown on the blog page

  - qdrant

  - benchmarks

  - performance

---



It's time for an update to Qdrant's benchmarks! 



We've compared how Qdrant performs against the other vector search engines to give you a thorough performance analysis. Let's get into what's new and what remains the same in our approach. 



### What's Changed?



#### All engines have improved
tags: # Change this, related by tags posts will be shown on the blog page

  - qdrant

  - benchmarks

  - performance

---



It's time for an update to Qdrant's benchmarks! 



We've compared how Qdrant performs against the other vector search engines to give you a thorough performance analysis. Let's get into what's new and what remains the same in our approach. 



### What's Changed?



#### All engines have improved
All of those engines might be easily used in combination with the vector search offered by Qdrant. But the 

exact way how to combine the results of both algorithms to achieve the best search precision might be still 

unclear. So we need to understand how to do it effectively. We will be using reference datasets to benchmark 

the search quality.



## Why not linear combination?



It's often proposed to use full-text and vector search scores to form a linear combination formula to rerank"
What models does Qdrant support for embedding generation?,"Qdrant supports a reasonable range of models, including a few multilingual ones.","["". So this is taken care of. So whatever is your best practices for the Embedding model, make sure you use it when you're using it with Qdrant or just in isolation as well."", "". So this is taken care of. So whatever is your best practices for the Embedding model, make sure you use it when you're using it with Qdrant or just in isolation as well."", ""### Dataset Combinations\n\n\n\nFor those exploring the integration of text embedding models with Qdrant, it's crucial to consider various model configurations for optimal performance. The dataset combinations defined above illustrate different configurations to test against Qdrant. These combinations vary by two primary attributes:"", ""### Dataset Combinations\n\n\n\nFor those exploring the integration of text embedding models with Qdrant, it's crucial to consider various model configurations for optimal performance. The dataset combinations defined above illustrate different configurations to test against Qdrant. These combinations vary by two primary attributes:"", '2. **Dimensions**: This refers to the size of the vector embeddings produced by the model. Options range from 512 to 3072 dimensions. Higher dimensions could lead to more precise embeddings but might also increase the search time and memory usage in Qdrant.']",". So this is taken care of. So whatever is your best practices for the Embedding model, make sure you use it when you're using it with Qdrant or just in isolation as well.
. So this is taken care of. So whatever is your best practices for the Embedding model, make sure you use it when you're using it with Qdrant or just in isolation as well.
### Dataset Combinations



For those exploring the integration of text embedding models with Qdrant, it's crucial to consider various model configurations for optimal performance. The dataset combinations defined above illustrate different configurations to test against Qdrant. These combinations vary by two primary attributes:
### Dataset Combinations



For those exploring the integration of text embedding models with Qdrant, it's crucial to consider various model configurations for optimal performance. The dataset combinations defined above illustrate different configurations to test against Qdrant. These combinations vary by two primary attributes:
2. **Dimensions**: This refers to the size of the vector embeddings produced by the model. Options range from 512 to 3072 dimensions. Higher dimensions could lead to more precise embeddings but might also increase the search time and memory usage in Qdrant."
How can you parallelize the upload of a large dataset using shards in Qdrant?,"By creating multiple shards in Qdrant, you can parallelize the upload of a large dataset. It is recommended to have 2 to 4 shards per machine for efficient processing. When creating a collection in Qdrant, you can specify the number of shards to use for that collection. ","['## Parallel upload into multiple shards\n\n\n\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\n\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python', '## Parallel upload into multiple shards\n\n\n\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\n\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python', 'Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.\n\n\n\n### Moving shards\n\n\n\n*Available as of v0.9.0*', 'Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.\n\n\n\n### Moving shards\n\n\n\n*Available as of v0.9.0*', 'By design, Qdrant offers three levels of isolation. We initially introduced collection-based isolation, but your scaled setup has to move beyond this level. In this scenario, you will leverage payload-based isolation (from multitenancy) and resource-based isolation (from sharding). The ultimate goal is to have a single collection, where you can manipulate and customize placement of shards inside your cluster more precisely and avoid any kind of overhead']","## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""shard_number"": 2

}

```



```python
## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""shard_number"": 2

}

```



```python
Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.



### Moving shards



*Available as of v0.9.0*
Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.



### Moving shards



*Available as of v0.9.0*
By design, Qdrant offers three levels of isolation. We initially introduced collection-based isolation, but your scaled setup has to move beyond this level. In this scenario, you will leverage payload-based isolation (from multitenancy) and resource-based isolation (from sharding). The ultimate goal is to have a single collection, where you can manipulate and customize placement of shards inside your cluster more precisely and avoid any kind of overhead"
What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?,"Maximizing the distance between all points in the response when utilizing vector similarity for diversity search is crucial as it allows for the algorithm to sequentially output dissimilar results. By doing so, a diverse selection of the collection can be made possible without the need for any labeling efforts. This approach ensures that the algorithm can effectively identify and retrieve a wide range of diverse items or data points.","['{{< figure src=/articles_data/vector-similarity-beyond-search/diversity.png caption=""Diversity Search"" >}}\n\n\n\n\n\nSome forms of diversity sampling are already used in the industry and are known as [Maximum Margin Relevance](https://python.langchain.com/docs/integrations/vectorstores/qdrant#maximum-marginal-relevance-search-mmr) (MMR). Techniques like this were developed to enhance similarity on a universal search API.\n\nHowever, there is still room for new ideas, particularly regarding diversity retrieval.', '{{< figure src=/articles_data/vector-similarity-beyond-search/diversity.png caption=""Diversity Search"" >}}\n\n\n\n\n\nSome forms of diversity sampling are already used in the industry and are known as [Maximum Margin Relevance](https://python.langchain.com/docs/integrations/vectorstores/qdrant#maximum-marginal-relevance-search-mmr) (MMR). Techniques like this were developed to enhance similarity on a universal search API.\n\nHowever, there is still room for new ideas, particularly regarding diversity retrieval.', 'Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.\n\n\n\n\n\n## Diversity Search\n\n\n\nEven with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.\n\n\n\nThe naive approach is to do random sampling. \n\nHowever, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.', 'Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.\n\n\n\n\n\n## Diversity Search\n\n\n\nEven with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.\n\n\n\nThe naive approach is to do random sampling. \n\nHowever, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.', '{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=""Example of similarity-based sampling"" >}}\n\n\n\n\n\nThe power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.\n\nBy maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.']","{{< figure src=/articles_data/vector-similarity-beyond-search/diversity.png caption=""Diversity Search"" >}}





Some forms of diversity sampling are already used in the industry and are known as [Maximum Margin Relevance](https://python.langchain.com/docs/integrations/vectorstores/qdrant#maximum-marginal-relevance-search-mmr) (MMR). Techniques like this were developed to enhance similarity on a universal search API.

However, there is still room for new ideas, particularly regarding diversity retrieval.
{{< figure src=/articles_data/vector-similarity-beyond-search/diversity.png caption=""Diversity Search"" >}}





Some forms of diversity sampling are already used in the industry and are known as [Maximum Margin Relevance](https://python.langchain.com/docs/integrations/vectorstores/qdrant#maximum-marginal-relevance-search-mmr) (MMR). Techniques like this were developed to enhance similarity on a universal search API.

However, there is still room for new ideas, particularly regarding diversity retrieval.
Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.





## Diversity Search



Even with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.



The naive approach is to do random sampling. 

However, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.
Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.





## Diversity Search



Even with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.



The naive approach is to do random sampling. 

However, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.
{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=""Example of similarity-based sampling"" >}}





The power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.

By maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results."
How can you ensure that collection shards are replicated in Qdrant after adding a new node to the cluster?,"To ensure that collection shards are replicated in Qdrant after adding a new node to the cluster, you can use the Replicate Shard Operation. This operation allows you to create another copy of the shard on the newly connected node. It's important to note that Qdrant does not automatically balance shards as it is considered an expensive operation.","['Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.\n\nUse the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.', 'Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.\n\nUse the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.', 'Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.\n\n\n\n### Moving shards\n\n\n\n*Available as of v0.9.0*', 'Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.\n\n\n\n### Moving shards\n\n\n\n*Available as of v0.9.0*', 'Once all shards of the collection are recovered, the collection will become operational again.\n\n\n\n## Consistency guarantees\n\n\n\nBy default, Qdrant focuses on availability and maximum throughput of search operations.\n\nFor the majority of use cases, this is a preferable trade-off.\n\n\n\nDuring the normal state of operation, it is possible to search and modify data from any peers in the cluster.']","Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.

Use the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.
Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.

Use the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.
Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.



### Moving shards



*Available as of v0.9.0*
Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.



### Moving shards



*Available as of v0.9.0*
Once all shards of the collection are recovered, the collection will become operational again.



## Consistency guarantees



By default, Qdrant focuses on availability and maximum throughput of search operations.

For the majority of use cases, this is a preferable trade-off.



During the normal state of operation, it is possible to search and modify data from any peers in the cluster."
Why would someone use a vector database?,"A vector database is used for various reasons, primarily for its efficiency in handling complex data structures and enabling advanced search capabilities. In the context of search and discovery, a vector database allows for state-of-the-art vector-search functionalities, making it ideal for applications requiring similarity search, recommendation systems, and content-based search. By leveraging vectors to represent data points, a vector database can efficiently compute similarities between vectors, enabling quick and accurate retrieval of relevant information","['Vector Databases, which are designed to store and process immense amounts of vectors, are the first candidates to implement these new techniques and allow users to exploit their data to its fullest.', 'Vector Databases, which are designed to store and process immense amounts of vectors, are the first candidates to implement these new techniques and allow users to exploit their data to its fullest.', '## Summary\n\nUltimately, you do not need a vector database if you are looking for a simple vector search functionality with a small amount of data. We genuinely recommend starting with whatever you already have in your stack to prototype. But you need one if you are looking to do more out of it, and it is the central functionality of your application. It is just like using a multi-tool to make something quick or using a dedicated instrument highly optimized for the use case.', '## Summary\n\nUltimately, you do not need a vector database if you are looking for a simple vector search functionality with a small amount of data. We genuinely recommend starting with whatever you already have in your stack to prototype. But you need one if you are looking to do more out of it, and it is the central functionality of your application. It is just like using a multi-tool to make something quick or using a dedicated instrument highly optimized for the use case.', '### Vector Database Use Cases\n\n\n\nIf we had to summarize the use cases for vector databases into a single word, it would be ""match"". They are great at finding non-obvious ways to correspond or “match” data with a given query. Whether it\'s through similarity in images, text, user preferences, or patterns in data.\n\n\n\nHere’s some examples on how to take advantage of using vector databases:']","Vector Databases, which are designed to store and process immense amounts of vectors, are the first candidates to implement these new techniques and allow users to exploit their data to its fullest.
Vector Databases, which are designed to store and process immense amounts of vectors, are the first candidates to implement these new techniques and allow users to exploit their data to its fullest.
## Summary

Ultimately, you do not need a vector database if you are looking for a simple vector search functionality with a small amount of data. We genuinely recommend starting with whatever you already have in your stack to prototype. But you need one if you are looking to do more out of it, and it is the central functionality of your application. It is just like using a multi-tool to make something quick or using a dedicated instrument highly optimized for the use case.
## Summary

Ultimately, you do not need a vector database if you are looking for a simple vector search functionality with a small amount of data. We genuinely recommend starting with whatever you already have in your stack to prototype. But you need one if you are looking to do more out of it, and it is the central functionality of your application. It is just like using a multi-tool to make something quick or using a dedicated instrument highly optimized for the use case.
### Vector Database Use Cases



If we had to summarize the use cases for vector databases into a single word, it would be ""match"". They are great at finding non-obvious ways to correspond or “match” data with a given query. Whether it's through similarity in images, text, user preferences, or patterns in data.



Here’s some examples on how to take advantage of using vector databases:"
What benefits does Qdrant Cloud on Microsoft Azure offer for rapid application development?,"Qdrant Cloud on Microsoft Azure offers the benefit of rapid application development by allowing users to deploy their own cluster through the Qdrant Cloud Console within seconds. This means that users can set up their environment on Azure quickly, reducing deployment time and enabling them to scale their resources as needed. This rapid deployment capability enables users to hit the ground running with their development projects, facilitating faster development cycles and improved scalability.","['---\n\ndraft: false\n\ntitle: Introducing Qdrant Cloud on Microsoft Azure\n\nslug: qdrant-cloud-on-microsoft-azure\n\nshort_description: Qdrant Cloud is now available on Microsoft Azure\n\ndescription: ""Learn the benefits of Qdrant Cloud on Azure.""\n\npreview_image: /blog/from_cms/qdrant-azure-2-1.png\n\ndate: 2024-01-17T08:40:42Z\n\nauthor: Manuel Meyer\n\nfeatured: false\n\ntags:\n\n  - Data Science\n\n  - Vector Database\n\n  - Machine Learning\n\n  - Information Retrieval\n\n  - Cloud\n\n  - Azure\n\n---', '---\n\ndraft: false\n\ntitle: Introducing Qdrant Cloud on Microsoft Azure\n\nslug: qdrant-cloud-on-microsoft-azure\n\nshort_description: Qdrant Cloud is now available on Microsoft Azure\n\ndescription: ""Learn the benefits of Qdrant Cloud on Azure.""\n\npreview_image: /blog/from_cms/qdrant-azure-2-1.png\n\ndate: 2024-01-17T08:40:42Z\n\nauthor: Manuel Meyer\n\nfeatured: false\n\ntags:\n\n  - Data Science\n\n  - Vector Database\n\n  - Machine Learning\n\n  - Information Retrieval\n\n  - Cloud\n\n  - Azure\n\n---', ""What this means for you:\n\n\n\n- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.\n\n- **Billion vector scale**: Seamlessly grow and handle large-scale datasets with billions of vectors. Leverage Qdrant features like horizontal scaling and binary quantization with Microsoft Azure's scalable infrastructure."", ""What this means for you:\n\n\n\n- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.\n\n- **Billion vector scale**: Seamlessly grow and handle large-scale datasets with billions of vectors. Leverage Qdrant features like horizontal scaling and binary quantization with Microsoft Azure's scalable infrastructure."", ""- Information Retrieval\n\n  - Cloud\n\n  - Azure\n\n---\n\nGreat news! We've expanded Qdrant's managed vector database offering — [Qdrant Cloud](https://cloud.qdrant.io/) — to be available on Microsoft Azure. \n\nYou can now effortlessly set up your environment on Azure, which reduces deployment time, so you can hit the ground running.\n\n\n\n[Get started](https://cloud.qdrant.io/)\n\n\n\nWhat this means for you:""]","---

draft: false

title: Introducing Qdrant Cloud on Microsoft Azure

slug: qdrant-cloud-on-microsoft-azure

short_description: Qdrant Cloud is now available on Microsoft Azure

description: ""Learn the benefits of Qdrant Cloud on Azure.""

preview_image: /blog/from_cms/qdrant-azure-2-1.png

date: 2024-01-17T08:40:42Z

author: Manuel Meyer

featured: false

tags:

  - Data Science

  - Vector Database

  - Machine Learning

  - Information Retrieval

  - Cloud

  - Azure

---
---

draft: false

title: Introducing Qdrant Cloud on Microsoft Azure

slug: qdrant-cloud-on-microsoft-azure

short_description: Qdrant Cloud is now available on Microsoft Azure

description: ""Learn the benefits of Qdrant Cloud on Azure.""

preview_image: /blog/from_cms/qdrant-azure-2-1.png

date: 2024-01-17T08:40:42Z

author: Manuel Meyer

featured: false

tags:

  - Data Science

  - Vector Database

  - Machine Learning

  - Information Retrieval

  - Cloud

  - Azure

---
What this means for you:



- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.

- **Billion vector scale**: Seamlessly grow and handle large-scale datasets with billions of vectors. Leverage Qdrant features like horizontal scaling and binary quantization with Microsoft Azure's scalable infrastructure.
What this means for you:



- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.

- **Billion vector scale**: Seamlessly grow and handle large-scale datasets with billions of vectors. Leverage Qdrant features like horizontal scaling and binary quantization with Microsoft Azure's scalable infrastructure.
- Information Retrieval

  - Cloud

  - Azure

---

Great news! We've expanded Qdrant's managed vector database offering — [Qdrant Cloud](https://cloud.qdrant.io/) — to be available on Microsoft Azure. 

You can now effortlessly set up your environment on Azure, which reduces deployment time, so you can hit the ground running.



[Get started](https://cloud.qdrant.io/)



What this means for you:"
What is the purpose of the `groupBy` parameter?,"The `groupBy` parameter is used to specify how the results should be grouped based on a specific field, like ""document_id"". By setting the `groupBy` parameter to ""document_id"", the results are organized into groups where each group contains all the hits associated with a particular document_id.","['The groups format assumes that all objects split into groups of similar objects. All objects inside\n\none group are similar, and all other objects outside this group considered dissimilar to them.\n\n\n\nBut in the case of pairs, we can only assume similarity between explicitly specified pairs of objects.\n\n\n\nWe can apply any of the approaches with our data, but pairs one seems more intuitive.\n\n\n\nThe format in which Similarity is represented determines which loss can be used.', 'The groups format assumes that all objects split into groups of similar objects. All objects inside\n\none group are similar, and all other objects outside this group considered dissimilar to them.\n\n\n\nBut in the case of pairs, we can only assume similarity between explicitly specified pairs of objects.\n\n\n\nWe can apply any of the approaches with our data, but pairs one seems more intuitive.\n\n\n\nThe format in which Similarity is represented determines which loss can be used.', ""simply don't want to for reasons we don't need to know.\n\n\n\n### Grouping requests\n\n\n\nEmbeddings are great for capturing the semantics of the documents, but we rarely encode larger pieces \n\nof data into a single vector. Having a summary of a book may sound attractive, but in reality, we \n\ndivide it into paragraphs or some different parts to have higher granularity. That pays off when we \n\nperform the semantic search, as we can return the relevant pieces only. That's also how modern tools"", ""simply don't want to for reasons we don't need to know.\n\n\n\n### Grouping requests\n\n\n\nEmbeddings are great for capturing the semantics of the documents, but we rarely encode larger pieces \n\nof data into a single vector. Having a summary of a book may sound attractive, but in reality, we \n\ndivide it into paragraphs or some different parts to have higher granularity. That pays off when we \n\nperform the semantic search, as we can return the relevant pieces only. That's also how modern tools"", 'One way of optimizing storage when using groups is to store the information shared by the points with the same group id in a single point in another collection. Then, when using the [**groups** API](#grouping-api), add the `with_lookup` parameter to bring the information from those points into each group.\n\n\n\n![Group id matches point id](/docs/lookup_id_linking.png)\n\n\n\nThis has the extra benefit of having a single point to update when the information shared by the points in a group changes.']","The groups format assumes that all objects split into groups of similar objects. All objects inside

one group are similar, and all other objects outside this group considered dissimilar to them.



But in the case of pairs, we can only assume similarity between explicitly specified pairs of objects.



We can apply any of the approaches with our data, but pairs one seems more intuitive.



The format in which Similarity is represented determines which loss can be used.
The groups format assumes that all objects split into groups of similar objects. All objects inside

one group are similar, and all other objects outside this group considered dissimilar to them.



But in the case of pairs, we can only assume similarity between explicitly specified pairs of objects.



We can apply any of the approaches with our data, but pairs one seems more intuitive.



The format in which Similarity is represented determines which loss can be used.
simply don't want to for reasons we don't need to know.



### Grouping requests



Embeddings are great for capturing the semantics of the documents, but we rarely encode larger pieces 

of data into a single vector. Having a summary of a book may sound attractive, but in reality, we 

divide it into paragraphs or some different parts to have higher granularity. That pays off when we 

perform the semantic search, as we can return the relevant pieces only. That's also how modern tools
simply don't want to for reasons we don't need to know.



### Grouping requests



Embeddings are great for capturing the semantics of the documents, but we rarely encode larger pieces 

of data into a single vector. Having a summary of a book may sound attractive, but in reality, we 

divide it into paragraphs or some different parts to have higher granularity. That pays off when we 

perform the semantic search, as we can return the relevant pieces only. That's also how modern tools
One way of optimizing storage when using groups is to store the information shared by the points with the same group id in a single point in another collection. Then, when using the [**groups** API](#grouping-api), add the `with_lookup` parameter to bring the information from those points into each group.



![Group id matches point id](/docs/lookup_id_linking.png)



This has the extra benefit of having a single point to update when the information shared by the points in a group changes."
How can one change or correct Qdrant's behavior and default collection settings using configuration files?,"To change or correct Qdrant's behavior and default collection settings, one can utilize configuration files. The default configuration file for Qdrant is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml). If you wish to modify the default configuration, you can add a new configuration file and specify the path using `--config-path path/to/custom_config.yaml","[""---\n\ntitle: Configuration\n\nweight: 160\n\naliases:\n\n  - ../configuration\n\n---\n\n\n\n# Configuration\n\n\n\nTo change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.\n\n\n\nThe default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\n\n\n\nTo change the default configuration, add a new configuration file and specify"", ""---\n\ntitle: Configuration\n\nweight: 160\n\naliases:\n\n  - ../configuration\n\n---\n\n\n\n# Configuration\n\n\n\nTo change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.\n\n\n\nThe default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\n\n\n\nTo change the default configuration, add a new configuration file and specify"", '| [Installation](guides/installation/) | Different ways to install Qdrant. | [Collections](concepts/collections/) | Learn about the central concept behind Qdrant. |                  \n\n| [Configuration](guides/configuration/)   | Update the default configuration.    | [Bulk Upload](tutorials/bulk-upload/) |   Efficiently upload a large number of vectors. |', '| [Installation](guides/installation/) | Different ways to install Qdrant. | [Collections](concepts/collections/) | Learn about the central concept behind Qdrant. |                  \n\n| [Configuration](guides/configuration/)   | Update the default configuration.    | [Bulk Upload](tutorials/bulk-upload/) |   Efficiently upload a large number of vectors. |', 'This feature enables administrators to prevent a qdrant process from using more disk space while permitting users to search and delete unnecessary data.\n\n\n\nYou can optionally provide the error message that should be used for error responses to users.\n\n\n\n## Recovery mode\n\n\n\n*Available as of v1.2.0*\n\n\n\nRecovery mode can help in situations where Qdrant fails to start repeatedly.\n\nWhen starting in recovery mode, Qdrant only loads collection metadata to prevent']","---

title: Configuration

weight: 160

aliases:

  - ../configuration

---



# Configuration



To change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.



The default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).



To change the default configuration, add a new configuration file and specify
---

title: Configuration

weight: 160

aliases:

  - ../configuration

---



# Configuration



To change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.



The default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).



To change the default configuration, add a new configuration file and specify
| [Installation](guides/installation/) | Different ways to install Qdrant. | [Collections](concepts/collections/) | Learn about the central concept behind Qdrant. |                  

| [Configuration](guides/configuration/)   | Update the default configuration.    | [Bulk Upload](tutorials/bulk-upload/) |   Efficiently upload a large number of vectors. |
| [Installation](guides/installation/) | Different ways to install Qdrant. | [Collections](concepts/collections/) | Learn about the central concept behind Qdrant. |                  

| [Configuration](guides/configuration/)   | Update the default configuration.    | [Bulk Upload](tutorials/bulk-upload/) |   Efficiently upload a large number of vectors. |
This feature enables administrators to prevent a qdrant process from using more disk space while permitting users to search and delete unnecessary data.



You can optionally provide the error message that should be used for error responses to users.



## Recovery mode



*Available as of v1.2.0*



Recovery mode can help in situations where Qdrant fails to start repeatedly.

When starting in recovery mode, Qdrant only loads collection metadata to prevent"
What are the two ways of creating batches supported by the Qdrant API and how do they differ internally?,"The Qdrant API supports two ways of creating batches - record-oriented and column-oriented. Internally, these options do not differ and are made only for the convenience of interaction. This means that both record-oriented and column-oriented batch creation methods achieve the same result in terms of processing and storing data within the Qdrant system.","['Now, thanks to the new batch search, you don’t need to worry about that. Qdrant will handle multiple search requests in just one API call and will perform those requests in the most optimal way.\n\n\n\n## An example of using the batch search\n\n\n\nWe’ve used the official Python client to show how the batch search might be integrated with your application. Since there have been some changes in the interfaces of Qdrant 0.10.0, we’ll go step by step.\n\n\n\n## Creating the collection', 'Now, thanks to the new batch search, you don’t need to worry about that. Qdrant will handle multiple search requests in just one API call and will perform those requests in the most optimal way.\n\n\n\n## An example of using the batch search\n\n\n\nWe’ve used the official Python client to show how the batch search might be integrated with your application. Since there have been some changes in the interfaces of Qdrant 0.10.0, we’ll go step by step.\n\n\n\n## Creating the collection', '## Summary\n\n\n\nBatch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!', '## Summary\n\n\n\nBatch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!', '## Benchmark\n\n\n\nThe batch search is fairly easy to be integrated into your application, but if you prefer to see some numbers before deciding to switch, then it’s worth comparing four different options:\n\n\n\n1. Querying the database sequentially.\n\n2. Using many threads/processes with individual requests.\n\n3. Utilizing the batch search of Qdrant in a single request.\n\n4. Combining parallel processing and batch search.']","Now, thanks to the new batch search, you don’t need to worry about that. Qdrant will handle multiple search requests in just one API call and will perform those requests in the most optimal way.



## An example of using the batch search



We’ve used the official Python client to show how the batch search might be integrated with your application. Since there have been some changes in the interfaces of Qdrant 0.10.0, we’ll go step by step.



## Creating the collection
Now, thanks to the new batch search, you don’t need to worry about that. Qdrant will handle multiple search requests in just one API call and will perform those requests in the most optimal way.



## An example of using the batch search



We’ve used the official Python client to show how the batch search might be integrated with your application. Since there have been some changes in the interfaces of Qdrant 0.10.0, we’ll go step by step.



## Creating the collection
## Summary



Batch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!
## Summary



Batch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!
## Benchmark



The batch search is fairly easy to be integrated into your application, but if you prefer to see some numbers before deciding to switch, then it’s worth comparing four different options:



1. Querying the database sequentially.

2. Using many threads/processes with individual requests.

3. Utilizing the batch search of Qdrant in a single request.

4. Combining parallel processing and batch search."
How can you create an index for a specific field in a payload using the Qdrant library?,"To create an index for a specific field in a payload using the Qdrant library, you can utilize the `CreatePayloadIndexAsync` method provided by the client. This method takes in the parameters `collectionName` and `fieldName`. ","['Once you create the field index, Qdrant will preserve all values of the indexed field in RAM regardless of the payload storage type.\n\n\n\nYou can specify the desired type of payload storage with [configuration file](../../guides/configuration/) or with collection parameter `on_disk_payload` during [creation](../collections/#create-collection) of the collection.\n\n\n\n## Versioning\n\n\n\nTo ensure data integrity, Qdrant performs all data changes in 2 stages.', 'Once you create the field index, Qdrant will preserve all values of the indexed field in RAM regardless of the payload storage type.\n\n\n\nYou can specify the desired type of payload storage with [configuration file](../../guides/configuration/) or with collection parameter `on_disk_payload` during [creation](../collections/#create-collection) of the collection.\n\n\n\n## Versioning\n\n\n\nTo ensure data integrity, Qdrant performs all data changes in 2 stages.', 'to set up the Qdrant [payload index](/documentation/concepts/indexing/#payload-index), so the search \n\nis more efficient. \n\n\n\n```python\n\nfrom qdrant_client import models\n\n\n\nclient.create_payload_index(\n\n    collection_name=""my_collection"",\n\n    field_name=""metadata.library"",\n\n    field_type=models.PayloadSchemaType.KEYWORD,\n\n)\n\n```\n\n\n\nThe payload index is not the only thing we want to change. Since none of the search', 'to set up the Qdrant [payload index](/documentation/concepts/indexing/#payload-index), so the search \n\nis more efficient. \n\n\n\n```python\n\nfrom qdrant_client import models\n\n\n\nclient.create_payload_index(\n\n    collection_name=""my_collection"",\n\n    field_name=""metadata.library"",\n\n    field_type=models.PayloadSchemaType.KEYWORD,\n\n)\n\n```\n\n\n\nThe payload index is not the only thing we want to change. Since none of the search', ""There are several possible reasons for that:\n\n\n\n- **Using filters without payload index** -- If you're performing a search with a filter but you don't have a payload index, Qdrant will have to load whole payload data from disk to check the filtering condition. Ensure you have adequately configured [payload indexes](../../concepts/indexing/#payload-index).""]","Once you create the field index, Qdrant will preserve all values of the indexed field in RAM regardless of the payload storage type.



You can specify the desired type of payload storage with [configuration file](../../guides/configuration/) or with collection parameter `on_disk_payload` during [creation](../collections/#create-collection) of the collection.



## Versioning



To ensure data integrity, Qdrant performs all data changes in 2 stages.
Once you create the field index, Qdrant will preserve all values of the indexed field in RAM regardless of the payload storage type.



You can specify the desired type of payload storage with [configuration file](../../guides/configuration/) or with collection parameter `on_disk_payload` during [creation](../collections/#create-collection) of the collection.



## Versioning



To ensure data integrity, Qdrant performs all data changes in 2 stages.
to set up the Qdrant [payload index](/documentation/concepts/indexing/#payload-index), so the search 

is more efficient. 



```python

from qdrant_client import models



client.create_payload_index(

    collection_name=""my_collection"",

    field_name=""metadata.library"",

    field_type=models.PayloadSchemaType.KEYWORD,

)

```



The payload index is not the only thing we want to change. Since none of the search
to set up the Qdrant [payload index](/documentation/concepts/indexing/#payload-index), so the search 

is more efficient. 



```python

from qdrant_client import models



client.create_payload_index(

    collection_name=""my_collection"",

    field_name=""metadata.library"",

    field_type=models.PayloadSchemaType.KEYWORD,

)

```



The payload index is not the only thing we want to change. Since none of the search
There are several possible reasons for that:



- **Using filters without payload index** -- If you're performing a search with a filter but you don't have a payload index, Qdrant will have to load whole payload data from disk to check the filtering condition. Ensure you have adequately configured [payload indexes](../../concepts/indexing/#payload-index)."
What is the purpose of the Quantization?,"Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces. In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.","[""Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear"", ""Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear"", 'Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\n\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\n\n\n\nThere are tradeoffs associated with quantization.\n\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.', 'Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\n\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\n\n\n\nThere are tradeoffs associated with quantization.\n\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.', "". So without further ado, let's jump into this, Andrey, we're talking about binary quantization, but let's maybe start a step back. Why do we need any quantization at all? Why not just use original vectors?""]","Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear
Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear
Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.

In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.



There are tradeoffs associated with quantization.

On the one hand, quantization allows for significant reductions in storage requirements and faster search times.
Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.

In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.



There are tradeoffs associated with quantization.

On the one hand, quantization allows for significant reductions in storage requirements and faster search times.
. So without further ado, let's jump into this, Andrey, we're talking about binary quantization, but let's maybe start a step back. Why do we need any quantization at all? Why not just use original vectors?"
How can the retrieval quality of an approximation be measured in the context of semantic search?,"The retrieval quality of an approximation in semantic search can be measured using various quality metrics. Some of these metrics include Precision@k, Mean Reciprocal Rank (MRR), and DCG and NDCG.","['search quality.\n\n\n\n### Quality metrics\n\n\n\nThere are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), \n\nare based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),', 'search quality.\n\n\n\n### Quality metrics\n\n\n\nThere are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), \n\nare based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),', '---\n\ntitle: Measure retrieval quality\n\nweight: 21\n\n---\n\n\n\n# Measure retrieval quality\n\n\n\n| Time: 30 min | Level: Intermediate |  |    |\n\n|--------------|---------------------|--|----|\n\n\n\nSemantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might\n\nbe far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another', '---\n\ntitle: Measure retrieval quality\n\nweight: 21\n\n---\n\n\n\n# Measure retrieval quality\n\n\n\n| Time: 30 min | Level: Intermediate |  |    |\n\n|--------------|---------------------|--|----|\n\n\n\nSemantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might\n\nbe far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another', '- **Search Limits**: We specify the number of results from the search process. We experimented with various search limits to measure their impact the accuracy and efficiency. We explored the trade-offs between search depth and performance. The results provide insight for applications with different precision and speed requirements.']","search quality.



### Quality metrics



There are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), 

are based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),
search quality.



### Quality metrics



There are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), 

are based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),
---

title: Measure retrieval quality

weight: 21

---



# Measure retrieval quality



| Time: 30 min | Level: Intermediate |  |    |

|--------------|---------------------|--|----|



Semantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might

be far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another
---

title: Measure retrieval quality

weight: 21

---



# Measure retrieval quality



| Time: 30 min | Level: Intermediate |  |    |

|--------------|---------------------|--|----|



Semantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might

be far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another
- **Search Limits**: We specify the number of results from the search process. We experimented with various search limits to measure their impact the accuracy and efficiency. We explored the trade-offs between search depth and performance. The results provide insight for applications with different precision and speed requirements."
Why does Qdrant deliberately exclude libraries or algorithm implementations in their benchmark comparisons?,"Qdrant excludes libraries or algorithm implementations in their benchmark comparisons because their primary focus is on vector databases. By limiting their comparisons to open-source solutions and avoiding external cloud components, Qdrant ensures hardware parity and minimizes biases. This allows them to provide accurate and unbiased benchmarks specifically tailored to the performance of vector databases, enabling users to make informed decisions based on the data provided.","['We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.\n\n\n\nIn order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.\n\n\n\nAdditionally, Qdrant uses a bunch of internal heuristics to optimize the performance.\n\nTo better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.\n\nWith this information, we can make Qdrant faster for everyone.', 'We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.\n\n\n\nIn order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.\n\n\n\nAdditionally, Qdrant uses a bunch of internal heuristics to optimize the performance.\n\nTo better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.\n\nWith this information, we can make Qdrant faster for everyone.', '. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.', '. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.', ""5. Testing and transition: Following the data migration, thoroughly test the Qdrant system. Once you're assured of the Qdrant system's stability and performance, you can make the switch.\n\n\n\n6. Monitoring and fine-tuning: After transitioning to Qdrant, maintain a close watch on its performance. It's key to continue refining the system for optimal results as needed.\n\n\n\n## Next steps""]","We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.



In order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.



Additionally, Qdrant uses a bunch of internal heuristics to optimize the performance.

To better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.

With this information, we can make Qdrant faster for everyone.
We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.



In order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.



Additionally, Qdrant uses a bunch of internal heuristics to optimize the performance.

To better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.

With this information, we can make Qdrant faster for everyone.
. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.
. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.
5. Testing and transition: Following the data migration, thoroughly test the Qdrant system. Once you're assured of the Qdrant system's stability and performance, you can make the switch.



6. Monitoring and fine-tuning: After transitioning to Qdrant, maintain a close watch on its performance. It's key to continue refining the system for optimal results as needed.



## Next steps"
What is the primary purpose of a Vector Database and why would someone choose to use it over traditional databases?,"A Vector Database is a specialized database system that is specifically designed for efficiently indexing, querying, and retrieving high-dimensional vector data. The primary purpose of a Vector Database is to enable advanced data analysis and similarity-search operations that go beyond the capabilities of traditional, structured query approaches used in conventional databases.","[""or, in other words, to understand how far apart they are.\n\n\n\nNow that we know what vector databases are and how they are structurally different than other \n\ndatabases, let's go over why they are important.\n\n\n\n## Why do we need Vector Databases?\n\n\n\nVector databases play a crucial role in various applications that require similarity search, such \n\nas recommendation systems, content-based image retrieval, and personalized search. By taking"", ""or, in other words, to understand how far apart they are.\n\n\n\nNow that we know what vector databases are and how they are structurally different than other \n\ndatabases, let's go over why they are important.\n\n\n\n## Why do we need Vector Databases?\n\n\n\nVector databases play a crucial role in various applications that require similarity search, such \n\nas recommendation systems, content-based image retrieval, and personalized search. By taking"", 'Here are [just](https://nextword.substack.com/p/vector-database-is-not-a-separate) a [few](https://stackoverflow.blog/2023/09/20/do-you-need-a-specialized-vector-database-to-implement-vector-search-well/) of [them](https://www.singlestore.com/blog/why-your-vector-database-should-not-be-a-vector-database/).\n\n\n\n\n\nThis article presents our vision and arguments on the topic .\n\nWe will:\n\n\n\n1. Explain why and when you actually need a dedicated vector solution', 'Here are [just](https://nextword.substack.com/p/vector-database-is-not-a-separate) a [few](https://stackoverflow.blog/2023/09/20/do-you-need-a-specialized-vector-database-to-implement-vector-search-well/) of [them](https://www.singlestore.com/blog/why-your-vector-database-should-not-be-a-vector-database/).\n\n\n\n\n\nThis article presents our vision and arguments on the topic .\n\nWe will:\n\n\n\n1. Explain why and when you actually need a dedicated vector solution', '## Summary\n\nUltimately, you do not need a vector database if you are looking for a simple vector search functionality with a small amount of data. We genuinely recommend starting with whatever you already have in your stack to prototype. But you need one if you are looking to do more out of it, and it is the central functionality of your application. It is just like using a multi-tool to make something quick or using a dedicated instrument highly optimized for the use case.']","or, in other words, to understand how far apart they are.



Now that we know what vector databases are and how they are structurally different than other 

databases, let's go over why they are important.



## Why do we need Vector Databases?



Vector databases play a crucial role in various applications that require similarity search, such 

as recommendation systems, content-based image retrieval, and personalized search. By taking
or, in other words, to understand how far apart they are.



Now that we know what vector databases are and how they are structurally different than other 

databases, let's go over why they are important.



## Why do we need Vector Databases?



Vector databases play a crucial role in various applications that require similarity search, such 

as recommendation systems, content-based image retrieval, and personalized search. By taking
Here are [just](https://nextword.substack.com/p/vector-database-is-not-a-separate) a [few](https://stackoverflow.blog/2023/09/20/do-you-need-a-specialized-vector-database-to-implement-vector-search-well/) of [them](https://www.singlestore.com/blog/why-your-vector-database-should-not-be-a-vector-database/).





This article presents our vision and arguments on the topic .

We will:



1. Explain why and when you actually need a dedicated vector solution
Here are [just](https://nextword.substack.com/p/vector-database-is-not-a-separate) a [few](https://stackoverflow.blog/2023/09/20/do-you-need-a-specialized-vector-database-to-implement-vector-search-well/) of [them](https://www.singlestore.com/blog/why-your-vector-database-should-not-be-a-vector-database/).





This article presents our vision and arguments on the topic .

We will:



1. Explain why and when you actually need a dedicated vector solution
## Summary

Ultimately, you do not need a vector database if you are looking for a simple vector search functionality with a small amount of data. We genuinely recommend starting with whatever you already have in your stack to prototype. But you need one if you are looking to do more out of it, and it is the central functionality of your application. It is just like using a multi-tool to make something quick or using a dedicated instrument highly optimized for the use case."
"How does oversampling impact the performance of machine learning models, especially in the context of imbalanced datasets?","Oversampling is a technique used in machine learning to address imbalances in datasets, where one class significantly outnumbers others. This imbalance can lead to skewed model performance, as the model may favor the majority class at the expense of minority classes. By generating additional samples from the minority classes, oversampling helps to equalize the representation of different classes in the training dataset.","['oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.', 'oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.', ""The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one."", ""The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one."", 'Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects.\n\n\n\n![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png)\n\n\n\n### Leveraging Binary Quantization: Best Practices']","oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one.
The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one.
Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects.



![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png)



### Leveraging Binary Quantization: Best Practices"
How does binary quantization work in the context of vector comparisons?,"Binary quantization is a method used in indexing and data compression, particularly by Qdrant, that involves splitting a data point's vector in half at a certain point. This process essentially divides the vector into two parts, marking everything above the split point as ""1"" and everything below as ""0"". The result is a string of bits that represents the original vector in a compressed form. This quantized code is much smaller and easier to compare. Especially for OpenAI embeddings, this type of quantization has proven to achieve a massive performance improvement at a lower cost of accuracy.","['This is the fastest quantization method, since it lets you perform a vector comparison with a few CPU instructions.\n\n\n\nBinary quantization can achieve up to a **40x** speedup compared to the original vectors.\n\n\n\nHowever, binary quantization is only efficient for high-dimensional vectors and require a centered distribution of vector components. \n\n\n\nAt the moment, binary quantization shows good accuracy results with the following models:', 'This is the fastest quantization method, since it lets you perform a vector comparison with a few CPU instructions.\n\n\n\nBinary quantization can achieve up to a **40x** speedup compared to the original vectors.\n\n\n\nHowever, binary quantization is only efficient for high-dimensional vectors and require a centered distribution of vector components. \n\n\n\nAt the moment, binary quantization shows good accuracy results with the following models:', "". So without further ado, let's jump into this, Andrey, we're talking about binary quantization, but let's maybe start a step back. Why do we need any quantization at all? Why not just use original vectors?"", "". So without further ado, let's jump into this, Andrey, we're talking about binary quantization, but let's maybe start a step back. Why do we need any quantization at all? Why not just use original vectors?"", 'preview_dir: /articles_data/binary-quantization/preview\n\nweight: -40\n\nauthor: Nirant Kasliwal\n\nauthor_link: \n\ndate: 2023-09-18T13:00:00+03:00\n\ndraft: false\n\nkeywords:\n\n  - vector search\n\n  - binary quantization\n\n  - memory optimization\n\n---\n\n\n\n#### Optimizing high-dimensional vectors']","This is the fastest quantization method, since it lets you perform a vector comparison with a few CPU instructions.



Binary quantization can achieve up to a **40x** speedup compared to the original vectors.



However, binary quantization is only efficient for high-dimensional vectors and require a centered distribution of vector components. 



At the moment, binary quantization shows good accuracy results with the following models:
This is the fastest quantization method, since it lets you perform a vector comparison with a few CPU instructions.



Binary quantization can achieve up to a **40x** speedup compared to the original vectors.



However, binary quantization is only efficient for high-dimensional vectors and require a centered distribution of vector components. 



At the moment, binary quantization shows good accuracy results with the following models:
. So without further ado, let's jump into this, Andrey, we're talking about binary quantization, but let's maybe start a step back. Why do we need any quantization at all? Why not just use original vectors?
. So without further ado, let's jump into this, Andrey, we're talking about binary quantization, but let's maybe start a step back. Why do we need any quantization at all? Why not just use original vectors?
preview_dir: /articles_data/binary-quantization/preview

weight: -40

author: Nirant Kasliwal

author_link: 

date: 2023-09-18T13:00:00+03:00

draft: false

keywords:

  - vector search

  - binary quantization

  - memory optimization

---



#### Optimizing high-dimensional vectors"
What is the significance of the 'always_ram' parameter in the context of vector quantization in Qdrant?,"In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times.","[""However, in some setups you might want to keep quantized vectors in RAM to speed up the search process. Then set `always_ram` to `true`.\n\n\n\n### Searching with Quantization\n\n\n\nOnce you have configured quantization for a collection, you don't need to do anything extra to search with quantization.\n\nQdrant will automatically use quantized vectors if they are available.\n\n\n\nHowever, there are a few options that you can use to control the search process:\n\n\n\n```http"", ""However, in some setups you might want to keep quantized vectors in RAM to speed up the search process. Then set `always_ram` to `true`.\n\n\n\n### Searching with Quantization\n\n\n\nOnce you have configured quantization for a collection, you don't need to do anything extra to search with quantization.\n\nQdrant will automatically use quantized vectors if they are available.\n\n\n\nHowever, there are a few options that you can use to control the search process:\n\n\n\n```http"", '}\n\n);\n\n```\n\n\n\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\n\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\n\n\n\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\n\n\n\n### Setting up Product Quantization', '}\n\n);\n\n```\n\n\n\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\n\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\n\n\n\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\n\n\n\n### Setting up Product Quantization', 'It provides configurable trade-offs between RAM usage and search speed.\n\n\n\nWe are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.\n\nPlease feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!']","However, in some setups you might want to keep quantized vectors in RAM to speed up the search process. Then set `always_ram` to `true`.



### Searching with Quantization



Once you have configured quantization for a collection, you don't need to do anything extra to search with quantization.

Qdrant will automatically use quantized vectors if they are available.



However, there are a few options that you can use to control the search process:



```http
However, in some setups you might want to keep quantized vectors in RAM to speed up the search process. Then set `always_ram` to `true`.



### Searching with Quantization



Once you have configured quantization for a collection, you don't need to do anything extra to search with quantization.

Qdrant will automatically use quantized vectors if they are available.



However, there are a few options that you can use to control the search process:



```http
}

);

```



`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.

However, in some setups you might want to keep quantized vectors in RAM to speed up the search process.



In this case, you can set `always_ram` to `true` to store quantized vectors in RAM.



### Setting up Product Quantization
}

);

```



`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.

However, in some setups you might want to keep quantized vectors in RAM to speed up the search process.



In this case, you can set `always_ram` to `true` to store quantized vectors in RAM.



### Setting up Product Quantization
It provides configurable trade-offs between RAM usage and search speed.



We are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.

Please feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!"
How can automatic backups be set up for clusters using the Cloud UI?,"Automatic backups for clusters can be set up using the Cloud UI by following the procedures listed on the page. These procedures allow you to configure snapshots on a daily, weekly, or monthly basis. You have the flexibility to keep as many snapshots as needed and can restore a cluster from the snapshot of your choice. It is important to note that during the restoration of a snapshot, the affected cluster will not be available.","['- [Create a cluster](/documentation/cloud/create-cluster/)\n\n- Set up [Authentication](/documentation/cloud/authentication/)\n\n- Configure one or more [Collections](/documentation/concepts/collections/)\n\n\n\n## Automatic backups\n\n\n\nYou can set up automatic backups of your clusters with our Cloud UI. With the\n\nprocedures listed in this page, you can set up\n\nsnapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you\n\nneed. You can restore a cluster from the snapshot of your choice.', '- [Create a cluster](/documentation/cloud/create-cluster/)\n\n- Set up [Authentication](/documentation/cloud/authentication/)\n\n- Configure one or more [Collections](/documentation/concepts/collections/)\n\n\n\n## Automatic backups\n\n\n\nYou can set up automatic backups of your clusters with our Cloud UI. With the\n\nprocedures listed in this page, you can set up\n\nsnapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you\n\nneed. You can restore a cluster from the snapshot of your choice.', '[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:\n\n\n\n1. Sign in to the dashboard\n\n1. Select Clusters.\n\n1. Select the cluster that you want to back up.\n\n   ![Select a cluster](/documentation/cloud/select-cluster.png)\n\n1. Find and select the **Backups** tab.\n\n1. Now you can set up a backup schedule.\n\n   The **Days of Retention** is the number of days after a backup snapshot is\n\n   deleted.\n\n1. Alternatively, you can select **Backup now** to take an immediate snapshot.', '[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:\n\n\n\n1. Sign in to the dashboard\n\n1. Select Clusters.\n\n1. Select the cluster that you want to back up.\n\n   ![Select a cluster](/documentation/cloud/select-cluster.png)\n\n1. Find and select the **Backups** tab.\n\n1. Now you can set up a backup schedule.\n\n   The **Days of Retention** is the number of days after a backup snapshot is\n\n   deleted.\n\n1. Alternatively, you can select **Backup now** to take an immediate snapshot.', ""![Configure a cluster backup](/documentation/cloud/backup-schedule.png)\n\n\n\n### Restore a backup\n\n\n\nIf you have a backup, it appears in the list of **Available Backups**. You can\n\nchoose to restore or delete the backups of your choice.\n\n\n\n![Restore or delete a cluster backup](/documentation/cloud/restore-delete.png)\n\n\n\n<!-- I think we should move this to the Snapshot page, but I'll do it later -->\n\n\n\n## Backups with a snapshot\n\n\n\nQdrant also offers a snapshot API which allows you to create a snapshot""]","- [Create a cluster](/documentation/cloud/create-cluster/)

- Set up [Authentication](/documentation/cloud/authentication/)

- Configure one or more [Collections](/documentation/concepts/collections/)



## Automatic backups



You can set up automatic backups of your clusters with our Cloud UI. With the

procedures listed in this page, you can set up

snapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you

need. You can restore a cluster from the snapshot of your choice.
- [Create a cluster](/documentation/cloud/create-cluster/)

- Set up [Authentication](/documentation/cloud/authentication/)

- Configure one or more [Collections](/documentation/concepts/collections/)



## Automatic backups



You can set up automatic backups of your clusters with our Cloud UI. With the

procedures listed in this page, you can set up

snapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you

need. You can restore a cluster from the snapshot of your choice.
[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:



1. Sign in to the dashboard

1. Select Clusters.

1. Select the cluster that you want to back up.

   ![Select a cluster](/documentation/cloud/select-cluster.png)

1. Find and select the **Backups** tab.

1. Now you can set up a backup schedule.

   The **Days of Retention** is the number of days after a backup snapshot is

   deleted.

1. Alternatively, you can select **Backup now** to take an immediate snapshot.
[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:



1. Sign in to the dashboard

1. Select Clusters.

1. Select the cluster that you want to back up.

   ![Select a cluster](/documentation/cloud/select-cluster.png)

1. Find and select the **Backups** tab.

1. Now you can set up a backup schedule.

   The **Days of Retention** is the number of days after a backup snapshot is

   deleted.

1. Alternatively, you can select **Backup now** to take an immediate snapshot.
![Configure a cluster backup](/documentation/cloud/backup-schedule.png)



### Restore a backup



If you have a backup, it appears in the list of **Available Backups**. You can

choose to restore or delete the backups of your choice.



![Restore or delete a cluster backup](/documentation/cloud/restore-delete.png)



<!-- I think we should move this to the Snapshot page, but I'll do it later -->



## Backups with a snapshot



Qdrant also offers a snapshot API which allows you to create a snapshot"
What are snapshots in the context of Qdrant Cloud and how are they used in a distributed setup?,"Snapshots in Qdrant Cloud are `tar` archive files that contain data and configuration of a specific collection on a specific node at a specific time. In a distributed setup with multiple nodes in a cluster, snapshots must be created for each node separately when dealing with a single collection. These snapshots can be used to archive data or easily replicate an existing deployment.","['<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>\n\n\n\n### Create full storage snapshot\n\n\n\n```http\n\nPOST /snapshots\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_full_snapshot()\n\n```', '<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>\n\n\n\n### Create full storage snapshot\n\n\n\n```http\n\nPOST /snapshots\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_full_snapshot()\n\n```', '## Store snapshots\n\n\n\nThe target directory used to store generated snapshots is controlled through the [configuration](../../guides/configuration) or using the ENV variable: `QDRANT__STORAGE__SNAPSHOTS_PATH=./snapshots`.\n\n\n\nYou can set the snapshots storage directory from the [config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml) file. If no value is given, default is `./snapshots`.\n\n\n\n```yaml\n\nstorage:\n\n  # Specify where you want to store snapshots.', '## Store snapshots\n\n\n\nThe target directory used to store generated snapshots is controlled through the [configuration](../../guides/configuration) or using the ENV variable: `QDRANT__STORAGE__SNAPSHOTS_PATH=./snapshots`.\n\n\n\nYou can set the snapshots storage directory from the [config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml) file. If no value is given, default is `./snapshots`.\n\n\n\n```yaml\n\nstorage:\n\n  # Specify where you want to store snapshots.', ""## Next steps\n\n\n\n1. If you aren't ready yet, [try out Qdrant locally](/documentation/quick-start/) or sign up for [Qdrant Cloud](https://cloud.qdrant.io/).\n\n\n\n2. For more basic information on Qdrant read our [Overview](overview/) section or learn more about Qdrant Cloud's [Free Tier](documentation/cloud/).\n\n\n\n3. If ready to migrate, please consult our [Comprehensive Guide](https://github.com/NirantK/qdrant_tools) for further details on migration steps.""]","<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>



### Create full storage snapshot



```http

POST /snapshots

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.create_full_snapshot()

```
<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>



### Create full storage snapshot



```http

POST /snapshots

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.create_full_snapshot()

```
## Store snapshots



The target directory used to store generated snapshots is controlled through the [configuration](../../guides/configuration) or using the ENV variable: `QDRANT__STORAGE__SNAPSHOTS_PATH=./snapshots`.



You can set the snapshots storage directory from the [config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml) file. If no value is given, default is `./snapshots`.



```yaml

storage:

  # Specify where you want to store snapshots.
## Store snapshots



The target directory used to store generated snapshots is controlled through the [configuration](../../guides/configuration) or using the ENV variable: `QDRANT__STORAGE__SNAPSHOTS_PATH=./snapshots`.



You can set the snapshots storage directory from the [config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml) file. If no value is given, default is `./snapshots`.



```yaml

storage:

  # Specify where you want to store snapshots.
## Next steps



1. If you aren't ready yet, [try out Qdrant locally](/documentation/quick-start/) or sign up for [Qdrant Cloud](https://cloud.qdrant.io/).



2. For more basic information on Qdrant read our [Overview](overview/) section or learn more about Qdrant Cloud's [Free Tier](documentation/cloud/).



3. If ready to migrate, please consult our [Comprehensive Guide](https://github.com/NirantK/qdrant_tools) for further details on migration steps."
What is the significance of the lowercase parameter in the context of text filters in Qdrant?,"The lowercase parameter in Qdrant is used to specify whether the index should be case-insensitive or not. When set to true, Qdrant will convert all the texts to lowercase before indexing them. This means that during searches, the case of the letters in the query will not affect the results.","['This feature is implemented as additional filters during the search and will enable you to incorporate custom logic on top of semantic similarity.\n\n\n\nDuring the filtering, Qdrant will check the conditions over those values that match the type of the filtering condition. If the stored value type does not fit the filtering condition - it will be considered not satisfied.\n\n\n\nFor example, you will get an empty output if you apply the [range condition](../filtering/#range) on the string data.', 'This feature is implemented as additional filters during the search and will enable you to incorporate custom logic on top of semantic similarity.\n\n\n\nDuring the filtering, Qdrant will check the conditions over those values that match the type of the filtering condition. If the stored value type does not fit the filtering condition - it will be considered not satisfied.\n\n\n\nFor example, you will get an empty output if you apply the [range condition](../filtering/#range) on the string data.', 'additional filters on top of the semantic search. Up until version 0.10, Qdrant only supported keyword filters. With the \n\nrelease of Qdrant 0.10, [you can now use full-text filters](https://blog.qdrant.tech/qdrant-introduces-full-text-filters-and-indexes-9a032fcb5fa) \n\nas well. This new filter type can be used on its own or in combination with other filter types to provide even more \n\nflexibility in your searches.', 'additional filters on top of the semantic search. Up until version 0.10, Qdrant only supported keyword filters. With the \n\nrelease of Qdrant 0.10, [you can now use full-text filters](https://blog.qdrant.tech/qdrant-introduces-full-text-filters-and-indexes-9a032fcb5fa) \n\nas well. This new filter type can be used on its own or in combination with other filter types to provide even more \n\nflexibility in your searches.', '. So we cannot do pre filtering or post filtering. So we had to find a database that do filtering and matching and semantic matching and search at the same time. And so Qdrant is one of them, you have other one in the market.']","This feature is implemented as additional filters during the search and will enable you to incorporate custom logic on top of semantic similarity.



During the filtering, Qdrant will check the conditions over those values that match the type of the filtering condition. If the stored value type does not fit the filtering condition - it will be considered not satisfied.



For example, you will get an empty output if you apply the [range condition](../filtering/#range) on the string data.
This feature is implemented as additional filters during the search and will enable you to incorporate custom logic on top of semantic similarity.



During the filtering, Qdrant will check the conditions over those values that match the type of the filtering condition. If the stored value type does not fit the filtering condition - it will be considered not satisfied.



For example, you will get an empty output if you apply the [range condition](../filtering/#range) on the string data.
additional filters on top of the semantic search. Up until version 0.10, Qdrant only supported keyword filters. With the 

release of Qdrant 0.10, [you can now use full-text filters](https://blog.qdrant.tech/qdrant-introduces-full-text-filters-and-indexes-9a032fcb5fa) 

as well. This new filter type can be used on its own or in combination with other filter types to provide even more 

flexibility in your searches.
additional filters on top of the semantic search. Up until version 0.10, Qdrant only supported keyword filters. With the 

release of Qdrant 0.10, [you can now use full-text filters](https://blog.qdrant.tech/qdrant-introduces-full-text-filters-and-indexes-9a032fcb5fa) 

as well. This new filter type can be used on its own or in combination with other filter types to provide even more 

flexibility in your searches.
. So we cannot do pre filtering or post filtering. So we had to find a database that do filtering and matching and semantic matching and search at the same time. And so Qdrant is one of them, you have other one in the market."
How does adjusting the `write_consistency_factor` parameter impact write operations in a distributed deployment using Qdrant?,"The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully.","['- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.', '- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.', 'In addition, you have to make sure:\n\n\n\n* To use a performant [persistent storage](#storage) for your data\n\n* To configure the [security settings](/documentation/guides/security/) for your deployment\n\n* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)\n\n* To set up a load balancer for your Qdrant cluster\n\n* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data', 'In addition, you have to make sure:\n\n\n\n* To use a performant [persistent storage](#storage) for your data\n\n* To configure the [security settings](/documentation/guides/security/) for your deployment\n\n* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)\n\n* To set up a load balancer for your Qdrant cluster\n\n* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data', '- Locking applies to a single node only. It is necessary to call lock on all the desired nodes in a distributed deployment setup.\n\n\n\nLock request sample:\n\n\n\n```http\n\nPOST /locks\n\n{\n\n    ""error_message"": ""write is forbidden"",\n\n    ""write"": true\n\n}\n\n```\n\n\n\nWrite flags enables/disables write lock.\n\nIf the write lock is set to true, qdrant doesn\'t allow creating new collections or adding new data to the existing storage.\n\nHowever, deletion operations or updates are not forbidden under the write lock.']","- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
In addition, you have to make sure:



* To use a performant [persistent storage](#storage) for your data

* To configure the [security settings](/documentation/guides/security/) for your deployment

* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)

* To set up a load balancer for your Qdrant cluster

* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data
In addition, you have to make sure:



* To use a performant [persistent storage](#storage) for your data

* To configure the [security settings](/documentation/guides/security/) for your deployment

* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)

* To set up a load balancer for your Qdrant cluster

* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data
- Locking applies to a single node only. It is necessary to call lock on all the desired nodes in a distributed deployment setup.



Lock request sample:



```http

POST /locks

{

    ""error_message"": ""write is forbidden"",

    ""write"": true

}

```



Write flags enables/disables write lock.

If the write lock is set to true, qdrant doesn't allow creating new collections or adding new data to the existing storage.

However, deletion operations or updates are not forbidden under the write lock."
What are some of the features and support services that come pre-configured with each instance in Qdrant Cloud?,"Each instance in Qdrant Cloud comes pre-configured with the following tools, features, and support services:1. Automatically created with the latest available version of Qdrant.
2. Upgradeable to later versions of Qdrant as they are released.
3. Equipped with monitoring and logging to observe the health of each cluster.
4. Accessible through the Qdrant Cloud Console.
5. Vertically scalable.
6. Offered on AWS and GCP, with Azure currently in development.
","['3. [**Spin up a Qdrant Cloud cluster:**](cloud/) the recommended method to run Qdrant in production. Read [Quickstart](cloud/quickstart-cloud/) to setup your first instance.\n\n\n\n### Recommended Workflow:\n\n\n\n![Local mode workflow](https://raw.githubusercontent.com/qdrant/qdrant-client/master/docs/images/try-develop-deploy.png)', '3. [**Spin up a Qdrant Cloud cluster:**](cloud/) the recommended method to run Qdrant in production. Read [Quickstart](cloud/quickstart-cloud/) to setup your first instance.\n\n\n\n### Recommended Workflow:\n\n\n\n![Local mode workflow](https://raw.githubusercontent.com/qdrant/qdrant-client/master/docs/images/try-develop-deploy.png)', ""## Next steps\n\n\n\n1. If you aren't ready yet, [try out Qdrant locally](/documentation/quick-start/) or sign up for [Qdrant Cloud](https://cloud.qdrant.io/).\n\n\n\n2. For more basic information on Qdrant read our [Overview](overview/) section or learn more about Qdrant Cloud's [Free Tier](documentation/cloud/).\n\n\n\n3. If ready to migrate, please consult our [Comprehensive Guide](https://github.com/NirantK/qdrant_tools) for further details on migration steps."", ""## Next steps\n\n\n\n1. If you aren't ready yet, [try out Qdrant locally](/documentation/quick-start/) or sign up for [Qdrant Cloud](https://cloud.qdrant.io/).\n\n\n\n2. For more basic information on Qdrant read our [Overview](overview/) section or learn more about Qdrant Cloud's [Free Tier](documentation/cloud/).\n\n\n\n3. If ready to migrate, please consult our [Comprehensive Guide](https://github.com/NirantK/qdrant_tools) for further details on migration steps."", '## Prerequisites\n\n\n\nBefore you start, make sure you have the following:\n\n\n\n1. A Qdrant instance to connect to. You can get free cloud instance [cloud.qdrant.io](https://cloud.qdrant.io/). \n\n2. An account at Make.com. You can register yourself [here](https://www.make.com/en/register).\n\n\n\n## Setting up a connection\n\n\n\nNavigate to your scenario on the Make dashboard and select a Qdrant app module to start a connection.\n\n![Qdrant Make connection](/documentation/frameworks/make/connection.png)']","3. [**Spin up a Qdrant Cloud cluster:**](cloud/) the recommended method to run Qdrant in production. Read [Quickstart](cloud/quickstart-cloud/) to setup your first instance.



### Recommended Workflow:



![Local mode workflow](https://raw.githubusercontent.com/qdrant/qdrant-client/master/docs/images/try-develop-deploy.png)
3. [**Spin up a Qdrant Cloud cluster:**](cloud/) the recommended method to run Qdrant in production. Read [Quickstart](cloud/quickstart-cloud/) to setup your first instance.



### Recommended Workflow:



![Local mode workflow](https://raw.githubusercontent.com/qdrant/qdrant-client/master/docs/images/try-develop-deploy.png)
## Next steps



1. If you aren't ready yet, [try out Qdrant locally](/documentation/quick-start/) or sign up for [Qdrant Cloud](https://cloud.qdrant.io/).



2. For more basic information on Qdrant read our [Overview](overview/) section or learn more about Qdrant Cloud's [Free Tier](documentation/cloud/).



3. If ready to migrate, please consult our [Comprehensive Guide](https://github.com/NirantK/qdrant_tools) for further details on migration steps.
## Next steps



1. If you aren't ready yet, [try out Qdrant locally](/documentation/quick-start/) or sign up for [Qdrant Cloud](https://cloud.qdrant.io/).



2. For more basic information on Qdrant read our [Overview](overview/) section or learn more about Qdrant Cloud's [Free Tier](documentation/cloud/).



3. If ready to migrate, please consult our [Comprehensive Guide](https://github.com/NirantK/qdrant_tools) for further details on migration steps.
## Prerequisites



Before you start, make sure you have the following:



1. A Qdrant instance to connect to. You can get free cloud instance [cloud.qdrant.io](https://cloud.qdrant.io/). 

2. An account at Make.com. You can register yourself [here](https://www.make.com/en/register).



## Setting up a connection



Navigate to your scenario on the Make dashboard and select a Qdrant app module to start a connection.

![Qdrant Make connection](/documentation/frameworks/make/connection.png)"
How is the `must_not` clause used in conjunction with other clauses in filtering data?,"The `must_not` clause in filtering data is used to exclude documents that meet specific criteria. The `must_not` clause can be used in combination with the `must` clause to create a more refined filter. Specifically, the `must_not` clause is used to specify conditions that must not be met for a document to be included in the results.","['{ ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }\n\n]\n\n```\n\n\n\nWhen using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.\n\nIn this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.\n\n\n\n### Clauses combination\n\n\n\nIt is also possible to use several clauses simultaneously:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""must"": [', '{ ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }\n\n]\n\n```\n\n\n\nWhen using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.\n\nIn this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.\n\n\n\n### Clauses combination\n\n\n\nIt is also possible to use several clauses simultaneously:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""must"": [', ""---\n\ndraft: false\n\nid: 5\n\ntitle:\n\ndescription: '<b> Updated: Feb 2023 </b>'\n\n\n\nfilter_data: /benchmarks/filter-result-2023-02-03.json\n\ndate: 2023-02-13\n\nweight: 4\n\n---\n\n\n\n\n\n## Filtered Results\n\n\n\nAs you can see from the charts, there are three main patterns:\n\n\n\n- **Speed boost** - for some engines/queries, the filtered search is faster than the unfiltered one. It might happen if the filter is restrictive enough, to completely avoid the usage of the vector index."", ""---\n\ndraft: false\n\nid: 5\n\ntitle:\n\ndescription: '<b> Updated: Feb 2023 </b>'\n\n\n\nfilter_data: /benchmarks/filter-result-2023-02-03.json\n\ndate: 2023-02-13\n\nweight: 4\n\n---\n\n\n\n\n\n## Filtered Results\n\n\n\nAs you can see from the charts, there are three main patterns:\n\n\n\n- **Speed boost** - for some engines/queries, the filtered search is faster than the unfiltered one. It might happen if the filter is restrictive enough, to completely avoid the usage of the vector index."", '[{ ""id"": 2, ""city"": ""London"", ""color"": ""red"" }]\n\n```\n\n\n\nWhen using `must`, the clause becomes `true` only if every condition listed inside `must` is satisfied.\n\nIn this sense, `must` is equivalent to the operator `AND`.\n\n\n\n### Should\n\n\n\nExample:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""should"": [\n\n            { ""key"": ""city"", ""match"": { ""value"": ""London"" } },\n\n            { ""key"": ""color"", ""match"": { ""value"": ""red"" } }\n\n        ]\n\n    }\n\n}\n\n```']","{ ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }

]

```



When using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.

In this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.



### Clauses combination



It is also possible to use several clauses simultaneously:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""must"": [
{ ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }

]

```



When using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.

In this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.



### Clauses combination



It is also possible to use several clauses simultaneously:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""must"": [
---

draft: false

id: 5

title:

description: '<b> Updated: Feb 2023 </b>'



filter_data: /benchmarks/filter-result-2023-02-03.json

date: 2023-02-13

weight: 4

---





## Filtered Results



As you can see from the charts, there are three main patterns:



- **Speed boost** - for some engines/queries, the filtered search is faster than the unfiltered one. It might happen if the filter is restrictive enough, to completely avoid the usage of the vector index.
---

draft: false

id: 5

title:

description: '<b> Updated: Feb 2023 </b>'



filter_data: /benchmarks/filter-result-2023-02-03.json

date: 2023-02-13

weight: 4

---





## Filtered Results



As you can see from the charts, there are three main patterns:



- **Speed boost** - for some engines/queries, the filtered search is faster than the unfiltered one. It might happen if the filter is restrictive enough, to completely avoid the usage of the vector index.
[{ ""id"": 2, ""city"": ""London"", ""color"": ""red"" }]

```



When using `must`, the clause becomes `true` only if every condition listed inside `must` is satisfied.

In this sense, `must` is equivalent to the operator `AND`.



### Should



Example:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""should"": [

            { ""key"": ""city"", ""match"": { ""value"": ""London"" } },

            { ""key"": ""color"", ""match"": { ""value"": ""red"" } }

        ]

    }

}

```"
What are some advantages of vector search over keyword-based search?,"Vector search has clear advantages over keyword-based search in various scenarios as outlined in the document chunk. Some of these advantages include:

1. Multi-lingual & multi-modal search: Vector search is effective in handling searches across multiple languages and modalities.
2. For short texts with typos and ambiguous content-dependent meanings: Vector search excels in situations where the search queries are short, contain typos, or have ambiguous meanings.
3. Specialized domains with tuned encoder models: Vector search is beneficial in specialized domains where encoder models can be fine-tuned for better search results.
4. Document-as-a-Query similarity search: Vector search allows for similarity searches where the entire document can be used as a query to find similar documents.

While vector search offers these advantages, it is important to note that keyword-based search still has its relevance in certain cases. For example, in out-of-domain search.","[""2. Vector search with keyword-based search. This one is covered in this article.\n\n3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.\n\n\n\n## Why do we still need keyword search?\n\n\n\nA keyword-based search was the obvious choice for search engines in the past. It struggled with some\n\ncommon issues, but since we didn't have any alternatives, we had to overcome them with additional"", ""2. Vector search with keyword-based search. This one is covered in this article.\n\n3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.\n\n\n\n## Why do we still need keyword search?\n\n\n\nA keyword-based search was the obvious choice for search engines in the past. It struggled with some\n\ncommon issues, but since we didn't have any alternatives, we had to overcome them with additional"", 'There is no one-size-fits-all approach that would not compromise on performance or flexibility.\n\nSo if your use case utilizes vector search in any significant way, it is worth investing in a dedicated vector search engine, aka vector database.', 'There is no one-size-fits-all approach that would not compromise on performance or flexibility.\n\nSo if your use case utilizes vector search in any significant way, it is worth investing in a dedicated vector search engine, aka vector database.', ""In search engines and research databases, they enhance the user experience by providing results that are **semantically** similar to the query. They do not rely solely on the exact words typed into the search bar.\n\n\n\nIf you're new to the vector search space, this article explains the key concepts and relationships that you need to know.\n\n\n\nSo let's get into it.\n\n\n\n\n\n## What is Vector Data?\n\n\n\nTo understand vector databases, let's begin by defining what is a 'vector' or 'vector data'.""]","2. Vector search with keyword-based search. This one is covered in this article.

3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.



## Why do we still need keyword search?



A keyword-based search was the obvious choice for search engines in the past. It struggled with some

common issues, but since we didn't have any alternatives, we had to overcome them with additional
2. Vector search with keyword-based search. This one is covered in this article.

3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.



## Why do we still need keyword search?



A keyword-based search was the obvious choice for search engines in the past. It struggled with some

common issues, but since we didn't have any alternatives, we had to overcome them with additional
There is no one-size-fits-all approach that would not compromise on performance or flexibility.

So if your use case utilizes vector search in any significant way, it is worth investing in a dedicated vector search engine, aka vector database.
There is no one-size-fits-all approach that would not compromise on performance or flexibility.

So if your use case utilizes vector search in any significant way, it is worth investing in a dedicated vector search engine, aka vector database.
In search engines and research databases, they enhance the user experience by providing results that are **semantically** similar to the query. They do not rely solely on the exact words typed into the search bar.



If you're new to the vector search space, this article explains the key concepts and relationships that you need to know.



So let's get into it.





## What is Vector Data?



To understand vector databases, let's begin by defining what is a 'vector' or 'vector data'."
What is the purpose of the DocArray library and how does it relate to the Qdrant engine?,"The DocArray library is designed to define a structure for unstructured data, making it easier to process a collection of documents that may include various data types such as audio, video, and text. This library simplifies the handling of these diverse types of data within a single framework. On the other hand, the Qdrant engine focuses on enabling the scaling of vector search and storage capabilities. By integrating the DocArray library with the Qdrant engine, users can benefit from efficient processing of unstructured data and enhanced scalability for vector search operations. This integration streamlines the handling of different data types while also optimizing the performance of vector search and storage functions.","['To install DocArray with Qdrant support, please do\n\n\n\n```bash\n\npip install ""docarray[qdrant]""\n\n```\n\n\n\nMore information can be found in [DocArray\'s documentations](https://docarray.jina.ai/advanced/document-store/qdrant/).', 'To install DocArray with Qdrant support, please do\n\n\n\n```bash\n\npip install ""docarray[qdrant]""\n\n```\n\n\n\nMore information can be found in [DocArray\'s documentations](https://docarray.jina.ai/advanced/document-store/qdrant/).', 'DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,\n\nincluding audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.\n\n\n\nRead more about the integration by this [link](/documentation/install/#docarray)', 'DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,\n\nincluding audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.\n\n\n\nRead more about the integration by this [link](/documentation/install/#docarray)', '---\n\ntitle: DocArray\n\nweight: 300\n\naliases: [ ../integrations/docarray/ ]\n\n---\n\n\n\n# DocArray\n\nYou can use Qdrant natively in DocArray, where Qdrant serves as a high-performance document store to enable scalable vector search.\n\n\n\nDocArray is a library from Jina AI for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc.\n\nIt allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API.']","To install DocArray with Qdrant support, please do



```bash

pip install ""docarray[qdrant]""

```



More information can be found in [DocArray's documentations](https://docarray.jina.ai/advanced/document-store/qdrant/).
To install DocArray with Qdrant support, please do



```bash

pip install ""docarray[qdrant]""

```



More information can be found in [DocArray's documentations](https://docarray.jina.ai/advanced/document-store/qdrant/).
DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,

including audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.



Read more about the integration by this [link](/documentation/install/#docarray)
DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,

including audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.



Read more about the integration by this [link](/documentation/install/#docarray)
---

title: DocArray

weight: 300

aliases: [ ../integrations/docarray/ ]

---



# DocArray

You can use Qdrant natively in DocArray, where Qdrant serves as a high-performance document store to enable scalable vector search.



DocArray is a library from Jina AI for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc.

It allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API."
What are the search-time parameters that can be used to tune the search accuracy and speed?,"The search-time parameters that can be used to tune the search accuracy and speed are ""hnsw_ef"" and ""exact"". The ""hnsw_ef"" parameter is set to 128, which controls the search accuracy by specifying the number of neighbors to inspect during the search process. The ""exact"" parameter is set to false, indicating that an approximate search method is used for faster retrieval of results. These parameters are essential for optimizing the search process in the context of the QdrantClient for efficient retrieval of relevant data points.","['Of course, some of them support a different subset of functionalities, and those might be a key factor to make the decision.\n\nBut in general, we all care about the search precision, speed, and resources required to achieve it.', 'Of course, some of them support a different subset of functionalities, and those might be a key factor to make the decision.\n\nBut in general, we all care about the search precision, speed, and resources required to achieve it.', '- **Search Limits**: We specify the number of results from the search process. We experimented with various search limits to measure their impact the accuracy and efficiency. We explored the trade-offs between search depth and performance. The results provide insight for applications with different precision and speed requirements.', '- **Search Limits**: We specify the number of results from the search process. We experimented with various search limits to measure their impact the accuracy and efficiency. We explored the trade-offs between search depth and performance. The results provide insight for applications with different precision and speed requirements.', 'search quality.\n\n\n\n### Quality metrics\n\n\n\nThere are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), \n\nare based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),']","Of course, some of them support a different subset of functionalities, and those might be a key factor to make the decision.

But in general, we all care about the search precision, speed, and resources required to achieve it.
Of course, some of them support a different subset of functionalities, and those might be a key factor to make the decision.

But in general, we all care about the search precision, speed, and resources required to achieve it.
- **Search Limits**: We specify the number of results from the search process. We experimented with various search limits to measure their impact the accuracy and efficiency. We explored the trade-offs between search depth and performance. The results provide insight for applications with different precision and speed requirements.
- **Search Limits**: We specify the number of results from the search process. We experimented with various search limits to measure their impact the accuracy and efficiency. We explored the trade-offs between search depth and performance. The results provide insight for applications with different precision and speed requirements.
search quality.



### Quality metrics



There are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), 

are based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),"
What is the significance of using named vectors in a collection within the Qdrant system?,"Named vectors in a collection within the Qdrant system allow for the inclusion of multiple vectors in a single point, with each vector having its own dimensionality and metric requirements. This feature enables more flexibility in organizing and structuring data within a collection, as different vectors can represent distinct aspects or features of the data points. By utilizing named vectors, users can better tailor the representation of their data to suit specific analysis or search requirements, enhancing the overall efficiency and effectiveness of the system.","[""with_vectors=False, \n\n    )\n\n)\n\n```\n\n\n\n### Qdrant web user interface\n\n\n\nWe are excited to announce a more user-friendly way to organize and work with your collections inside of Qdrant. Our dashboard's design is simple, but very intuitive and easy to access."", ""with_vectors=False, \n\n    )\n\n)\n\n```\n\n\n\n### Qdrant web user interface\n\n\n\nWe are excited to announce a more user-friendly way to organize and work with your collections inside of Qdrant. Our dashboard's design is simple, but very intuitive and easy to access."", '- **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.\n\n\n\n\n\n#### Memory and speed tuning\n\n\n\nIn this section, we will discuss how to tune the memory and speed of the search process with quantization.\n\n\n\nThere are 3 possible modes to place storage of vectors within the qdrant collection:', '- **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.\n\n\n\n\n\n#### Memory and speed tuning\n\n\n\nIn this section, we will discuss how to tune the memory and speed of the search process with quantization.\n\n\n\nThere are 3 possible modes to place storage of vectors within the qdrant collection:', 'To move onto some more complex examples of vector search, read our [Tutorials](../tutorials/) and create your own app with the help of our [Examples](../examples/).\n\n\n\n**Note:** There is another way of running Qdrant locally. If you are a Python developer, we recommend that you try Local Mode in [Qdrant Client](https://github.com/qdrant/qdrant-client), as it only takes a few moments to get setup.']","with_vectors=False, 

    )

)

```



### Qdrant web user interface



We are excited to announce a more user-friendly way to organize and work with your collections inside of Qdrant. Our dashboard's design is simple, but very intuitive and easy to access.
with_vectors=False, 

    )

)

```



### Qdrant web user interface



We are excited to announce a more user-friendly way to organize and work with your collections inside of Qdrant. Our dashboard's design is simple, but very intuitive and easy to access.
- **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.





#### Memory and speed tuning



In this section, we will discuss how to tune the memory and speed of the search process with quantization.



There are 3 possible modes to place storage of vectors within the qdrant collection:
- **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.





#### Memory and speed tuning



In this section, we will discuss how to tune the memory and speed of the search process with quantization.



There are 3 possible modes to place storage of vectors within the qdrant collection:
To move onto some more complex examples of vector search, read our [Tutorials](../tutorials/) and create your own app with the help of our [Examples](../examples/).



**Note:** There is another way of running Qdrant locally. If you are a Python developer, we recommend that you try Local Mode in [Qdrant Client](https://github.com/qdrant/qdrant-client), as it only takes a few moments to get setup."
What parameters can be configured in the configuration file to improve performance in HNSW indexing?,"In the context of HNSW indexing, the parameters that can be configured in the configuration file to enhance performance are `m`, `ef_construct`, and `ef`. 

- `m`: This parameter represents the number of edges per node in the index graph. A higher value for `m` leads to more accurate search results but also requires more space. By adjusting `m`, you can control the trade-off between search accuracy and space efficiency.

- `ef_construct`: When building the index, `ef_construct` specifies the number of neighbors to consider. Increasing the value of `ef_construct` improves the accuracy of the search but also increases the time required to build the index. 

- `ef`: This parameter is used when searching for targets. It determines the search range for finding nearest neighbors. By setting an appropriate value for `ef`, you can optimize the search process to balance efficiency and accuracy in finding the nearest neighbors within the specified search range.","['### Indexing vectors in HNSW\n\n\n\nIn some cases, you might be surprised the value of `indexed_vectors_count` is lower than `vectors_count`. This is an intended behaviour and\n\ndepends on the [optimizer configuration](../optimizer). A new index segment is built if the size of non-indexed vectors is higher than the\n\nvalue of `indexing_threshold`(in kB).  If your collection is very small or the dimensionality of the vectors is low, there might be no HNSW segment', '### Indexing vectors in HNSW\n\n\n\nIn some cases, you might be surprised the value of `indexed_vectors_count` is lower than `vectors_count`. This is an intended behaviour and\n\ndepends on the [optimizer configuration](../optimizer). A new index segment is built if the size of non-indexed vectors is higher than the\n\nvalue of `indexing_threshold`(in kB).  If your collection is very small or the dimensionality of the vectors is low, there might be no HNSW segment', 'In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.\n\n\n\nThe corresponding parameters could be configured in the configuration file:\n\n\n\n```yaml\n\nstorage:\n\n  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually\n\n  hnsw_index:\n\n    # Number of edges per node in the index graph.', 'In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.\n\n\n\nThe corresponding parameters could be configured in the configuration file:\n\n\n\n```yaml\n\nstorage:\n\n  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually\n\n  hnsw_index:\n\n    # Number of edges per node in the index graph.', '# If `max_optimization_threads = 0`, optimization will be disabled.\n\n    max_optimization_threads: 1\n\n\n\n  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually\n\n  hnsw_index:\n\n    # Number of edges per node in the index graph. Larger the value - more accurate the search, more space required.\n\n    m: 16\n\n    # Number of neighbours to consider during the index building. Larger the value - more accurate the search, more time required to build index.']","### Indexing vectors in HNSW



In some cases, you might be surprised the value of `indexed_vectors_count` is lower than `vectors_count`. This is an intended behaviour and

depends on the [optimizer configuration](../optimizer). A new index segment is built if the size of non-indexed vectors is higher than the

value of `indexing_threshold`(in kB).  If your collection is very small or the dimensionality of the vectors is low, there might be no HNSW segment
### Indexing vectors in HNSW



In some cases, you might be surprised the value of `indexed_vectors_count` is lower than `vectors_count`. This is an intended behaviour and

depends on the [optimizer configuration](../optimizer). A new index segment is built if the size of non-indexed vectors is higher than the

value of `indexing_threshold`(in kB).  If your collection is very small or the dimensionality of the vectors is low, there might be no HNSW segment
In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.



The corresponding parameters could be configured in the configuration file:



```yaml

storage:

  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually

  hnsw_index:

    # Number of edges per node in the index graph.
In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.



The corresponding parameters could be configured in the configuration file:



```yaml

storage:

  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually

  hnsw_index:

    # Number of edges per node in the index graph.
# If `max_optimization_threads = 0`, optimization will be disabled.

    max_optimization_threads: 1



  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually

  hnsw_index:

    # Number of edges per node in the index graph. Larger the value - more accurate the search, more space required.

    m: 16

    # Number of neighbours to consider during the index building. Larger the value - more accurate the search, more time required to build index."
How does product quantization differ from scalar quantization in terms of compression and computational efficiency?,"Product quantization involves dividing data into chunks and quantizing each segment individually, approximating each chunk with a centroid index that represents the original vector component. The positions of these centroids are determined through clustering algorithms like k-means, with Qdrant currently utilizing 256 centroids, allowing each centroid index to be represented by a single byte. Product quantization can achieve higher compression factors compared to scalar quantization. However, there are tradeoffs to consider. Product quantization distance calculations are not SIMD-friendly, resulting in slower computational speed compared to scalar quantization. Additionally, product quantization introduces a loss of accuracy, making it more suitable for high-dimensional vectors. To optimize quantization parameters for specific use cases, it is recommended to refer to the ""Quantization Tips"" section for more information.","['. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.', '. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.', 'But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\n\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\n\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n## How to choose the right quantization method', 'But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\n\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\n\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n## How to choose the right quantization method', '* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.\n\n\n\n## Setting up Quantization in Qdrant\n\n\n\nYou can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.']",". For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method
But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method
* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.



## Setting up Quantization in Qdrant



You can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration."
What is the significance of Qdrant's approach to storing multiple vectors per object in data indexing?,"Qdrant's approach to storing multiple vectors per object in data indexing is significant as it opens up new possibilities in data representation and retrieval. By allowing multiple vectors to be associated with a single object, Qdrant enables more nuanced and detailed analysis of the data. This approach can be particularly beneficial in applications such as data science, neural networks, database management, and similarity search. It provides a more comprehensive understanding of the relationships between objects and enhances the accuracy and efficiency of search and retrieval processes. Overall, Qdrant's innovative vector storage strategy contributes to pushing the boundaries of data indexing and offers practical applications and benefits in various fields.","[""---\n\ndraft: false\n\ntitle: Storing multiple vectors per object in Qdrant\n\nslug: storing-multiple-vectors-per-object-in-qdrant\n\nshort_description: Qdrant's approach to storing multiple vectors per object,\n\n  unraveling new possibilities in data representation and retrieval.\n\ndescription: Discover how Qdrant continues to push the boundaries of data\n\n  indexing, providing insights into the practical applications and benefits of\n\n  this novel vector storage strategy."", ""---\n\ndraft: false\n\ntitle: Storing multiple vectors per object in Qdrant\n\nslug: storing-multiple-vectors-per-object-in-qdrant\n\nshort_description: Qdrant's approach to storing multiple vectors per object,\n\n  unraveling new possibilities in data representation and retrieval.\n\ndescription: Discover how Qdrant continues to push the boundaries of data\n\n  indexing, providing insights into the practical applications and benefits of\n\n  this novel vector storage strategy."", 'author_link: https://medium.com/@lukawskikacper\n\ndate: 2022-09-19T13:30:00+02:00\n\ndraft: false\n\n---\n\n\n\n[Qdrant 0.10 is a new version](https://github.com/qdrant/qdrant/releases/tag/v0.10.0) that brings a lot of performance \n\nimprovements, but also some new features which were heavily requested by our users. Here is an overview of what has changed.\n\n\n\n## Storing multiple vectors per object', 'author_link: https://medium.com/@lukawskikacper\n\ndate: 2022-09-19T13:30:00+02:00\n\ndraft: false\n\n---\n\n\n\n[Qdrant 0.10 is a new version](https://github.com/qdrant/qdrant/releases/tag/v0.10.0) that brings a lot of performance \n\nimprovements, but also some new features which were heavily requested by our users. Here is an overview of what has changed.\n\n\n\n## Storing multiple vectors per object', '## Usage with Qdrant\n\n\n\nQdrant is a Vector Store, offering a comprehensive, efficient, and scalable solution for modern machine learning and AI applications. Whether you are dealing with billions of data points, require a low latency performant vector solution, or specialized quantization methods – [Qdrant is engineered](https://qdrant.tech/documentation/overview/)\xa0to meet those demands head-on.']","---

draft: false

title: Storing multiple vectors per object in Qdrant

slug: storing-multiple-vectors-per-object-in-qdrant

short_description: Qdrant's approach to storing multiple vectors per object,

  unraveling new possibilities in data representation and retrieval.

description: Discover how Qdrant continues to push the boundaries of data

  indexing, providing insights into the practical applications and benefits of

  this novel vector storage strategy.
---

draft: false

title: Storing multiple vectors per object in Qdrant

slug: storing-multiple-vectors-per-object-in-qdrant

short_description: Qdrant's approach to storing multiple vectors per object,

  unraveling new possibilities in data representation and retrieval.

description: Discover how Qdrant continues to push the boundaries of data

  indexing, providing insights into the practical applications and benefits of

  this novel vector storage strategy.
author_link: https://medium.com/@lukawskikacper

date: 2022-09-19T13:30:00+02:00

draft: false

---



[Qdrant 0.10 is a new version](https://github.com/qdrant/qdrant/releases/tag/v0.10.0) that brings a lot of performance 

improvements, but also some new features which were heavily requested by our users. Here is an overview of what has changed.



## Storing multiple vectors per object
author_link: https://medium.com/@lukawskikacper

date: 2022-09-19T13:30:00+02:00

draft: false

---



[Qdrant 0.10 is a new version](https://github.com/qdrant/qdrant/releases/tag/v0.10.0) that brings a lot of performance 

improvements, but also some new features which were heavily requested by our users. Here is an overview of what has changed.



## Storing multiple vectors per object
## Usage with Qdrant



Qdrant is a Vector Store, offering a comprehensive, efficient, and scalable solution for modern machine learning and AI applications. Whether you are dealing with billions of data points, require a low latency performant vector solution, or specialized quantization methods – [Qdrant is engineered](https://qdrant.tech/documentation/overview/) to meet those demands head-on."
What configuration is required to enable in-memory quantization with on-disk original vectors in Qdrant?,"To configure in-memory quantization with on-disk original vectors in Qdrant, you need to create a collection with the following configuration:

```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 768,

        ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": true

        }

    }

}
```

Additionally, in the Python code snippet provided, you can use the QdrantClient library to create the collection with the specified configuration parameters. This includes defining vector size, distance metric (such as Cosine similarity), memmap threshold for optimization, and scalar quantization configuration with type set to INT8","[""```\n\n\n\nWhile Qdrant offers various options to store some parts of the data on disk, starting \n\nfrom version 1.1.0, you can also optimize your memory by compressing the embeddings. \n\nWe've implemented the mechanism of **Scalar Quantization**! It turns out to have not \n\nonly a positive impact on memory but also on the performance. \n\n\n\n## Scalar Quantization\n\n\n\nScalar quantization is a data compression technique that converts floating point values"", ""```\n\n\n\nWhile Qdrant offers various options to store some parts of the data on disk, starting \n\nfrom version 1.1.0, you can also optimize your memory by compressing the embeddings. \n\nWe've implemented the mechanism of **Scalar Quantization**! It turns out to have not \n\nonly a positive impact on memory but also on the performance. \n\n\n\n## Scalar Quantization\n\n\n\nScalar quantization is a data compression technique that converts floating point values"", '5. RAM: Store the full vectors and payload on disk. Limit what you load from memory to the binary quantization index. This helps reduce the memory footprint and improve the overall efficiency of the system. The incremental latency from the disk read is negligible compared to the latency savings from the binary scoring in Qdrant, which uses SIMD instructions where possible.\n\n\n\nWant to discuss these findings and learn more about Binary Quantization? [Join our Discord community.](https://discord.gg/qdrant)', '5. RAM: Store the full vectors and payload on disk. Limit what you load from memory to the binary quantization index. This helps reduce the memory footprint and improve the overall efficiency of the system. The incremental latency from the disk read is negligible compared to the latency savings from the binary scoring in Qdrant, which uses SIMD instructions where possible.\n\n\n\nWant to discuss these findings and learn more about Binary Quantization? [Join our Discord community.](https://discord.gg/qdrant)', '- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself\n\n- Situations where the dimensionality of the original vectors is sufficiently high\n\n- Cases where indexing speed is not a critical factor\n\n\n\nIn circumstances that do not align with the above, Scalar Quantization should be the preferred choice.\n\n\n\nQdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization)']","```



While Qdrant offers various options to store some parts of the data on disk, starting 

from version 1.1.0, you can also optimize your memory by compressing the embeddings. 

We've implemented the mechanism of **Scalar Quantization**! It turns out to have not 

only a positive impact on memory but also on the performance. 



## Scalar Quantization



Scalar quantization is a data compression technique that converts floating point values
```



While Qdrant offers various options to store some parts of the data on disk, starting 

from version 1.1.0, you can also optimize your memory by compressing the embeddings. 

We've implemented the mechanism of **Scalar Quantization**! It turns out to have not 

only a positive impact on memory but also on the performance. 



## Scalar Quantization



Scalar quantization is a data compression technique that converts floating point values
5. RAM: Store the full vectors and payload on disk. Limit what you load from memory to the binary quantization index. This helps reduce the memory footprint and improve the overall efficiency of the system. The incremental latency from the disk read is negligible compared to the latency savings from the binary scoring in Qdrant, which uses SIMD instructions where possible.



Want to discuss these findings and learn more about Binary Quantization? [Join our Discord community.](https://discord.gg/qdrant)
5. RAM: Store the full vectors and payload on disk. Limit what you load from memory to the binary quantization index. This helps reduce the memory footprint and improve the overall efficiency of the system. The incremental latency from the disk read is negligible compared to the latency savings from the binary scoring in Qdrant, which uses SIMD instructions where possible.



Want to discuss these findings and learn more about Binary Quantization? [Join our Discord community.](https://discord.gg/qdrant)
- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself

- Situations where the dimensionality of the original vectors is sufficiently high

- Cases where indexing speed is not a critical factor



In circumstances that do not align with the above, Scalar Quantization should be the preferred choice.



Qdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization)"
How can dissimilarity search be used for outlier detection in a dataset?,"Dissimilarity search can be utilized for outlier detection in a dataset by first establishing a set of reference points that are considered ""normal"". These reference points serve as a benchmark for comparison. Subsequently, the dissimilarity search algorithm is applied to identify the data points that are the most dissimilar to the reference set. These identified points are then flagged as potential outliers or anomalies in the dataset. This approach allows for the detection of abnormal data points even in cases where labels are not available, enabling the identification of outliers based on their deviation from the established ""normal"" reference points. This method can be a valuable tool in data analysis and anomaly detection tasks.","['### Case: Outlier Detection\n\n\n\nIn some cases, we might not even have labels, but it is still possible to try to detect anomalies in our dataset.\n\nDissimilarity search can be used for this purpose as well.\n\n\n\n{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/anomaly-detection.png caption=""Anomaly Detection"" >}}\n\n\n\nThe only thing we need is a bunch of reference points that we consider ""normal"".', '### Case: Outlier Detection\n\n\n\nIn some cases, we might not even have labels, but it is still possible to try to detect anomalies in our dataset.\n\nDissimilarity search can be used for this purpose as well.\n\n\n\n{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/anomaly-detection.png caption=""Anomaly Detection"" >}}\n\n\n\nThe only thing we need is a bunch of reference points that we consider ""normal"".', ""On the contrary, if the proportion of out-of-place elements is high enough, outlier search methods are likely to be useless.\n\n\n\n### Similarity search\n\n\n\nThe idea behind similarity search is to measure semantic similarity between related parts of the data.\n\nE.g. between category title and item images.\n\nThe hypothesis is, that unsuitable items will be less similar.\n\n\n\nWe can't directly compare text and image data.\n\nFor this we need an intermediate representation - embeddings."", ""On the contrary, if the proportion of out-of-place elements is high enough, outlier search methods are likely to be useless.\n\n\n\n### Similarity search\n\n\n\nThe idea behind similarity search is to measure semantic similarity between related parts of the data.\n\nE.g. between category title and item images.\n\nThe hypothesis is, that unsuitable items will be less similar.\n\n\n\nWe can't directly compare text and image data.\n\nFor this we need an intermediate representation - embeddings."", 'There is no silver bullet. You should validate your dataset thoroughly, and you need tools for this.\n\n\n\nWhen you are sure that there are not many objects placed in the wrong category, they can be considered outliers or anomalies.\n\nThus, you can train a model or a bunch of models capable of looking for anomalies, e.g. autoencoder and a classifier on it.\n\nHowever, this is again a resource-intensive task, both in terms of time and manual labour, since labels have to be provided for classification.']","### Case: Outlier Detection



In some cases, we might not even have labels, but it is still possible to try to detect anomalies in our dataset.

Dissimilarity search can be used for this purpose as well.



{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/anomaly-detection.png caption=""Anomaly Detection"" >}}



The only thing we need is a bunch of reference points that we consider ""normal"".
### Case: Outlier Detection



In some cases, we might not even have labels, but it is still possible to try to detect anomalies in our dataset.

Dissimilarity search can be used for this purpose as well.



{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/anomaly-detection.png caption=""Anomaly Detection"" >}}



The only thing we need is a bunch of reference points that we consider ""normal"".
On the contrary, if the proportion of out-of-place elements is high enough, outlier search methods are likely to be useless.



### Similarity search



The idea behind similarity search is to measure semantic similarity between related parts of the data.

E.g. between category title and item images.

The hypothesis is, that unsuitable items will be less similar.



We can't directly compare text and image data.

For this we need an intermediate representation - embeddings.
On the contrary, if the proportion of out-of-place elements is high enough, outlier search methods are likely to be useless.



### Similarity search



The idea behind similarity search is to measure semantic similarity between related parts of the data.

E.g. between category title and item images.

The hypothesis is, that unsuitable items will be less similar.



We can't directly compare text and image data.

For this we need an intermediate representation - embeddings.
There is no silver bullet. You should validate your dataset thoroughly, and you need tools for this.



When you are sure that there are not many objects placed in the wrong category, they can be considered outliers or anomalies.

Thus, you can train a model or a bunch of models capable of looking for anomalies, e.g. autoencoder and a classifier on it.

However, this is again a resource-intensive task, both in terms of time and manual labour, since labels have to be provided for classification."
How does binary quantization work and what benefits does it offer in terms of query processing times and data compression?,"Binary quantization is a method used by Qdrant for fast indexing and data compression. In this process, each data point is represented as a vector. The quantization splits the vector in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This results in a string of bits that represents the original vector, making it much smaller and easier to compare. By supporting vector comparisons, binary quantization can significantly speed up query processing times, up to 40 times faster. This method is especially effective for OpenAI embeddings, where it has been shown to achieve a massive performance improvement at a lower cost of accuracy.","['#### What is Binary Quantization?\n\n\n\nQuantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.\n\n\n\n[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).', '#### What is Binary Quantization?\n\n\n\nQuantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.\n\n\n\n[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).', '![](/blog/openai/Accuracy_Models.png)\n\n\n\nThe efficiency gains from Binary Quantization are as follows: \n\n\n\n- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. \n\n- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. \n\n- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.', '![](/blog/openai/Accuracy_Models.png)\n\n\n\nThe efficiency gains from Binary Quantization are as follows: \n\n\n\n- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. \n\n- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. \n\n- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.', 'Here, we show how the accuracy of binary quantization is quite good across different dimensions -- for both the models. \n\n\n\n## Enhanced Performance and Efficiency with Binary Quantization']","#### What is Binary Quantization?



Quantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.



[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).
#### What is Binary Quantization?



Quantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.



[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).
![](/blog/openai/Accuracy_Models.png)



The efficiency gains from Binary Quantization are as follows: 



- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. 

- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. 

- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.
![](/blog/openai/Accuracy_Models.png)



The efficiency gains from Binary Quantization are as follows: 



- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. 

- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. 

- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.
Here, we show how the accuracy of binary quantization is quite good across different dimensions -- for both the models. 



## Enhanced Performance and Efficiency with Binary Quantization"
What is the primary function of vector embeddings in the context of machine learning and artificial intelligence?,"Vector embeddings play a crucial role in machine learning and artificial intelligence by converting complex data into a format that machines can easily understand and process. These embeddings represent data points in a multi-dimensional space, where similar data points are closer together and dissimilar data points are farther apart. By capturing the underlying relationships and similarities between data points, vector embeddings enable algorithms to perform tasks such as similarity search, recommendation systems, and natural language processing more effectively. In essence, vector embeddings help in transforming raw data into a structured and meaningful representation that can be utilized by machine learning models to make accurate predictions and decisions.","['> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.\n\n\n\n\n\n### Creating Vector Embeddings\n\n\n\nEmbeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.', '> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.\n\n\n\n\n\n### Creating Vector Embeddings\n\n\n\nEmbeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.', ""The **creation** of vector data (so we can store this high-dimensional data on our vector database) is primarily done through **embeddings**.\n\n\n\n![](/articles_data/what-is-a-vector-database/Vector-Data.jpg)\n\n\n\n### How do Embeddings Work?\n\n\n\nEmbeddings translate this high-dimensional data into a more manageable, **lower-dimensional** vector form that's more suitable for machine learning and data processing applications, typically through **neural network models**."", ""The **creation** of vector data (so we can store this high-dimensional data on our vector database) is primarily done through **embeddings**.\n\n\n\n![](/articles_data/what-is-a-vector-database/Vector-Data.jpg)\n\n\n\n### How do Embeddings Work?\n\n\n\nEmbeddings translate this high-dimensional data into a more manageable, **lower-dimensional** vector form that's more suitable for machine learning and data processing applications, typically through **neural network models**."", '## How do embeddings work?\n\n\n\nEmbeddings are created through neural networks. They capture complex relationships and semantics into [dense vectors](https://www1.se.cuhk.edu.hk/~seem5680/lecture/semantics-with-dense-vectors-2018.pdf) which are more suitable for machine learning and data processing applications. They can then project these vectors into a proper **high-dimensional** space, specifically, a [Vector Database](https://qdrant.tech/articles/what-is-a-vector-database/).']","> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.





### Creating Vector Embeddings



Embeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.
> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.





### Creating Vector Embeddings



Embeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.
The **creation** of vector data (so we can store this high-dimensional data on our vector database) is primarily done through **embeddings**.



![](/articles_data/what-is-a-vector-database/Vector-Data.jpg)



### How do Embeddings Work?



Embeddings translate this high-dimensional data into a more manageable, **lower-dimensional** vector form that's more suitable for machine learning and data processing applications, typically through **neural network models**.
The **creation** of vector data (so we can store this high-dimensional data on our vector database) is primarily done through **embeddings**.



![](/articles_data/what-is-a-vector-database/Vector-Data.jpg)



### How do Embeddings Work?



Embeddings translate this high-dimensional data into a more manageable, **lower-dimensional** vector form that's more suitable for machine learning and data processing applications, typically through **neural network models**.
## How do embeddings work?



Embeddings are created through neural networks. They capture complex relationships and semantics into [dense vectors](https://www1.se.cuhk.edu.hk/~seem5680/lecture/semantics-with-dense-vectors-2018.pdf) which are more suitable for machine learning and data processing applications. They can then project these vectors into a proper **high-dimensional** space, specifically, a [Vector Database](https://qdrant.tech/articles/what-is-a-vector-database/)."
What is the concept of the `best_score` strategy introduced in version 1.6.0 of the recommendation system?,"The `best_score` strategy, introduced in version 1.6.0 of the recommendation system, is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided.","['This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.\n\n\n\n### Best score strategy\n\n\n\n*Available as of v1.6.0*\n\n\n\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.', 'This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.\n\n\n\n### Best score strategy\n\n\n\n*Available as of v1.6.0*\n\n\n\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.', 'embedding. Internally, that mechanism is based on the averages of positive and negative examples and was calculated with the \n\nfollowing formula:\n\n\n\n$$\n\n\\text{average vector} = \\text{avg}(\\text{positive vectors}) + \\left( \\text{avg}(\\text{positive vectors}) - \\text{avg}(\\text{negative vectors}) \\right)\n\n$$\n\n\n\nThe `average_vector` converts the problem of recommendations into a single vector search.\n\n\n\n### The new hotness - Best score', 'embedding. Internally, that mechanism is based on the averages of positive and negative examples and was calculated with the \n\nfollowing formula:\n\n\n\n$$\n\n\\text{average vector} = \\text{avg}(\\text{positive vectors}) + \\left( \\text{avg}(\\text{positive vectors}) - \\text{avg}(\\text{negative vectors}) \\right)\n\n$$\n\n\n\nThe `average_vector` converts the problem of recommendations into a single vector search.\n\n\n\n### The new hotness - Best score', 'recommendation API.\n\n\n\n### Average vector\n\n\n\nThe default strategy, called `average_vector` is the previous one, based on the average of positive and negative examples. It \n\nsimplifies the recommendations process and converts it into a single vector search. It supports both point IDs and vectors as \n\nparameters. For example, you can get recommendations based on past interactions with existing points combined with query vector']","This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
embedding. Internally, that mechanism is based on the averages of positive and negative examples and was calculated with the 

following formula:



$$

\text{average vector} = \text{avg}(\text{positive vectors}) + \left( \text{avg}(\text{positive vectors}) - \text{avg}(\text{negative vectors}) \right)

$$



The `average_vector` converts the problem of recommendations into a single vector search.



### The new hotness - Best score
embedding. Internally, that mechanism is based on the averages of positive and negative examples and was calculated with the 

following formula:



$$

\text{average vector} = \text{avg}(\text{positive vectors}) + \left( \text{avg}(\text{positive vectors}) - \text{avg}(\text{negative vectors}) \right)

$$



The `average_vector` converts the problem of recommendations into a single vector search.



### The new hotness - Best score
recommendation API.



### Average vector



The default strategy, called `average_vector` is the previous one, based on the average of positive and negative examples. It 

simplifies the recommendations process and converts it into a single vector search. It supports both point IDs and vectors as 

parameters. For example, you can get recommendations based on past interactions with existing points combined with query vector"
How does Qdrant handle sparse vectors and what changes have been made to accommodate them?,"Qdrant, until recently, was not able to handle sparse vectors natively. The majority of dimensions in sparse vectors are usually zeros, making them theoretically high dimensional. However, Qdrant stores sparse vectors differently by only keeping track of the non-zero dimensions. Previously, some users attempted to convert sparse vectors to dense vectors as a workaround, but this was not considered the best solution. In response to user demand for a single tool that can handle both sparse and dense vectors, Qdrant has now introduced support for sparse vectors. This change allows users to work with both sparse and dense vectors seamlessly within the Qdrant tool, eliminating the need for alternative solutions or tools for keyword lookup.","[""But let's first take a look at how you can work with sparse vectors in Qdrant.\n\n\n\n## Practical Implementation in Python\n\n\n\nLet's dive into how Qdrant handles sparse vectors with an example. Here is what we will cover:\n\n\n\n1. Setting Up Qdrant Client: Initially, we establish a connection with Qdrant using the QdrantClient. This setup is crucial for subsequent operations."", ""But let's first take a look at how you can work with sparse vectors in Qdrant.\n\n\n\n## Practical Implementation in Python\n\n\n\nLet's dive into how Qdrant handles sparse vectors with an example. Here is what we will cover:\n\n\n\n1. Setting Up Qdrant Client: Initially, we establish a connection with Qdrant using the QdrantClient. This setup is crucial for subsequent operations."", 'Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.', 'Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.', '2. Creating a Collection with Sparse Vector Support: In Qdrant, a collection is a container for your vectors. Here, we create a collection specifically designed to support sparse vectors. This is done using the recreate_collection method where we define the parameters for sparse vectors, such as setting the index configuration.']","But let's first take a look at how you can work with sparse vectors in Qdrant.



## Practical Implementation in Python



Let's dive into how Qdrant handles sparse vectors with an example. Here is what we will cover:



1. Setting Up Qdrant Client: Initially, we establish a connection with Qdrant using the QdrantClient. This setup is crucial for subsequent operations.
But let's first take a look at how you can work with sparse vectors in Qdrant.



## Practical Implementation in Python



Let's dive into how Qdrant handles sparse vectors with an example. Here is what we will cover:



1. Setting Up Qdrant Client: Initially, we establish a connection with Qdrant using the QdrantClient. This setup is crucial for subsequent operations.
Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.
Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.
2. Creating a Collection with Sparse Vector Support: In Qdrant, a collection is a container for your vectors. Here, we create a collection specifically designed to support sparse vectors. This is done using the recreate_collection method where we define the parameters for sparse vectors, such as setting the index configuration."
What is the purpose of the `prefix` parameter in the configuration options for the Qdrant instance?,"The `prefix` parameter in the configuration options for the Qdrant instance serves the purpose of adding a specific prefix to the REST URL path. For example, setting the `prefix` parameter to `service/v1` would result in the REST API endpoint being accessed at `http://localhost:6333/service/v1/{qdrant-endpoint}`. This allows for customization and organization of the API endpoints under a specific path, making it easier to manage and access different functionalities within the Qdrant instance.","['. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.', '. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.', ""---\n\ntitle: Configuration\n\nweight: 160\n\naliases:\n\n  - ../configuration\n\n---\n\n\n\n# Configuration\n\n\n\nTo change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.\n\n\n\nThe default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\n\n\n\nTo change the default configuration, add a new configuration file and specify"", ""---\n\ntitle: Configuration\n\nweight: 160\n\naliases:\n\n  - ../configuration\n\n---\n\n\n\n# Configuration\n\n\n\nTo change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.\n\n\n\nThe default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\n\n\n\nTo change the default configuration, add a new configuration file and specify"", '## Release notes\n\n\n\n[Our release notes](https://github.com/qdrant/qdrant/releases/tag/v1.7.0) are a place to go if you are interested in more details. Please remember that Qdrant is an open source project, so feel free to [contribute](https://github.com/qdrant/qdrant/issues)!']",". Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.
. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.
---

title: Configuration

weight: 160

aliases:

  - ../configuration

---



# Configuration



To change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.



The default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).



To change the default configuration, add a new configuration file and specify
---

title: Configuration

weight: 160

aliases:

  - ../configuration

---



# Configuration



To change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.



The default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).



To change the default configuration, add a new configuration file and specify
## Release notes



[Our release notes](https://github.com/qdrant/qdrant/releases/tag/v1.7.0) are a place to go if you are interested in more details. Please remember that Qdrant is an open source project, so feel free to [contribute](https://github.com/qdrant/qdrant/issues)!"
What is the default scoring metric used in Qdrant for search operations?,"The default scoring metric used in Qdrant for search operations is `Dot product`. It is not necessary to specify this metric explicitly as it is the default setting. However, Qdrant also supports other distance metrics such as Cosine for scoring. When performing a search operation in Qdrant, if no specific metric is mentioned, the system will automatically use the Dot product as the scoring metric.","[""The `recommend` endpoint works roughly the same as `search_points`, but instead of searching for a vector, Qdrant searches for one or more points (you can also give negative example points the search engine will try to avoid in the results). It was built to help drive recommendation engines, saving the round-trip of sending the current point's vector back to Qdrant to find more similar ones"", ""The `recommend` endpoint works roughly the same as `search_points`, but instead of searching for a vector, Qdrant searches for one or more points (you can also give negative example points the search engine will try to avoid in the results). It was built to help drive recommendation engines, saving the round-trip of sending the current point's vector back to Qdrant to find more similar ones"", '. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.', '. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.', 'This feature enables administrators to prevent a qdrant process from using more disk space while permitting users to search and delete unnecessary data.\n\n\n\nYou can optionally provide the error message that should be used for error responses to users.\n\n\n\n## Recovery mode\n\n\n\n*Available as of v1.2.0*\n\n\n\nRecovery mode can help in situations where Qdrant fails to start repeatedly.\n\nWhen starting in recovery mode, Qdrant only loads collection metadata to prevent']","The `recommend` endpoint works roughly the same as `search_points`, but instead of searching for a vector, Qdrant searches for one or more points (you can also give negative example points the search engine will try to avoid in the results). It was built to help drive recommendation engines, saving the round-trip of sending the current point's vector back to Qdrant to find more similar ones
The `recommend` endpoint works roughly the same as `search_points`, but instead of searching for a vector, Qdrant searches for one or more points (you can also give negative example points the search engine will try to avoid in the results). It was built to help drive recommendation engines, saving the round-trip of sending the current point's vector back to Qdrant to find more similar ones
. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.
. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.
This feature enables administrators to prevent a qdrant process from using more disk space while permitting users to search and delete unnecessary data.



You can optionally provide the error message that should be used for error responses to users.



## Recovery mode



*Available as of v1.2.0*



Recovery mode can help in situations where Qdrant fails to start repeatedly.

When starting in recovery mode, Qdrant only loads collection metadata to prevent"
What options are available for using Qdrant if the official clients do not support the language you are using?,"If the official clients for Qdrant do not support the language you are using, you have the option to either use the REST API directly or generate a client for your language using OpenAPI definitions or protobuf definitions. By utilizing the OpenAPI definitions available at [this link](https://github.com/qdrant/qdrant/blob/master/docs/redoc/master/openapi.json) or the protobuf definitions at [this link](https://github.com/qdrant/qdrant/tree/master/lib/api/src/grpc/proto), you can interact with Qdrant's functionalities even if there is no official client available for your specific programming language. This flexibility allows developers to integrate Qdrant into a wide range of applications and environments.","[""You guarantee Qdrant that you have all the rights and authorizations necessary to use and publicize such Content and that you can grant Qdrant and its subcontractors a license under these terms.\n\n\n\nYou undertake to publish only legal content that does not infringe on public order, good morals, third-party’s rights, legislative or regulatory provisions, and, more generally, is in no way likely to jeopardize Qdrant's civil or criminal liability."", ""You guarantee Qdrant that you have all the rights and authorizations necessary to use and publicize such Content and that you can grant Qdrant and its subcontractors a license under these terms.\n\n\n\nYou undertake to publish only legal content that does not infringe on public order, good morals, third-party’s rights, legislative or regulatory provisions, and, more generally, is in no way likely to jeopardize Qdrant's civil or criminal liability."", 'Any other liability of Qdrant shall be excluded.\n\n\n\nMoreover, Qdrant shall not be liable if the alleged fault results from the incorrect application of the recommendations and advice given in the course of the Solution and/or by the Documentation.\n\n\n\n\n\n### 16. Complaint\n\n\n\nFor any complaint related to the use of the Solution and/or the Website, you may contact Qdrant at the following address: contact@qdrant.com.', 'Any other liability of Qdrant shall be excluded.\n\n\n\nMoreover, Qdrant shall not be liable if the alleged fault results from the incorrect application of the recommendations and advice given in the course of the Solution and/or by the Documentation.\n\n\n\n\n\n### 16. Complaint\n\n\n\nFor any complaint related to the use of the Solution and/or the Website, you may contact Qdrant at the following address: contact@qdrant.com.', 'While we will assist with break/fix troubleshooting of issues and errors specific to our products, Qdrant is not accountable for reviewing, writing (or rewriting), or debugging custom code.']","You guarantee Qdrant that you have all the rights and authorizations necessary to use and publicize such Content and that you can grant Qdrant and its subcontractors a license under these terms.



You undertake to publish only legal content that does not infringe on public order, good morals, third-party’s rights, legislative or regulatory provisions, and, more generally, is in no way likely to jeopardize Qdrant's civil or criminal liability.
You guarantee Qdrant that you have all the rights and authorizations necessary to use and publicize such Content and that you can grant Qdrant and its subcontractors a license under these terms.



You undertake to publish only legal content that does not infringe on public order, good morals, third-party’s rights, legislative or regulatory provisions, and, more generally, is in no way likely to jeopardize Qdrant's civil or criminal liability.
Any other liability of Qdrant shall be excluded.



Moreover, Qdrant shall not be liable if the alleged fault results from the incorrect application of the recommendations and advice given in the course of the Solution and/or by the Documentation.





### 16. Complaint



For any complaint related to the use of the Solution and/or the Website, you may contact Qdrant at the following address: contact@qdrant.com.
Any other liability of Qdrant shall be excluded.



Moreover, Qdrant shall not be liable if the alleged fault results from the incorrect application of the recommendations and advice given in the course of the Solution and/or by the Documentation.





### 16. Complaint



For any complaint related to the use of the Solution and/or the Website, you may contact Qdrant at the following address: contact@qdrant.com.
While we will assist with break/fix troubleshooting of issues and errors specific to our products, Qdrant is not accountable for reviewing, writing (or rewriting), or debugging custom code."
What are the two main interfaces available for interacting with Qdrant and where can the documentation for each be found?,"The two main interfaces available for interacting with Qdrant are REST API and gRPC API. The documentation for REST API can be found in the [OpenAPI Specification](https://qdrant.github.io/qdrant/redoc/index.html), while the documentation for gRPC API can be found [here](https://github.com/qdrant/qdrant/blob/master/docs/grpc/docs.md). The gRPC methods in Qdrant follow the same principles as REST, with each REST endpoint having a corresponding gRPC method. The gRPC interface in Qdrant is available on a specified port as outlined in the configuration file, with the default port being 6334. If choosing to use gRPC, it is necessary to expose the port when starting Qdrant, such as when running the service inside a Docker container.","['There are a lot of scenarios in which you may need to perform multiple non-related tasks at the same time. Previously, you only could send several requests to Qdrant API on your own. But multiple parallel requests may cause significant network overhead and slow down the process, especially in case of poor connection speed.', 'There are a lot of scenarios in which you may need to perform multiple non-related tasks at the same time. Previously, you only could send several requests to Qdrant API on your own. But multiple parallel requests may cause significant network overhead and slow down the process, especially in case of poor connection speed.', '## Release notes\n\n\n\n[Our release notes](https://github.com/qdrant/qdrant/releases/tag/v1.7.0) are a place to go if you are interested in more details. Please remember that Qdrant is an open source project, so feel free to [contribute](https://github.com/qdrant/qdrant/issues)!', '## Release notes\n\n\n\n[Our release notes](https://github.com/qdrant/qdrant/releases/tag/v1.7.0) are a place to go if you are interested in more details. Please remember that Qdrant is an open source project, so feel free to [contribute](https://github.com/qdrant/qdrant/issues)!', 'Any other liability of Qdrant shall be excluded.\n\n\n\nMoreover, Qdrant shall not be liable if the alleged fault results from the incorrect application of the recommendations and advice given in the course of the Solution and/or by the Documentation.\n\n\n\n\n\n### 16. Complaint\n\n\n\nFor any complaint related to the use of the Solution and/or the Website, you may contact Qdrant at the following address: contact@qdrant.com.']","There are a lot of scenarios in which you may need to perform multiple non-related tasks at the same time. Previously, you only could send several requests to Qdrant API on your own. But multiple parallel requests may cause significant network overhead and slow down the process, especially in case of poor connection speed.
There are a lot of scenarios in which you may need to perform multiple non-related tasks at the same time. Previously, you only could send several requests to Qdrant API on your own. But multiple parallel requests may cause significant network overhead and slow down the process, especially in case of poor connection speed.
## Release notes



[Our release notes](https://github.com/qdrant/qdrant/releases/tag/v1.7.0) are a place to go if you are interested in more details. Please remember that Qdrant is an open source project, so feel free to [contribute](https://github.com/qdrant/qdrant/issues)!
## Release notes



[Our release notes](https://github.com/qdrant/qdrant/releases/tag/v1.7.0) are a place to go if you are interested in more details. Please remember that Qdrant is an open source project, so feel free to [contribute](https://github.com/qdrant/qdrant/issues)!
Any other liability of Qdrant shall be excluded.



Moreover, Qdrant shall not be liable if the alleged fault results from the incorrect application of the recommendations and advice given in the course of the Solution and/or by the Documentation.





### 16. Complaint



For any complaint related to the use of the Solution and/or the Website, you may contact Qdrant at the following address: contact@qdrant.com."
When should one use gRPC with Qdrant and what factors should be considered in making this choice?,"gRPC is recommended for use with Qdrant when optimizing the performance of an application is a priority and when the user is already familiar with Qdrant. The decision to use gRPC over the REST API is a trade-off between convenience and speed. gRPC is a binary protocol that offers higher performance compared to REST API, but it can be more challenging to debug. Therefore, if the goal is to maximize performance and the user is comfortable with the potential debugging complexities associated with gRPC, it is advisable to opt for gRPC when interacting with Qdrant.","[""qdrant/qdrant\n\n```\n\n\n\n**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.\n\n\n\n## Qdrant Web UI\n\n\n\nQdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points."", ""qdrant/qdrant\n\n```\n\n\n\n**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.\n\n\n\n## Qdrant Web UI\n\n\n\nQdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points."", '* `6334` - For the [gRPC](/documentation/interfaces/#grpc-interface) API\n\n* `6335` - For [Distributed deployment](/documentation/guides/distributed_deployment/)\n\n\n\nAll Qdrant instances in a cluster must be able to:\n\n\n\n- Communicate with each other over these ports\n\n- Allow incoming connections to ports `6333` and `6334` from clients that use Qdrant.\n\n\n\n## Installation options\n\n\n\nQdrant can be installed in different ways depending on your needs:', '* `6334` - For the [gRPC](/documentation/interfaces/#grpc-interface) API\n\n* `6335` - For [Distributed deployment](/documentation/guides/distributed_deployment/)\n\n\n\nAll Qdrant instances in a cluster must be able to:\n\n\n\n- Communicate with each other over these ports\n\n- Allow incoming connections to ports `6333` and `6334` from clients that use Qdrant.\n\n\n\n## Installation options\n\n\n\nQdrant can be installed in different ways depending on your needs:', '. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.']","qdrant/qdrant

```



**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.



## Qdrant Web UI



Qdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points.
qdrant/qdrant

```



**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.



## Qdrant Web UI



Qdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points.
* `6334` - For the [gRPC](/documentation/interfaces/#grpc-interface) API

* `6335` - For [Distributed deployment](/documentation/guides/distributed_deployment/)



All Qdrant instances in a cluster must be able to:



- Communicate with each other over these ports

- Allow incoming connections to ports `6333` and `6334` from clients that use Qdrant.



## Installation options



Qdrant can be installed in different ways depending on your needs:
* `6334` - For the [gRPC](/documentation/interfaces/#grpc-interface) API

* `6335` - For [Distributed deployment](/documentation/guides/distributed_deployment/)



All Qdrant instances in a cluster must be able to:



- Communicate with each other over these ports

- Allow incoming connections to ports `6333` and `6334` from clients that use Qdrant.



## Installation options



Qdrant can be installed in different ways depending on your needs:
. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner."
Where can the GUI for Qdrant be accessed?,"The GUI for Qdrant can be accessed at `http://localhost:6333/dashboard`. This web address will lead you to the graphical user interface (GUI) for Qdrant, providing a visual interface for interacting with the Qdrant system. The GUI allows users to perform various operations and tasks related to Qdrant, such as managing indexes, querying data, and configuring settings. By accessing the GUI at the specified URL, users can easily navigate and interact with the Qdrant system in a user-friendly and intuitive manner.","['While we will assist with break/fix troubleshooting of issues and errors specific to our products, Qdrant is not accountable for reviewing, writing (or rewriting), or debugging custom code.', 'While we will assist with break/fix troubleshooting of issues and errors specific to our products, Qdrant is not accountable for reviewing, writing (or rewriting), or debugging custom code.', '## Why Qdrant?', '## Why Qdrant?', '## What sets Qdrant apart?']","While we will assist with break/fix troubleshooting of issues and errors specific to our products, Qdrant is not accountable for reviewing, writing (or rewriting), or debugging custom code.
While we will assist with break/fix troubleshooting of issues and errors specific to our products, Qdrant is not accountable for reviewing, writing (or rewriting), or debugging custom code.
## Why Qdrant?
## Why Qdrant?
## What sets Qdrant apart?"
What are the three ways to use Qdrant?,"According to the documentation, there are three ways to use Qdrant:

1. Run a Docker image: This method is recommended for users who do not have a Python development environment. By running a Docker image, users can quickly set up a local Qdrant server and storage.

2. Get the Python client: Users familiar with Python can install the Qdrant client by using `pip install qdrant-client`. The Python client also supports an in-memory database.

3. Spin up a Qdrant Cloud cluster: This method is recommended for running Qdrant in a production environment. Users can set up their first instance by following the Quickstart guide provided in the documentation.","[""You guarantee Qdrant that you have all the rights and authorizations necessary to use and publicize such Content and that you can grant Qdrant and its subcontractors a license under these terms.\n\n\n\nYou undertake to publish only legal content that does not infringe on public order, good morals, third-party’s rights, legislative or regulatory provisions, and, more generally, is in no way likely to jeopardize Qdrant's civil or criminal liability."", ""You guarantee Qdrant that you have all the rights and authorizations necessary to use and publicize such Content and that you can grant Qdrant and its subcontractors a license under these terms.\n\n\n\nYou undertake to publish only legal content that does not infringe on public order, good morals, third-party’s rights, legislative or regulatory provisions, and, more generally, is in no way likely to jeopardize Qdrant's civil or criminal liability."", 'Any other liability of Qdrant shall be excluded.\n\n\n\nMoreover, Qdrant shall not be liable if the alleged fault results from the incorrect application of the recommendations and advice given in the course of the Solution and/or by the Documentation.\n\n\n\n\n\n### 16. Complaint\n\n\n\nFor any complaint related to the use of the Solution and/or the Website, you may contact Qdrant at the following address: contact@qdrant.com.', 'Any other liability of Qdrant shall be excluded.\n\n\n\nMoreover, Qdrant shall not be liable if the alleged fault results from the incorrect application of the recommendations and advice given in the course of the Solution and/or by the Documentation.\n\n\n\n\n\n### 16. Complaint\n\n\n\nFor any complaint related to the use of the Solution and/or the Website, you may contact Qdrant at the following address: contact@qdrant.com.', ""5. Testing and transition: Following the data migration, thoroughly test the Qdrant system. Once you're assured of the Qdrant system's stability and performance, you can make the switch.\n\n\n\n6. Monitoring and fine-tuning: After transitioning to Qdrant, maintain a close watch on its performance. It's key to continue refining the system for optimal results as needed.\n\n\n\n## Next steps""]","You guarantee Qdrant that you have all the rights and authorizations necessary to use and publicize such Content and that you can grant Qdrant and its subcontractors a license under these terms.



You undertake to publish only legal content that does not infringe on public order, good morals, third-party’s rights, legislative or regulatory provisions, and, more generally, is in no way likely to jeopardize Qdrant's civil or criminal liability.
You guarantee Qdrant that you have all the rights and authorizations necessary to use and publicize such Content and that you can grant Qdrant and its subcontractors a license under these terms.



You undertake to publish only legal content that does not infringe on public order, good morals, third-party’s rights, legislative or regulatory provisions, and, more generally, is in no way likely to jeopardize Qdrant's civil or criminal liability.
Any other liability of Qdrant shall be excluded.



Moreover, Qdrant shall not be liable if the alleged fault results from the incorrect application of the recommendations and advice given in the course of the Solution and/or by the Documentation.





### 16. Complaint



For any complaint related to the use of the Solution and/or the Website, you may contact Qdrant at the following address: contact@qdrant.com.
Any other liability of Qdrant shall be excluded.



Moreover, Qdrant shall not be liable if the alleged fault results from the incorrect application of the recommendations and advice given in the course of the Solution and/or by the Documentation.





### 16. Complaint



For any complaint related to the use of the Solution and/or the Website, you may contact Qdrant at the following address: contact@qdrant.com.
5. Testing and transition: Following the data migration, thoroughly test the Qdrant system. Once you're assured of the Qdrant system's stability and performance, you can make the switch.



6. Monitoring and fine-tuning: After transitioning to Qdrant, maintain a close watch on its performance. It's key to continue refining the system for optimal results as needed.



## Next steps"
What are the recommended ways for Qdrant users to provide feedback or contribute to the project?,"Qdrant encourages feedback and contributions from its users, specifically Data Scientists, ML Engineers, and MLOps professionals. The best way for Qdrant users to contribute is by providing feedback on their experience with Qdrant. This can include reporting problems, unexpected behaviors, or suggesting improvements in documentation. Users can share their feedback through various channels such as creating an issue on the Qdrant GitHub repository, starting a discussion on GitHub, or sending a message on Discord. Additionally, users who use Qdrant or Metric Learning in their projects are encouraged to share their stories, articles, and demos with the Qdrant community. For those familiar with Rust, there is a specific contribution guide available on the Qdrant GitHub repository. If users encounter problems with understanding the code or architecture, they can reach out to the Qdrant team at any time for assistance.","['While we will assist with break/fix troubleshooting of issues and errors specific to our products, Qdrant is not accountable for reviewing, writing (or rewriting), or debugging custom code.', 'While we will assist with break/fix troubleshooting of issues and errors specific to our products, Qdrant is not accountable for reviewing, writing (or rewriting), or debugging custom code.', 'Subscribe to the\xa0[Qdrant user’s group](https://discord.gg/tdtYvXjC4h)\xa0if you want to be updated on latest Qdrant news and features.', 'Subscribe to the\xa0[Qdrant user’s group](https://discord.gg/tdtYvXjC4h)\xa0if you want to be updated on latest Qdrant news and features.', 'Any other liability of Qdrant shall be excluded.\n\n\n\nMoreover, Qdrant shall not be liable if the alleged fault results from the incorrect application of the recommendations and advice given in the course of the Solution and/or by the Documentation.\n\n\n\n\n\n### 16. Complaint\n\n\n\nFor any complaint related to the use of the Solution and/or the Website, you may contact Qdrant at the following address: contact@qdrant.com.']","While we will assist with break/fix troubleshooting of issues and errors specific to our products, Qdrant is not accountable for reviewing, writing (or rewriting), or debugging custom code.
While we will assist with break/fix troubleshooting of issues and errors specific to our products, Qdrant is not accountable for reviewing, writing (or rewriting), or debugging custom code.
Subscribe to the [Qdrant user’s group](https://discord.gg/tdtYvXjC4h) if you want to be updated on latest Qdrant news and features.
Subscribe to the [Qdrant user’s group](https://discord.gg/tdtYvXjC4h) if you want to be updated on latest Qdrant news and features.
Any other liability of Qdrant shall be excluded.



Moreover, Qdrant shall not be liable if the alleged fault results from the incorrect application of the recommendations and advice given in the course of the Solution and/or by the Documentation.





### 16. Complaint



For any complaint related to the use of the Solution and/or the Website, you may contact Qdrant at the following address: contact@qdrant.com."
What is Aleph Alpha and what capabilities does their API offer?,"Aleph Alpha is a multimodal and multilingual embeddings provider. Their API allows for the creation of embeddings for both text and images, placing them in the same latent space. They offer an official Python client that can be installed using pip. The API provides both synchronous and asynchronous clients. Users can obtain embeddings for images and store them into Qdrant by utilizing the provided Python client and following the specified steps in the documentation.","[""---\n\ntitle: Aleph Alpha\n\nweight: 900\n\naliases: [ ../integrations/aleph-alpha/ ]\n\n---\n\n\n\nAleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both \n\nin the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be \n\ninstalled with pip:\n\n\n\n```bash\n\npip install aleph-alpha-client\n\n```"", ""---\n\ntitle: Aleph Alpha\n\nweight: 900\n\naliases: [ ../integrations/aleph-alpha/ ]\n\n---\n\n\n\nAleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both \n\nin the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be \n\ninstalled with pip:\n\n\n\n```bash\n\npip install aleph-alpha-client\n\n```"", '---\n\ntitle: Aleph Alpha Search\n\nweight: 16\n\n---\n\n\n\n# Multimodal Semantic Search with Aleph Alpha\n\n\n\n| Time: 30 min | Level: Beginner |  |    |\n\n| --- | ----------- | ----------- |----------- |\n\n\n\nThis tutorial shows you how to run a proper multimodal semantic search system with a few lines of code, without the need to annotate the data or train your networks.', '---\n\ntitle: Aleph Alpha Search\n\nweight: 16\n\n---\n\n\n\n# Multimodal Semantic Search with Aleph Alpha\n\n\n\n| Time: 30 min | Level: Beginner |  |    |\n\n| --- | ----------- | ----------- |----------- |\n\n\n\nThis tutorial shows you how to run a proper multimodal semantic search system with a few lines of code, without the need to annotate the data or train your networks.', '| [Build a Search with Aleph Alpha](tutorials/aleph-alpha-search/)           | Build a simple semantic search that combines text and image data.                  | Qdrant, Aleph Alpha | \n\n| [Developing Recommendations Systems](https://githubtocolab.com/qdrant/examples/blob/master/qdrant_101_getting_started/getting_started.ipynb)    | Learn how to get started building semantic search and recommendation systems. | Qdrant |']","---

title: Aleph Alpha

weight: 900

aliases: [ ../integrations/aleph-alpha/ ]

---



Aleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both 

in the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be 

installed with pip:



```bash

pip install aleph-alpha-client

```
---

title: Aleph Alpha

weight: 900

aliases: [ ../integrations/aleph-alpha/ ]

---



Aleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both 

in the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be 

installed with pip:



```bash

pip install aleph-alpha-client

```
---

title: Aleph Alpha Search

weight: 16

---



# Multimodal Semantic Search with Aleph Alpha



| Time: 30 min | Level: Beginner |  |    |

| --- | ----------- | ----------- |----------- |



This tutorial shows you how to run a proper multimodal semantic search system with a few lines of code, without the need to annotate the data or train your networks.
---

title: Aleph Alpha Search

weight: 16

---



# Multimodal Semantic Search with Aleph Alpha



| Time: 30 min | Level: Beginner |  |    |

| --- | ----------- | ----------- |----------- |



This tutorial shows you how to run a proper multimodal semantic search system with a few lines of code, without the need to annotate the data or train your networks.
| [Build a Search with Aleph Alpha](tutorials/aleph-alpha-search/)           | Build a simple semantic search that combines text and image data.                  | Qdrant, Aleph Alpha | 

| [Developing Recommendations Systems](https://githubtocolab.com/qdrant/examples/blob/master/qdrant_101_getting_started/getting_started.ipynb)    | Learn how to get started building semantic search and recommendation systems. | Qdrant |"
What is the purpose of the `task_type` parameter when obtaining Nomic embeddings?,"The `task_type` parameter in the `nomic-embed-text-v1` model is used to define the type of embeddings that are obtained for documents. When obtaining Nomic embeddings for documents, you need to set the `task_type` parameter to `search_document`. This ensures that the embeddings generated are specifically tailored for document search purposes. By specifying the `task_type` as `search_document`, you are instructing the model to encode the text in a way that is optimized for searching and retrieving relevant documents based on the input query. This parameter helps customize the embeddings to suit the specific requirements of document retrieval tasks within the Nomic framework.","['6. `task_type_unspecified`: Unset value, which will default to one of the other values.\n\n\n\n\n\nIf you\'re building a semantic search application, such as RAG, you should use `task_type=""retrieval_document""` for the indexed documents and `task_type=""retrieval_query""` for the search queries. \n\n\n\nThe following example shows how to do this with Qdrant:\n\n\n\n## Setup\n\n\n\n```bash\n\npip install google-generativeai\n\n```\n\n\n\nLet\'s see how to use the Embedding Model API to embed a document for retrieval.', '6. `task_type_unspecified`: Unset value, which will default to one of the other values.\n\n\n\n\n\nIf you\'re building a semantic search application, such as RAG, you should use `task_type=""retrieval_document""` for the indexed documents and `task_type=""retrieval_query""` for the search queries. \n\n\n\nThe following example shows how to do this with Qdrant:\n\n\n\n## Setup\n\n\n\n```bash\n\npip install google-generativeai\n\n```\n\n\n\nLet\'s see how to use the Embedding Model API to embed a document for retrieval.', 'Once installed, you can configure it with the official Python client or through direct HTTP requests.\n\n\n\n<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>\n\n\n\nYou can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings\n\nare obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.\n\nFor documents, set the `task_type` to `search_document`:\n\n\n\n```python', 'Once installed, you can configure it with the official Python client or through direct HTTP requests.\n\n\n\n<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>\n\n\n\nYou can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings\n\nare obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.\n\nFor documents, set the `task_type` to `search_document`:\n\n\n\n```python', '```python\n\nfrom qdrant_client import QdrantClient, models\n\nfrom nomic import embed\n\n\n\noutput = embed.text(\n\n    texts=[""Qdrant is the best vector database!""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_document"",\n\n)\n\n\n\nqdrant_client = QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""my-collection"",\n\n    points=models.Batch(\n\n        ids=[1],\n\n        vectors=output[""embeddings""],\n\n    ),\n\n)\n\n```\n\n\n\nTo query the collection, set the `task_type` to `search_query`:\n\n\n\n```python']","6. `task_type_unspecified`: Unset value, which will default to one of the other values.





If you're building a semantic search application, such as RAG, you should use `task_type=""retrieval_document""` for the indexed documents and `task_type=""retrieval_query""` for the search queries. 



The following example shows how to do this with Qdrant:



## Setup



```bash

pip install google-generativeai

```



Let's see how to use the Embedding Model API to embed a document for retrieval.
6. `task_type_unspecified`: Unset value, which will default to one of the other values.





If you're building a semantic search application, such as RAG, you should use `task_type=""retrieval_document""` for the indexed documents and `task_type=""retrieval_query""` for the search queries. 



The following example shows how to do this with Qdrant:



## Setup



```bash

pip install google-generativeai

```



Let's see how to use the Embedding Model API to embed a document for retrieval.
Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

    texts=[""Qdrant is the best vector database!""],

    model=""nomic-embed-text-v1"",

    task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

    collection_name=""my-collection"",

    points=models.Batch(

        ids=[1],

        vectors=output[""embeddings""],

    ),

)

```



To query the collection, set the `task_type` to `search_query`:



```python"
How can you use the Nomic library to upsert data into a collection and query it using text embeddings?,"To upsert data into a collection and query it using text embeddings with the Nomic library, you first need to use the `embed.text` method with the `task_type` set to `search_document` to generate embeddings for the text data you want to upsert. After obtaining the embeddings, you can use the `QdrantClient` to upsert the data into a specific collection by providing the collection name and the batch of points containing the IDs and corresponding vectors.

To query the collection, you need to set the `task_type` to `search_query` in the `embed.text` method with the query text. Once you have the query embeddings, you can use the `QdrantClient` to search the collection by providing the collection name and the query embedding to retrieve relevant results based on similarity.

For more detailed information and examples on using text , refer to : https://qdrant.tech/documentation/embeddings/nomic/","['```python\n\noutput = embed.text(\n\n    texts=[""What is the best vector database?""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_query"",\n\n)\n\n\n\nqdrant_client.search(\n\n    collection_name=""my-collection"",\n\n    query=output[""embeddings""][0],\n\n)\n\n```\n\n\n\nFor more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).', '```python\n\noutput = embed.text(\n\n    texts=[""What is the best vector database?""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_query"",\n\n)\n\n\n\nqdrant_client.search(\n\n    collection_name=""my-collection"",\n\n    query=output[""embeddings""][0],\n\n)\n\n```\n\n\n\nFor more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).', '---\n\ntitle: ""Nomic""\n\nweight: 1100\n\n---\n\n\n\n# Nomic\n\n\n\nThe `nomic-embed-text-v1` model is an open source [8192 context length](https://github.com/nomic-ai/contrastors) text encoder.\n\nWhile you can find it on the [Hugging Face Hub](https://huggingface.co/nomic-ai/nomic-embed-text-v1), \n\nyou may find it easier to obtain them through the [Nomic Text Embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).', '---\n\ntitle: ""Nomic""\n\nweight: 1100\n\n---\n\n\n\n# Nomic\n\n\n\nThe `nomic-embed-text-v1` model is an open source [8192 context length](https://github.com/nomic-ai/contrastors) text encoder.\n\nWhile you can find it on the [Hugging Face Hub](https://huggingface.co/nomic-ai/nomic-embed-text-v1), \n\nyou may find it easier to obtain them through the [Nomic Text Embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).', 'Once installed, you can configure it with the official Python client or through direct HTTP requests.\n\n\n\n<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>\n\n\n\nYou can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings\n\nare obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.\n\nFor documents, set the `task_type` to `search_document`:\n\n\n\n```python']","```python

output = embed.text(

    texts=[""What is the best vector database?""],

    model=""nomic-embed-text-v1"",

    task_type=""search_query"",

)



qdrant_client.search(

    collection_name=""my-collection"",

    query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
```python

output = embed.text(

    texts=[""What is the best vector database?""],

    model=""nomic-embed-text-v1"",

    task_type=""search_query"",

)



qdrant_client.search(

    collection_name=""my-collection"",

    query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
---

title: ""Nomic""

weight: 1100

---



# Nomic



The `nomic-embed-text-v1` model is an open source [8192 context length](https://github.com/nomic-ai/contrastors) text encoder.

While you can find it on the [Hugging Face Hub](https://huggingface.co/nomic-ai/nomic-embed-text-v1), 

you may find it easier to obtain them through the [Nomic Text Embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
---

title: ""Nomic""

weight: 1100

---



# Nomic



The `nomic-embed-text-v1` model is an open source [8192 context length](https://github.com/nomic-ai/contrastors) text encoder.

While you can find it on the [Hugging Face Hub](https://huggingface.co/nomic-ai/nomic-embed-text-v1), 

you may find it easier to obtain them through the [Nomic Text Embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python"
What is the purpose of the `task_type` parameter in the Gemini Embedding Model API?,"The `task_type` parameter in the Gemini Embedding Model API serves to designate the intended purpose for the embeddings utilized. It allows users to specify the type of task they want to perform with the given text. The supported task types include `retrieval_query` for search/retrieval queries, `retrieval_document` for documents from the corpus being searched, `semantic_similarity` for Semantic Text Similarity, and `classification` for text classification. This parameter helps in customizing the functionality of the Gemini Embedding Model API based on the specific task requirements.","['---\n\ntitle: Gemini\n\nweight: 700\n\n---\n\n\n\n# Gemini\n\n\n\nQdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:\n\n\n\nGemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. \n\n\n\nIn the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.', '---\n\ntitle: Gemini\n\nweight: 700\n\n---\n\n\n\n# Gemini\n\n\n\nQdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:\n\n\n\nGemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. \n\n\n\nIn the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.', 'task_type=""retrieval_document"",\n\n    title=""Qdrant x Gemini"",\n\n)\n\n```\n\n\n\nThe returned result is a dictionary with a key: `embedding`. The value of this key is a list of floats representing the embedding of the document.\n\n\n\n## Indexing documents with Qdrant\n\n\n\n```python\n\nfrom qdrant_client.http.models import Batch\n\n\n\nqdrant_client = qdrant_client.QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""GeminiCollection"",\n\n    points=Batch(\n\n        ids=[1],\n\n        vectors=genai.embed_content(', 'task_type=""retrieval_document"",\n\n    title=""Qdrant x Gemini"",\n\n)\n\n```\n\n\n\nThe returned result is a dictionary with a key: `embedding`. The value of this key is a list of floats representing the embedding of the document.\n\n\n\n## Indexing documents with Qdrant\n\n\n\n```python\n\nfrom qdrant_client.http.models import Batch\n\n\n\nqdrant_client = qdrant_client.QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""GeminiCollection"",\n\n    points=Batch(\n\n        ids=[1],\n\n        vectors=genai.embed_content(', 'ids=[1],\n\n        vectors=genai.embed_content(\n\n            model=""models/embedding-001"",\n\n            content=""Qdrant is the best vector search engine to use with Gemini"",\n\n            task_type=""retrieval_document"",\n\n            title=""Qdrant x Gemini"",\n\n        )[""embedding""],\n\n    ),\n\n)\n\n```\n\n\n\n## Searching for documents with Qdrant\n\n\n\nOnce the documents are indexed, you can search for the most relevant documents using the same model with the `retrieval_query` task type:\n\n\n\n```python']","---

title: Gemini

weight: 700

---



# Gemini



Qdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:



Gemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. 



In the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.
---

title: Gemini

weight: 700

---



# Gemini



Qdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:



Gemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. 



In the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.
task_type=""retrieval_document"",

    title=""Qdrant x Gemini"",

)

```



The returned result is a dictionary with a key: `embedding`. The value of this key is a list of floats representing the embedding of the document.



## Indexing documents with Qdrant



```python

from qdrant_client.http.models import Batch



qdrant_client = qdrant_client.QdrantClient()

qdrant_client.upsert(

    collection_name=""GeminiCollection"",

    points=Batch(

        ids=[1],

        vectors=genai.embed_content(
task_type=""retrieval_document"",

    title=""Qdrant x Gemini"",

)

```



The returned result is a dictionary with a key: `embedding`. The value of this key is a list of floats representing the embedding of the document.



## Indexing documents with Qdrant



```python

from qdrant_client.http.models import Batch



qdrant_client = qdrant_client.QdrantClient()

qdrant_client.upsert(

    collection_name=""GeminiCollection"",

    points=Batch(

        ids=[1],

        vectors=genai.embed_content(
ids=[1],

        vectors=genai.embed_content(

            model=""models/embedding-001"",

            content=""Qdrant is the best vector search engine to use with Gemini"",

            task_type=""retrieval_document"",

            title=""Qdrant x Gemini"",

        )[""embedding""],

    ),

)

```



## Searching for documents with Qdrant



Once the documents are indexed, you can search for the most relevant documents using the same model with the `retrieval_query` task type:



```python"
What is the maximum number of tokens that Jina embeddings allow for model input lengths?,"Jina embeddings allow for model input lengths of up to 8192 tokens. This means that the models utilizing Jina embeddings can process sequences with a maximum length of 8192 tokens, providing flexibility and capability for handling large amounts of text data efficiently.","['---\n\ntitle: Jina Embeddings\n\nweight: 800\n\naliases: [ ../integrations/jina-embeddings/ ]\n\n---\n\n\n\n# Jina Embeddings\n\n\n\nQdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.', '---\n\ntitle: Jina Embeddings\n\nweight: 800\n\naliases: [ ../integrations/jina-embeddings/ ]\n\n---\n\n\n\n# Jina Embeddings\n\n\n\nQdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.', '# Provide Jina API key and choose one of the available models.\n\n# You can get a free trial key here: https://jina.ai/embeddings/\n\nJINA_API_KEY = ""jina_xxxxxxxxxxx""\n\nMODEL = ""jina-embeddings-v2-base-en""  # or ""jina-embeddings-v2-base-en""\n\nEMBEDDING_SIZE = 768  # 512 for small variant\n\n\n\n# Get embeddings from the API\n\nurl = ""https://api.jina.ai/v1/embeddings""\n\n\n\nheaders = {\n\n    ""Content-Type"": ""application/json"",\n\n    ""Authorization"": f""Bearer {JINA_API_KEY}"",\n\n}\n\n\n\ndata = {', '# Provide Jina API key and choose one of the available models.\n\n# You can get a free trial key here: https://jina.ai/embeddings/\n\nJINA_API_KEY = ""jina_xxxxxxxxxxx""\n\nMODEL = ""jina-embeddings-v2-base-en""  # or ""jina-embeddings-v2-base-en""\n\nEMBEDDING_SIZE = 768  # 512 for small variant\n\n\n\n# Get embeddings from the API\n\nurl = ""https://api.jina.ai/v1/embeddings""\n\n\n\nheaders = {\n\n    ""Content-Type"": ""application/json"",\n\n    ""Authorization"": f""Bearer {JINA_API_KEY}"",\n\n}\n\n\n\ndata = {', 'To call their endpoint, all you need is an API key obtainable [here](https://jina.ai/embeddings/). By the way, our friends from **Jina AI** provided us with a code (**QDRANT**) that will grant you a **10% discount** if you plan to use Jina Embeddings in production.\n\n\n\n```python\n\nimport qdrant_client\n\nimport requests\n\n\n\nfrom qdrant_client.http.models import Distance, VectorParams\n\nfrom qdrant_client.http.models import Batch\n\n\n\n# Provide Jina API key and choose one of the available models.']","---

title: Jina Embeddings

weight: 800

aliases: [ ../integrations/jina-embeddings/ ]

---



# Jina Embeddings



Qdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.
---

title: Jina Embeddings

weight: 800

aliases: [ ../integrations/jina-embeddings/ ]

---



# Jina Embeddings



Qdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.
# Provide Jina API key and choose one of the available models.

# You can get a free trial key here: https://jina.ai/embeddings/

JINA_API_KEY = ""jina_xxxxxxxxxxx""

MODEL = ""jina-embeddings-v2-base-en""  # or ""jina-embeddings-v2-base-en""

EMBEDDING_SIZE = 768  # 512 for small variant



# Get embeddings from the API

url = ""https://api.jina.ai/v1/embeddings""



headers = {

    ""Content-Type"": ""application/json"",

    ""Authorization"": f""Bearer {JINA_API_KEY}"",

}



data = {
# Provide Jina API key and choose one of the available models.

# You can get a free trial key here: https://jina.ai/embeddings/

JINA_API_KEY = ""jina_xxxxxxxxxxx""

MODEL = ""jina-embeddings-v2-base-en""  # or ""jina-embeddings-v2-base-en""

EMBEDDING_SIZE = 768  # 512 for small variant



# Get embeddings from the API

url = ""https://api.jina.ai/v1/embeddings""



headers = {

    ""Content-Type"": ""application/json"",

    ""Authorization"": f""Bearer {JINA_API_KEY}"",

}



data = {
To call their endpoint, all you need is an API key obtainable [here](https://jina.ai/embeddings/). By the way, our friends from **Jina AI** provided us with a code (**QDRANT**) that will grant you a **10% discount** if you plan to use Jina Embeddings in production.



```python

import qdrant_client

import requests



from qdrant_client.http.models import Distance, VectorParams

from qdrant_client.http.models import Batch



# Provide Jina API key and choose one of the available models."
Why does Qdrant by default not return vectors in search results?,"By default, Qdrant tries to minimize network traffic and doesn't return vectors in search results. This default behavior is in place to optimize performance and reduce unnecessary data transfer. However, if you specifically require the vectors to be included in the search results, you can override this default setting by setting the `with_vector` parameter of the Search/Scroll to `true`. This allows you to retrieve the vectors along with other search results, providing you with the option to access the vector data when needed.","['. In this article, we will compare how Qdrant performs against the other vector search engines.', '. In this article, we will compare how Qdrant performs against the other vector search engines.', '**Upload data to Qdrant**\n\n\n\nNow once we have the vectors prepared and the search engine running, we can start uploading the data. To interact with Qdrant from python, I recommend using an out-of-the-box client library.\n\n\n\nTo install it, use the following command\n\n\n\n`pip install qdrant-client`', '**Upload data to Qdrant**\n\n\n\nNow once we have the vectors prepared and the search engine running, we can start uploading the data. To interact with Qdrant from python, I recommend using an out-of-the-box client library.\n\n\n\nTo install it, use the following command\n\n\n\n`pip install qdrant-client`', ""---\n\ntitle: Qdrant Documentation\n\nweight: 10\n\n---\n\n# Documentation\n\n\n\n**Qdrant (read: quadrant)** is a vector similarity search engine. Use our documentation to develop a production-ready service with a convenient API to store, search, and manage vectors with an additional payload. Qdrant's expanding features allow for all sorts of neural network or semantic-based matching, faceted search, and other applications.\n\n\n\n## First-Time Users:\n\n\n\nThere are three ways to use Qdrant:""]",". In this article, we will compare how Qdrant performs against the other vector search engines.
. In this article, we will compare how Qdrant performs against the other vector search engines.
**Upload data to Qdrant**



Now once we have the vectors prepared and the search engine running, we can start uploading the data. To interact with Qdrant from python, I recommend using an out-of-the-box client library.



To install it, use the following command



`pip install qdrant-client`
**Upload data to Qdrant**



Now once we have the vectors prepared and the search engine running, we can start uploading the data. To interact with Qdrant from python, I recommend using an out-of-the-box client library.



To install it, use the following command



`pip install qdrant-client`
---

title: Qdrant Documentation

weight: 10

---

# Documentation



**Qdrant (read: quadrant)** is a vector similarity search engine. Use our documentation to develop a production-ready service with a convenient API to store, search, and manage vectors with an additional payload. Qdrant's expanding features allow for all sorts of neural network or semantic-based matching, faceted search, and other applications.



## First-Time Users:



There are three ways to use Qdrant:"
What does Qdrant not plan to support in the future?,"In the future, Qdrant does not plan to support BM25 or other non-vector-based retrieval or ranking functions, built-in ontologies or knowledge graphs, and query analyzers and other NLP tools. Qdrant aims to introduce support for sparse vectors, as used in SPLADE or similar models, but does not intend to incorporate the aforementioned features into its functionality. Users can combine Qdrant with specialized tools, including full-text search engines, to complement its capabilities.","['## Release notes\n\n\n\n[Our release notes](https://github.com/qdrant/qdrant/releases/tag/v1.7.0) are a place to go if you are interested in more details. Please remember that Qdrant is an open source project, so feel free to [contribute](https://github.com/qdrant/qdrant/issues)!', '## Release notes\n\n\n\n[Our release notes](https://github.com/qdrant/qdrant/releases/tag/v1.7.0) are a place to go if you are interested in more details. Please remember that Qdrant is an open source project, so feel free to [contribute](https://github.com/qdrant/qdrant/issues)!', ""You guarantee Qdrant that you have all the rights and authorizations necessary to use and publicize such Content and that you can grant Qdrant and its subcontractors a license under these terms.\n\n\n\nYou undertake to publish only legal content that does not infringe on public order, good morals, third-party’s rights, legislative or regulatory provisions, and, more generally, is in no way likely to jeopardize Qdrant's civil or criminal liability."", ""You guarantee Qdrant that you have all the rights and authorizations necessary to use and publicize such Content and that you can grant Qdrant and its subcontractors a license under these terms.\n\n\n\nYou undertake to publish only legal content that does not infringe on public order, good morals, third-party’s rights, legislative or regulatory provisions, and, more generally, is in no way likely to jeopardize Qdrant's civil or criminal liability."", 'While we will assist with break/fix troubleshooting of issues and errors specific to our products, Qdrant is not accountable for reviewing, writing (or rewriting), or debugging custom code.']","## Release notes



[Our release notes](https://github.com/qdrant/qdrant/releases/tag/v1.7.0) are a place to go if you are interested in more details. Please remember that Qdrant is an open source project, so feel free to [contribute](https://github.com/qdrant/qdrant/issues)!
## Release notes



[Our release notes](https://github.com/qdrant/qdrant/releases/tag/v1.7.0) are a place to go if you are interested in more details. Please remember that Qdrant is an open source project, so feel free to [contribute](https://github.com/qdrant/qdrant/issues)!
You guarantee Qdrant that you have all the rights and authorizations necessary to use and publicize such Content and that you can grant Qdrant and its subcontractors a license under these terms.



You undertake to publish only legal content that does not infringe on public order, good morals, third-party’s rights, legislative or regulatory provisions, and, more generally, is in no way likely to jeopardize Qdrant's civil or criminal liability.
You guarantee Qdrant that you have all the rights and authorizations necessary to use and publicize such Content and that you can grant Qdrant and its subcontractors a license under these terms.



You undertake to publish only legal content that does not infringe on public order, good morals, third-party’s rights, legislative or regulatory provisions, and, more generally, is in no way likely to jeopardize Qdrant's civil or criminal liability.
While we will assist with break/fix troubleshooting of issues and errors specific to our products, Qdrant is not accountable for reviewing, writing (or rewriting), or debugging custom code."
 What is the recommended approach for configuring multitenancy in terms of the number of collections to create?,"In most cases, the recommended approach for configuring multitenancy is to create only a single collection with payload-based partitioning. This approach, known as multitenancy, is efficient for the majority of users. By using a single collection with payload-based partitioning, each user can access their own vectors while ensuring that they cannot see the vectors of other users. However, it is important to note that this approach requires additional configuration to set up properly.","['## Setting up multitenancy\n\n\n\n**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called [multitenancy](https://en.wikipedia.org/wiki/Multitenancy). It is efficient for most of users, but it requires additional configuration. [Learn how to set it up](../../tutorials/multiple-partitions/)', '## Setting up multitenancy\n\n\n\n**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called [multitenancy](https://en.wikipedia.org/wiki/Multitenancy). It is efficient for most of users, but it requires additional configuration. [Learn how to set it up](../../tutorials/multiple-partitions/)', '---\n\ntitle: Multitenancy\n\nweight: 12\n\naliases:\n\n  - ../tutorials/multiple-partitions\n\n---\n\n# Configure Multitenancy\n\n\n\n**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.', '---\n\ntitle: Multitenancy\n\nweight: 12\n\naliases:\n\n  - ../tutorials/multiple-partitions\n\n---\n\n# Configure Multitenancy\n\n\n\n**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.', 'In addition to metrics and vector size, each collection uses its own set of parameters that controls collection optimization, index construction, and vacuum.\n\nThese settings can be changed at any time by a corresponding request.\n\n\n\n## Setting up multitenancy']","## Setting up multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called [multitenancy](https://en.wikipedia.org/wiki/Multitenancy). It is efficient for most of users, but it requires additional configuration. [Learn how to set it up](../../tutorials/multiple-partitions/)
## Setting up multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called [multitenancy](https://en.wikipedia.org/wiki/Multitenancy). It is efficient for most of users, but it requires additional configuration. [Learn how to set it up](../../tutorials/multiple-partitions/)
---

title: Multitenancy

weight: 12

aliases:

  - ../tutorials/multiple-partitions

---

# Configure Multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.
---

title: Multitenancy

weight: 12

aliases:

  - ../tutorials/multiple-partitions

---

# Configure Multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.
In addition to metrics and vector size, each collection uses its own set of parameters that controls collection optimization, index construction, and vacuum.

These settings can be changed at any time by a corresponding request.



## Setting up multitenancy"
What is the purpose of the locking feature in Qdrant administration tools?,"The locking feature in Qdrant administration tools serves the purpose of restricting the possible operations on a Qdrant process. It allows users to control the behavior of a Qdrant instance at runtime without manually changing its configuration. When a lock is applied, specific operations such as creating new collections or adding new data to the existing storage can be disabled. It is important to note that the locking configuration is not persistent, so it needs to be reapplied after a restart. Additionally, the locking feature applies to a single node only, requiring the lock to be set on all desired nodes in a distributed deployment setup. The locking API in Qdrant enables users to manage and control the access and operations on the Qdrant instance effectively.","['---\n\ntitle: Administration\n\nweight: 10\n\naliases:\n\n  - ../administration\n\n---\n\n\n\n# Administration\n\n\n\nQdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.\n\n\n\n## Locking\n\n\n\nA locking API enables users to restrict the possible operations on a qdrant process.\n\nIt is important to mention that:\n\n- The configuration is not persistent therefore it is necessary to lock again following a restart.', '---\n\ntitle: Administration\n\nweight: 10\n\naliases:\n\n  - ../administration\n\n---\n\n\n\n# Administration\n\n\n\nQdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.\n\n\n\n## Locking\n\n\n\nA locking API enables users to restrict the possible operations on a qdrant process.\n\nIt is important to mention that:\n\n- The configuration is not persistent therefore it is necessary to lock again following a restart.', '- Locking applies to a single node only. It is necessary to call lock on all the desired nodes in a distributed deployment setup.\n\n\n\nLock request sample:\n\n\n\n```http\n\nPOST /locks\n\n{\n\n    ""error_message"": ""write is forbidden"",\n\n    ""write"": true\n\n}\n\n```\n\n\n\nWrite flags enables/disables write lock.\n\nIf the write lock is set to true, qdrant doesn\'t allow creating new collections or adding new data to the existing storage.\n\nHowever, deletion operations or updates are not forbidden under the write lock.', '- Locking applies to a single node only. It is necessary to call lock on all the desired nodes in a distributed deployment setup.\n\n\n\nLock request sample:\n\n\n\n```http\n\nPOST /locks\n\n{\n\n    ""error_message"": ""write is forbidden"",\n\n    ""write"": true\n\n}\n\n```\n\n\n\nWrite flags enables/disables write lock.\n\nIf the write lock is set to true, qdrant doesn\'t allow creating new collections or adding new data to the existing storage.\n\nHowever, deletion operations or updates are not forbidden under the write lock.', ""5. Testing and transition: Following the data migration, thoroughly test the Qdrant system. Once you're assured of the Qdrant system's stability and performance, you can make the switch.\n\n\n\n6. Monitoring and fine-tuning: After transitioning to Qdrant, maintain a close watch on its performance. It's key to continue refining the system for optimal results as needed.\n\n\n\n## Next steps""]","---

title: Administration

weight: 10

aliases:

  - ../administration

---



# Administration



Qdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.



## Locking



A locking API enables users to restrict the possible operations on a qdrant process.

It is important to mention that:

- The configuration is not persistent therefore it is necessary to lock again following a restart.
---

title: Administration

weight: 10

aliases:

  - ../administration

---



# Administration



Qdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.



## Locking



A locking API enables users to restrict the possible operations on a qdrant process.

It is important to mention that:

- The configuration is not persistent therefore it is necessary to lock again following a restart.
- Locking applies to a single node only. It is necessary to call lock on all the desired nodes in a distributed deployment setup.



Lock request sample:



```http

POST /locks

{

    ""error_message"": ""write is forbidden"",

    ""write"": true

}

```



Write flags enables/disables write lock.

If the write lock is set to true, qdrant doesn't allow creating new collections or adding new data to the existing storage.

However, deletion operations or updates are not forbidden under the write lock.
- Locking applies to a single node only. It is necessary to call lock on all the desired nodes in a distributed deployment setup.



Lock request sample:



```http

POST /locks

{

    ""error_message"": ""write is forbidden"",

    ""write"": true

}

```



Write flags enables/disables write lock.

If the write lock is set to true, qdrant doesn't allow creating new collections or adding new data to the existing storage.

However, deletion operations or updates are not forbidden under the write lock.
5. Testing and transition: Following the data migration, thoroughly test the Qdrant system. Once you're assured of the Qdrant system's stability and performance, you can make the switch.



6. Monitoring and fine-tuning: After transitioning to Qdrant, maintain a close watch on its performance. It's key to continue refining the system for optimal results as needed.



## Next steps"
What is the significance of the `on_disk_payload` setting?,"The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes.","['Read more about the payload storage in the [Storage](../../concepts/storage/#payload-storage) section.\n\n\n\n\n\n## Storage focused configuration\n\n\n\nIf your priority is to serve large amount of vectors with an average search latency, it is recommended to configure [mmap storage](../../concepts/storage/#configuring-memmap-storage).\n\nIn this case vectors will be stored on the disc in memory-mapped files, and only the most frequently used vectors will be kept in RAM.', 'Read more about the payload storage in the [Storage](../../concepts/storage/#payload-storage) section.\n\n\n\n\n\n## Storage focused configuration\n\n\n\nIf your priority is to serve large amount of vectors with an average search latency, it is recommended to configure [mmap storage](../../concepts/storage/#configuring-memmap-storage).\n\nIn this case vectors will be stored on the disc in memory-mapped files, and only the most frequently used vectors will be kept in RAM.', ""temp_path: null\n\n\n\n  # If true - point's payload will not be stored in memory.\n\n  # It will be read from the disk every time it is requested.\n\n  # This setting saves RAM by (slightly) increasing the response time.\n\n  # Note: those payload values that are involved in filtering and are indexed - remain in RAM.\n\n  on_disk_payload: true\n\n\n\n  # Maximum number of concurrent updates to shard replicas\n\n  # If `null` - maximum concurrency is used.\n\n  update_concurrency: null"", ""temp_path: null\n\n\n\n  # If true - point's payload will not be stored in memory.\n\n  # It will be read from the disk every time it is requested.\n\n  # This setting saves RAM by (slightly) increasing the response time.\n\n  # Note: those payload values that are involved in filtering and are indexed - remain in RAM.\n\n  on_disk_payload: true\n\n\n\n  # Maximum number of concurrent updates to shard replicas\n\n  # If `null` - maximum concurrency is used.\n\n  update_concurrency: null"", 'Here are the results:\n\n\n\n600mb - 50 rps\n\n300mb - 13 rps\n\n200md - 8 rps\n\n150mb - 7 rps\n\n\n\n-->\n\n\n\nTo measure the impact of disk parameters on search speed, we used the `fio` tool to test the speed of different types of disks.\n\n\n\n```bash\n\n# Install fio\n\nsudo apt-get install fio\n\n\n\n# Run fio to check the random reads speed\n\nfio --randrepeat=1 \\\n\n    --ioengine=libaio \\\n\n    --direct=1 \\\n\n    --gtod_reduce=1 \\\n\n    --name=fiotest \\\n\n    --filename=testfio \\\n\n    --bs=4k \\\n\n    --iodepth=64 \\\n\n    --size=8G \\']","Read more about the payload storage in the [Storage](../../concepts/storage/#payload-storage) section.





## Storage focused configuration



If your priority is to serve large amount of vectors with an average search latency, it is recommended to configure [mmap storage](../../concepts/storage/#configuring-memmap-storage).

In this case vectors will be stored on the disc in memory-mapped files, and only the most frequently used vectors will be kept in RAM.
Read more about the payload storage in the [Storage](../../concepts/storage/#payload-storage) section.





## Storage focused configuration



If your priority is to serve large amount of vectors with an average search latency, it is recommended to configure [mmap storage](../../concepts/storage/#configuring-memmap-storage).

In this case vectors will be stored on the disc in memory-mapped files, and only the most frequently used vectors will be kept in RAM.
temp_path: null



  # If true - point's payload will not be stored in memory.

  # It will be read from the disk every time it is requested.

  # This setting saves RAM by (slightly) increasing the response time.

  # Note: those payload values that are involved in filtering and are indexed - remain in RAM.

  on_disk_payload: true



  # Maximum number of concurrent updates to shard replicas

  # If `null` - maximum concurrency is used.

  update_concurrency: null
temp_path: null



  # If true - point's payload will not be stored in memory.

  # It will be read from the disk every time it is requested.

  # This setting saves RAM by (slightly) increasing the response time.

  # Note: those payload values that are involved in filtering and are indexed - remain in RAM.

  on_disk_payload: true



  # Maximum number of concurrent updates to shard replicas

  # If `null` - maximum concurrency is used.

  update_concurrency: null
Here are the results:



600mb - 50 rps

300mb - 13 rps

200md - 8 rps

150mb - 7 rps



-->



To measure the impact of disk parameters on search speed, we used the `fio` tool to test the speed of different types of disks.



```bash

# Install fio

sudo apt-get install fio



# Run fio to check the random reads speed

fio --randrepeat=1 \

    --ioengine=libaio \

    --direct=1 \

    --gtod_reduce=1 \

    --name=fiotest \

    --filename=testfio \

    --bs=4k \

    --iodepth=64 \

    --size=8G \"
What is the significance of the parameter `max_segment_size_kb` in the context of vector indexation?,"The `max_segment_size_kb` parameter in the context of vector indexation determines the maximum size, in kilobytes, that a segment can have. Segments larger than this specified size might lead to disproportionately long indexation times. Therefore, it is essential to limit the size of segments to optimize the indexation process. The choice of this parameter should be based on the priority between indexation speed and search speed. If indexation speed is more critical, the parameter should be set lower. Conversely, if search speed is more important, the parameter should be set higher. It is important to note that 1Kb is equivalent to 1 vector of size 256. If the `max_segment_size_kb` parameter is not explicitly set, it will be automatically selected based on the number of available CPUs.","['# If search speed is more important - make this parameter higher.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    # If not set, will be automatically selected considering the number of available CPUs.\n\n    max_segment_size_kb: null\n\n\n\n    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.\n\n    # Segments larger than this threshold will be stored as read-only memmaped file.\n\n    # To enable memmap storage, lower the threshold\n\n    # Note: 1Kb = 1 vector of size 256', '# If search speed is more important - make this parameter higher.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    # If not set, will be automatically selected considering the number of available CPUs.\n\n    max_segment_size_kb: null\n\n\n\n    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.\n\n    # Segments larger than this threshold will be stored as read-only memmaped file.\n\n    # To enable memmap storage, lower the threshold\n\n    # Note: 1Kb = 1 vector of size 256', '# so that each segment would be handled evenly by one of the threads.\n\n    # If `default_segment_number = 0`, will be automatically selected by the number of available CPUs\n\n    default_segment_number: 0\n\n\n\n    # Do not create segments larger this size (in KiloBytes).\n\n    # Large segments might require disproportionately long indexation times,\n\n    # therefore it makes sense to limit the size of segments.\n\n    #\n\n    # If indexation speed have more priority for your - make this parameter lower.', '# so that each segment would be handled evenly by one of the threads.\n\n    # If `default_segment_number = 0`, will be automatically selected by the number of available CPUs\n\n    default_segment_number: 0\n\n\n\n    # Do not create segments larger this size (in KiloBytes).\n\n    # Large segments might require disproportionately long indexation times,\n\n    # therefore it makes sense to limit the size of segments.\n\n    #\n\n    # If indexation speed have more priority for your - make this parameter lower.', '# Segments larger than this threshold will be stored as read-only memmaped file.\n\n    # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value.\n\n    # To disable memmap storage, set this to `0`.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    memmap_threshold_kb: 200000\n\n\n\n    # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing']","# If search speed is more important - make this parameter higher.

    # Note: 1Kb = 1 vector of size 256

    # If not set, will be automatically selected considering the number of available CPUs.

    max_segment_size_kb: null



    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.

    # Segments larger than this threshold will be stored as read-only memmaped file.

    # To enable memmap storage, lower the threshold

    # Note: 1Kb = 1 vector of size 256
# If search speed is more important - make this parameter higher.

    # Note: 1Kb = 1 vector of size 256

    # If not set, will be automatically selected considering the number of available CPUs.

    max_segment_size_kb: null



    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.

    # Segments larger than this threshold will be stored as read-only memmaped file.

    # To enable memmap storage, lower the threshold

    # Note: 1Kb = 1 vector of size 256
# so that each segment would be handled evenly by one of the threads.

    # If `default_segment_number = 0`, will be automatically selected by the number of available CPUs

    default_segment_number: 0



    # Do not create segments larger this size (in KiloBytes).

    # Large segments might require disproportionately long indexation times,

    # therefore it makes sense to limit the size of segments.

    #

    # If indexation speed have more priority for your - make this parameter lower.
# so that each segment would be handled evenly by one of the threads.

    # If `default_segment_number = 0`, will be automatically selected by the number of available CPUs

    default_segment_number: 0



    # Do not create segments larger this size (in KiloBytes).

    # Large segments might require disproportionately long indexation times,

    # therefore it makes sense to limit the size of segments.

    #

    # If indexation speed have more priority for your - make this parameter lower.
# Segments larger than this threshold will be stored as read-only memmaped file.

    # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value.

    # To disable memmap storage, set this to `0`.

    # Note: 1Kb = 1 vector of size 256

    memmap_threshold_kb: 200000



    # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing"
What is the significance of the `indexing_threshold_kb` parameter?,"The `indexing_threshold_kb` parameter specifies the maximum size, in Kilobytes, allowed for vectors for plain index. The default value for this parameter is set to 20000 KB. It is important to note that 1 KB is equivalent to 1 vector of size 256. This parameter plays a crucial role in determining the maximum size of vectors that can be used for indexing. If the `indexing_threshold_kb` parameter is not explicitly set, the default value will be used. Additionally, setting this parameter to `0` will explicitly disable vector indexing. This parameter directly impacts the efficiency and performance of the indexing process for the given vectors.","[');\n\n```\n\n\n\nThe rule of thumb to set the memmap threshold parameter is simple:\n\n\n\n- if you have a balanced use scenario - set memmap threshold the same as `indexing_threshold` (default is 20000). In this case the optimizer will not make any extra runs and will optimize all thresholds at once.', ');\n\n```\n\n\n\nThe rule of thumb to set the memmap threshold parameter is simple:\n\n\n\n- if you have a balanced use scenario - set memmap threshold the same as `indexing_threshold` (default is 20000). In this case the optimizer will not make any extra runs and will optimize all thresholds at once.', 'So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan.\n\n\n\nThe Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached.\n\n\n\nThe criteria for starting the optimizer are defined in the configuration file.\n\n\n\nHere is an example of parameter values:\n\n\n\n```yaml\n\nstorage:\n\n  optimizers:\n\n    # Maximum size (in kilobytes) of vectors to store in-memory per segment.', 'So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan.\n\n\n\nThe Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached.\n\n\n\nThe criteria for starting the optimizer are defined in the configuration file.\n\n\n\nHere is an example of parameter values:\n\n\n\n```yaml\n\nstorage:\n\n  optimizers:\n\n    # Maximum size (in kilobytes) of vectors to store in-memory per segment.', "". And that's how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself.""]",");

```



The rule of thumb to set the memmap threshold parameter is simple:



- if you have a balanced use scenario - set memmap threshold the same as `indexing_threshold` (default is 20000). In this case the optimizer will not make any extra runs and will optimize all thresholds at once.
);

```



The rule of thumb to set the memmap threshold parameter is simple:



- if you have a balanced use scenario - set memmap threshold the same as `indexing_threshold` (default is 20000). In this case the optimizer will not make any extra runs and will optimize all thresholds at once.
So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan.



The Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached.



The criteria for starting the optimizer are defined in the configuration file.



Here is an example of parameter values:



```yaml

storage:

  optimizers:

    # Maximum size (in kilobytes) of vectors to store in-memory per segment.
So, for example, if the number of points is less than 10000, using any index would be less efficient than a brute force scan.



The Indexing Optimizer is used to implement the enabling of indexes and memmap storage when the minimal amount of records is reached.



The criteria for starting the optimizer are defined in the configuration file.



Here is an example of parameter values:



```yaml

storage:

  optimizers:

    # Maximum size (in kilobytes) of vectors to store in-memory per segment.
. And that's how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself."
How can you optimize Qdrant for minimizing latency in search requests?,"To optimize Qdrant for minimizing latency in search requests, you can set up the system to use as many cores as possible for a single request. This can be achieved by setting the number of segments in the collection to be equal to the number of cores in the system. By doing this, each segment will be processed in parallel, leading to a faster final result. This approach allows for the efficient utilization of system resources and can significantly reduce the time taken from the moment a request is submitted to the moment a response is received. By optimizing for latency in this manner, you can enhance the overall speed and responsiveness of the search functionality in Qdrant.","['There are a lot of scenarios in which you may need to perform multiple non-related tasks at the same time. Previously, you only could send several requests to Qdrant API on your own. But multiple parallel requests may cause significant network overhead and slow down the process, especially in case of poor connection speed.', 'There are a lot of scenarios in which you may need to perform multiple non-related tasks at the same time. Previously, you only could send several requests to Qdrant API on your own. But multiple parallel requests may cause significant network overhead and slow down the process, especially in case of poor connection speed.', 'It provides configurable trade-offs between RAM usage and search speed.\n\n\n\nWe are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.\n\nPlease feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!', 'It provides configurable trade-offs between RAM usage and search speed.\n\n\n\nWe are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.\n\nPlease feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!', ""## Minor improvements\n\n\n\nBeyond introducing new features, Qdrant 1.7.0 enhances performance and addresses various minor issues. Here's a rundown of the key improvements:\n\n\n\n1. Improvement of HNSW Index Building on High CPU Systems ([PR#2869](https://github.com/qdrant/qdrant/pull/2869)).\n\n\n\n2. Improving [Search Tail Latencies](https://github.com/qdrant/qdrant/pull/2931): improvement for high CPU systems with many parallel searches, directly impacting the user experience by reducing latency.""]","There are a lot of scenarios in which you may need to perform multiple non-related tasks at the same time. Previously, you only could send several requests to Qdrant API on your own. But multiple parallel requests may cause significant network overhead and slow down the process, especially in case of poor connection speed.
There are a lot of scenarios in which you may need to perform multiple non-related tasks at the same time. Previously, you only could send several requests to Qdrant API on your own. But multiple parallel requests may cause significant network overhead and slow down the process, especially in case of poor connection speed.
It provides configurable trade-offs between RAM usage and search speed.



We are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.

Please feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!
It provides configurable trade-offs between RAM usage and search speed.



We are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.

Please feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!
## Minor improvements



Beyond introducing new features, Qdrant 1.7.0 enhances performance and addresses various minor issues. Here's a rundown of the key improvements:



1. Improvement of HNSW Index Building on High CPU Systems ([PR#2869](https://github.com/qdrant/qdrant/pull/2869)).



2. Improving [Search Tail Latencies](https://github.com/qdrant/qdrant/pull/2931): improvement for high CPU systems with many parallel searches, directly impacting the user experience by reducing latency."
How can Qdrant be configured to optimize throughput by utilizing multiple cores for parallel processing?,"To optimize throughput in Qdrant and utilize multiple cores for parallel processing, you can configure the system to use minimal number of segments, typically set to 2. By setting the default_segment_number to 2 in the optimizers_config section of the PUT request for creating a collection, you can instruct Qdrant to use a smaller number of segments. This approach benefits from the size of the index and reduces the overall number of vector comparisons needed to find the nearest neighbors. However, it is important to note that larger segments require more time to build the index. By adjusting this configuration parameter, you can strike a balance between maximizing throughput and optimizing the efficiency of vector search operations in Qdrant.","['. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.', '. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.', 'There are a lot of scenarios in which you may need to perform multiple non-related tasks at the same time. Previously, you only could send several requests to Qdrant API on your own. But multiple parallel requests may cause significant network overhead and slow down the process, especially in case of poor connection speed.', 'There are a lot of scenarios in which you may need to perform multiple non-related tasks at the same time. Previously, you only could send several requests to Qdrant API on your own. But multiple parallel requests may cause significant network overhead and slow down the process, especially in case of poor connection speed.', 'We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.\n\n\n\nIn order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.\n\n\n\nAdditionally, Qdrant uses a bunch of internal heuristics to optimize the performance.\n\nTo better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.\n\nWith this information, we can make Qdrant faster for everyone.']",". Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.
. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.
There are a lot of scenarios in which you may need to perform multiple non-related tasks at the same time. Previously, you only could send several requests to Qdrant API on your own. But multiple parallel requests may cause significant network overhead and slow down the process, especially in case of poor connection speed.
There are a lot of scenarios in which you may need to perform multiple non-related tasks at the same time. Previously, you only could send several requests to Qdrant API on your own. But multiple parallel requests may cause significant network overhead and slow down the process, especially in case of poor connection speed.
We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.



In order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.



Additionally, Qdrant uses a bunch of internal heuristics to optimize the performance.

To better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.

With this information, we can make Qdrant faster for everyone."
How can one enable distributed deployment mode in Qdrant?,"To enable distributed deployment mode in Qdrant, one needs to enable the cluster mode in the configuration or set the ENV variable `QDRANT__CLUSTER__ENABLED=true`. This allows multiple Qdrant services to communicate with each other, distributing data across peers to extend storage capabilities and increase stability. In the configuration file, under the `cluster` section, set `enabled: true` to run Qdrant in distributed deployment mode. Additionally, configure the internal communication port between peers using the `p2p` section with the specified port number. Furthermore, adjust the parameters related to the distributed consensus algorithm as needed for optimal performance.","['---\n\ntitle: Distributed Deployment\n\nweight: 100\n\naliases:\n\n  - ../distributed_deployment\n\n---\n\n\n\n# Distributed deployment\n\n\n\nSince version v0.8.0 Qdrant supports a distributed deployment mode.\n\nIn this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.', '---\n\ntitle: Distributed Deployment\n\nweight: 100\n\naliases:\n\n  - ../distributed_deployment\n\n---\n\n\n\n# Distributed deployment\n\n\n\nSince version v0.8.0 Qdrant supports a distributed deployment mode.\n\nIn this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.', 'To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.\n\n\n\n```yaml\n\ncluster:\n\n  # Use `enabled: true` to run Qdrant in distributed deployment mode\n\n  enabled: true\n\n  # Configuration of the inter-cluster communication\n\n  p2p:\n\n    # Port for internal communication between peers\n\n    port: 6335\n\n\n\n  # Configuration related to distributed consensus algorithm\n\n  consensus:', 'To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.\n\n\n\n```yaml\n\ncluster:\n\n  # Use `enabled: true` to run Qdrant in distributed deployment mode\n\n  enabled: true\n\n  # Configuration of the inter-cluster communication\n\n  p2p:\n\n    # Port for internal communication between peers\n\n    port: 6335\n\n\n\n  # Configuration related to distributed consensus algorithm\n\n  consensus:', 'In addition, you have to make sure:\n\n\n\n* To use a performant [persistent storage](#storage) for your data\n\n* To configure the [security settings](/documentation/guides/security/) for your deployment\n\n* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)\n\n* To set up a load balancer for your Qdrant cluster\n\n* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data']","---

title: Distributed Deployment

weight: 100

aliases:

  - ../distributed_deployment

---



# Distributed deployment



Since version v0.8.0 Qdrant supports a distributed deployment mode.

In this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.
---

title: Distributed Deployment

weight: 100

aliases:

  - ../distributed_deployment

---



# Distributed deployment



Since version v0.8.0 Qdrant supports a distributed deployment mode.

In this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.
To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.



```yaml

cluster:

  # Use `enabled: true` to run Qdrant in distributed deployment mode

  enabled: true

  # Configuration of the inter-cluster communication

  p2p:

    # Port for internal communication between peers

    port: 6335



  # Configuration related to distributed consensus algorithm

  consensus:
To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.



```yaml

cluster:

  # Use `enabled: true` to run Qdrant in distributed deployment mode

  enabled: true

  # Configuration of the inter-cluster communication

  p2p:

    # Port for internal communication between peers

    port: 6335



  # Configuration related to distributed consensus algorithm

  consensus:
In addition, you have to make sure:



* To use a performant [persistent storage](#storage) for your data

* To configure the [security settings](/documentation/guides/security/) for your deployment

* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)

* To set up a load balancer for your Qdrant cluster

* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data"
What is the recommended approach for determining the number of shards when creating a collection using Qdrant?,"When creating a collection using Qdrant, it is recommended to set the number of shards to be a multiple of the number of nodes currently running in your cluster. For example, if you have 3 nodes in your cluster, setting the number of shards to 6 could be a good option. Shards are evenly distributed across all existing nodes when a collection is first created. It is important to note that Qdrant does not automatically rebalance shards if your cluster size or replication factor changes, as this operation can be expensive on large clusters. If scaling operations require moving shards, there are specific procedures outlined for this purpose, available as of version 0.9.0 of Qdrant.","['Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.\n\n\n\n### Moving shards\n\n\n\n*Available as of v0.9.0*', 'Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.\n\n\n\n### Moving shards\n\n\n\n*Available as of v0.9.0*', 'Once all shards of the collection are recovered, the collection will become operational again.\n\n\n\n## Consistency guarantees\n\n\n\nBy default, Qdrant focuses on availability and maximum throughput of search operations.\n\nFor the majority of use cases, this is a preferable trade-off.\n\n\n\nDuring the normal state of operation, it is possible to search and modify data from any peers in the cluster.', 'Once all shards of the collection are recovered, the collection will become operational again.\n\n\n\n## Consistency guarantees\n\n\n\nBy default, Qdrant focuses on availability and maximum throughput of search operations.\n\nFor the majority of use cases, this is a preferable trade-off.\n\n\n\nDuring the normal state of operation, it is possible to search and modify data from any peers in the cluster.', 'Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.\n\nUse the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.']","Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.



### Moving shards



*Available as of v0.9.0*
Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.



### Moving shards



*Available as of v0.9.0*
Once all shards of the collection are recovered, the collection will become operational again.



## Consistency guarantees



By default, Qdrant focuses on availability and maximum throughput of search operations.

For the majority of use cases, this is a preferable trade-off.



During the normal state of operation, it is possible to search and modify data from any peers in the cluster.
Once all shards of the collection are recovered, the collection will become operational again.



## Consistency guarantees



By default, Qdrant focuses on availability and maximum throughput of search operations.

For the majority of use cases, this is a preferable trade-off.



During the normal state of operation, it is possible to search and modify data from any peers in the cluster.
Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.

Use the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node."
How is the discovery score computed in the context provided?,"In the context provided, the discovery score is computed using the formula: 

Discovery Score = sigmoid(s(v_t)) + Σ rank(v_i^+, v_i^-)

Here, s(v_t) represents the similarity function of the target vector, and v_i^+ and v_i^- represent the positive and negative examples, respectively. The sigmoid function is utilized to normalize the score within the range of 0 to 1. The sum of ranks is used to penalize vectors that are closer to negative examples than to positive ones. This means that the sum of individual ranks determines how many positive zones a point is in, while the closeness hierarchy serves as a secondary factor in the computation of the discovery score.","['The Discovery API can be used in two ways - either with or without the target point. The first case is called a **discovery search**, while the second is called a **context search**.\n\n\n\n#### Discovery search\n\n\n\n*Discovery search* is an operation that uses a target point to find the most relevant points in the collection, while performing the search in the preferred areas only. That is basically a search operation with more control over the search space.', 'The Discovery API can be used in two ways - either with or without the target point. The first case is called a **discovery search**, while the second is called a **context search**.\n\n\n\n#### Discovery search\n\n\n\n*Discovery search* is an operation that uses a target point to find the most relevant points in the collection, while performing the search in the preferred areas only. That is basically a search operation with more control over the search space.', '### Discovery API\n\n\n\nThe recently launched [Discovery API](/documentation/concepts/explore/#discovery-api) extends the range of scenarios for leveraging vectors. While its interface mirrors the [Recommendation API](/documentation/concepts/explore/#recommendation-api), it focuses on refining the search parameters for greater precision.', '### Discovery API\n\n\n\nThe recently launched [Discovery API](/documentation/concepts/explore/#discovery-api) extends the range of scenarios for leveraging vectors. While its interface mirrors the [Recommendation API](/documentation/concepts/explore/#recommendation-api), it focuses on refining the search parameters for greater precision.', 'The interface for providing context is similar to the recommendation API (ids or raw vectors). Still, in this case, they need to be provided in the form of positive-negative pairs.\n\n\n\nDiscovery API lets you do two new types of search:\n\n- **Discovery search**: Uses the context (the pairs of positive-negative vectors) and a target to return the points more similar to the target, but constrained by the context.']","The Discovery API can be used in two ways - either with or without the target point. The first case is called a **discovery search**, while the second is called a **context search**.



#### Discovery search



*Discovery search* is an operation that uses a target point to find the most relevant points in the collection, while performing the search in the preferred areas only. That is basically a search operation with more control over the search space.
The Discovery API can be used in two ways - either with or without the target point. The first case is called a **discovery search**, while the second is called a **context search**.



#### Discovery search



*Discovery search* is an operation that uses a target point to find the most relevant points in the collection, while performing the search in the preferred areas only. That is basically a search operation with more control over the search space.
### Discovery API



The recently launched [Discovery API](/documentation/concepts/explore/#discovery-api) extends the range of scenarios for leveraging vectors. While its interface mirrors the [Recommendation API](/documentation/concepts/explore/#recommendation-api), it focuses on refining the search parameters for greater precision.
### Discovery API



The recently launched [Discovery API](/documentation/concepts/explore/#discovery-api) extends the range of scenarios for leveraging vectors. While its interface mirrors the [Recommendation API](/documentation/concepts/explore/#recommendation-api), it focuses on refining the search parameters for greater precision.
The interface for providing context is similar to the recommendation API (ids or raw vectors). Still, in this case, they need to be provided in the form of positive-negative pairs.



Discovery API lets you do two new types of search:

- **Discovery search**: Uses the context (the pairs of positive-negative vectors) and a target to return the points more similar to the target, but constrained by the context."
How does Qdrant optimize storage at the segment level?,"Qdrant optimizes storage at the segment level by applying changes in batches rather than individually. When optimization is needed, the segment to be optimized remains readable during the rebuild process. This is achieved by wrapping the segment into a proxy that handles data changes transparently. Changed data is placed in a copy-on-write segment, prioritizing retrieval and subsequent updates. This approach ensures efficiency in storage optimization within Qdrant.","['. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.', '. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.', '---\n\ntitle: Optimizer\n\nweight: 70\n\naliases:\n\n  - ../optimizer\n\n---\n\n\n\n# Optimizer\n\n\n\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\n\n\n\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).', '---\n\ntitle: Optimizer\n\nweight: 70\n\naliases:\n\n  - ../optimizer\n\n---\n\n\n\n# Optimizer\n\n\n\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\n\n\n\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).', '---\n\ntitle: Roadmap\n\nweight: 32\n\ndraft: true\n\n---\n\n\n\n# Qdrant 2023 Roadmap\n\n\n\nGoals of the release:\n\n\n\n* **Maintain easy upgrades** - we plan to keep backward compatibility for at least one major version back. \n\n  * That means that you can upgrade Qdrant without any downtime and without any changes in your client code within one major version.\n\n  * Storage should be compatible between any two consequent versions, so you can upgrade Qdrant with automatic data migration between consecutive versions.']",". Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.
. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.
---

title: Optimizer

weight: 70

aliases:

  - ../optimizer

---



# Optimizer



It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.



Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).
---

title: Optimizer

weight: 70

aliases:

  - ../optimizer

---



# Optimizer



It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.



Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).
---

title: Roadmap

weight: 32

draft: true

---



# Qdrant 2023 Roadmap



Goals of the release:



* **Maintain easy upgrades** - we plan to keep backward compatibility for at least one major version back. 

  * That means that you can upgrade Qdrant without any downtime and without any changes in your client code within one major version.

  * Storage should be compatible between any two consequent versions, so you can upgrade Qdrant with automatic data migration between consecutive versions."
What is the purpose of the Vacuum Optimizer in the Qdrant database system?,"The Vacuum Optimizer in the Qdrant database system is utilized to address the issue of accumulated deleted records within segments. When records are marked as deleted instead of being immediately removed, they can accumulate over time, occupying memory and slowing down the system. The Vacuum Optimizer is triggered when a segment has accumulated a significant number of deleted records, as defined by the criteria set in the configuration file. This optimizer helps in optimizing segments by removing these accumulated deleted records, thereby improving system performance and efficiency. The configuration file specifies parameters such as the minimal fraction of deleted vectors and the minimal number of vectors in a segment required to trigger the Vacuum Optimizer. By running the Vacuum Optimizer, the system can effectively manage and optimize segments to prevent performance degradation due to accumulated deleted records.","['Like many other databases, Qdrant does not delete entries immediately after a query.\n\nInstead, it marks records as deleted and ignores them for future queries.\n\n\n\nThis strategy allows us to minimize disk access - one of the slowest operations.\n\nHowever, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.\n\n\n\nTo avoid these adverse effects, Vacuum Optimizer is used.\n\nIt is used if the segment has accumulated too many deleted records.', 'Like many other databases, Qdrant does not delete entries immediately after a query.\n\nInstead, it marks records as deleted and ignores them for future queries.\n\n\n\nThis strategy allows us to minimize disk access - one of the slowest operations.\n\nHowever, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.\n\n\n\nTo avoid these adverse effects, Vacuum Optimizer is used.\n\nIt is used if the segment has accumulated too many deleted records.', '---\n\ntitle: Optimizer\n\nweight: 70\n\naliases:\n\n  - ../optimizer\n\n---\n\n\n\n# Optimizer\n\n\n\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\n\n\n\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).', '---\n\ntitle: Optimizer\n\nweight: 70\n\naliases:\n\n  - ../optimizer\n\n---\n\n\n\n# Optimizer\n\n\n\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\n\n\n\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).', ""use case of your organization and the features of the database you ultimately choose.\n\n\n\nLet's now evaluate, at a high-level, the way Qdrant is architected.\n\n\n\n## High-Level Overview of Qdrant's Architecture\n\n\n\n![qdrant](https://raw.githubusercontent.com/ramonpzg/mlops-sydney-2023/main/images/qdrant_overview_high_level.png)\n\n\n\nThe diagram above represents a high-level overview of some of the main components of Qdrant. Here \n\nare the terminologies you should get familiar with.""]","Like many other databases, Qdrant does not delete entries immediately after a query.

Instead, it marks records as deleted and ignores them for future queries.



This strategy allows us to minimize disk access - one of the slowest operations.

However, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.



To avoid these adverse effects, Vacuum Optimizer is used.

It is used if the segment has accumulated too many deleted records.
Like many other databases, Qdrant does not delete entries immediately after a query.

Instead, it marks records as deleted and ignores them for future queries.



This strategy allows us to minimize disk access - one of the slowest operations.

However, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.



To avoid these adverse effects, Vacuum Optimizer is used.

It is used if the segment has accumulated too many deleted records.
---

title: Optimizer

weight: 70

aliases:

  - ../optimizer

---



# Optimizer



It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.



Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).
---

title: Optimizer

weight: 70

aliases:

  - ../optimizer

---



# Optimizer



It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.



Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).
use case of your organization and the features of the database you ultimately choose.



Let's now evaluate, at a high-level, the way Qdrant is architected.



## High-Level Overview of Qdrant's Architecture



![qdrant](https://raw.githubusercontent.com/ramonpzg/mlops-sydney-2023/main/images/qdrant_overview_high_level.png)



The diagram above represents a high-level overview of some of the main components of Qdrant. Here 

are the terminologies you should get familiar with."
What is the purpose of the `payload` field in the data points?,"The `payload` field in the data points being upserted using the QdrantClient serves as a container for additional metadata or information related to the vector data being stored. It allows users to associate supplementary details with each data point, such as the city name, price, or any other custom attributes that provide context or additional insights into the vector data. This metadata can be used for filtering, searching, or categorizing the vectors during retrieval or query operations, enabling more efficient and targeted data analysis and retrieval processes. In the examples given, the `payload` field includes information like city names, prices, and other relevant details specific to each data point, enhancing the overall utility and relevance of the stored vector data.","['## Payload\n\n\n\nA [Payload](/documentation/concepts/payload/) describes information that you can store with vectors.\n\n\n\n## Points\n\n\n\n[Points](/documentation/concepts/points/) are a record which consists of a vector and an optional payload. \n\n\n\n## Search\n\n\n\n[Search](/documentation/concepts/search/) describes _similarity search_, which set up related objects close to each other in vector space.\n\n\n\n## Explore', '## Payload\n\n\n\nA [Payload](/documentation/concepts/payload/) describes information that you can store with vectors.\n\n\n\n## Points\n\n\n\n[Points](/documentation/concepts/points/) are a record which consists of a vector and an optional payload. \n\n\n\n## Search\n\n\n\n[Search](/documentation/concepts/search/) describes _similarity search_, which set up related objects close to each other in vector space.\n\n\n\n## Explore', 'Payload index may occupy some additional memory, so it is recommended to only use index for those fields that are used in filtering conditions.\n\nIf you need to filter by many fields and the memory limits does not allow to index all of them, it is recommended to choose the field that limits the search result the most.\n\nAs a rule, the more different values a payload value has, the more efficiently the index will be used.\n\n\n\n### Full-text index\n\n\n\n*Available as of v0.10.0*', 'Payload index may occupy some additional memory, so it is recommended to only use index for those fields that are used in filtering conditions.\n\nIf you need to filter by many fields and the memory limits does not allow to index all of them, it is recommended to choose the field that limits the search result the most.\n\nAs a rule, the more different values a payload value has, the more efficiently the index will be used.\n\n\n\n### Full-text index\n\n\n\n*Available as of v0.10.0*', '```\n\n\n\nAvailable field types are:\n\n\n\n* `keyword` - for [keyword](../payload/#keyword) payload, affects [Match](../filtering/#match) filtering conditions.\n\n* `integer` - for [integer](../payload/#integer) payload, affects [Match](../filtering/#match) and [Range](../filtering/#range) filtering conditions.\n\n* `float` - for [float](../payload/#float) payload, affects [Range](../filtering/#range) filtering conditions.']","## Payload



A [Payload](/documentation/concepts/payload/) describes information that you can store with vectors.



## Points



[Points](/documentation/concepts/points/) are a record which consists of a vector and an optional payload. 



## Search



[Search](/documentation/concepts/search/) describes _similarity search_, which set up related objects close to each other in vector space.



## Explore
## Payload



A [Payload](/documentation/concepts/payload/) describes information that you can store with vectors.



## Points



[Points](/documentation/concepts/points/) are a record which consists of a vector and an optional payload. 



## Search



[Search](/documentation/concepts/search/) describes _similarity search_, which set up related objects close to each other in vector space.



## Explore
Payload index may occupy some additional memory, so it is recommended to only use index for those fields that are used in filtering conditions.

If you need to filter by many fields and the memory limits does not allow to index all of them, it is recommended to choose the field that limits the search result the most.

As a rule, the more different values a payload value has, the more efficiently the index will be used.



### Full-text index



*Available as of v0.10.0*
Payload index may occupy some additional memory, so it is recommended to only use index for those fields that are used in filtering conditions.

If you need to filter by many fields and the memory limits does not allow to index all of them, it is recommended to choose the field that limits the search result the most.

As a rule, the more different values a payload value has, the more efficiently the index will be used.



### Full-text index



*Available as of v0.10.0*
```



Available field types are:



* `keyword` - for [keyword](../payload/#keyword) payload, affects [Match](../filtering/#match) filtering conditions.

* `integer` - for [integer](../payload/#integer) payload, affects [Match](../filtering/#match) and [Range](../filtering/#range) filtering conditions.

* `float` - for [float](../payload/#float) payload, affects [Range](../filtering/#range) filtering conditions."
What is the purpose of shard replication in Qdrant?,"Shard replication in Qdrant increases the reliability of the cluster by keeping several copies of a shard spread across the cluster. This ensures the availability of the data in case of node failures, except if all replicas are lost.","['order to the target shard to bring it into the same state as the source. There\n\nare two important benefits: 1. It transfers index and quantization data, so that\n\nthe shard does not have to be optimized again on the target node, making them\n\nimmediately available. This way, Qdrant ensures that there will be no\n\ndegradation in performance at the end of the transfer. Especially on large\n\nshards, this can give a huge performance improvement. 2. The consistency and', 'order to the target shard to bring it into the same state as the source. There\n\nare two important benefits: 1. It transfers index and quantization data, so that\n\nthe shard does not have to be optimized again on the target node, making them\n\nimmediately available. This way, Qdrant ensures that there will be no\n\ndegradation in performance at the end of the transfer. Especially on large\n\nshards, this can give a huge performance improvement. 2. The consistency and', '- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', '- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', 'Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.\n\nIf there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.\n\n\n\n### Recover from a URL or local file\n\n\n\n*Available as of v0.11.3*']","order to the target shard to bring it into the same state as the source. There

are two important benefits: 1. It transfers index and quantization data, so that

the shard does not have to be optimized again on the target node, making them

immediately available. This way, Qdrant ensures that there will be no

degradation in performance at the end of the transfer. Especially on large

shards, this can give a huge performance improvement. 2. The consistency and
order to the target shard to bring it into the same state as the source. There

are two important benefits: 1. It transfers index and quantization data, so that

the shard does not have to be optimized again on the target node, making them

immediately available. This way, Qdrant ensures that there will be no

degradation in performance at the end of the transfer. Especially on large

shards, this can give a huge performance improvement. 2. The consistency and
- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.

If there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.



### Recover from a URL or local file



*Available as of v0.11.3*"
What are the steps to set up product quantization in QdrantClient?,"To set up product quantization in QdrantClient, you need to specify the quantization parameters in the quantization_config section of the collection configuration. The compression ratio can be set to x16 and the always_ram parameter can be set to true to store quantized vectors in RAM. The vectors_config section should also be specified with the desired vector size and distance metric. Finally, you can use the create_collection method of the QdrantClient to create the collection with the specified configurations.","[""customizable, so you can find the sweet spot between memory usage and search precision. This article \n\ncovers all the steps required to perform Product Quantization and the way it's implemented in Qdrant.\n\n\n\nLet’s assume we have a few vectors being added to the collection and that our optimizer decided \n\nto start creating a new segment.\n\n\n\n![A list of raw vectors](/articles_data/product-quantization/raw-vectors.png)\n\n\n\n### Cutting the vector into pieces"", ""customizable, so you can find the sweet spot between memory usage and search precision. This article \n\ncovers all the steps required to perform Product Quantization and the way it's implemented in Qdrant.\n\n\n\nLet’s assume we have a few vectors being added to the collection and that our optimizer decided \n\nto start creating a new segment.\n\n\n\n![A list of raw vectors](/articles_data/product-quantization/raw-vectors.png)\n\n\n\n### Cutting the vector into pieces"", '* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.\n\n\n\n## Setting up Quantization in Qdrant\n\n\n\nYou can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.', '* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.\n\n\n\n## Setting up Quantization in Qdrant\n\n\n\nYou can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.', 'But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\n\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\n\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n## How to choose the right quantization method']","customizable, so you can find the sweet spot between memory usage and search precision. This article 

covers all the steps required to perform Product Quantization and the way it's implemented in Qdrant.



Let’s assume we have a few vectors being added to the collection and that our optimizer decided 

to start creating a new segment.



![A list of raw vectors](/articles_data/product-quantization/raw-vectors.png)



### Cutting the vector into pieces
customizable, so you can find the sweet spot between memory usage and search precision. This article 

covers all the steps required to perform Product Quantization and the way it's implemented in Qdrant.



Let’s assume we have a few vectors being added to the collection and that our optimizer decided 

to start creating a new segment.



![A list of raw vectors](/articles_data/product-quantization/raw-vectors.png)



### Cutting the vector into pieces
* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.



## Setting up Quantization in Qdrant



You can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.
* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.



## Setting up Quantization in Qdrant



You can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.
But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method"
What are the different write ordering options available in Qdrant?,"Qdrant provides three different write ordering options: weak, medium, and strong. Weak ordering does not provide any additional guarantees and allows write operations to be freely reordered. Medium ordering serializes write operations through a dynamically elected leader, which may cause minor inconsistencies in case of leader change. Strong ordering serializes write operations through the permanent leader, providing strong consistency but may result in write operations being unavailable if the leader is down.","['While we will assist with break/fix troubleshooting of issues and errors specific to our products, Qdrant is not accountable for reviewing, writing (or rewriting), or debugging custom code.', 'While we will assist with break/fix troubleshooting of issues and errors specific to our products, Qdrant is not accountable for reviewing, writing (or rewriting), or debugging custom code.', ""5. Testing and transition: Following the data migration, thoroughly test the Qdrant system. Once you're assured of the Qdrant system's stability and performance, you can make the switch.\n\n\n\n6. Monitoring and fine-tuning: After transitioning to Qdrant, maintain a close watch on its performance. It's key to continue refining the system for optimal results as needed.\n\n\n\n## Next steps"", ""5. Testing and transition: Following the data migration, thoroughly test the Qdrant system. Once you're assured of the Qdrant system's stability and performance, you can make the switch.\n\n\n\n6. Monitoring and fine-tuning: After transitioning to Qdrant, maintain a close watch on its performance. It's key to continue refining the system for optimal results as needed.\n\n\n\n## Next steps"", 'Any other liability of Qdrant shall be excluded.\n\n\n\nMoreover, Qdrant shall not be liable if the alleged fault results from the incorrect application of the recommendations and advice given in the course of the Solution and/or by the Documentation.\n\n\n\n\n\n### 16. Complaint\n\n\n\nFor any complaint related to the use of the Solution and/or the Website, you may contact Qdrant at the following address: contact@qdrant.com.']","While we will assist with break/fix troubleshooting of issues and errors specific to our products, Qdrant is not accountable for reviewing, writing (or rewriting), or debugging custom code.
While we will assist with break/fix troubleshooting of issues and errors specific to our products, Qdrant is not accountable for reviewing, writing (or rewriting), or debugging custom code.
5. Testing and transition: Following the data migration, thoroughly test the Qdrant system. Once you're assured of the Qdrant system's stability and performance, you can make the switch.



6. Monitoring and fine-tuning: After transitioning to Qdrant, maintain a close watch on its performance. It's key to continue refining the system for optimal results as needed.



## Next steps
5. Testing and transition: Following the data migration, thoroughly test the Qdrant system. Once you're assured of the Qdrant system's stability and performance, you can make the switch.



6. Monitoring and fine-tuning: After transitioning to Qdrant, maintain a close watch on its performance. It's key to continue refining the system for optimal results as needed.



## Next steps
Any other liability of Qdrant shall be excluded.



Moreover, Qdrant shall not be liable if the alleged fault results from the incorrect application of the recommendations and advice given in the course of the Solution and/or by the Documentation.





### 16. Complaint



For any complaint related to the use of the Solution and/or the Website, you may contact Qdrant at the following address: contact@qdrant.com."
How can Qdrant be optimized for low memory footprint and high speed search?,"To optimize Qdrant for low memory footprint and high speed search, you can use vector quantization. This involves converting vectors into a more compact representation that can be stored in memory and used for search. By using smaller vectors, you can cache more in RAM and reduce the number of disk reads. To configure in-memory quantization with on-disk original vectors, you need to create a collection with the specified configuration.","['It provides configurable trade-offs between RAM usage and search speed.\n\n\n\nWe are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.\n\nPlease feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!', 'It provides configurable trade-offs between RAM usage and search speed.\n\n\n\nWe are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.\n\nPlease feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!', '. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.', '. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.', 'We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.\n\n\n\nIn order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.\n\n\n\nAdditionally, Qdrant uses a bunch of internal heuristics to optimize the performance.\n\nTo better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.\n\nWith this information, we can make Qdrant faster for everyone.']","It provides configurable trade-offs between RAM usage and search speed.



We are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.

Please feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!
It provides configurable trade-offs between RAM usage and search speed.



We are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.

Please feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!
. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.
. Qdrant basically optimizes a lot using for the compute resources and this also helped us to scale the whole infrastructure in a really efficient manner.
We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.



In order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.



Additionally, Qdrant uses a bunch of internal heuristics to optimize the performance.

To better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.

With this information, we can make Qdrant faster for everyone."
How does Qdrant optimize memory and search speed for sparse vectors?,"Qdrant optimizes memory and search speed for sparse vectors by utilizing an inverted index structure to store vectors for each non-zero dimension. This approach allows Qdrant to efficiently represent sparse vectors, which are characterized by a high proportion of zeroes. By only storing information about non-zero dimensions, Qdrant reduces the memory footprint required to store sparse vectors and also speeds up search operations by focusing only on relevant dimensions during indexing and querying processes. This optimization ensures that Qdrant can handle sparse vectors effectively while maintaining efficient memory usage and search performance.","[""But let's first take a look at how you can work with sparse vectors in Qdrant.\n\n\n\n## Practical Implementation in Python\n\n\n\nLet's dive into how Qdrant handles sparse vectors with an example. Here is what we will cover:\n\n\n\n1. Setting Up Qdrant Client: Initially, we establish a connection with Qdrant using the QdrantClient. This setup is crucial for subsequent operations."", ""But let's first take a look at how you can work with sparse vectors in Qdrant.\n\n\n\n## Practical Implementation in Python\n\n\n\nLet's dive into how Qdrant handles sparse vectors with an example. Here is what we will cover:\n\n\n\n1. Setting Up Qdrant Client: Initially, we establish a connection with Qdrant using the QdrantClient. This setup is crucial for subsequent operations."", 'Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.', 'Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.', '2. Creating a Collection with Sparse Vector Support: In Qdrant, a collection is a container for your vectors. Here, we create a collection specifically designed to support sparse vectors. This is done using the recreate_collection method where we define the parameters for sparse vectors, such as setting the index configuration.']","But let's first take a look at how you can work with sparse vectors in Qdrant.



## Practical Implementation in Python



Let's dive into how Qdrant handles sparse vectors with an example. Here is what we will cover:



1. Setting Up Qdrant Client: Initially, we establish a connection with Qdrant using the QdrantClient. This setup is crucial for subsequent operations.
But let's first take a look at how you can work with sparse vectors in Qdrant.



## Practical Implementation in Python



Let's dive into how Qdrant handles sparse vectors with an example. Here is what we will cover:



1. Setting Up Qdrant Client: Initially, we establish a connection with Qdrant using the QdrantClient. This setup is crucial for subsequent operations.
Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.
Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.
2. Creating a Collection with Sparse Vector Support: In Qdrant, a collection is a container for your vectors. Here, we create a collection specifically designed to support sparse vectors. This is done using the recreate_collection method where we define the parameters for sparse vectors, such as setting the index configuration."
How does Qdrant address the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters?,"Qdrant addresses the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters by extending the HNSW graph with additional edges based on the stored payload values. This extension allows for more efficient searching of nearby vectors using the HNSW index while applying filters during the search in the graph. By adding extra edges, Qdrant minimizes the overhead on condition checks since the conditions only need to be calculated for a small fraction of the points involved in the search. This approach improves the performance of the HNSW index in cases where the traditional methods of weak filters or complete rescore are not optimal, providing a more balanced solution for scenarios that fall in between.","['HNSW is chosen for several reasons.\n\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\n\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\n\n\n\n*Available as of v1.1.1*\n\n\n\nThe HNSW parameters can also be configured on a collection and named vector\n\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\n\nperformance.', 'HNSW is chosen for several reasons.\n\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\n\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\n\n\n\n*Available as of v1.1.1*\n\n\n\nThe HNSW parameters can also be configured on a collection and named vector\n\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\n\nperformance.', 'Read more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.', 'Read more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.', 'All right, keep going. I like it.\n\n\n\nRishabh Bhardwaj:\n\nYeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.\n\n\n\nDemetrios:\n\nRight.\n\n\n\nRishabh Bhardwaj:']","HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
Read more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.
Read more about the Qdrant approach in our [Filtrable HNSW](/articles/filtrable-hnsw/) article.
All right, keep going. I like it.



Rishabh Bhardwaj:

Yeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.



Demetrios:

Right.



Rishabh Bhardwaj:"
How is metric learning utilized in addressing the challenge of detecting anomalies in coffee beans?,"Metric learning is utilized in addressing the challenge of detecting anomalies in coffee beans by encoding images in an n-dimensional vector space and using learned similarities to label images during the inference process. This approach involves representing the images in a vector space where similar images are closer together in the space. By doing so, the model can effectively classify and detect anomalies in coffee beans based on the similarities learned during training. The KNN (K-Nearest Neighbors) classification method is commonly used in this approach as it simplifies the process of determining the class label of an image based on the labels of its nearest neighbors in the vector space. This method allows for the adaptation to new types of defects and changing shooting conditions, making the model more robust and accurate in detecting anomalies in coffee beans.","['---\n\ntitle: Metric Learning for Anomaly Detection\n\nshort_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""\n\ndescription: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.\n\nsocial_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg\n\npreview_dir: /articles_data/detecting-coffee-anomalies/preview', '---\n\ntitle: Metric Learning for Anomaly Detection\n\nshort_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""\n\ndescription: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.\n\nsocial_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg\n\npreview_dir: /articles_data/detecting-coffee-anomalies/preview', '{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}\n\n\n\nWe should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it\'s own.\n\nIn the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.\n\n\n\nLet\'s find out how metric learning might help to address this challenge.\n\n\n\n## Metric Learning Approach', '{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}\n\n\n\nWe should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it\'s own.\n\nIn the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.\n\n\n\nLet\'s find out how metric learning might help to address this challenge.\n\n\n\n## Metric Learning Approach', '* Hard to maintain - you would need to re-train the model repeatedly in response to changes in the data distribution.\n\n\n\nThese are not desirable features if you want to put your model into production in a rapidly-changing environment.\n\nAnd, despite all the mentioned difficulties, they do not necessarily offer superior performance compared to the alternatives.\n\nIn this post, we will detail the lessons learned from such a use case.\n\n\n\n## Coffee Beans']","---

title: Metric Learning for Anomaly Detection

short_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""

description: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.

social_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg

preview_dir: /articles_data/detecting-coffee-anomalies/preview
---

title: Metric Learning for Anomaly Detection

short_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""

description: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.

social_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg

preview_dir: /articles_data/detecting-coffee-anomalies/preview
{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}



We should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it's own.

In the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.



Let's find out how metric learning might help to address this challenge.



## Metric Learning Approach
{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}



We should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it's own.

In the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.



Let's find out how metric learning might help to address this challenge.



## Metric Learning Approach
* Hard to maintain - you would need to re-train the model repeatedly in response to changes in the data distribution.



These are not desirable features if you want to put your model into production in a rapidly-changing environment.

And, despite all the mentioned difficulties, they do not necessarily offer superior performance compared to the alternatives.

In this post, we will detail the lessons learned from such a use case.



## Coffee Beans"
How can one reproduce the benchmark for Open Source vector databases?,"To reproduce the benchmark for Open Source vector databases, one can access the source code available on Github at https://github.com/qdrant/vector-db-benchmark. The repository contains a `README.md` file that provides detailed instructions on how to run the benchmark for a specific engine. By following the guidelines outlined in the README file, individuals can easily replicate the benchmarks conducted on the Open Source vector databases. Additionally, the document emphasizes the importance of transparency in the benchmarking process and invites contributions from individuals who may be able to provide insights, identify misconfigurations, or suggest improvements. Interested parties can contribute to the benchmarking efforts by accessing the benchmark repository at https://github.com/qdrant/vector-db-benchmark.","[""If you're interested in testing the benchmark yourself or want to contribute to its development, head over to our [benchmark repository](https://github.com/qdrant/vector-db-benchmark). We appreciate your support and involvement in improving the performance of vector databases."", ""If you're interested in testing the benchmark yourself or want to contribute to its development, head over to our [benchmark repository](https://github.com/qdrant/vector-db-benchmark). We appreciate your support and involvement in improving the performance of vector databases."", 'That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.\n\n\n\nThis is not the final list, and we’ll continue benchmarking as many different engines as possible.\n\n\n\n## How to reproduce the benchmark?\n\n\n\nThe source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.', 'That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.\n\n\n\nThis is not the final list, and we’ll continue benchmarking as many different engines as possible.\n\n\n\n## How to reproduce the benchmark?\n\n\n\nThe source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.', 'Vector Databases, which are designed to store and process immense amounts of vectors, are the first candidates to implement these new techniques and allow users to exploit their data to its fullest.']","If you're interested in testing the benchmark yourself or want to contribute to its development, head over to our [benchmark repository](https://github.com/qdrant/vector-db-benchmark). We appreciate your support and involvement in improving the performance of vector databases.
If you're interested in testing the benchmark yourself or want to contribute to its development, head over to our [benchmark repository](https://github.com/qdrant/vector-db-benchmark). We appreciate your support and involvement in improving the performance of vector databases.
That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.



This is not the final list, and we’ll continue benchmarking as many different engines as possible.



## How to reproduce the benchmark?



The source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.
That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.



This is not the final list, and we’ll continue benchmarking as many different engines as possible.



## How to reproduce the benchmark?



The source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.
Vector Databases, which are designed to store and process immense amounts of vectors, are the first candidates to implement these new techniques and allow users to exploit their data to its fullest."
What is the purpose of the `ordering` parameter in Qdrant and when should it be used?,"The `ordering` parameter in Qdrant is used with update and delete operations to ensure that the operations are executed in the same order on all replicas. When this option is enabled, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This is beneficial in preventing data inconsistency that may arise from concurrent updates of the same documents. The `ordering` parameter is particularly recommended when read operations are more frequent than updates and when search performance is critical. By enforcing a specific order of operations across replicas, the `ordering` parameter helps maintain data consistency and integrity in distributed environments.","['We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.\n\n\n\nIn order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.\n\n\n\nAdditionally, Qdrant uses a bunch of internal heuristics to optimize the performance.\n\nTo better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.\n\nWith this information, we can make Qdrant faster for everyone.', 'We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.\n\n\n\nIn order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.\n\n\n\nAdditionally, Qdrant uses a bunch of internal heuristics to optimize the performance.\n\nTo better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.\n\nWith this information, we can make Qdrant faster for everyone.', ""5. Testing and transition: Following the data migration, thoroughly test the Qdrant system. Once you're assured of the Qdrant system's stability and performance, you can make the switch.\n\n\n\n6. Monitoring and fine-tuning: After transitioning to Qdrant, maintain a close watch on its performance. It's key to continue refining the system for optimal results as needed.\n\n\n\n## Next steps"", ""5. Testing and transition: Following the data migration, thoroughly test the Qdrant system. Once you're assured of the Qdrant system's stability and performance, you can make the switch.\n\n\n\n6. Monitoring and fine-tuning: After transitioning to Qdrant, maintain a close watch on its performance. It's key to continue refining the system for optimal results as needed.\n\n\n\n## Next steps"", 'Any other liability of Qdrant shall be excluded.\n\n\n\nMoreover, Qdrant shall not be liable if the alleged fault results from the incorrect application of the recommendations and advice given in the course of the Solution and/or by the Documentation.\n\n\n\n\n\n### 16. Complaint\n\n\n\nFor any complaint related to the use of the Solution and/or the Website, you may contact Qdrant at the following address: contact@qdrant.com.']","We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.



In order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.



Additionally, Qdrant uses a bunch of internal heuristics to optimize the performance.

To better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.

With this information, we can make Qdrant faster for everyone.
We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.



In order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.



Additionally, Qdrant uses a bunch of internal heuristics to optimize the performance.

To better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.

With this information, we can make Qdrant faster for everyone.
5. Testing and transition: Following the data migration, thoroughly test the Qdrant system. Once you're assured of the Qdrant system's stability and performance, you can make the switch.



6. Monitoring and fine-tuning: After transitioning to Qdrant, maintain a close watch on its performance. It's key to continue refining the system for optimal results as needed.



## Next steps
5. Testing and transition: Following the data migration, thoroughly test the Qdrant system. Once you're assured of the Qdrant system's stability and performance, you can make the switch.



6. Monitoring and fine-tuning: After transitioning to Qdrant, maintain a close watch on its performance. It's key to continue refining the system for optimal results as needed.



## Next steps
Any other liability of Qdrant shall be excluded.



Moreover, Qdrant shall not be liable if the alleged fault results from the incorrect application of the recommendations and advice given in the course of the Solution and/or by the Documentation.





### 16. Complaint



For any complaint related to the use of the Solution and/or the Website, you may contact Qdrant at the following address: contact@qdrant.com."
How can you enable API key based authentication in Qdrant instances for security purposes?,"To enable API key based authentication in Qdrant instances for security purposes, you must specify a key in the configuration. This can be done by adding the following code snippet to your configuration file:

```yaml
service:
  # Set an api-key.
  # If set, all requests must include a header with the api-key.
  # example header: `api-key: <API-KEY>`
  #
  # If you enable this you should also enable TLS.
  # (Either above or via an external service like nginx.)
  # Sending an api-key over an unencrypted channel is insecure.
  api_key: your_secret_api_key_here
```

By adding this configuration, all requests to your Qdrant instance must include a header with the specified API key. This simple form of client authentication helps secure your instance and is available starting from version 1.2.0","['This can be used to secure your instance.\n\n\n\nTo enable API key based authentication in your own Qdrant instance you must\n\nspecify a key in the configuration:\n\n\n\n```yaml\n\nservice:\n\n  # Set an api-key.\n\n  # If set, all requests must include a header with the api-key.\n\n  # example header: `api-key: <API-KEY>`\n\n  #\n\n  # If you enable this you should also enable TLS.\n\n  # (Either above or via an external service like nginx.)\n\n  # Sending an api-key over an unencrypted channel is insecure.', 'This can be used to secure your instance.\n\n\n\nTo enable API key based authentication in your own Qdrant instance you must\n\nspecify a key in the configuration:\n\n\n\n```yaml\n\nservice:\n\n  # Set an api-key.\n\n  # If set, all requests must include a header with the api-key.\n\n  # example header: `api-key: <API-KEY>`\n\n  #\n\n  # If you enable this you should also enable TLS.\n\n  # (Either above or via an external service like nginx.)\n\n  # Sending an api-key over an unencrypted channel is insecure.', '---\n\ntitle: Security\n\nweight: 165\n\naliases:\n\n  - ../security\n\n---\n\n\n\n# Security\n\n\n\n\n\n\n\nPlease read this page carefully. Although there are various ways to secure your Qdrant instances, **they are unsecured by default**. \n\nYou need to enable security measures before production use. Otherwise, they are completely open to anyone\n\n\n\n## Authentication\n\n\n\n*Available as of v1.2.0*\n\n\n\nQdrant supports a simple form of client authentication using a static API key.\n\nThis can be used to secure your instance.', '---\n\ntitle: Security\n\nweight: 165\n\naliases:\n\n  - ../security\n\n---\n\n\n\n# Security\n\n\n\n\n\n\n\nPlease read this page carefully. Although there are various ways to secure your Qdrant instances, **they are unsecured by default**. \n\nYou need to enable security measures before production use. Otherwise, they are completely open to anyone\n\n\n\n## Authentication\n\n\n\n*Available as of v1.2.0*\n\n\n\nQdrant supports a simple form of client authentication using a static API key.\n\nThis can be used to secure your instance.', 'api_key: your_secret_api_key_here\n\n```\n\n\n\nOr alternatively, you can use the environment variable:\n\n\n\n```bash\n\nexport QDRANT__SERVICE__API_KEY=your_secret_api_key_here\n\n```\n\n\n\n<aside role=""alert""><a href=""#tls"">TLS</a> must be used to prevent leaking the API key over an unencrypted connection.</aside>\n\n\n\nFor using API key based authentication in Qdrant cloud see the cloud\n\n[Authentication](https://qdrant.tech/documentation/cloud/authentication)\n\nsection.']","This can be used to secure your instance.



To enable API key based authentication in your own Qdrant instance you must

specify a key in the configuration:



```yaml

service:

  # Set an api-key.

  # If set, all requests must include a header with the api-key.

  # example header: `api-key: <API-KEY>`

  #

  # If you enable this you should also enable TLS.

  # (Either above or via an external service like nginx.)

  # Sending an api-key over an unencrypted channel is insecure.
This can be used to secure your instance.



To enable API key based authentication in your own Qdrant instance you must

specify a key in the configuration:



```yaml

service:

  # Set an api-key.

  # If set, all requests must include a header with the api-key.

  # example header: `api-key: <API-KEY>`

  #

  # If you enable this you should also enable TLS.

  # (Either above or via an external service like nginx.)

  # Sending an api-key over an unencrypted channel is insecure.
---

title: Security

weight: 165

aliases:

  - ../security

---



# Security







Please read this page carefully. Although there are various ways to secure your Qdrant instances, **they are unsecured by default**. 

You need to enable security measures before production use. Otherwise, they are completely open to anyone



## Authentication



*Available as of v1.2.0*



Qdrant supports a simple form of client authentication using a static API key.

This can be used to secure your instance.
---

title: Security

weight: 165

aliases:

  - ../security

---



# Security







Please read this page carefully. Although there are various ways to secure your Qdrant instances, **they are unsecured by default**. 

You need to enable security measures before production use. Otherwise, they are completely open to anyone



## Authentication



*Available as of v1.2.0*



Qdrant supports a simple form of client authentication using a static API key.

This can be used to secure your instance.
api_key: your_secret_api_key_here

```



Or alternatively, you can use the environment variable:



```bash

export QDRANT__SERVICE__API_KEY=your_secret_api_key_here

```



<aside role=""alert""><a href=""#tls"">TLS</a> must be used to prevent leaking the API key over an unencrypted connection.</aside>



For using API key based authentication in Qdrant cloud see the cloud

[Authentication](https://qdrant.tech/documentation/cloud/authentication)

section."
