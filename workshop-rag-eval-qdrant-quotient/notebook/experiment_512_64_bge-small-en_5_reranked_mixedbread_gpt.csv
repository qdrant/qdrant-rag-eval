success,failure_reason,Completion Time (ms),content.id,content.input_text,content.answer,content.completion,content.context,content.documents,content.formatted_content,content.raw_content.input_text,content.raw_content.answer,content.raw_content.documents,content.raw_content.context,rouge1_precision,rouge1_recall,rouge1_fmeasure,rouge2_precision,rouge2_recall,rouge2_fmeasure,rougeL_precision,rougeL_recall,ROUGE-L,rougeLsum_precision,rougeLsum_recall,rougeLsum_fmeasure,BERT Sentence Similarity,bert_score_precision,bert_score_recall,BERTScore,Faithfulness,Context Relevance,Completion Verbosity,Verbosity Ratio,Chunk Relevance
True,,5867.82,ac67d706,What is vaccum optimizer ?,"The Vacuum Optimizer is a feature in the Qdrant database designed to manage and improve system performance by removing obsolete data. In Qdrant, when records are deleted, they are not immediately removed from the database. Instead, they are marked as deleted and subsequently ignored in future queries. This approach helps minimize disk access but results in accumulated deleted records that can occupy valuable memory and degrade performance over time.
The Vacuum Optimizer addresses this issue by systematically clearing out these marked-as-deleted records once they reach a certain threshold. This threshold is configurable and includes conditions such as the minimum fraction of deleted vectors in a segment and the minimum number of vectors present in a segment. ","A vacuum optimizer is a component of a system that is responsible for optimizing the storage of data by performing tasks such as segment optimization. In the context provided, the vacuum optimizer is configured with parameters such as the minimal fraction of deleted vectors in a segment required to perform optimization and the minimal number of vectors in a segment required for optimization. These parameters help in determining when and how the optimizer should optimize segments in the storage system.","The criteria for starting the optimizer are defined in the configuration file.



Here is an example of parameter values:



```yaml

storage:

  optimizers:

    # The minimal fraction of deleted vectors in a segment, required to perform segment optimization

    deleted_threshold: 0.2

    # The minimal number of vectors in a segment, required to perform segment optimization

    vacuum_min_vector_number: 1000

```



## Merge Optimizer



The service may require the creation of temporary segments.
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
default_segment_number=5,

        indexing_threshold=0,

    ),

    quantization_config=models.BinaryQuantization(

        binary=models.BinaryQuantizationConfig(always_ram=True),

    ),

)

```



#### What is happening in the OptimizerConfig? 



We're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded
optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),

    hnsw_config=models.HnswConfigDiff(on_disk=True),

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createCollection(""{collection_name}"", {

  vectors: {

    size: 768,

    distance: ""Cosine"",

  },

  optimizers_config: {

    memmap_threshold: 20000,

  },

  hnsw_config: {

    on_disk: true,

  },

});

```
config: Some(Config::Params(VectorParams {

                size: 768,

                distance: Distance::Cosine.into(),

                ..Default::default()

            })),

        }),

        optimizers_config: Some(OptimizersConfigDiff {

            memmap_threshold: Some(20000),

            ..Default::default()

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;","['The criteria for starting the optimizer are defined in the configuration file.\n\n\n\nHere is an example of parameter values:\n\n\n\n```yaml\n\nstorage:\n\n  optimizers:\n\n    # The minimal fraction of deleted vectors in a segment, required to perform segment optimization\n\n    deleted_threshold: 0.2\n\n    # The minimal number of vectors in a segment, required to perform segment optimization\n\n    vacuum_min_vector_number: 1000\n\n```\n\n\n\n## Merge Optimizer\n\n\n\nThe service may require the creation of temporary segments.', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', ""default_segment_number=5,\n\n        indexing_threshold=0,\n\n    ),\n\n    quantization_config=models.BinaryQuantization(\n\n        binary=models.BinaryQuantizationConfig(always_ram=True),\n\n    ),\n\n)\n\n```\n\n\n\n#### What is happening in the OptimizerConfig? \n\n\n\nWe're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded"", 'optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),\n\n    hnsw_config=models.HnswConfigDiff(on_disk=True),\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createCollection(""{collection_name}"", {\n\n  vectors: {\n\n    size: 768,\n\n    distance: ""Cosine"",\n\n  },\n\n  optimizers_config: {\n\n    memmap_threshold: 20000,\n\n  },\n\n  hnsw_config: {\n\n    on_disk: true,\n\n  },\n\n});\n\n```', 'config: Some(Config::Params(VectorParams {\n\n                size: 768,\n\n                distance: Distance::Cosine.into(),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        optimizers_config: Some(OptimizersConfigDiff {\n\n            memmap_threshold: Some(20000),\n\n            ..Default::default()\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;']","Question: What is vaccum optimizer ?\n\nContext: The criteria for starting the optimizer are defined in the configuration file.



Here is an example of parameter values:



```yaml

storage:

optimizers:

# The minimal fraction of deleted vectors in a segment, required to perform segment optimization

deleted_threshold: 0.2

# The minimal number of vectors in a segment, required to perform segment optimization

vacuum_min_vector_number: 1000

```



## Merge Optimizer



The service may require the creation of temporary segments.
""on_disk_payload"": false

},

""hnsw_config"": {

""m"": 16,

""ef_construct"": 100,

""full_scan_threshold"": 10000,

""max_indexing_threads"": 0

},

""optimizer_config"": {

""deleted_threshold"": 0.2,

""vacuum_min_vector_number"": 1000,

""default_segment_number"": 0,

""max_segment_size"": null,
default_segment_number=5,

indexing_threshold=0,

),

quantization_config=models.BinaryQuantization(

binary=models.BinaryQuantizationConfig(always_ram=True),

),

)

```



#### What is happening in the OptimizerConfig? 



We're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded
optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),

hnsw_config=models.HnswConfigDiff(on_disk=True),

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createCollection(""{collection_name}"", {

vectors: {

size: 768,

distance: ""Cosine"",

},

optimizers_config: {

memmap_threshold: 20000,

},

hnsw_config: {

on_disk: true,

},

});

```
config: Some(Config::Params(VectorParams {

size: 768,

distance: Distance::Cosine.into(),

..Default::default()

})),

}),

optimizers_config: Some(OptimizersConfigDiff {

memmap_threshold: Some(20000),

..Default::default()

}),

..Default::default()

})

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;\n\nAnswer:",What is vaccum optimizer ?,"The Vacuum Optimizer is a feature in the Qdrant database designed to manage and improve system performance by removing obsolete data. In Qdrant, when records are deleted, they are not immediately removed from the database. Instead, they are marked as deleted and subsequently ignored in future queries. This approach helps minimize disk access but results in accumulated deleted records that can occupy valuable memory and degrade performance over time.
The Vacuum Optimizer addresses this issue by systematically clearing out these marked-as-deleted records once they reach a certain threshold. This threshold is configurable and includes conditions such as the minimum fraction of deleted vectors in a segment and the minimum number of vectors present in a segment. ","['The criteria for starting the optimizer are defined in the configuration file.\n\n\n\nHere is an example of parameter values:\n\n\n\n```yaml\n\nstorage:\n\n  optimizers:\n\n    # The minimal fraction of deleted vectors in a segment, required to perform segment optimization\n\n    deleted_threshold: 0.2\n\n    # The minimal number of vectors in a segment, required to perform segment optimization\n\n    vacuum_min_vector_number: 1000\n\n```\n\n\n\n## Merge Optimizer\n\n\n\nThe service may require the creation of temporary segments.', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', ""default_segment_number=5,\n\n        indexing_threshold=0,\n\n    ),\n\n    quantization_config=models.BinaryQuantization(\n\n        binary=models.BinaryQuantizationConfig(always_ram=True),\n\n    ),\n\n)\n\n```\n\n\n\n#### What is happening in the OptimizerConfig? \n\n\n\nWe're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded"", 'optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),\n\n    hnsw_config=models.HnswConfigDiff(on_disk=True),\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createCollection(""{collection_name}"", {\n\n  vectors: {\n\n    size: 768,\n\n    distance: ""Cosine"",\n\n  },\n\n  optimizers_config: {\n\n    memmap_threshold: 20000,\n\n  },\n\n  hnsw_config: {\n\n    on_disk: true,\n\n  },\n\n});\n\n```', 'config: Some(Config::Params(VectorParams {\n\n                size: 768,\n\n                distance: Distance::Cosine.into(),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        optimizers_config: Some(OptimizersConfigDiff {\n\n            memmap_threshold: Some(20000),\n\n            ..Default::default()\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;']","The criteria for starting the optimizer are defined in the configuration file.



Here is an example of parameter values:



```yaml

storage:

  optimizers:

    # The minimal fraction of deleted vectors in a segment, required to perform segment optimization

    deleted_threshold: 0.2

    # The minimal number of vectors in a segment, required to perform segment optimization

    vacuum_min_vector_number: 1000

```



## Merge Optimizer



The service may require the creation of temporary segments.
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
default_segment_number=5,

        indexing_threshold=0,

    ),

    quantization_config=models.BinaryQuantization(

        binary=models.BinaryQuantizationConfig(always_ram=True),

    ),

)

```



#### What is happening in the OptimizerConfig? 



We're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded
optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),

    hnsw_config=models.HnswConfigDiff(on_disk=True),

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createCollection(""{collection_name}"", {

  vectors: {

    size: 768,

    distance: ""Cosine"",

  },

  optimizers_config: {

    memmap_threshold: 20000,

  },

  hnsw_config: {

    on_disk: true,

  },

});

```
config: Some(Config::Params(VectorParams {

                size: 768,

                distance: Distance::Cosine.into(),

                ..Default::default()

            })),

        }),

        optimizers_config: Some(OptimizersConfigDiff {

            memmap_threshold: Some(20000),

            ..Default::default()

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;",0.5441176470588235,0.3425925925925926,0.4204545454545454,0.19402985074626866,0.12149532710280374,0.14942528735632182,0.36764705882352944,0.23148148148148148,0.2840909090909091,0.5147058823529411,0.32407407407407407,0.3977272727272727,0.7215760946273804,0.6863101720809937,0.6092772483825684,0.645503580570221,0.866562552129229,0.6303131952881813,80,0.6896551724137931,0.7808929351587122
True,,4193.159,7b389a25,Tell me about ‘always_ram’ parameter?,"In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times.","The `always_ram` parameter in the quantization config is used to specify whether the quantized data should always be kept in RAM (Random Access Memory) or not. When `always_ram` is set to `false`, it indicates that the quantized data can be stored in memory-mapped files (mmap storage) instead of keeping it in RAM all the time. This can be useful for optimizing memory usage and handling large datasets where storing all the quantized data in RAM may not be feasible.","This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": false

        }

    }

}

```



```python

from qdrant_client import QdrantClient, models
always_ram: true,

    },

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, BinaryQuantization,

        CreateCollection, Distance, QuantizationConfig, VectorParams, VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".to_string(),
""size"": 1536,

      ""distance"": ""Cosine""

    },

    ""quantization_config"": {

        ""binary"": {

            ""always_ram"": true

        }

    }

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),
always_ram: Some(true),

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.BinaryQuantization;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.QuantizationConfig;
type: ""int8"",

      always_ram: false,

    },

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,

        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,

        VectorParams, VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {","['This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models', 'always_ram: true,\n\n    },\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, BinaryQuantization,\n\n        CreateCollection, Distance, QuantizationConfig, VectorParams, VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".to_string(),', '""size"": 1536,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""quantization_config"": {\n\n        ""binary"": {\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),', 'always_ram: Some(true),\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.BinaryQuantization;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.QuantizationConfig;', 'type: ""int8"",\n\n      always_ram: false,\n\n    },\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,\n\n        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,\n\n        VectorParams, VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {']","Question: Tell me about ‘always_ram’ parameter?\n\nContext: This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""optimizers_config"": {

""memmap_threshold"": 20000

},

""quantization_config"": {

""scalar"": {

""type"": ""int8"",

""always_ram"": false

}

}

}

```



```python

from qdrant_client import QdrantClient, models
always_ram: true,

},

},

});

```



```rust

use qdrant_client::{

client::QdrantClient,

qdrant::{

quantization_config::Quantization, vectors_config::Config, BinaryQuantization,

CreateCollection, Distance, QuantizationConfig, VectorParams, VectorsConfig,

},

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

.create_collection(&CreateCollection {

collection_name: ""{collection_name}"".to_string(),
""size"": 1536,

""distance"": ""Cosine""

},

""quantization_config"": {

""binary"": {

""always_ram"": true

}

}

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),
always_ram: Some(true),

})),

}),

..Default::default()

})

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.BinaryQuantization;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.QuantizationConfig;
type: ""int8"",

always_ram: false,

},

},

});

```



```rust

use qdrant_client::{

client::QdrantClient,

qdrant::{

quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,

OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,

VectorParams, VectorsConfig,

},

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

.create_collection(&CreateCollection {\n\nAnswer:",Tell me about ‘always_ram’ parameter?,"In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times.","['This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models', 'always_ram: true,\n\n    },\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, BinaryQuantization,\n\n        CreateCollection, Distance, QuantizationConfig, VectorParams, VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".to_string(),', '""size"": 1536,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""quantization_config"": {\n\n        ""binary"": {\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),', 'always_ram: Some(true),\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.BinaryQuantization;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;\n\nimport io.qdrant.client.grpc.Collections.QuantizationConfig;', 'type: ""int8"",\n\n      always_ram: false,\n\n    },\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,\n\n        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,\n\n        VectorParams, VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {']","This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": false

        }

    }

}

```



```python

from qdrant_client import QdrantClient, models
always_ram: true,

    },

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, BinaryQuantization,

        CreateCollection, Distance, QuantizationConfig, VectorParams, VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".to_string(),
""size"": 1536,

      ""distance"": ""Cosine""

    },

    ""quantization_config"": {

        ""binary"": {

            ""always_ram"": true

        }

    }

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),
always_ram: Some(true),

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.BinaryQuantization;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.Distance;

import io.qdrant.client.grpc.Collections.QuantizationConfig;
type: ""int8"",

      always_ram: false,

    },

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,

        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,

        VectorParams, VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {",0.3815789473684211,0.5471698113207547,0.44961240310077516,0.12,0.17307692307692307,0.14173228346456693,0.2631578947368421,0.37735849056603776,0.3100775193798449,0.32894736842105265,0.4716981132075472,0.3875968992248062,0.819019079208374,0.6223134398460388,0.6566934585571289,0.6390413641929626,0.983725535372893,0.5370436335603397,79,1.4107142857142858,0.6965130725803854
True,,6230.197,04a14de6,What is difference between scalar and product quantization?,"While both methods aim to reduce the memory footprint and potentially speed up operations, scalar quantization offers faster processing with SIMD-friendly operations and minimal accuracy loss, making it suitable for scenarios where speed is critical. Product quantization achieves higher compression rates and is better suited for handling very high-dimensional vectors where memory space is more of a concern than computation speed.","Scalar quantization and product quantization are two different methods of compressing vectors to minimize memory usage in the context of vector search engines. Here are the key differences between the two:

1. **Scalar Quantization**:
   - Involves compressing vectors by reducing the number of bits used to represent each vector component.
   - For example, Qdrant may use 32-bit floating-point numbers to represent original vector components and then reduce the number of bits used to 8 through `float32 -> uint8` conversion for each component.
   - Suitable for low-RAM environments where disk reads are the limiting factor rather than vector comparison, high-dimensional vectors, and scenarios where indexing speed is not critical.
   - Recommended when circumstances align with the above criteria.

2. **Product Quantization**:
   - Involves compressing vectors by dividing them into chunks and quantizing each segment individually.
   - Has a tradeoff of loss of accuracy, so it is suitable for high-dimensional vectors.
   - May not be SIMD-friendly and can be slower than scalar quantization in some cases.
   - Effective for minimizing memory usage and compressing vectors up to a certain factor, such as 64 times the original size in some implementations.
   - Recommended for high-dimensional vectors and situations where the loss of accuracy is acceptable.

In summary, scalar quantization focuses on reducing the number of bits used for individual vector components, while product quantization divides vectors into segments for compression, with considerations for accuracy and performance tradeoffs. The choice between the two methods depends on factors such as vector dimensionality, memory constraints, speed requirements, and the tolerance for accuracy loss.","But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method
| 0        | 0        | 0                |



</details>



As you can see, both functions are equal up to a constant factor, which makes similarity search equivalent.

Binary quantization makes it efficient to compare vectors using this representation.





## Product Quantization



*Available as of v1.2.0*



Product quantization is a method of compressing vectors to minimize their memory usage by dividing them into 

chunks and quantizing each segment individually.
. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
## Scalar Quantization



*Available as of v1.1.0*



Scalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.





For instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.

In other words, Qdrant performs `float32 -> uint8` conversion for each vector component.
- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself

- Situations where the dimensionality of the original vectors is sufficiently high

- Cases where indexing speed is not a critical factor



In circumstances that do not align with the above, Scalar Quantization should be the preferred choice.



Qdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization)","['But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\n\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\n\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n## How to choose the right quantization method', '| 0        | 0        | 0                |\n\n\n\n</details>\n\n\n\nAs you can see, both functions are equal up to a constant factor, which makes similarity search equivalent.\n\nBinary quantization makes it efficient to compare vectors using this representation.\n\n\n\n\n\n## Product Quantization\n\n\n\n*Available as of v1.2.0*\n\n\n\nProduct quantization is a method of compressing vectors to minimize their memory usage by dividing them into \n\nchunks and quantizing each segment individually.', '. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.', '## Scalar Quantization\n\n\n\n*Available as of v1.1.0*\n\n\n\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\n\n\n\n\n\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\n\nIn other words, Qdrant performs `float32 -> uint8` conversion for each vector component.', '- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself\n\n- Situations where the dimensionality of the original vectors is sufficiently high\n\n- Cases where indexing speed is not a critical factor\n\n\n\nIn circumstances that do not align with the above, Scalar Quantization should be the preferred choice.\n\n\n\nQdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization)']","Question: What is difference between scalar and product quantization?\n\nContext: But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method
| 0        | 0        | 0                |



</details>



As you can see, both functions are equal up to a constant factor, which makes similarity search equivalent.

Binary quantization makes it efficient to compare vectors using this representation.





## Product Quantization



*Available as of v1.2.0*



Product quantization is a method of compressing vectors to minimize their memory usage by dividing them into 

chunks and quantizing each segment individually.
. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
## Scalar Quantization



*Available as of v1.1.0*



Scalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.





For instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.

In other words, Qdrant performs `float32 -> uint8` conversion for each vector component.
- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself

- Situations where the dimensionality of the original vectors is sufficiently high

- Cases where indexing speed is not a critical factor



In circumstances that do not align with the above, Scalar Quantization should be the preferred choice.



Qdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization)\n\nAnswer:",What is difference between scalar and product quantization?,"While both methods aim to reduce the memory footprint and potentially speed up operations, scalar quantization offers faster processing with SIMD-friendly operations and minimal accuracy loss, making it suitable for scenarios where speed is critical. Product quantization achieves higher compression rates and is better suited for handling very high-dimensional vectors where memory space is more of a concern than computation speed.","['But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\n\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\n\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n## How to choose the right quantization method', '| 0        | 0        | 0                |\n\n\n\n</details>\n\n\n\nAs you can see, both functions are equal up to a constant factor, which makes similarity search equivalent.\n\nBinary quantization makes it efficient to compare vectors using this representation.\n\n\n\n\n\n## Product Quantization\n\n\n\n*Available as of v1.2.0*\n\n\n\nProduct quantization is a method of compressing vectors to minimize their memory usage by dividing them into \n\nchunks and quantizing each segment individually.', '. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.', '## Scalar Quantization\n\n\n\n*Available as of v1.1.0*\n\n\n\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\n\n\n\n\n\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\n\nIn other words, Qdrant performs `float32 -> uint8` conversion for each vector component.', '- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself\n\n- Situations where the dimensionality of the original vectors is sufficiently high\n\n- Cases where indexing speed is not a critical factor\n\n\n\nIn circumstances that do not align with the above, Scalar Quantization should be the preferred choice.\n\n\n\nQdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization)']","But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method
| 0        | 0        | 0                |



</details>



As you can see, both functions are equal up to a constant factor, which makes similarity search equivalent.

Binary quantization makes it efficient to compare vectors using this representation.





## Product Quantization



*Available as of v1.2.0*



Product quantization is a method of compressing vectors to minimize their memory usage by dividing them into 

chunks and quantizing each segment individually.
. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
## Scalar Quantization



*Available as of v1.1.0*



Scalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.





For instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.

In other words, Qdrant performs `float32 -> uint8` conversion for each vector component.
- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself

- Situations where the dimensionality of the original vectors is sufficiently high

- Cases where indexing speed is not a critical factor



In circumstances that do not align with the above, Scalar Quantization should be the preferred choice.



Qdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization)",0.16317991631799164,0.639344262295082,0.26,0.037815126050420166,0.15,0.060402684563758385,0.1087866108786611,0.4262295081967213,0.17333333333333334,0.15481171548117154,0.6065573770491803,0.24666666666666667,0.8601665496826172,0.5208744406700134,0.6197026968002319,0.5660069584846497,0.8584816999500617,0.9671527249738574,247,4.049180327868853,0.3707686920853407
True,,3375.214,b0327964,What is ‘best_score’ strategy?,"The `best_score` strategy is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided.","The 'average_vector' strategy is a recommendation strategy where the recommendation system calculates the average vector representation of the positive examples provided and uses this average vector as the reference point to find similar vectors. This strategy is based on the assumption that vectors similar to the average vector of positive examples are likely to be relevant recommendations.

In this strategy, the system computes the average vector of the positive examples and then compares this average vector with other vectors to find the most similar ones. This approach can be useful in scenarios where a collective representation of positive examples is more meaningful than individual examples.

Overall, the 'average_vector' strategy aims to provide recommendations based on the collective characteristics of positive examples rather than individual instances, potentially leading to a more generalized recommendation approach.","This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:



```rust

let score = if best_positive_score > best_negative_score {

    best_positive_score;

} else {

    -(best_negative_score * best_negative_score);

};

```



<aside role=""alert"">

The performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.

</aside>
##### One positive and one negative example



<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>



The more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there 

is just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making 

the variety richer.
},

  ""strategy"": ""average_vector"",

  ""limit"": 3

}

```



There are two key changes in the request. First of all, we can adjust the strategy of search and set it to `average_vector` (the 

default) or `best_score`. Moreover, we can pass both IDs (`718`) and embeddings (`[0.2, 0.3, 0.4, 0.5]`) as both positive and 

negative examples. 



## HNSW ANN example and strategy
```



Example result of this API would be



```json

{

  ""result"": [

    { ""id"": 10, ""score"": 0.81 },

    { ""id"": 14, ""score"": 0.75 },

    { ""id"": 11, ""score"": 0.73 }

  ],

  ""status"": ""ok"",

  ""time"": 0.001

}

```



The algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.



### Average vector strategy","['This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.\n\n\n\n### Best score strategy\n\n\n\n*Available as of v1.6.0*\n\n\n\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.', 'The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\n\n\n\n```rust\n\nlet score = if best_positive_score > best_negative_score {\n\n    best_positive_score;\n\n} else {\n\n    -(best_negative_score * best_negative_score);\n\n};\n\n```\n\n\n\n<aside role=""alert"">\n\nThe performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.\n\n</aside>', '##### One positive and one negative example\n\n\n\n<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>\n\n\n\nThe more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there \n\nis just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making \n\nthe variety richer.', '},\n\n  ""strategy"": ""average_vector"",\n\n  ""limit"": 3\n\n}\n\n```\n\n\n\nThere are two key changes in the request. First of all, we can adjust the strategy of search and set it to `average_vector` (the \n\ndefault) or `best_score`. Moreover, we can pass both IDs (`718`) and embeddings (`[0.2, 0.3, 0.4, 0.5]`) as both positive and \n\nnegative examples. \n\n\n\n## HNSW ANN example and strategy', '```\n\n\n\nExample result of this API would be\n\n\n\n```json\n\n{\n\n  ""result"": [\n\n    { ""id"": 10, ""score"": 0.81 },\n\n    { ""id"": 14, ""score"": 0.75 },\n\n    { ""id"": 11, ""score"": 0.73 }\n\n  ],\n\n  ""status"": ""ok"",\n\n  ""time"": 0.001\n\n}\n\n```\n\n\n\nThe algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.\n\n\n\n### Average vector strategy']","Question: What is ‘best_score’ strategy?\n\nContext: This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:



```rust

let score = if best_positive_score > best_negative_score {

best_positive_score;

} else {

-(best_negative_score * best_negative_score);

};

```



<aside role=""alert"">

The performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.

</aside>
##### One positive and one negative example



<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>



The more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there 

is just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making 

the variety richer.
},

""strategy"": ""average_vector"",

""limit"": 3

}

```



There are two key changes in the request. First of all, we can adjust the strategy of search and set it to `average_vector` (the 

default) or `best_score`. Moreover, we can pass both IDs (`718`) and embeddings (`[0.2, 0.3, 0.4, 0.5]`) as both positive and 

negative examples. 



## HNSW ANN example and strategy
```



Example result of this API would be



```json

{

""result"": [

{ ""id"": 10, ""score"": 0.81 },

{ ""id"": 14, ""score"": 0.75 },

{ ""id"": 11, ""score"": 0.73 }

],

""status"": ""ok"",

""time"": 0.001

}

```



The algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.



### Average vector strategy\n\nAnswer:",What is ‘best_score’ strategy?,"The `best_score` strategy is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided.","['This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.\n\n\n\n### Best score strategy\n\n\n\n*Available as of v1.6.0*\n\n\n\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.', 'The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\n\n\n\n```rust\n\nlet score = if best_positive_score > best_negative_score {\n\n    best_positive_score;\n\n} else {\n\n    -(best_negative_score * best_negative_score);\n\n};\n\n```\n\n\n\n<aside role=""alert"">\n\nThe performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.\n\n</aside>', '##### One positive and one negative example\n\n\n\n<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>\n\n\n\nThe more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there \n\nis just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making \n\nthe variety richer.', '},\n\n  ""strategy"": ""average_vector"",\n\n  ""limit"": 3\n\n}\n\n```\n\n\n\nThere are two key changes in the request. First of all, we can adjust the strategy of search and set it to `average_vector` (the \n\ndefault) or `best_score`. Moreover, we can pass both IDs (`718`) and embeddings (`[0.2, 0.3, 0.4, 0.5]`) as both positive and \n\nnegative examples. \n\n\n\n## HNSW ANN example and strategy', '```\n\n\n\nExample result of this API would be\n\n\n\n```json\n\n{\n\n  ""result"": [\n\n    { ""id"": 10, ""score"": 0.81 },\n\n    { ""id"": 14, ""score"": 0.75 },\n\n    { ""id"": 11, ""score"": 0.73 }\n\n  ],\n\n  ""status"": ""ok"",\n\n  ""time"": 0.001\n\n}\n\n```\n\n\n\nThe algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.\n\n\n\n### Average vector strategy']","This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:



```rust

let score = if best_positive_score > best_negative_score {

    best_positive_score;

} else {

    -(best_negative_score * best_negative_score);

};

```



<aside role=""alert"">

The performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.

</aside>
##### One positive and one negative example



<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>



The more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there 

is just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making 

the variety richer.
},

  ""strategy"": ""average_vector"",

  ""limit"": 3

}

```



There are two key changes in the request. First of all, we can adjust the strategy of search and set it to `average_vector` (the 

default) or `best_score`. Moreover, we can pass both IDs (`718`) and embeddings (`[0.2, 0.3, 0.4, 0.5]`) as both positive and 

negative examples. 



## HNSW ANN example and strategy
```



Example result of this API would be



```json

{

  ""result"": [

    { ""id"": 10, ""score"": 0.81 },

    { ""id"": 14, ""score"": 0.75 },

    { ""id"": 11, ""score"": 0.73 }

  ],

  ""status"": ""ok"",

  ""time"": 0.001

}

```



The algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.



### Average vector strategy",0.31092436974789917,0.3627450980392157,0.334841628959276,0.09322033898305085,0.10891089108910891,0.10045662100456619,0.19327731092436976,0.22549019607843138,0.20814479638009048,0.2605042016806723,0.30392156862745096,0.28054298642533937,0.6105151176452637,0.5832219123840332,0.5854649543762207,0.584341287612915,0.6711405426263809,0.9855274494737387,133,1.1271186440677967,0.42261550919143137
True,,7277.937,6ddba4b0,How does oversampling helps?,"Defines how many extra vectors should be pre-selected using quantized index, and then re-scored using original vectors. For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring. Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.","Oversampling helps in several ways:

1. **Equalizing Class Representation**: By oversampling, we can balance the representation of different classes in the training dataset. This is particularly important in scenarios where there is an imbalance in the number of samples across different classes. Having a more balanced dataset can lead to more accurate and fair modeling of real-world scenarios.

2. **Search Speed vs. Search Quality Tradeoff Tuning**: Oversampling allows us to tune the tradeoff between search speed and search quality during query time. By adjusting the oversampling factor, we can control how many potential candidates are considered during the search process, impacting both the efficiency of the search and the accuracy of the results.

3. **Improving Search Precision**: Oversampling can lead to improved search precision. By increasing the number of potential candidates (determined by the oversampling factor and limit value), we can enhance the recall of correct results. This is especially important in applications where search accuracy is crucial.

4. **Optimizing Efficiency**: By using an optimal oversampling factor, such as 3 as mentioned in the example, we can strike a balance between accuracy and efficiency in the search process. This factor is considered suitable for a broad range of applications, providing a good tradeoff between accuracy and computational resources.

5. **Enhancing Accuracy through Rescoring**: Enabling rescoring, in conjunction with oversampling, can further improve the accuracy of search results. By selecting a subset of vectors using a quantized index based on the oversampling factor and limit, and then performing a more accurate search on this subset, we can refine the search results and enhance their accuracy.

In summary, oversampling plays a crucial role in improving the fairness, accuracy, efficiency, and overall performance of search algorithms, particularly in scenarios where class imbalances and search precision are important considerations.","oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.

Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.



## Quantization tips



#### Accuracy tuning



In this section, we will discuss how to tune the search precision.
We see that ""correct"" results i.e. recall increases as the number of potential ""candidates"" increase (limit x oversampling). To highlight the impact of changing the `limit`, different limit values are broken apart into different curves. For example, we see that the lowest recall for limit 50 is around 94 correct, with 100 candidates. This also implies we used an oversampling of 2.0



As oversampling increases, we see a general improvement in results – but that does not hold in every case.
3. Oversampling: Use an oversampling factor of 3 for the best balance between accuracy and efficiency. This factor is suitable for a wide range of applications.

4. Rescoring: Enable rescoring to improve the accuracy of search results.
For example, if `oversampling=2.0` and the `limit=100`, then 200 vectors will first be selected using a quantized index. For those 200 vectors, the full 32 bit vector will be used with their HNSW index to a much more accurate 100 item result set. As opposed to doing a full HNSW search, we oversample a preliminary search and then only do the full search on this much smaller set of vectors.



## Improved Storage Efficiency","['oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.', 'For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.\n\nOversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.\n\n\n\n## Quantization tips\n\n\n\n#### Accuracy tuning\n\n\n\nIn this section, we will discuss how to tune the search precision.', 'We see that ""correct"" results i.e. recall increases as the number of potential ""candidates"" increase (limit x oversampling). To highlight the impact of changing the `limit`, different limit values are broken apart into different curves. For example, we see that the lowest recall for limit 50 is around 94 correct, with 100 candidates. This also implies we used an oversampling of 2.0\n\n\n\nAs oversampling increases, we see a general improvement in results – but that does not hold in every case.', '3. Oversampling: Use an oversampling factor of 3 for the best balance between accuracy and efficiency. This factor is suitable for a wide range of applications.\n\n4. Rescoring: Enable rescoring to improve the accuracy of search results.', 'For example, if `oversampling=2.0` and the `limit=100`, then 200 vectors will first be selected using a quantized index. For those 200 vectors, the full 32 bit vector will be used with their HNSW index to a much more accurate 100 item result set. As opposed to doing a full HNSW search, we oversample a preliminary search and then only do the full search on this much smaller set of vectors.\n\n\n\n## Improved Storage Efficiency']","Question: How does oversampling helps?\n\nContext: oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.

Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.



## Quantization tips



#### Accuracy tuning



In this section, we will discuss how to tune the search precision.
We see that ""correct"" results i.e. recall increases as the number of potential ""candidates"" increase (limit x oversampling). To highlight the impact of changing the `limit`, different limit values are broken apart into different curves. For example, we see that the lowest recall for limit 50 is around 94 correct, with 100 candidates. This also implies we used an oversampling of 2.0



As oversampling increases, we see a general improvement in results – but that does not hold in every case.
3. Oversampling: Use an oversampling factor of 3 for the best balance between accuracy and efficiency. This factor is suitable for a wide range of applications.

4. Rescoring: Enable rescoring to improve the accuracy of search results.
For example, if `oversampling=2.0` and the `limit=100`, then 200 vectors will first be selected using a quantized index. For those 200 vectors, the full 32 bit vector will be used with their HNSW index to a much more accurate 100 item result set. As opposed to doing a full HNSW search, we oversample a preliminary search and then only do the full search on this much smaller set of vectors.



## Improved Storage Efficiency\n\nAnswer:",How does oversampling helps?,"Defines how many extra vectors should be pre-selected using quantized index, and then re-scored using original vectors. For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring. Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.","['oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.', 'For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.\n\nOversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.\n\n\n\n## Quantization tips\n\n\n\n#### Accuracy tuning\n\n\n\nIn this section, we will discuss how to tune the search precision.', 'We see that ""correct"" results i.e. recall increases as the number of potential ""candidates"" increase (limit x oversampling). To highlight the impact of changing the `limit`, different limit values are broken apart into different curves. For example, we see that the lowest recall for limit 50 is around 94 correct, with 100 candidates. This also implies we used an oversampling of 2.0\n\n\n\nAs oversampling increases, we see a general improvement in results – but that does not hold in every case.', '3. Oversampling: Use an oversampling factor of 3 for the best balance between accuracy and efficiency. This factor is suitable for a wide range of applications.\n\n4. Rescoring: Enable rescoring to improve the accuracy of search results.', 'For example, if `oversampling=2.0` and the `limit=100`, then 200 vectors will first be selected using a quantized index. For those 200 vectors, the full 32 bit vector will be used with their HNSW index to a much more accurate 100 item result set. As opposed to doing a full HNSW search, we oversample a preliminary search and then only do the full search on this much smaller set of vectors.\n\n\n\n## Improved Storage Efficiency']","oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.

Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.



## Quantization tips



#### Accuracy tuning



In this section, we will discuss how to tune the search precision.
We see that ""correct"" results i.e. recall increases as the number of potential ""candidates"" increase (limit x oversampling). To highlight the impact of changing the `limit`, different limit values are broken apart into different curves. For example, we see that the lowest recall for limit 50 is around 94 correct, with 100 candidates. This also implies we used an oversampling of 2.0



As oversampling increases, we see a general improvement in results – but that does not hold in every case.
3. Oversampling: Use an oversampling factor of 3 for the best balance between accuracy and efficiency. This factor is suitable for a wide range of applications.

4. Rescoring: Enable rescoring to improve the accuracy of search results.
For example, if `oversampling=2.0` and the `limit=100`, then 200 vectors will first be selected using a quantized index. For those 200 vectors, the full 32 bit vector will be used with their HNSW index to a much more accurate 100 item result set. As opposed to doing a full HNSW search, we oversample a preliminary search and then only do the full search on this much smaller set of vectors.



## Improved Storage Efficiency",0.12734082397003746,0.5,0.20298507462686569,0.05263157894736842,0.208955223880597,0.08408408408408408,0.06741573033707865,0.2647058823529412,0.10746268656716416,0.12359550561797752,0.4852941176470588,0.19701492537313434,0.5114958882331848,0.5315836071968079,0.6084301471710205,0.5674168467521667,0.8142402000084985,0.8791107690582672,295,4.609375,0.5204066308783901
True,,2287.153,75485d0a,What is the purpose of ‘CreatePayloadIndexAsync’?,CreatePayloadIndexAsync is a method in the Qdrant library that enables the creation of a payload index in Qdrant. A payload index is a data structure designed to store supplemental information about the data stored in Qdrant. This method requires a Qdrant instance and a payload index name as input parameters.,"The purpose of the `CreatePayloadIndexAsync` method is to create an index for a specific field in a collection within the Qdrant database. The method allows you to specify the collection name, the field name to index, the schema type of the payload (e.g., Keyword or Text), and any additional parameters related to the indexing process, such as text indexing parameters in the case of text schema type. The method enables you to define how the data in the specified field should be indexed and searched within the Qdrant database.","client

    .createPayloadIndexAsync(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        PayloadSchemaType.Keyword,

        null,

        null,

        null,

        null)

    .get();

```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");

```



Available field types are:
QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createPayloadIndexAsync(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        PayloadSchemaType.Text,

        PayloadIndexParams.newBuilder()

            .setTextIndexParams(

                TextIndexParams.newBuilder()

                    .setTokenizer(TokenizerType.Word)

                    .setMinTokenLen(2)

                    .setMaxTokenLen(10)
.setMaxTokenLen(10)

                    .setLowercase(true)

                    .build())

            .build(),

        null,

        null,

        null)

    .get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(

	collectionName: ""{collection_name}"",

	fieldName: ""name_of_the_field_to_index"",

	schemaType: PayloadSchemaType.Text,
client

    .create_field_index(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        FieldType::Keyword,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createPayloadIndexAsync(
FieldType::Keyword,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createPayloadIndexAsync(

        ""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null)

    .get();

```","['client\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        PayloadSchemaType.Keyword,\n\n        null,\n\n        null,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");\n\n```\n\n\n\nAvailable field types are:', 'QdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        PayloadSchemaType.Text,\n\n        PayloadIndexParams.newBuilder()\n\n            .setTextIndexParams(\n\n                TextIndexParams.newBuilder()\n\n                    .setTokenizer(TokenizerType.Word)\n\n                    .setMinTokenLen(2)\n\n                    .setMaxTokenLen(10)', '.setMaxTokenLen(10)\n\n                    .setLowercase(true)\n\n                    .build())\n\n            .build(),\n\n        null,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tfieldName: ""name_of_the_field_to_index"",\n\n\tschemaType: PayloadSchemaType.Text,', 'client\n\n    .create_field_index(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(', 'FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null)\n\n    .get();\n\n```']","Question: What is the purpose of ‘CreatePayloadIndexAsync’?\n\nContext: client

.createPayloadIndexAsync(

""{collection_name}"",

""name_of_the_field_to_index"",

PayloadSchemaType.Keyword,

null,

null,

null,

null)

.get();

```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");

```



Available field types are:
QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createPayloadIndexAsync(

""{collection_name}"",

""name_of_the_field_to_index"",

PayloadSchemaType.Text,

PayloadIndexParams.newBuilder()

.setTextIndexParams(

TextIndexParams.newBuilder()

.setTokenizer(TokenizerType.Word)

.setMinTokenLen(2)

.setMaxTokenLen(10)
.setMaxTokenLen(10)

.setLowercase(true)

.build())

.build(),

null,

null,

null)

.get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(

collectionName: ""{collection_name}"",

fieldName: ""name_of_the_field_to_index"",

schemaType: PayloadSchemaType.Text,
client

.create_field_index(

""{collection_name}"",

""name_of_the_field_to_index"",

FieldType::Keyword,

None,

None,

)

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createPayloadIndexAsync(
FieldType::Keyword,

None,

None,

)

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createPayloadIndexAsync(

""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null)

.get();

```\n\nAnswer:",What is the purpose of ‘CreatePayloadIndexAsync’?,CreatePayloadIndexAsync is a method in the Qdrant library that enables the creation of a payload index in Qdrant. A payload index is a data structure designed to store supplemental information about the data stored in Qdrant. This method requires a Qdrant instance and a payload index name as input parameters.,"['client\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        PayloadSchemaType.Keyword,\n\n        null,\n\n        null,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");\n\n```\n\n\n\nAvailable field types are:', 'QdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        PayloadSchemaType.Text,\n\n        PayloadIndexParams.newBuilder()\n\n            .setTextIndexParams(\n\n                TextIndexParams.newBuilder()\n\n                    .setTokenizer(TokenizerType.Word)\n\n                    .setMinTokenLen(2)\n\n                    .setMaxTokenLen(10)', '.setMaxTokenLen(10)\n\n                    .setLowercase(true)\n\n                    .build())\n\n            .build(),\n\n        null,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tfieldName: ""name_of_the_field_to_index"",\n\n\tschemaType: PayloadSchemaType.Text,', 'client\n\n    .create_field_index(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(', 'FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null)\n\n    .get();\n\n```']","client

    .createPayloadIndexAsync(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        PayloadSchemaType.Keyword,

        null,

        null,

        null,

        null)

    .get();

```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");

```



Available field types are:
QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createPayloadIndexAsync(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        PayloadSchemaType.Text,

        PayloadIndexParams.newBuilder()

            .setTextIndexParams(

                TextIndexParams.newBuilder()

                    .setTokenizer(TokenizerType.Word)

                    .setMinTokenLen(2)

                    .setMaxTokenLen(10)
.setMaxTokenLen(10)

                    .setLowercase(true)

                    .build())

            .build(),

        null,

        null,

        null)

    .get();

```



```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(

	collectionName: ""{collection_name}"",

	fieldName: ""name_of_the_field_to_index"",

	schemaType: PayloadSchemaType.Text,
client

    .create_field_index(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        FieldType::Keyword,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createPayloadIndexAsync(
FieldType::Keyword,

        None,

        None,

    )

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.PayloadSchemaType;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createPayloadIndexAsync(

        ""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null)

    .get();

```",0.2876712328767123,0.5121951219512195,0.3684210526315789,0.013888888888888888,0.025,0.017857142857142856,0.1643835616438356,0.2926829268292683,0.21052631578947367,0.2602739726027397,0.4634146341463415,0.3333333333333333,0.8207826614379883,0.6342048645019531,0.6885201930999756,0.6602473258972168,0.9356802018980185,0.9484348384042581,89,1.78,0.5731939368168172
True,,1414.173,52da4f1d,What is the purpose of ef_construct in HNSW ?,"In HNSW algorithm the ef_construct parameter is the number of neighbours to consider during the index building. The larger the value, the higher the precision, but the longer the indexing time. The default values of this parameters 100","The purpose of `ef_construct` in HNSW is to determine the number of neighbors to consider during the index building process. Increasing the value of `ef_construct` can lead to higher precision in search results, but it also requires more space and longer indexing time. This parameter affects the quality of the search results by influencing the number of neighbors considered during the indexing phase.","The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of 

neighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.

The default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and
""m"": 32,

                ""ef_construct"": 123

            },

            ""quantization_config"": {

                ""product"": {

                    ""compression"": ""x32"",

                    ""always_ram"": true

                }

            },

            ""on_disk"": true

        }

    },

    ""hnsw_config"": {

        ""ef_construct"": 123

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""quantile"": 0.8,
```http

PATCH /collections/{collection_name}

{

    ""vectors"": {

        ""my_vector"": {

            ""hnsw_config"": {

                ""m"": 32,

                ""ef_construct"": 123

            },

            ""quantization_config"": {

                ""product"": {

                    ""compression"": ""x32"",

                    ""always_ram"": true

                }

            },

            ""on_disk"": true

        }

    },

    ""hnsw_config"": {

        ""ef_construct"": 123

    },
""hnsw_config"": {

        ""ef_construct"": 123

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""quantile"": 0.8,

            ""always_ram"": false

        }

    }

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

  -H 'Content-Type: application/json' \

  --data-raw '{

    ""vectors"": {

        ""my_vector"": {

            ""hnsw_config"": {

                ""m"": 32,

                ""ef_construct"": 123
(""my_vector"".into()),

                        VectorParamsDiff {

                            hnsw_config: Some(HnswConfigDiff {

                                m: Some(32),

                                ef_construct: Some(123),

                                ..Default::default()

                            }),

                            ..Default::default()

                        },

                    )]),

                },

            )),

        }),","[""The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \n\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\n\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and"", '""m"": 32,\n\n                ""ef_construct"": 123\n\n            },\n\n            ""quantization_config"": {\n\n                ""product"": {\n\n                    ""compression"": ""x32"",\n\n                    ""always_ram"": true\n\n                }\n\n            },\n\n            ""on_disk"": true\n\n        }\n\n    },\n\n    ""hnsw_config"": {\n\n        ""ef_construct"": 123\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""quantile"": 0.8,', '```http\n\nPATCH /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""my_vector"": {\n\n            ""hnsw_config"": {\n\n                ""m"": 32,\n\n                ""ef_construct"": 123\n\n            },\n\n            ""quantization_config"": {\n\n                ""product"": {\n\n                    ""compression"": ""x32"",\n\n                    ""always_ram"": true\n\n                }\n\n            },\n\n            ""on_disk"": true\n\n        }\n\n    },\n\n    ""hnsw_config"": {\n\n        ""ef_construct"": 123\n\n    },', '""hnsw_config"": {\n\n        ""ef_construct"": 123\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""quantile"": 0.8,\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""vectors"": {\n\n        ""my_vector"": {\n\n            ""hnsw_config"": {\n\n                ""m"": 32,\n\n                ""ef_construct"": 123', '(""my_vector"".into()),\n\n                        VectorParamsDiff {\n\n                            hnsw_config: Some(HnswConfigDiff {\n\n                                m: Some(32),\n\n                                ef_construct: Some(123),\n\n                                ..Default::default()\n\n                            }),\n\n                            ..Default::default()\n\n                        },\n\n                    )]),\n\n                },\n\n            )),\n\n        }),']","Question: What is the purpose of ef_construct in HNSW ?\n\nContext: The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of 

neighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.

The default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and
""m"": 32,

""ef_construct"": 123

},

""quantization_config"": {

""product"": {

""compression"": ""x32"",

""always_ram"": true

}

},

""on_disk"": true

}

},

""hnsw_config"": {

""ef_construct"": 123

},

""quantization_config"": {

""scalar"": {

""type"": ""int8"",

""quantile"": 0.8,
```http

PATCH /collections/{collection_name}

{

""vectors"": {

""my_vector"": {

""hnsw_config"": {

""m"": 32,

""ef_construct"": 123

},

""quantization_config"": {

""product"": {

""compression"": ""x32"",

""always_ram"": true

}

},

""on_disk"": true

}

},

""hnsw_config"": {

""ef_construct"": 123

},
""hnsw_config"": {

""ef_construct"": 123

},

""quantization_config"": {

""scalar"": {

""type"": ""int8"",

""quantile"": 0.8,

""always_ram"": false

}

}

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

-H 'Content-Type: application/json' \

--data-raw '{

""vectors"": {

""my_vector"": {

""hnsw_config"": {

""m"": 32,

""ef_construct"": 123
(""my_vector"".into()),

VectorParamsDiff {

hnsw_config: Some(HnswConfigDiff {

m: Some(32),

ef_construct: Some(123),

..Default::default()

}),

..Default::default()

},

)]),

},

)),

}),\n\nAnswer:",What is the purpose of ef_construct in HNSW ?,"In HNSW algorithm the ef_construct parameter is the number of neighbours to consider during the index building. The larger the value, the higher the precision, but the longer the indexing time. The default values of this parameters 100","[""The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \n\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\n\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and"", '""m"": 32,\n\n                ""ef_construct"": 123\n\n            },\n\n            ""quantization_config"": {\n\n                ""product"": {\n\n                    ""compression"": ""x32"",\n\n                    ""always_ram"": true\n\n                }\n\n            },\n\n            ""on_disk"": true\n\n        }\n\n    },\n\n    ""hnsw_config"": {\n\n        ""ef_construct"": 123\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""quantile"": 0.8,', '```http\n\nPATCH /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""my_vector"": {\n\n            ""hnsw_config"": {\n\n                ""m"": 32,\n\n                ""ef_construct"": 123\n\n            },\n\n            ""quantization_config"": {\n\n                ""product"": {\n\n                    ""compression"": ""x32"",\n\n                    ""always_ram"": true\n\n                }\n\n            },\n\n            ""on_disk"": true\n\n        }\n\n    },\n\n    ""hnsw_config"": {\n\n        ""ef_construct"": 123\n\n    },', '""hnsw_config"": {\n\n        ""ef_construct"": 123\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""quantile"": 0.8,\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""vectors"": {\n\n        ""my_vector"": {\n\n            ""hnsw_config"": {\n\n                ""m"": 32,\n\n                ""ef_construct"": 123', '(""my_vector"".into()),\n\n                        VectorParamsDiff {\n\n                            hnsw_config: Some(HnswConfigDiff {\n\n                                m: Some(32),\n\n                                ef_construct: Some(123),\n\n                                ..Default::default()\n\n                            }),\n\n                            ..Default::default()\n\n                        },\n\n                    )]),\n\n                },\n\n            )),\n\n        }),']","The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of 

neighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.

The default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and
""m"": 32,

                ""ef_construct"": 123

            },

            ""quantization_config"": {

                ""product"": {

                    ""compression"": ""x32"",

                    ""always_ram"": true

                }

            },

            ""on_disk"": true

        }

    },

    ""hnsw_config"": {

        ""ef_construct"": 123

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""quantile"": 0.8,
```http

PATCH /collections/{collection_name}

{

    ""vectors"": {

        ""my_vector"": {

            ""hnsw_config"": {

                ""m"": 32,

                ""ef_construct"": 123

            },

            ""quantization_config"": {

                ""product"": {

                    ""compression"": ""x32"",

                    ""always_ram"": true

                }

            },

            ""on_disk"": true

        }

    },

    ""hnsw_config"": {

        ""ef_construct"": 123

    },
""hnsw_config"": {

        ""ef_construct"": 123

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""quantile"": 0.8,

            ""always_ram"": false

        }

    }

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

  -H 'Content-Type: application/json' \

  --data-raw '{

    ""vectors"": {

        ""my_vector"": {

            ""hnsw_config"": {

                ""m"": 32,

                ""ef_construct"": 123
(""my_vector"".into()),

                        VectorParamsDiff {

                            hnsw_config: Some(HnswConfigDiff {

                                m: Some(32),

                                ef_construct: Some(123),

                                ..Default::default()

                            }),

                            ..Default::default()

                        },

                    )]),

                },

            )),

        }),",0.38596491228070173,0.7586206896551724,0.5116279069767441,0.21428571428571427,0.42857142857142855,0.2857142857142857,0.3333333333333333,0.6551724137931034,0.4418604651162791,0.38596491228070173,0.7586206896551724,0.5116279069767441,0.8710167407989502,0.6346986889839172,0.6957376599311829,0.6638180017471313,0.9778485903516412,0.5893875658512115,63,1.6578947368421053,0.41745930905108175
True,,1670.81,c1fc08da,How do you use ‘ordering’ parameter?,"Write ordering can be specified for any write request to serialize it through a single “leader” node, which ensures that all write operations (issued with the same ordering) are performed and observed sequentially. It is of 3 types weak , medium and strong and is used in python with additional param ordering=models.WriteOrdering.STRONG to upsert request.","The `ordering` parameter can be used with update and delete operations in Qdrant to ensure that the operations are executed in the same order on all replicas. By using this parameter, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This helps to avoid data inconsistency in cases where concurrent updates of the same documents may occur.","- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
key=""city"",

            match=models.MatchValue(

                value=""London"",

            ),

        )

    ]

)



recommend_queries = [

    models.RecommendRequest(

        positive=[100, 231], negative=[718], filter=filter, limit=3

    ),

    models.RecommendRequest(positive=[200, 67], negative=[300], filter=filter, limit=3),

]



client.recommend_batch(collection_name=""{collection_name}"", requests=recommend_queries)

```



```typescript
WITH ENGINE = ""qdrant"",

PARAMETERS = {

    ""location"": "":memory:"",

    ""collection_config"": {

        ""size"": 386,

        ""distance"": ""Cosine""

    }

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
using: Some(""image"".to_string()),

        limit: 10,

        ..Default::default()

    })

    .await?;

```



```java

import java.util.List;



import static io.qdrant.client.PointIdFactory.id;



import io.qdrant.client.grpc.Points.RecommendPoints;



client

    .recommendAsync(

        RecommendPoints.newBuilder()

            .setCollectionName(""{collection_name}"")

            .addAllPositive(List.of(id(100), id(231)))

            .addAllNegative(List.of(id(718)))
```http

POST /collections/{collection_name}/points/payload

{

    ""payload"": {

        ""property1"": ""string"",

        ""property2"": ""string""

    },

    ""filter"": {

        ""must"": [

            {

                ""key"": ""color"",

                ""match"": {

                    ""value"": ""red""

                }

            }

        ]

    }

}

```



```python

client.set_payload(

    collection_name=""{collection_name}"",

    payload={

        ""property1"": ""string"",","['- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', 'key=""city"",\n\n            match=models.MatchValue(\n\n                value=""London"",\n\n            ),\n\n        )\n\n    ]\n\n)\n\n\n\nrecommend_queries = [\n\n    models.RecommendRequest(\n\n        positive=[100, 231], negative=[718], filter=filter, limit=3\n\n    ),\n\n    models.RecommendRequest(positive=[200, 67], negative=[300], filter=filter, limit=3),\n\n]\n\n\n\nclient.recommend_batch(collection_name=""{collection_name}"", requests=recommend_queries)\n\n```\n\n\n\n```typescript', 'WITH ENGINE = ""qdrant"",\n\nPARAMETERS = {\n\n    ""location"": "":memory:"",\n\n    ""collection_config"": {\n\n        ""size"": 386,\n\n        ""distance"": ""Cosine""\n\n    }\n\n}\n\n```\n\n\n\nThe available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).\n\n\n\n## Creating a new table', 'using: Some(""image"".to_string()),\n\n        limit: 10,\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport java.util.List;\n\n\n\nimport static io.qdrant.client.PointIdFactory.id;\n\n\n\nimport io.qdrant.client.grpc.Points.RecommendPoints;\n\n\n\nclient\n\n    .recommendAsync(\n\n        RecommendPoints.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .addAllPositive(List.of(id(100), id(231)))\n\n            .addAllNegative(List.of(id(718)))', '```http\n\nPOST /collections/{collection_name}/points/payload\n\n{\n\n    ""payload"": {\n\n        ""property1"": ""string"",\n\n        ""property2"": ""string""\n\n    },\n\n    ""filter"": {\n\n        ""must"": [\n\n            {\n\n                ""key"": ""color"",\n\n                ""match"": {\n\n                    ""value"": ""red""\n\n                }\n\n            }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.set_payload(\n\n    collection_name=""{collection_name}"",\n\n    payload={\n\n        ""property1"": ""string"",']","Question: How do you use ‘ordering’ parameter?\n\nContext: - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
key=""city"",

match=models.MatchValue(

value=""London"",

),

)

]

)



recommend_queries = [

models.RecommendRequest(

positive=[100, 231], negative=[718], filter=filter, limit=3

),

models.RecommendRequest(positive=[200, 67], negative=[300], filter=filter, limit=3),

]



client.recommend_batch(collection_name=""{collection_name}"", requests=recommend_queries)

```



```typescript
WITH ENGINE = ""qdrant"",

PARAMETERS = {

""location"": "":memory:"",

""collection_config"": {

""size"": 386,

""distance"": ""Cosine""

}

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
using: Some(""image"".to_string()),

limit: 10,

..Default::default()

})

.await?;

```



```java

import java.util.List;



import static io.qdrant.client.PointIdFactory.id;



import io.qdrant.client.grpc.Points.RecommendPoints;



client

.recommendAsync(

RecommendPoints.newBuilder()

.setCollectionName(""{collection_name}"")

.addAllPositive(List.of(id(100), id(231)))

.addAllNegative(List.of(id(718)))
```http

POST /collections/{collection_name}/points/payload

{

""payload"": {

""property1"": ""string"",

""property2"": ""string""

},

""filter"": {

""must"": [

{

""key"": ""color"",

""match"": {

""value"": ""red""

}

}

]

}

}

```



```python

client.set_payload(

collection_name=""{collection_name}"",

payload={

""property1"": ""string"",\n\nAnswer:",How do you use ‘ordering’ parameter?,"Write ordering can be specified for any write request to serialize it through a single “leader” node, which ensures that all write operations (issued with the same ordering) are performed and observed sequentially. It is of 3 types weak , medium and strong and is used in python with additional param ordering=models.WriteOrdering.STRONG to upsert request.","['- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', 'key=""city"",\n\n            match=models.MatchValue(\n\n                value=""London"",\n\n            ),\n\n        )\n\n    ]\n\n)\n\n\n\nrecommend_queries = [\n\n    models.RecommendRequest(\n\n        positive=[100, 231], negative=[718], filter=filter, limit=3\n\n    ),\n\n    models.RecommendRequest(positive=[200, 67], negative=[300], filter=filter, limit=3),\n\n]\n\n\n\nclient.recommend_batch(collection_name=""{collection_name}"", requests=recommend_queries)\n\n```\n\n\n\n```typescript', 'WITH ENGINE = ""qdrant"",\n\nPARAMETERS = {\n\n    ""location"": "":memory:"",\n\n    ""collection_config"": {\n\n        ""size"": 386,\n\n        ""distance"": ""Cosine""\n\n    }\n\n}\n\n```\n\n\n\nThe available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).\n\n\n\n## Creating a new table', 'using: Some(""image"".to_string()),\n\n        limit: 10,\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport java.util.List;\n\n\n\nimport static io.qdrant.client.PointIdFactory.id;\n\n\n\nimport io.qdrant.client.grpc.Points.RecommendPoints;\n\n\n\nclient\n\n    .recommendAsync(\n\n        RecommendPoints.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .addAllPositive(List.of(id(100), id(231)))\n\n            .addAllNegative(List.of(id(718)))', '```http\n\nPOST /collections/{collection_name}/points/payload\n\n{\n\n    ""payload"": {\n\n        ""property1"": ""string"",\n\n        ""property2"": ""string""\n\n    },\n\n    ""filter"": {\n\n        ""must"": [\n\n            {\n\n                ""key"": ""color"",\n\n                ""match"": {\n\n                    ""value"": ""red""\n\n                }\n\n            }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.set_payload(\n\n    collection_name=""{collection_name}"",\n\n    payload={\n\n        ""property1"": ""string"",']","- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
key=""city"",

            match=models.MatchValue(

                value=""London"",

            ),

        )

    ]

)



recommend_queries = [

    models.RecommendRequest(

        positive=[100, 231], negative=[718], filter=filter, limit=3

    ),

    models.RecommendRequest(positive=[200, 67], negative=[300], filter=filter, limit=3),

]



client.recommend_batch(collection_name=""{collection_name}"", requests=recommend_queries)

```



```typescript
WITH ENGINE = ""qdrant"",

PARAMETERS = {

    ""location"": "":memory:"",

    ""collection_config"": {

        ""size"": 386,

        ""distance"": ""Cosine""

    }

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
using: Some(""image"".to_string()),

        limit: 10,

        ..Default::default()

    })

    .await?;

```



```java

import java.util.List;



import static io.qdrant.client.PointIdFactory.id;



import io.qdrant.client.grpc.Points.RecommendPoints;



client

    .recommendAsync(

        RecommendPoints.newBuilder()

            .setCollectionName(""{collection_name}"")

            .addAllPositive(List.of(id(100), id(231)))

            .addAllNegative(List.of(id(718)))
```http

POST /collections/{collection_name}/points/payload

{

    ""payload"": {

        ""property1"": ""string"",

        ""property2"": ""string""

    },

    ""filter"": {

        ""must"": [

            {

                ""key"": ""color"",

                ""match"": {

                    ""value"": ""red""

                }

            }

        ]

    }

}

```



```python

client.set_payload(

    collection_name=""{collection_name}"",

    payload={

        ""property1"": ""string"",",0.32786885245901637,0.36363636363636365,0.3448275862068965,0.05,0.05555555555555555,0.052631578947368425,0.19672131147540983,0.21818181818181817,0.20689655172413796,0.2459016393442623,0.2727272727272727,0.2586206896551724,0.5104479789733887,0.6153050065040588,0.609060525894165,0.6121668815612793,0.9127926991010705,0.3691544532775879,70,1.25,0.24220537587345972
True,,2421.913,92a53f02,What is significance of ‘on_disk_payload’ setting?,"The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes.","The significance of the `on_disk_payload` setting is that it defines where to store the payload data associated with vectors in a collection. When set to `true`, it indicates that the payload data will be stored on disk only. This can be useful for limiting the RAM usage, especially in cases where the payload data is large. 

By storing payload data on disk, it helps in reducing the memory footprint of the collection and can improve the overall performance of the system, especially when dealing with large amounts of data.","* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.

* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.
To enable this, you need to set the `hnsw_config.on_disk` parameter to `true` during collection [creation](../collections/#create-a-collection) or [updating](../collections/#update-collection-parameters).



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""hnsw_config"": {

        ""on_disk"": true

    }

}

```



```python
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
""vectors_count"": 1068786,

        ""indexed_vectors_count"": 1024232,

        ""points_count"": 1068786,

        ""segments_count"": 31,

        ""config"": {

            ""params"": {

                ""vectors"": {

                    ""size"": 384,

                    ""distance"": ""Cosine""

                },

                ""shard_number"": 1,

                ""replication_factor"": 1,

                ""write_consistency_factor"": 1,

                ""on_disk_payload"": false

            },
""on_disk"": true

        }

    }

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

  -H 'Content-Type: application/json' \

  --data-raw '{

    ""vectors"": {

        ""my_vector"": { 

           ""on_disk"": true 

      }

    }

  }'

```



In the following example the HNSW index and quantization parameters are updated,

both for the whole collection, and for `my_vector` specifically:





```http

PATCH /collections/{collection_name}

{","['* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.\n\n* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\n\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.', 'To enable this, you need to set the `hnsw_config.on_disk` parameter to `true` during collection [creation](../collections/#create-a-collection) or [updating](../collections/#update-collection-parameters).\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""hnsw_config"": {\n\n        ""on_disk"": true\n\n    }\n\n}\n\n```\n\n\n\n```python', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', '""vectors_count"": 1068786,\n\n        ""indexed_vectors_count"": 1024232,\n\n        ""points_count"": 1068786,\n\n        ""segments_count"": 31,\n\n        ""config"": {\n\n            ""params"": {\n\n                ""vectors"": {\n\n                    ""size"": 384,\n\n                    ""distance"": ""Cosine""\n\n                },\n\n                ""shard_number"": 1,\n\n                ""replication_factor"": 1,\n\n                ""write_consistency_factor"": 1,\n\n                ""on_disk_payload"": false\n\n            },', '""on_disk"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""vectors"": {\n\n        ""my_vector"": { \n\n           ""on_disk"": true \n\n      }\n\n    }\n\n  }\'\n\n```\n\n\n\nIn the following example the HNSW index and quantization parameters are updated,\n\nboth for the whole collection, and for `my_vector` specifically:\n\n\n\n\n\n```http\n\nPATCH /collections/{collection_name}\n\n{']","Question: What is significance of ‘on_disk_payload’ setting?\n\nContext: * `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.

* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.
To enable this, you need to set the `hnsw_config.on_disk` parameter to `true` during collection [creation](../collections/#create-a-collection) or [updating](../collections/#update-collection-parameters).



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""optimizers_config"": {

""memmap_threshold"": 20000

},

""hnsw_config"": {

""on_disk"": true

}

}

```



```python
""on_disk_payload"": false

},

""hnsw_config"": {

""m"": 16,

""ef_construct"": 100,

""full_scan_threshold"": 10000,

""max_indexing_threads"": 0

},

""optimizer_config"": {

""deleted_threshold"": 0.2,

""vacuum_min_vector_number"": 1000,

""default_segment_number"": 0,

""max_segment_size"": null,
""vectors_count"": 1068786,

""indexed_vectors_count"": 1024232,

""points_count"": 1068786,

""segments_count"": 31,

""config"": {

""params"": {

""vectors"": {

""size"": 384,

""distance"": ""Cosine""

},

""shard_number"": 1,

""replication_factor"": 1,

""write_consistency_factor"": 1,

""on_disk_payload"": false

},
""on_disk"": true

}

}

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

-H 'Content-Type: application/json' \

--data-raw '{

""vectors"": {

""my_vector"": { 

""on_disk"": true 

}

}

}'

```



In the following example the HNSW index and quantization parameters are updated,

both for the whole collection, and for `my_vector` specifically:





```http

PATCH /collections/{collection_name}

{\n\nAnswer:",What is significance of ‘on_disk_payload’ setting?,"The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes.","['* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.\n\n* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\n\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.', 'To enable this, you need to set the `hnsw_config.on_disk` parameter to `true` during collection [creation](../collections/#create-a-collection) or [updating](../collections/#update-collection-parameters).\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""hnsw_config"": {\n\n        ""on_disk"": true\n\n    }\n\n}\n\n```\n\n\n\n```python', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', '""vectors_count"": 1068786,\n\n        ""indexed_vectors_count"": 1024232,\n\n        ""points_count"": 1068786,\n\n        ""segments_count"": 31,\n\n        ""config"": {\n\n            ""params"": {\n\n                ""vectors"": {\n\n                    ""size"": 384,\n\n                    ""distance"": ""Cosine""\n\n                },\n\n                ""shard_number"": 1,\n\n                ""replication_factor"": 1,\n\n                ""write_consistency_factor"": 1,\n\n                ""on_disk_payload"": false\n\n            },', '""on_disk"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""vectors"": {\n\n        ""my_vector"": { \n\n           ""on_disk"": true \n\n      }\n\n    }\n\n  }\'\n\n```\n\n\n\nIn the following example the HNSW index and quantization parameters are updated,\n\nboth for the whole collection, and for `my_vector` specifically:\n\n\n\n\n\n```http\n\nPATCH /collections/{collection_name}\n\n{']","* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.

* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.
To enable this, you need to set the `hnsw_config.on_disk` parameter to `true` during collection [creation](../collections/#create-a-collection) or [updating](../collections/#update-collection-parameters).



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""hnsw_config"": {

        ""on_disk"": true

    }

}

```



```python
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
""vectors_count"": 1068786,

        ""indexed_vectors_count"": 1024232,

        ""points_count"": 1068786,

        ""segments_count"": 31,

        ""config"": {

            ""params"": {

                ""vectors"": {

                    ""size"": 384,

                    ""distance"": ""Cosine""

                },

                ""shard_number"": 1,

                ""replication_factor"": 1,

                ""write_consistency_factor"": 1,

                ""on_disk_payload"": false

            },
""on_disk"": true

        }

    }

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

  -H 'Content-Type: application/json' \

  --data-raw '{

    ""vectors"": {

        ""my_vector"": { 

           ""on_disk"": true 

      }

    }

  }'

```



In the following example the HNSW index and quantization parameters are updated,

both for the whole collection, and for `my_vector` specifically:





```http

PATCH /collections/{collection_name}

{",0.4375,0.3723404255319149,0.4022988505747126,0.13924050632911392,0.11827956989247312,0.12790697674418605,0.275,0.23404255319148937,0.25287356321839083,0.425,0.3617021276595745,0.39080459770114945,0.8268088102340698,0.6372908353805542,0.6219008564949036,0.6295017600059509,0.984067220473662,0.4077657163143158,89,0.898989898989899,0.5990686819248437
True,,6495.793,62f8055d,What is the impact of ‘write_consistency_factor’ ?,"The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully.","The impact of the `write_consistency_factor` setting is significant in determining the durability and reliability of write operations in a distributed system. Here are some key points regarding its impact:

1. **Durability**: Setting a higher `write_consistency_factor` means that a write operation will only be considered successful if the specified number of replicas acknowledge the write. This ensures that data is reliably stored across multiple nodes in the cluster before the operation is confirmed as successful. This enhances the durability of data in the system.

2. **Tolerance to Network Partitions**: Increasing the `write_consistency_factor` can make the write operations more tolerant to network partitions in the cluster. By requiring a higher number of replicas to acknowledge the write, the system can continue to function effectively even if some nodes are temporarily disconnected or experiencing network issues.

3. **Availability**: On the flip side, setting a higher `write_consistency_factor` also means that a higher number of replicas need to be active and reachable for a write operation to succeed. This can impact the availability of the system, especially in scenarios where a significant portion of replicas are unavailable.

4. **Performance**: The `write_consistency_factor` setting can also impact the performance of write operations. Higher values may lead to longer write latencies since more replicas need to acknowledge the operation. This trade-off between consistency, availability, and performance needs to be carefully considered based on the specific requirements of the application.

In summary, the `write_consistency_factor` setting plays a crucial role in balancing data consistency, durability, availability, and performance in a distributed system. Adjusting this setting requires consideration of the trade-offs involved to ensure the system meets the desired level of reliability and performance.","- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

    ""write_consistency_factor"": 2,

}

```



```python
collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

	shardNumber: 6,

	replicationFactor: 2,

	writeConsistencyFactor: 2

);

```



Write operations will fail if the number of active replicas is less than the `write_consistency_factor`.



### Read consistency



Read `consistency` can be specified for most read requests and will ensure that the returned result

is consistent across cluster nodes.
""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

    shard_number=6,

    replication_factor=2,

    write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
""vectors_count"": 1068786,

        ""indexed_vectors_count"": 1024232,

        ""points_count"": 1068786,

        ""segments_count"": 31,

        ""config"": {

            ""params"": {

                ""vectors"": {

                    ""size"": 384,

                    ""distance"": ""Cosine""

                },

                ""shard_number"": 1,

                ""replication_factor"": 1,

                ""write_consistency_factor"": 1,

                ""on_disk_payload"": false

            },","['- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.', '### Write consistency factor\n\n\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\n\nIt can be configured at the collection\'s creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python', 'collectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\n\tshardNumber: 6,\n\n\treplicationFactor: 2,\n\n\twriteConsistencyFactor: 2\n\n);\n\n```\n\n\n\nWrite operations will fail if the number of active replicas is less than the `write_consistency_factor`.\n\n\n\n### Read consistency\n\n\n\nRead `consistency` can be specified for most read requests and will ensure that the returned result\n\nis consistent across cluster nodes.', '""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n\n    shard_number=6,\n\n    replication_factor=2,\n\n    write_consistency_factor=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', '""vectors_count"": 1068786,\n\n        ""indexed_vectors_count"": 1024232,\n\n        ""points_count"": 1068786,\n\n        ""segments_count"": 31,\n\n        ""config"": {\n\n            ""params"": {\n\n                ""vectors"": {\n\n                    ""size"": 384,\n\n                    ""distance"": ""Cosine""\n\n                },\n\n                ""shard_number"": 1,\n\n                ""replication_factor"": 1,\n\n                ""write_consistency_factor"": 1,\n\n                ""on_disk_payload"": false\n\n            },']","Question: What is the impact of ‘write_consistency_factor’ ?\n\nContext: - `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 300,

""distance"": ""Cosine""

},

""shard_number"": 6,

""replication_factor"": 2,

""write_consistency_factor"": 2,

}

```



```python
collectionName: ""{collection_name}"",

vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

shardNumber: 6,

replicationFactor: 2,

writeConsistencyFactor: 2

);

```



Write operations will fail if the number of active replicas is less than the `write_consistency_factor`.



### Read consistency



Read `consistency` can be specified for most read requests and will ensure that the returned result

is consistent across cluster nodes.
""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

shard_number=6,

replication_factor=2,

write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
""vectors_count"": 1068786,

""indexed_vectors_count"": 1024232,

""points_count"": 1068786,

""segments_count"": 31,

""config"": {

""params"": {

""vectors"": {

""size"": 384,

""distance"": ""Cosine""

},

""shard_number"": 1,

""replication_factor"": 1,

""write_consistency_factor"": 1,

""on_disk_payload"": false

},\n\nAnswer:",What is the impact of ‘write_consistency_factor’ ?,"The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully.","['- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.', '### Write consistency factor\n\n\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\n\nIt can be configured at the collection\'s creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python', 'collectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\n\tshardNumber: 6,\n\n\treplicationFactor: 2,\n\n\twriteConsistencyFactor: 2\n\n);\n\n```\n\n\n\nWrite operations will fail if the number of active replicas is less than the `write_consistency_factor`.\n\n\n\n### Read consistency\n\n\n\nRead `consistency` can be specified for most read requests and will ensure that the returned result\n\nis consistent across cluster nodes.', '""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n\n    shard_number=6,\n\n    replication_factor=2,\n\n    write_consistency_factor=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', '""vectors_count"": 1068786,\n\n        ""indexed_vectors_count"": 1024232,\n\n        ""points_count"": 1068786,\n\n        ""segments_count"": 31,\n\n        ""config"": {\n\n            ""params"": {\n\n                ""vectors"": {\n\n                    ""size"": 384,\n\n                    ""distance"": ""Cosine""\n\n                },\n\n                ""shard_number"": 1,\n\n                ""replication_factor"": 1,\n\n                ""write_consistency_factor"": 1,\n\n                ""on_disk_payload"": false\n\n            },']","- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

    ""write_consistency_factor"": 2,

}

```



```python
collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

	shardNumber: 6,

	replicationFactor: 2,

	writeConsistencyFactor: 2

);

```



Write operations will fail if the number of active replicas is less than the `write_consistency_factor`.



### Read consistency



Read `consistency` can be specified for most read requests and will ensure that the returned result

is consistent across cluster nodes.
""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

    shard_number=6,

    replication_factor=2,

    write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
""vectors_count"": 1068786,

        ""indexed_vectors_count"": 1024232,

        ""points_count"": 1068786,

        ""segments_count"": 31,

        ""config"": {

            ""params"": {

                ""vectors"": {

                    ""size"": 384,

                    ""distance"": ""Cosine""

                },

                ""shard_number"": 1,

                ""replication_factor"": 1,

                ""write_consistency_factor"": 1,

                ""on_disk_payload"": false

            },",0.1752988047808765,0.7857142857142857,0.28664495114006516,0.096,0.43636363636363634,0.15737704918032785,0.14342629482071714,0.6428571428571429,0.23452768729641696,0.16733067729083664,0.75,0.2736156351791531,0.810247004032135,0.572669267654419,0.6842742562294006,0.6235170364379883,0.5004409893470652,0.7825615395558998,273,4.403225806451613,0.7554829139960633
True,,2509.229,943b34a0,What is the purpose of oversampling in Qdrant search process?,Oversampling in Qdrant search process defines how many extra vectors should be pre-selected using quantized index and then re-scored using original vectors to improve search quality.,"The purpose of oversampling in the Qdrant search process is to control the precision of the search in real time. By using oversampling, Qdrant can internally retrieve more vectors from the quantized storage than actually needed. This allows for a more precise scoring when rescoring with the original vectors. 

In practical terms, if oversampling is set to 2.4 and the limit is 100, then 240 vectors will be pre-selected using the quantized index. After rescoring with original vectors, the top 100 vectors will be returned. This technique helps in tuning the tradeoff between search speed and search quality during the query time, allowing for a better balance between the two factors.","### Oversampling and Rescoring



A distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.

This enables the best combination of speed, accuracy, and RAM usage.



Qdrant stores the original vectors, so it is possible to rescore the top-k results with

the original vectors after doing the neighbours search in quantized space. That obviously
{

		Quantization = new QuantizationSearchParams

		{

			Ignore = false,

			Rescore = true,

			Oversampling = 2.0

		}

	},

	limit: 10

);

```



`ignore` - Toggle whether to ignore quantized vectors during the search process. By default, Qdrant will use quantized vectors if they are available.



`rescore` - Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors.
Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user
For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.

Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.



## Quantization tips



#### Accuracy tuning



In this section, we will discuss how to tune the search precision.
```http

POST /collections/{collection_name}/points/search

{

    ""params"": {

        ""quantization"": {

            ""ignore"": false,

            ""rescore"": true,

            ""oversampling"": 2.0

        }

    },

    ""vector"": [0.2, 0.1, 0.9, 0.7],

    ""limit"": 10

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.search(

    collection_name=""{collection_name}"",","['### Oversampling and Rescoring\n\n\n\nA distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.\n\nThis enables the best combination of speed, accuracy, and RAM usage.\n\n\n\nQdrant stores the original vectors, so it is possible to rescore the top-k results with\n\nthe original vectors after doing the neighbours search in quantized space. That obviously', '{\n\n\t\tQuantization = new QuantizationSearchParams\n\n\t\t{\n\n\t\t\tIgnore = false,\n\n\t\t\tRescore = true,\n\n\t\t\tOversampling = 2.0\n\n\t\t}\n\n\t},\n\n\tlimit: 10\n\n);\n\n```\n\n\n\n`ignore` - Toggle whether to ignore quantized vectors during the search process. By default, Qdrant will use quantized vectors if they are available.\n\n\n\n`rescore` - Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors.', 'Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user', 'For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.\n\nOversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.\n\n\n\n## Quantization tips\n\n\n\n#### Accuracy tuning\n\n\n\nIn this section, we will discuss how to tune the search precision.', '```http\n\nPOST /collections/{collection_name}/points/search\n\n{\n\n    ""params"": {\n\n        ""quantization"": {\n\n            ""ignore"": false,\n\n            ""rescore"": true,\n\n            ""oversampling"": 2.0\n\n        }\n\n    },\n\n    ""vector"": [0.2, 0.1, 0.9, 0.7],\n\n    ""limit"": 10\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.search(\n\n    collection_name=""{collection_name}"",']","Question: What is the purpose of oversampling in Qdrant search process?\n\nContext: ### Oversampling and Rescoring



A distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.

This enables the best combination of speed, accuracy, and RAM usage.



Qdrant stores the original vectors, so it is possible to rescore the top-k results with

the original vectors after doing the neighbours search in quantized space. That obviously
{

Quantization = new QuantizationSearchParams

{

Ignore = false,

Rescore = true,

Oversampling = 2.0

}

},

limit: 10

);

```



`ignore` - Toggle whether to ignore quantized vectors during the search process. By default, Qdrant will use quantized vectors if they are available.



`rescore` - Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors.
Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user
For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.

Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.



## Quantization tips



#### Accuracy tuning



In this section, we will discuss how to tune the search precision.
```http

POST /collections/{collection_name}/points/search

{

""params"": {

""quantization"": {

""ignore"": false,

""rescore"": true,

""oversampling"": 2.0

}

},

""vector"": [0.2, 0.1, 0.9, 0.7],

""limit"": 10

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.search(

collection_name=""{collection_name}"",\n\nAnswer:",What is the purpose of oversampling in Qdrant search process?,Oversampling in Qdrant search process defines how many extra vectors should be pre-selected using quantized index and then re-scored using original vectors to improve search quality.,"['### Oversampling and Rescoring\n\n\n\nA distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.\n\nThis enables the best combination of speed, accuracy, and RAM usage.\n\n\n\nQdrant stores the original vectors, so it is possible to rescore the top-k results with\n\nthe original vectors after doing the neighbours search in quantized space. That obviously', '{\n\n\t\tQuantization = new QuantizationSearchParams\n\n\t\t{\n\n\t\t\tIgnore = false,\n\n\t\t\tRescore = true,\n\n\t\t\tOversampling = 2.0\n\n\t\t}\n\n\t},\n\n\tlimit: 10\n\n);\n\n```\n\n\n\n`ignore` - Toggle whether to ignore quantized vectors during the search process. By default, Qdrant will use quantized vectors if they are available.\n\n\n\n`rescore` - Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors.', 'Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user', 'For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.\n\nOversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.\n\n\n\n## Quantization tips\n\n\n\n#### Accuracy tuning\n\n\n\nIn this section, we will discuss how to tune the search precision.', '```http\n\nPOST /collections/{collection_name}/points/search\n\n{\n\n    ""params"": {\n\n        ""quantization"": {\n\n            ""ignore"": false,\n\n            ""rescore"": true,\n\n            ""oversampling"": 2.0\n\n        }\n\n    },\n\n    ""vector"": [0.2, 0.1, 0.9, 0.7],\n\n    ""limit"": 10\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.search(\n\n    collection_name=""{collection_name}"",']","### Oversampling and Rescoring



A distinctive feature of the Qdrant architecture is the ability to combine the search for quantized and original vectors in a single query.

This enables the best combination of speed, accuracy, and RAM usage.



Qdrant stores the original vectors, so it is possible to rescore the top-k results with

the original vectors after doing the neighbours search in quantized space. That obviously
{

		Quantization = new QuantizationSearchParams

		{

			Ignore = false,

			Rescore = true,

			Oversampling = 2.0

		}

	},

	limit: 10

);

```



`ignore` - Toggle whether to ignore quantized vectors during the search process. By default, Qdrant will use quantized vectors if they are available.



`rescore` - Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors.
Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user
For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring.

Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time.



## Quantization tips



#### Accuracy tuning



In this section, we will discuss how to tune the search precision.
```http

POST /collections/{collection_name}/points/search

{

    ""params"": {

        ""quantization"": {

            ""ignore"": false,

            ""rescore"": true,

            ""oversampling"": 2.0

        }

    },

    ""vector"": [0.2, 0.1, 0.9, 0.7],

    ""limit"": 10

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.search(

    collection_name=""{collection_name}"",",0.21212121212121213,0.75,0.3307086614173228,0.11224489795918367,0.4074074074074074,0.176,0.16161616161616163,0.5714285714285714,0.25196850393700787,0.1919191919191919,0.6785714285714286,0.2992125984251968,0.913731575012207,0.5924439430236816,0.7084735631942749,0.6452843546867371,0.9988672779096911,0.9837962985038757,111,4.269230769230769,0.41232180969976345
True,,2150.949,b9c5b037,How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?,"Qdrant uses a different approach that does not require pre- or post-filtering, effectively addressing the accuracy problem while maintaining search efficiency.","Qdrant addresses the search accuracy problem by using a custom modification of the HNSW algorithm for Approximate Nearest Neighbor Search (ANN). This modification allows for querying the results with high speed and applying filters without compromising on accuracy. By leveraging the HNSW algorithm, which is known for its accuracy and speed according to public benchmarks, Qdrant ensures that search results are both fast and precise. Additionally, Qdrant allows for fine-tuning the HNSW parameters on a collection and named vector level to optimize search performance further. This combination of custom modifications and parameter tuning helps Qdrant outperform other search engines and provide an efficient solution for high-speed matching.","Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements
HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.

3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.

4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.

5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.
All right, keep going. I like it.



Rishabh Bhardwaj:

Yeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.



Demetrios:

Right.



Rishabh Bhardwaj:
In this episode, Rishabh dives into the nitty-gritty of creating a high-performance hotel matching solution with Qdrant, covering everything from data inconsistency challenges to the speed and accuracy enhancements achieved through the HNSW algorithm.



5 Keys to Learning from the Episode:



1. Discover the importance of data consistency and the challenges it poses when dealing with multiple sources and languages.","['Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements', 'HNSW is chosen for several reasons.\n\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\n\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\n\n\n\n*Available as of v1.1.1*\n\n\n\nThe HNSW parameters can also be configured on a collection and named vector\n\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\n\nperformance.', '2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.\n\n3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.\n\n4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.\n\n5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.', 'All right, keep going. I like it.\n\n\n\nRishabh Bhardwaj:\n\nYeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.\n\n\n\nDemetrios:\n\nRight.\n\n\n\nRishabh Bhardwaj:', 'In this episode, Rishabh dives into the nitty-gritty of creating a high-performance hotel matching solution with Qdrant, covering everything from data inconsistency challenges to the speed and accuracy enhancements achieved through the HNSW algorithm.\n\n\n\n5 Keys to Learning from the Episode:\n\n\n\n1. Discover the importance of data consistency and the challenges it poses when dealing with multiple sources and languages.']","Question: How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?\n\nContext: Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements
HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.

3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.

4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.

5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.
All right, keep going. I like it.



Rishabh Bhardwaj:

Yeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.



Demetrios:

Right.



Rishabh Bhardwaj:
In this episode, Rishabh dives into the nitty-gritty of creating a high-performance hotel matching solution with Qdrant, covering everything from data inconsistency challenges to the speed and accuracy enhancements achieved through the HNSW algorithm.



5 Keys to Learning from the Episode:



1. Discover the importance of data consistency and the challenges it poses when dealing with multiple sources and languages.\n\nAnswer:",How does Qdrant address the search accuracy problem in comparison to other search engines using HNSW?,"Qdrant uses a different approach that does not require pre- or post-filtering, effectively addressing the accuracy problem while maintaining search efficiency.","['Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements', 'HNSW is chosen for several reasons.\n\nFirst, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.\n\nSecond, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).\n\n\n\n*Available as of v1.1.1*\n\n\n\nThe HNSW parameters can also be configured on a collection and named vector\n\nlevel by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search\n\nperformance.', '2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.\n\n3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.\n\n4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.\n\n5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.', 'All right, keep going. I like it.\n\n\n\nRishabh Bhardwaj:\n\nYeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.\n\n\n\nDemetrios:\n\nRight.\n\n\n\nRishabh Bhardwaj:', 'In this episode, Rishabh dives into the nitty-gritty of creating a high-performance hotel matching solution with Qdrant, covering everything from data inconsistency challenges to the speed and accuracy enhancements achieved through the HNSW algorithm.\n\n\n\n5 Keys to Learning from the Episode:\n\n\n\n1. Discover the importance of data consistency and the challenges it poses when dealing with multiple sources and languages.']","Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements
HNSW is chosen for several reasons.

First, HNSW is well-compatible with the modification that allows Qdrant to use filters during a search.

Second, it is one of the most accurate and fastest algorithms, according to [public benchmarks](https://github.com/erikbern/ann-benchmarks).



*Available as of v1.1.1*



The HNSW parameters can also be configured on a collection and named vector

level by setting [`hnsw_config`](../indexing/#vector-index) to fine-tune search

performance.
2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.

3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.

4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.

5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.
All right, keep going. I like it.



Rishabh Bhardwaj:

Yeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.



Demetrios:

Right.



Rishabh Bhardwaj:
In this episode, Rishabh dives into the nitty-gritty of creating a high-performance hotel matching solution with Qdrant, covering everything from data inconsistency challenges to the speed and accuracy enhancements achieved through the HNSW algorithm.



5 Keys to Learning from the Episode:



1. Discover the importance of data consistency and the challenges it poses when dealing with multiple sources and languages.",0.0891089108910891,0.45,0.1487603305785124,0.01,0.05263157894736842,0.016806722689075633,0.0594059405940594,0.3,0.09917355371900825,0.07920792079207921,0.4,0.1322314049586777,0.6693063974380493,0.5327091217041016,0.6249747276306152,0.5751652121543884,0.9971966432174668,0.01722484827041626,107,5.095238095238095,0.2966446072280409
True,,3302.857,095ebaed,What is the difference between regular and neural search?,"Regular full-text search involves searching for keywords within a document, while neural search considers the real meaning of the query and documents, allowing for more accurate results.","Regular search and neural search differ in their underlying technology and capabilities:

1. Regular search: In a regular full-text search, like Google's, the search algorithm looks for specific keywords within documents. The algorithm does not consider the actual meaning of the query or the content of the documents beyond keyword matching. This can lead to missed relevant documents if they are not precisely aligned with the search terms.

2. Neural search: Neural search utilizes neural networks to understand the context, meaning, and relationships between words and documents. This technology can provide more accurate and relevant search results by interpreting the query in a more nuanced way. Neural search is particularly effective in scenarios where queries are vague, imprecise, or involve complex data types such as images, audio files, or lengthy texts.

In summary, while regular search relies on keyword matching, neural search leverages advanced machine learning techniques to enhance search capabilities and provide more sophisticated results.","These days, search technology is the heart of a variety of applications.

From web-pages search to product recommendations.

For many years, this technology didn't get much change until neural networks came into play.



In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?
* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



## What is neural search?



A regular full-text search, such as Google's, consists of searching for keywords inside a document.

For this reason, the algorithm can not take into account the real meaning of the query and documents.

Many documents that might be of interest to the user are not found because they use different wording.
**What is neural search?**



A regular full-text search, such as Google’s, consists of searching for keywords inside a document. For this reason, the algorithm can not take into account the real meaning of the query and documents. Many documents that might be of interest to the user are not found because they use different wording.
In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



**What is neural search?**
**What tasks is neural search good for?**



Neural search has the greatest advantage in areas where the query cannot be formulated precisely. Querying a table in a SQL database is not the best place for neural search.



On the contrary, if the query itself is fuzzy, or it cannot be formulated as a set of conditions — neural search can help you. If the search query is a picture, sound file or long text, neural network search is almost the only option.","[""These days, search technology is the heart of a variety of applications.\n\nFrom web-pages search to product recommendations.\n\nFor many years, this technology didn't get much change until neural networks came into play.\n\n\n\nIn this tutorial we are going to find answers to these questions:\n\n\n\n* What is the difference between regular and neural search?\n\n* What neural networks could be used for search?\n\n* In what tasks is neural network search useful?"", ""* In what tasks is neural network search useful?\n\n* How to build and deploy own neural search service step-by-step?\n\n\n\n## What is neural search?\n\n\n\nA regular full-text search, such as Google's, consists of searching for keywords inside a document.\n\nFor this reason, the algorithm can not take into account the real meaning of the query and documents.\n\nMany documents that might be of interest to the user are not found because they use different wording."", '**What is neural search?**\n\n\n\nA regular full-text search, such as Google’s, consists of searching for keywords inside a document. For this reason, the algorithm can not take into account the real meaning of the query and documents. Many documents that might be of interest to the user are not found because they use different wording.', 'In this tutorial we are going to find answers to these questions:\n\n\n\n* What is the difference between regular and neural search?\n\n* What neural networks could be used for search?\n\n* In what tasks is neural network search useful?\n\n* How to build and deploy own neural search service step-by-step?\n\n\n\n**What is neural search?**', '**What tasks is neural search good for?**\n\n\n\nNeural search has the greatest advantage in areas where the query cannot be formulated precisely. Querying a table in a SQL database is not the best place for neural search.\n\n\n\nOn the contrary, if the query itself is fuzzy, or it cannot be formulated as a set of conditions — neural search can help you. If the search query is a picture, sound file or long text, neural network search is almost the only option.']","Question: What is the difference between regular and neural search?\n\nContext: These days, search technology is the heart of a variety of applications.

From web-pages search to product recommendations.

For many years, this technology didn't get much change until neural networks came into play.



In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?
* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



## What is neural search?



A regular full-text search, such as Google's, consists of searching for keywords inside a document.

For this reason, the algorithm can not take into account the real meaning of the query and documents.

Many documents that might be of interest to the user are not found because they use different wording.
**What is neural search?**



A regular full-text search, such as Google’s, consists of searching for keywords inside a document. For this reason, the algorithm can not take into account the real meaning of the query and documents. Many documents that might be of interest to the user are not found because they use different wording.
In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



**What is neural search?**
**What tasks is neural search good for?**



Neural search has the greatest advantage in areas where the query cannot be formulated precisely. Querying a table in a SQL database is not the best place for neural search.



On the contrary, if the query itself is fuzzy, or it cannot be formulated as a set of conditions — neural search can help you. If the search query is a picture, sound file or long text, neural network search is almost the only option.\n\nAnswer:",What is the difference between regular and neural search?,"Regular full-text search involves searching for keywords within a document, while neural search considers the real meaning of the query and documents, allowing for more accurate results.","[""These days, search technology is the heart of a variety of applications.\n\nFrom web-pages search to product recommendations.\n\nFor many years, this technology didn't get much change until neural networks came into play.\n\n\n\nIn this tutorial we are going to find answers to these questions:\n\n\n\n* What is the difference between regular and neural search?\n\n* What neural networks could be used for search?\n\n* In what tasks is neural network search useful?"", ""* In what tasks is neural network search useful?\n\n* How to build and deploy own neural search service step-by-step?\n\n\n\n## What is neural search?\n\n\n\nA regular full-text search, such as Google's, consists of searching for keywords inside a document.\n\nFor this reason, the algorithm can not take into account the real meaning of the query and documents.\n\nMany documents that might be of interest to the user are not found because they use different wording."", '**What is neural search?**\n\n\n\nA regular full-text search, such as Google’s, consists of searching for keywords inside a document. For this reason, the algorithm can not take into account the real meaning of the query and documents. Many documents that might be of interest to the user are not found because they use different wording.', 'In this tutorial we are going to find answers to these questions:\n\n\n\n* What is the difference between regular and neural search?\n\n* What neural networks could be used for search?\n\n* In what tasks is neural network search useful?\n\n* How to build and deploy own neural search service step-by-step?\n\n\n\n**What is neural search?**', '**What tasks is neural search good for?**\n\n\n\nNeural search has the greatest advantage in areas where the query cannot be formulated precisely. Querying a table in a SQL database is not the best place for neural search.\n\n\n\nOn the contrary, if the query itself is fuzzy, or it cannot be formulated as a set of conditions — neural search can help you. If the search query is a picture, sound file or long text, neural network search is almost the only option.']","These days, search technology is the heart of a variety of applications.

From web-pages search to product recommendations.

For many years, this technology didn't get much change until neural networks came into play.



In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?
* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



## What is neural search?



A regular full-text search, such as Google's, consists of searching for keywords inside a document.

For this reason, the algorithm can not take into account the real meaning of the query and documents.

Many documents that might be of interest to the user are not found because they use different wording.
**What is neural search?**



A regular full-text search, such as Google’s, consists of searching for keywords inside a document. For this reason, the algorithm can not take into account the real meaning of the query and documents. Many documents that might be of interest to the user are not found because they use different wording.
In this tutorial we are going to find answers to these questions:



* What is the difference between regular and neural search?

* What neural networks could be used for search?

* In what tasks is neural network search useful?

* How to build and deploy own neural search service step-by-step?



**What is neural search?**
**What tasks is neural search good for?**



Neural search has the greatest advantage in areas where the query cannot be formulated precisely. Querying a table in a SQL database is not the best place for neural search.



On the contrary, if the query itself is fuzzy, or it cannot be formulated as a set of conditions — neural search can help you. If the search query is a picture, sound file or long text, neural network search is almost the only option.",0.14965986394557823,0.88,0.2558139534883721,0.0684931506849315,0.4166666666666667,0.1176470588235294,0.12244897959183673,0.72,0.20930232558139533,0.1292517006802721,0.76,0.22093023255813954,0.8787981867790222,0.5778165459632874,0.7147313952445984,0.6390225291252136,0.9393177847149283,0.9904457870870829,157,5.814814814814815,0.23219554732660247
True,,6252.6,222cf265,How can I use Qdrant as a vector store in Langchain Go?,"You can use Qdrant as a vector store in Langchain Go by installing the `langchain-go` project dependency and customizing the values for your configuration, such as the Qdrant REST URL and collection name.","To use Qdrant as a vector store in Langchain Go, you can follow these steps:

1. Install the `langchain-go` project dependency:

```bash
go get -u github.com/tmc/langchaingo
```

2. Customize the configuration values for your setup:

```go
import (
    ""log""
    ""net/url""

    ""github.com/tmc/langchaingo/vectorstores""
    ""github.com/tmc/langchaingo/vectorstores/qdrant""
)

llm, err := openai.New()
if err != nil {
    log.Fatal(err)
}

e, err := embeddings.NewEmbedder(llm)
if err != nil {
    log.Fatal(err)
}

url, err := url.Parse(""YOUR_QDRANT_REST_URL"")
if err != nil {
    log.Fatal(err)
}

store, err := qdrant.New(
    qdrant.WithURL(*url),
    qdrant.WithCollectionName(""YOUR_COLLECTION_NAME""),
    qdrant.WithEmbedder(e),
)

if err != nil {
    log.Fatal(err)
}
```

3. Use the `store` variable to interact with Qdrant as a vector store in your Langchain Go application.

Remember to replace `YOUR_QDRANT_REST_URL` and `YOUR_COLLECTION_NAME` with the appropriate values for your Qdrant setup.

Additionally, you can refer to the [Qdrant documentation](https://python.langchain.com/docs/modules/data_connection/vectorstores/#asynchronous-operations) for more information on using Qdrant with Langchain Go.","---

title: Langchain Go

weight: 120

---



# Langchain Go



[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.



You can use Qdrant as a vector store in Langchain Go.



## Setup



Install the `langchain-go` project dependency



```bash

go get -u github.com/tmc/langchaingo

```



## Usage



Before you use the following code sample, customize the following values for your configuration:
""github.com/tmc/langchaingo/vectorstores""

        ""github.com/tmc/langchaingo/vectorstores/qdrant""

)



 llm, err := openai.New()

 if err != nil {

  log.Fatal(err)

 }



 e, err := embeddings.NewEmbedder(llm)

 if err != nil {

  log.Fatal(err)

 }



 url, err := url.Parse(""YOUR_QDRANT_REST_URL"")

 if err != nil {

  log.Fatal(err)

 }



 store, err := qdrant.New(

  qdrant.WithURL(*url),

  qdrant.WithCollectionName(""YOUR_COLLECTION_NAME""),

  qdrant.WithEmbedder(e),

 )

 if err != nil {
index = VectorStoreIndex.from_vector_store(vector_store=vector_store)



```



The library [comes with a notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/vector_stores/QdrantIndexDemo.ipynb) 

that shows an end-to-end example of how to use Qdrant within LlamaIndex.
Qdrant is the only vector database with full coverage of async API in Langchain. Their documentation [describes how to use 

it](https://python.langchain.com/docs/modules/data_connection/vectorstores/#asynchronous-operations).
import io.qdrant.client.grpc.Collections.QuantizationConfig;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()","['---\n\ntitle: Langchain Go\n\nweight: 120\n\n---\n\n\n\n# Langchain Go\n\n\n\n[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.\n\n\n\nYou can use Qdrant as a vector store in Langchain Go.\n\n\n\n## Setup\n\n\n\nInstall the `langchain-go` project dependency\n\n\n\n```bash\n\ngo get -u github.com/tmc/langchaingo\n\n```\n\n\n\n## Usage\n\n\n\nBefore you use the following code sample, customize the following values for your configuration:', '""github.com/tmc/langchaingo/vectorstores""\n\n        ""github.com/tmc/langchaingo/vectorstores/qdrant""\n\n)\n\n\n\n llm, err := openai.New()\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n\n\n e, err := embeddings.NewEmbedder(llm)\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n\n\n url, err := url.Parse(""YOUR_QDRANT_REST_URL"")\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n\n\n store, err := qdrant.New(\n\n  qdrant.WithURL(*url),\n\n  qdrant.WithCollectionName(""YOUR_COLLECTION_NAME""),\n\n  qdrant.WithEmbedder(e),\n\n )\n\n if err != nil {', 'index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n\n\n\n```\n\n\n\nThe library [comes with a notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/vector_stores/QdrantIndexDemo.ipynb) \n\nthat shows an end-to-end example of how to use Qdrant within LlamaIndex.', 'Qdrant is the only vector database with full coverage of async API in Langchain. Their documentation [describes how to use \n\nit](https://python.langchain.com/docs/modules/data_connection/vectorstores/#asynchronous-operations).', 'import io.qdrant.client.grpc.Collections.QuantizationConfig;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()']","Question: How can I use Qdrant as a vector store in Langchain Go?\n\nContext: ---

title: Langchain Go

weight: 120

---



# Langchain Go



[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.



You can use Qdrant as a vector store in Langchain Go.



## Setup



Install the `langchain-go` project dependency



```bash

go get -u github.com/tmc/langchaingo

```



## Usage



Before you use the following code sample, customize the following values for your configuration:
""github.com/tmc/langchaingo/vectorstores""

""github.com/tmc/langchaingo/vectorstores/qdrant""

)



llm, err := openai.New()

if err != nil {

log.Fatal(err)

}



e, err := embeddings.NewEmbedder(llm)

if err != nil {

log.Fatal(err)

}



url, err := url.Parse(""YOUR_QDRANT_REST_URL"")

if err != nil {

log.Fatal(err)

}



store, err := qdrant.New(

qdrant.WithURL(*url),

qdrant.WithCollectionName(""YOUR_COLLECTION_NAME""),

qdrant.WithEmbedder(e),

)

if err != nil {
index = VectorStoreIndex.from_vector_store(vector_store=vector_store)



```



The library [comes with a notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/vector_stores/QdrantIndexDemo.ipynb) 

that shows an end-to-end example of how to use Qdrant within LlamaIndex.
Qdrant is the only vector database with full coverage of async API in Langchain. Their documentation [describes how to use 

it](https://python.langchain.com/docs/modules/data_connection/vectorstores/#asynchronous-operations).
import io.qdrant.client.grpc.Collections.QuantizationConfig;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createCollectionAsync(

CreateCollection.newBuilder()

.setCollectionName(""{collection_name}"")

.setVectorsConfig(

VectorsConfig.newBuilder()\n\nAnswer:",How can I use Qdrant as a vector store in Langchain Go?,"You can use Qdrant as a vector store in Langchain Go by installing the `langchain-go` project dependency and customizing the values for your configuration, such as the Qdrant REST URL and collection name.","['---\n\ntitle: Langchain Go\n\nweight: 120\n\n---\n\n\n\n# Langchain Go\n\n\n\n[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.\n\n\n\nYou can use Qdrant as a vector store in Langchain Go.\n\n\n\n## Setup\n\n\n\nInstall the `langchain-go` project dependency\n\n\n\n```bash\n\ngo get -u github.com/tmc/langchaingo\n\n```\n\n\n\n## Usage\n\n\n\nBefore you use the following code sample, customize the following values for your configuration:', '""github.com/tmc/langchaingo/vectorstores""\n\n        ""github.com/tmc/langchaingo/vectorstores/qdrant""\n\n)\n\n\n\n llm, err := openai.New()\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n\n\n e, err := embeddings.NewEmbedder(llm)\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n\n\n url, err := url.Parse(""YOUR_QDRANT_REST_URL"")\n\n if err != nil {\n\n  log.Fatal(err)\n\n }\n\n\n\n store, err := qdrant.New(\n\n  qdrant.WithURL(*url),\n\n  qdrant.WithCollectionName(""YOUR_COLLECTION_NAME""),\n\n  qdrant.WithEmbedder(e),\n\n )\n\n if err != nil {', 'index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n\n\n\n```\n\n\n\nThe library [comes with a notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/vector_stores/QdrantIndexDemo.ipynb) \n\nthat shows an end-to-end example of how to use Qdrant within LlamaIndex.', 'Qdrant is the only vector database with full coverage of async API in Langchain. Their documentation [describes how to use \n\nit](https://python.langchain.com/docs/modules/data_connection/vectorstores/#asynchronous-operations).', 'import io.qdrant.client.grpc.Collections.QuantizationConfig;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()']","---

title: Langchain Go

weight: 120

---



# Langchain Go



[Langchain Go](https://tmc.github.io/langchaingo/docs/) is a framework for developing data-aware applications powered by language models in Go.



You can use Qdrant as a vector store in Langchain Go.



## Setup



Install the `langchain-go` project dependency



```bash

go get -u github.com/tmc/langchaingo

```



## Usage



Before you use the following code sample, customize the following values for your configuration:
""github.com/tmc/langchaingo/vectorstores""

        ""github.com/tmc/langchaingo/vectorstores/qdrant""

)



 llm, err := openai.New()

 if err != nil {

  log.Fatal(err)

 }



 e, err := embeddings.NewEmbedder(llm)

 if err != nil {

  log.Fatal(err)

 }



 url, err := url.Parse(""YOUR_QDRANT_REST_URL"")

 if err != nil {

  log.Fatal(err)

 }



 store, err := qdrant.New(

  qdrant.WithURL(*url),

  qdrant.WithCollectionName(""YOUR_COLLECTION_NAME""),

  qdrant.WithEmbedder(e),

 )

 if err != nil {
index = VectorStoreIndex.from_vector_store(vector_store=vector_store)



```



The library [comes with a notebook](https://github.com/jerryjliu/llama_index/blob/main/docs/examples/vector_stores/QdrantIndexDemo.ipynb) 

that shows an end-to-end example of how to use Qdrant within LlamaIndex.
Qdrant is the only vector database with full coverage of async API in Langchain. Their documentation [describes how to use 

it](https://python.langchain.com/docs/modules/data_connection/vectorstores/#asynchronous-operations).
import io.qdrant.client.grpc.Collections.QuantizationConfig;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()",0.16071428571428573,0.9,0.27272727272727276,0.10778443113772455,0.6206896551724138,0.18367346938775514,0.14285714285714285,0.8,0.24242424242424243,0.1488095238095238,0.8333333333333334,0.25252525252525254,0.9244977831840515,0.5139115452766418,0.6573429107666016,0.5768449902534485,0.8978734117311736,0.9925860040821135,132,4.0,0.5641481553546551
True,,3135.389,e282cffc,How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?,Dust leveraged the control of the MMAP payload threshold and Scalar Quantization in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively.,"Dust leveraged the compression features in Qdrant, specifically the control of the MMAP payload threshold and Scalar Quantization. Scalar Quantization is a data compression technique that converts floating point values, reducing the footprint of vectors in memory. By implementing Scalar Quantization, Dust was able to effectively manage the balance between storing vectors on disk and keeping quantized vectors in RAM. This allowed them to reduce their cost significantly by not having to run lots of nodes in parallel and push the same instances further with the help of quantization. The combination of cutting-edge efficiency features in Qdrant, including compression options, multitenancy, and the ability to offload data to disk, helped Dust reduce memory consumption and optimize their deployment options.","compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP

payload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage

the balance between storing vectors on disk and keeping quantized vectors in RAM,

more effectively. “This allowed us to scale smoothly from there,” Polu says.
```



While Qdrant offers various options to store some parts of the data on disk, starting 

from version 1.1.0, you can also optimize your memory by compressing the embeddings. 

We've implemented the mechanism of **Scalar Quantization**! It turns out to have not 

only a positive impact on memory but also on the performance. 



## Scalar Quantization



Scalar quantization is a data compression technique that converts floating point values
![“We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as

we don’t have to run lots of nodes in parallel. While being memory-bound, we were

able to push the same instances further with the help of quantization. While you

get pressure on MMAP in this case you maintain very good performance even if the

RAM is fully used. With this we were able to reduce our cost by 2x.” - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)
Beyond performance, we provide our users with the most flexibility in cost savings and deployment options. A combination of cutting-edge efficiency features, like [built-in compression options](https://qdrant.tech/documentation/guides/quantization/), [multitenancy](https://qdrant.tech/documentation/guides/multiple-partitions/) and the ability to [offload data to disk](https://qdrant.tech/documentation/concepts/storage/), dramatically reduce memory consumption
So for instance, one of the reasons I mentioned binary quantization and product quantization is with something like binary quantization you can get 98% recall, but with 30 to 40 x memory savings because it discards all the extraneous bits and just keeps the zero or one bit of the embedding itself. And Qdrant has already measured it for you. So we know that it works for OpenAI and Cohere embeddings for sure. So you might want to use that to just massively scale while keeping your budgets as an engineer","['compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP\n\npayload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage\n\nthe balance between storing vectors on disk and keeping quantized vectors in RAM,\n\nmore effectively. “This allowed us to scale smoothly from there,” Polu says.', ""```\n\n\n\nWhile Qdrant offers various options to store some parts of the data on disk, starting \n\nfrom version 1.1.0, you can also optimize your memory by compressing the embeddings. \n\nWe've implemented the mechanism of **Scalar Quantization**! It turns out to have not \n\nonly a positive impact on memory but also on the performance. \n\n\n\n## Scalar Quantization\n\n\n\nScalar quantization is a data compression technique that converts floating point values"", '![“We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as\n\nwe don’t have to run lots of nodes in parallel. While being memory-bound, we were\n\nable to push the same instances further with the help of quantization. While you\n\nget pressure on MMAP in this case you maintain very good performance even if the\n\nRAM is fully used. With this we were able to reduce our cost by 2x.” - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)', 'Beyond performance, we provide our users with the most flexibility in cost savings and deployment options. A combination of cutting-edge efficiency features, like [built-in compression options](https://qdrant.tech/documentation/guides/quantization/), [multitenancy](https://qdrant.tech/documentation/guides/multiple-partitions/) and the ability to [offload data to disk](https://qdrant.tech/documentation/concepts/storage/), dramatically reduce memory consumption', 'So for instance, one of the reasons I mentioned binary quantization and product quantization is with something like binary quantization you can get 98% recall, but with 30 to 40 x memory savings because it discards all the extraneous bits and just keeps the zero or one bit of the embedding itself. And Qdrant has already measured it for you. So we know that it works for OpenAI and Cohere embeddings for sure. So you might want to use that to just massively scale while keeping your budgets as an engineer']","Question: How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?\n\nContext: compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP

payload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage

the balance between storing vectors on disk and keeping quantized vectors in RAM,

more effectively. “This allowed us to scale smoothly from there,” Polu says.
```



While Qdrant offers various options to store some parts of the data on disk, starting 

from version 1.1.0, you can also optimize your memory by compressing the embeddings. 

We've implemented the mechanism of **Scalar Quantization**! It turns out to have not 

only a positive impact on memory but also on the performance. 



## Scalar Quantization



Scalar quantization is a data compression technique that converts floating point values
![“We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as

we don’t have to run lots of nodes in parallel. While being memory-bound, we were

able to push the same instances further with the help of quantization. While you

get pressure on MMAP in this case you maintain very good performance even if the

RAM is fully used. With this we were able to reduce our cost by 2x.” - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)
Beyond performance, we provide our users with the most flexibility in cost savings and deployment options. A combination of cutting-edge efficiency features, like [built-in compression options](https://qdrant.tech/documentation/guides/quantization/), [multitenancy](https://qdrant.tech/documentation/guides/multiple-partitions/) and the ability to [offload data to disk](https://qdrant.tech/documentation/concepts/storage/), dramatically reduce memory consumption
So for instance, one of the reasons I mentioned binary quantization and product quantization is with something like binary quantization you can get 98% recall, but with 30 to 40 x memory savings because it discards all the extraneous bits and just keeps the zero or one bit of the embedding itself. And Qdrant has already measured it for you. So we know that it works for OpenAI and Cohere embeddings for sure. So you might want to use that to just massively scale while keeping your budgets as an engineer\n\nAnswer:",How did Dust leverage compression features in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively?,Dust leveraged the control of the MMAP payload threshold and Scalar Quantization in Qdrant to manage the balance between storing vectors on disk and keeping quantized vectors in RAM effectively.,"['compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP\n\npayload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage\n\nthe balance between storing vectors on disk and keeping quantized vectors in RAM,\n\nmore effectively. “This allowed us to scale smoothly from there,” Polu says.', ""```\n\n\n\nWhile Qdrant offers various options to store some parts of the data on disk, starting \n\nfrom version 1.1.0, you can also optimize your memory by compressing the embeddings. \n\nWe've implemented the mechanism of **Scalar Quantization**! It turns out to have not \n\nonly a positive impact on memory but also on the performance. \n\n\n\n## Scalar Quantization\n\n\n\nScalar quantization is a data compression technique that converts floating point values"", '![“We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as\n\nwe don’t have to run lots of nodes in parallel. While being memory-bound, we were\n\nable to push the same instances further with the help of quantization. While you\n\nget pressure on MMAP in this case you maintain very good performance even if the\n\nRAM is fully used. With this we were able to reduce our cost by 2x.” - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)', 'Beyond performance, we provide our users with the most flexibility in cost savings and deployment options. A combination of cutting-edge efficiency features, like [built-in compression options](https://qdrant.tech/documentation/guides/quantization/), [multitenancy](https://qdrant.tech/documentation/guides/multiple-partitions/) and the ability to [offload data to disk](https://qdrant.tech/documentation/concepts/storage/), dramatically reduce memory consumption', 'So for instance, one of the reasons I mentioned binary quantization and product quantization is with something like binary quantization you can get 98% recall, but with 30 to 40 x memory savings because it discards all the extraneous bits and just keeps the zero or one bit of the embedding itself. And Qdrant has already measured it for you. So we know that it works for OpenAI and Cohere embeddings for sure. So you might want to use that to just massively scale while keeping your budgets as an engineer']","compression features](https://qdrant.tech/documentation/guides/quantization/). In particular, Dust leveraged the control of the [MMAP

payload threshold](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage) as well as [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), which enabled Dust to manage

the balance between storing vectors on disk and keeping quantized vectors in RAM,

more effectively. “This allowed us to scale smoothly from there,” Polu says.
```



While Qdrant offers various options to store some parts of the data on disk, starting 

from version 1.1.0, you can also optimize your memory by compressing the embeddings. 

We've implemented the mechanism of **Scalar Quantization**! It turns out to have not 

only a positive impact on memory but also on the performance. 



## Scalar Quantization



Scalar quantization is a data compression technique that converts floating point values
![“We were able to reduce the footprint of vectors in memory, which led to a significant cost reduction as

we don’t have to run lots of nodes in parallel. While being memory-bound, we were

able to push the same instances further with the help of quantization. While you

get pressure on MMAP in this case you maintain very good performance even if the

RAM is fully used. With this we were able to reduce our cost by 2x.” - Stanislas Polu, Co-Founder of Dust](/case-studies/dust/Dust-Quote.jpg)
Beyond performance, we provide our users with the most flexibility in cost savings and deployment options. A combination of cutting-edge efficiency features, like [built-in compression options](https://qdrant.tech/documentation/guides/quantization/), [multitenancy](https://qdrant.tech/documentation/guides/multiple-partitions/) and the ability to [offload data to disk](https://qdrant.tech/documentation/concepts/storage/), dramatically reduce memory consumption
So for instance, one of the reasons I mentioned binary quantization and product quantization is with something like binary quantization you can get 98% recall, but with 30 to 40 x memory savings because it discards all the extraneous bits and just keeps the zero or one bit of the embedding itself. And Qdrant has already measured it for you. So we know that it works for OpenAI and Cohere embeddings for sure. So you might want to use that to just massively scale while keeping your budgets as an engineer",0.24545454545454545,1.0,0.39416058394160586,0.1926605504587156,0.8076923076923077,0.3111111111111111,0.22727272727272727,0.9259259259259259,0.36496350364963503,0.23636363636363636,0.9629629629629629,0.3795620437956204,0.9319639205932617,0.5993519425392151,0.7738453149795532,0.6755121350288391,0.8553165756165981,0.6338128745555878,119,3.966666666666667,0.5677444242938323
True,,3919.8689999999997,e96db044,Why do we still need keyword search?,"Keyword search is still useful in cases of out-of-domain search, where words are just words regardless of their meaning.","Keyword search continues to be important for several reasons:

1. **Historical Significance**: Keyword-based search has been the foundation of search engines for a long time. It was the obvious choice in the past, and even though new techniques have emerged, the familiarity and simplicity of keyword search make it a reliable option.

2. **User-Friendliness**: Keyword search has evolved over time to become more user-friendly. Techniques like converting words into their root forms, removing stopwords, and other optimizations have improved the search experience for users.

3. **Complementing Other Techniques**: While newer search technologies like semantic search are becoming more prevalent, keyword search can still complement these techniques. For example, using a mix of semantic and keyword-based search can provide a comprehensive search experience.

4. **Specificity**: Keyword search allows users to directly input the specific terms they are looking for, which can be crucial in certain contexts where precision is essential.

5. **Versatility**: Keyword search can be applied in various scenarios and industries, making it a versatile tool for information retrieval.

In conclusion, keyword search, despite its limitations, remains relevant and necessary in the digital landscape due to its historical significance, user-friendliness, complementarity with other search techniques, specificity, and versatility.","2. Vector search with keyword-based search. This one is covered in this article.

3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.



## Why do we still need keyword search?



A keyword-based search was the obvious choice for search engines in the past. It struggled with some

common issues, but since we didn't have any alternatives, we had to overcome them with additional
. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.
on those candidates only. This is an approach that makes it possible to use the power of semantic and keyword based 

   search together.



![The design of all the three experiments](/articles_data/hybrid-search/experiments-design.png)



### Quality metrics



There are various ways of how to measure the performance of search engines, and *[Recommender Systems: Machine Learning
. So like using some kind of hybrid search where someone does the embedding lookup first, and then does the keyword lookup, or vice versa, or does lookups parallel and then merges results together? Those kind of things where the search space is narrowed down always works for me.
needed to find what we’re looking for. Semantic search enables us to find things without knowing the exact 

terminology. For example, if an engineer wanted to understand “*What library is used for payment processing?*” 

a semantic code search engine would be able to retrieve results containing “*Stripe*” or “*PayPal*”. A traditional 

lexical search engine would not.



One peculiarity of this problem is that the **usefulness of the solution increases with the size of the code","[""2. Vector search with keyword-based search. This one is covered in this article.\n\n3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.\n\n\n\n## Why do we still need keyword search?\n\n\n\nA keyword-based search was the obvious choice for search engines in the past. It struggled with some\n\ncommon issues, but since we didn't have any alternatives, we had to overcome them with additional"", '. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.', 'on those candidates only. This is an approach that makes it possible to use the power of semantic and keyword based \n\n   search together.\n\n\n\n![The design of all the three experiments](/articles_data/hybrid-search/experiments-design.png)\n\n\n\n### Quality metrics\n\n\n\nThere are various ways of how to measure the performance of search engines, and *[Recommender Systems: Machine Learning', '. So like using some kind of hybrid search where someone does the embedding lookup first, and then does the keyword lookup, or vice versa, or does lookups parallel and then merges results together? Those kind of things where the search space is narrowed down always works for me.', 'needed to find what we’re looking for. Semantic search enables us to find things without knowing the exact \n\nterminology. For example, if an engineer wanted to understand “*What library is used for payment processing?*” \n\na semantic code search engine would be able to retrieve results containing “*Stripe*” or “*PayPal*”. A traditional \n\nlexical search engine would not.\n\n\n\nOne peculiarity of this problem is that the **usefulness of the solution increases with the size of the code']","Question: Why do we still need keyword search?\n\nContext: 2. Vector search with keyword-based search. This one is covered in this article.

3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.



## Why do we still need keyword search?



A keyword-based search was the obvious choice for search engines in the past. It struggled with some

common issues, but since we didn't have any alternatives, we had to overcome them with additional
. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.
on those candidates only. This is an approach that makes it possible to use the power of semantic and keyword based 

search together.



![The design of all the three experiments](/articles_data/hybrid-search/experiments-design.png)



### Quality metrics



There are various ways of how to measure the performance of search engines, and *[Recommender Systems: Machine Learning
. So like using some kind of hybrid search where someone does the embedding lookup first, and then does the keyword lookup, or vice versa, or does lookups parallel and then merges results together? Those kind of things where the search space is narrowed down always works for me.
needed to find what we’re looking for. Semantic search enables us to find things without knowing the exact 

terminology. For example, if an engineer wanted to understand “*What library is used for payment processing?*” 

a semantic code search engine would be able to retrieve results containing “*Stripe*” or “*PayPal*”. A traditional 

lexical search engine would not.



One peculiarity of this problem is that the **usefulness of the solution increases with the size of the code\n\nAnswer:",Why do we still need keyword search?,"Keyword search is still useful in cases of out-of-domain search, where words are just words regardless of their meaning.","[""2. Vector search with keyword-based search. This one is covered in this article.\n\n3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.\n\n\n\n## Why do we still need keyword search?\n\n\n\nA keyword-based search was the obvious choice for search engines in the past. It struggled with some\n\ncommon issues, but since we didn't have any alternatives, we had to overcome them with additional"", '. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.', 'on those candidates only. This is an approach that makes it possible to use the power of semantic and keyword based \n\n   search together.\n\n\n\n![The design of all the three experiments](/articles_data/hybrid-search/experiments-design.png)\n\n\n\n### Quality metrics\n\n\n\nThere are various ways of how to measure the performance of search engines, and *[Recommender Systems: Machine Learning', '. So like using some kind of hybrid search where someone does the embedding lookup first, and then does the keyword lookup, or vice versa, or does lookups parallel and then merges results together? Those kind of things where the search space is narrowed down always works for me.', 'needed to find what we’re looking for. Semantic search enables us to find things without knowing the exact \n\nterminology. For example, if an engineer wanted to understand “*What library is used for payment processing?*” \n\na semantic code search engine would be able to retrieve results containing “*Stripe*” or “*PayPal*”. A traditional \n\nlexical search engine would not.\n\n\n\nOne peculiarity of this problem is that the **usefulness of the solution increases with the size of the code']","2. Vector search with keyword-based search. This one is covered in this article.

3. A mix of dense and sparse vectors. That strategy will be covered in the upcoming article.



## Why do we still need keyword search?



A keyword-based search was the obvious choice for search engines in the past. It struggled with some

common issues, but since we didn't have any alternatives, we had to overcome them with additional
. We also started converting words into their root forms to cover more cases, removing stopwords, etc. Effectively we were becoming more and more user-friendly. Still, the idea behind the whole process is derived from the most straightforward keyword-based search known since the Middle Ages, with some tweaks.
on those candidates only. This is an approach that makes it possible to use the power of semantic and keyword based 

   search together.



![The design of all the three experiments](/articles_data/hybrid-search/experiments-design.png)



### Quality metrics



There are various ways of how to measure the performance of search engines, and *[Recommender Systems: Machine Learning
. So like using some kind of hybrid search where someone does the embedding lookup first, and then does the keyword lookup, or vice versa, or does lookups parallel and then merges results together? Those kind of things where the search space is narrowed down always works for me.
needed to find what we’re looking for. Semantic search enables us to find things without knowing the exact 

terminology. For example, if an engineer wanted to understand “*What library is used for payment processing?*” 

a semantic code search engine would be able to retrieve results containing “*Stripe*” or “*PayPal*”. A traditional 

lexical search engine would not.



One peculiarity of this problem is that the **usefulness of the solution increases with the size of the code",0.07329842931937172,0.6666666666666666,0.1320754716981132,0.005263157894736842,0.05,0.009523809523809523,0.041884816753926704,0.38095238095238093,0.07547169811320756,0.05235602094240838,0.47619047619047616,0.09433962264150943,0.7217075228691101,0.4633256494998932,0.6288548111915588,0.533546507358551,0.6691924152359466,0.002279341220855713,198,10.421052631578947,0.3169267760960123
True,,2677.643,7fded50c,What principles did Qdrant follow while designing benchmarks for vector search engines?,"Qdrant followed the principles of doing comparative benchmarks focusing on relative numbers rather than absolute numbers, and using affordable hardware for easy result reproduction.","The principles that Qdrant followed while designing benchmarks for vector search engines include:

1. **Scalability and Performance**: Qdrant prioritizes scalability and performance over transactional consistency. It is designed to function more like a search engine rather than a traditional database, focusing on efficient retrieval and searching of vector data.

2. **Understanding Application Needs**: Qdrant emphasizes the importance of understanding the application requirements and scalability when selecting a database for vector search tasks. Different applications may have varying needs, and the database choice should align with these requirements.

3. **Benchmarking Search Quality**: Qdrant uses reference datasets to benchmark the search quality of vector search engines and databases. By comparing performance metrics such as latency, requests per second (RPS), and overall search efficiency, Qdrant aims to assess and optimize the search quality of its engine.","## Project Overview



Qdrant is a powerful vector search engine widely used for similarity search and clustering. However, it lacked a user-friendly web-based UI for data visualization and exploration. My project aimed to bridge this gap by developing a web-based user interface that allows users to easily interact with and explore their vector data.



## Milestones and Achievements
---

title: Vector Database Benchmarks

description: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.

keywords:

  - vector databases comparative benchmark

  - ANN Benchmark

  - Qdrant vs Milvus

  - Qdrant vs Weaviate

  - Qdrant vs Redis

  - Qdrant vs ElasticSearch

  - benchmark

  - performance

  - latency

  - RPS

  - comparison

  - vector search

  - embedding

preview_image: /benchmarks/benchmark-1.png
- Qdrant

  - Vector Search Engine

  - Vector Database

---

> *""For systems like Qdrant, scalability and performance in my opinion, is much more important than transactional consistency, so it should be treated as a search engine rather than database.""*\

-- Andrey Vasnetsov

> 



Discussing core differences between search engines and databases, Andrey underlined the importance of application needs and scalability in database selection for vector search tasks.
All of those engines might be easily used in combination with the vector search offered by Qdrant. But the 

exact way how to combine the results of both algorithms to achieve the best search precision might be still 

unclear. So we need to understand how to do it effectively. We will be using reference datasets to benchmark 

the search quality.



## Why not linear combination?



It's often proposed to use full-text and vector search scores to form a linear combination formula to rerank
weight: -80

author: Kacper Łukawski

author_link: https://medium.com/@lukawskikacper

date: 2023-10-25T09:46:00.000Z

---



The most popular use case for vector search engines, such as Qdrant, is Semantic search with a single query vector. Given the 

query, we can vectorize (embed) it and find the closest points in the index. But [Vector Similarity beyond Search](/articles/vector-similarity-beyond-search/)","['## Project Overview\n\n\n\nQdrant is a powerful vector search engine widely used for similarity search and clustering. However, it lacked a user-friendly web-based UI for data visualization and exploration. My project aimed to bridge this gap by developing a web-based user interface that allows users to easily interact with and explore their vector data.\n\n\n\n## Milestones and Achievements', '---\n\ntitle: Vector Database Benchmarks\n\ndescription: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.\n\nkeywords:\n\n  - vector databases comparative benchmark\n\n  - ANN Benchmark\n\n  - Qdrant vs Milvus\n\n  - Qdrant vs Weaviate\n\n  - Qdrant vs Redis\n\n  - Qdrant vs ElasticSearch\n\n  - benchmark\n\n  - performance\n\n  - latency\n\n  - RPS\n\n  - comparison\n\n  - vector search\n\n  - embedding\n\npreview_image: /benchmarks/benchmark-1.png', '- Qdrant\n\n  - Vector Search Engine\n\n  - Vector Database\n\n---\n\n> *""For systems like Qdrant, scalability and performance in my opinion, is much more important than transactional consistency, so it should be treated as a search engine rather than database.""*\\\n\n-- Andrey Vasnetsov\n\n> \n\n\n\nDiscussing core differences between search engines and databases, Andrey underlined the importance of application needs and scalability in database selection for vector search tasks.', ""All of those engines might be easily used in combination with the vector search offered by Qdrant. But the \n\nexact way how to combine the results of both algorithms to achieve the best search precision might be still \n\nunclear. So we need to understand how to do it effectively. We will be using reference datasets to benchmark \n\nthe search quality.\n\n\n\n## Why not linear combination?\n\n\n\nIt's often proposed to use full-text and vector search scores to form a linear combination formula to rerank"", 'weight: -80\n\nauthor: Kacper Łukawski\n\nauthor_link: https://medium.com/@lukawskikacper\n\ndate: 2023-10-25T09:46:00.000Z\n\n---\n\n\n\nThe most popular use case for vector search engines, such as Qdrant, is Semantic search with a single query vector. Given the \n\nquery, we can vectorize (embed) it and find the closest points in the index. But [Vector Similarity beyond Search](/articles/vector-similarity-beyond-search/)']","Question: What principles did Qdrant follow while designing benchmarks for vector search engines?\n\nContext: ## Project Overview



Qdrant is a powerful vector search engine widely used for similarity search and clustering. However, it lacked a user-friendly web-based UI for data visualization and exploration. My project aimed to bridge this gap by developing a web-based user interface that allows users to easily interact with and explore their vector data.



## Milestones and Achievements
---

title: Vector Database Benchmarks

description: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.

keywords:

- vector databases comparative benchmark

- ANN Benchmark

- Qdrant vs Milvus

- Qdrant vs Weaviate

- Qdrant vs Redis

- Qdrant vs ElasticSearch

- benchmark

- performance

- latency

- RPS

- comparison

- vector search

- embedding

preview_image: /benchmarks/benchmark-1.png
- Qdrant

- Vector Search Engine

- Vector Database

---

> *""For systems like Qdrant, scalability and performance in my opinion, is much more important than transactional consistency, so it should be treated as a search engine rather than database.""*\

-- Andrey Vasnetsov

> 



Discussing core differences between search engines and databases, Andrey underlined the importance of application needs and scalability in database selection for vector search tasks.
All of those engines might be easily used in combination with the vector search offered by Qdrant. But the 

exact way how to combine the results of both algorithms to achieve the best search precision might be still 

unclear. So we need to understand how to do it effectively. We will be using reference datasets to benchmark 

the search quality.



## Why not linear combination?



It's often proposed to use full-text and vector search scores to form a linear combination formula to rerank
weight: -80

author: Kacper Łukawski

author_link: https://medium.com/@lukawskikacper

date: 2023-10-25T09:46:00.000Z

---



The most popular use case for vector search engines, such as Qdrant, is Semantic search with a single query vector. Given the 

query, we can vectorize (embed) it and find the closest points in the index. But [Vector Similarity beyond Search](/articles/vector-similarity-beyond-search/)\n\nAnswer:",What principles did Qdrant follow while designing benchmarks for vector search engines?,"Qdrant followed the principles of doing comparative benchmarks focusing on relative numbers rather than absolute numbers, and using affordable hardware for easy result reproduction.","['## Project Overview\n\n\n\nQdrant is a powerful vector search engine widely used for similarity search and clustering. However, it lacked a user-friendly web-based UI for data visualization and exploration. My project aimed to bridge this gap by developing a web-based user interface that allows users to easily interact with and explore their vector data.\n\n\n\n## Milestones and Achievements', '---\n\ntitle: Vector Database Benchmarks\n\ndescription: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.\n\nkeywords:\n\n  - vector databases comparative benchmark\n\n  - ANN Benchmark\n\n  - Qdrant vs Milvus\n\n  - Qdrant vs Weaviate\n\n  - Qdrant vs Redis\n\n  - Qdrant vs ElasticSearch\n\n  - benchmark\n\n  - performance\n\n  - latency\n\n  - RPS\n\n  - comparison\n\n  - vector search\n\n  - embedding\n\npreview_image: /benchmarks/benchmark-1.png', '- Qdrant\n\n  - Vector Search Engine\n\n  - Vector Database\n\n---\n\n> *""For systems like Qdrant, scalability and performance in my opinion, is much more important than transactional consistency, so it should be treated as a search engine rather than database.""*\\\n\n-- Andrey Vasnetsov\n\n> \n\n\n\nDiscussing core differences between search engines and databases, Andrey underlined the importance of application needs and scalability in database selection for vector search tasks.', ""All of those engines might be easily used in combination with the vector search offered by Qdrant. But the \n\nexact way how to combine the results of both algorithms to achieve the best search precision might be still \n\nunclear. So we need to understand how to do it effectively. We will be using reference datasets to benchmark \n\nthe search quality.\n\n\n\n## Why not linear combination?\n\n\n\nIt's often proposed to use full-text and vector search scores to form a linear combination formula to rerank"", 'weight: -80\n\nauthor: Kacper Łukawski\n\nauthor_link: https://medium.com/@lukawskikacper\n\ndate: 2023-10-25T09:46:00.000Z\n\n---\n\n\n\nThe most popular use case for vector search engines, such as Qdrant, is Semantic search with a single query vector. Given the \n\nquery, we can vectorize (embed) it and find the closest points in the index. But [Vector Similarity beyond Search](/articles/vector-similarity-beyond-search/)']","## Project Overview



Qdrant is a powerful vector search engine widely used for similarity search and clustering. However, it lacked a user-friendly web-based UI for data visualization and exploration. My project aimed to bridge this gap by developing a web-based user interface that allows users to easily interact with and explore their vector data.



## Milestones and Achievements
---

title: Vector Database Benchmarks

description: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.

keywords:

  - vector databases comparative benchmark

  - ANN Benchmark

  - Qdrant vs Milvus

  - Qdrant vs Weaviate

  - Qdrant vs Redis

  - Qdrant vs ElasticSearch

  - benchmark

  - performance

  - latency

  - RPS

  - comparison

  - vector search

  - embedding

preview_image: /benchmarks/benchmark-1.png
- Qdrant

  - Vector Search Engine

  - Vector Database

---

> *""For systems like Qdrant, scalability and performance in my opinion, is much more important than transactional consistency, so it should be treated as a search engine rather than database.""*\

-- Andrey Vasnetsov

> 



Discussing core differences between search engines and databases, Andrey underlined the importance of application needs and scalability in database selection for vector search tasks.
All of those engines might be easily used in combination with the vector search offered by Qdrant. But the 

exact way how to combine the results of both algorithms to achieve the best search precision might be still 

unclear. So we need to understand how to do it effectively. We will be using reference datasets to benchmark 

the search quality.



## Why not linear combination?



It's often proposed to use full-text and vector search scores to form a linear combination formula to rerank
weight: -80

author: Kacper Łukawski

author_link: https://medium.com/@lukawskikacper

date: 2023-10-25T09:46:00.000Z

---



The most popular use case for vector search engines, such as Qdrant, is Semantic search with a single query vector. Given the 

query, we can vectorize (embed) it and find the closest points in the index. But [Vector Similarity beyond Search](/articles/vector-similarity-beyond-search/)",0.10483870967741936,0.5652173913043478,0.17687074829931973,0.024390243902439025,0.13636363636363635,0.04137931034482759,0.056451612903225805,0.30434782608695654,0.09523809523809523,0.06451612903225806,0.34782608695652173,0.10884353741496598,0.6325840950012207,0.4871159791946411,0.6028186082839966,0.538826048374176,0.7366811069789239,0.026993095874786377,133,5.541666666666667,0.39478727399523106
True,,1129.058,786c2b9a,What models does Qdrant support for embedding generation?,"Qdrant supports a reasonable range of models, including a few multilingual ones.","Qdrant supports the following models for embedding generation:

1. Cohere
2. Gemini
3. Jina Embeddings
4. OpenAI
5. Aleph Alpha

These models offer seamless integration with Qdrant for generating embeddings tailored to different use cases and requirements.","Selecting the right embedding model for your use case is crucial to your application performance. Qdrant makes it easier by offering seamless integration with the best selection of embedding APIs, including [Cohere](https://qdrant.tech/documentation/embeddings/cohere/), [Gemini](https://qdrant.tech/documentation/embeddings/gemini/), [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/), [OpenAI](https://qdrant.tech/documentation/embeddings/openai/), [Aleph Alpha](https://qdrant
Through this detailed setup, our experiment sought to shed light on the nuanced interplay between Binary Quantization and the high-quality embeddings produced by OpenAI's models. By meticulously adjusting and observing the outcomes under different conditions, we aimed to uncover actionable insights that could empower users to harness the full potential of Qdrant in combination with OpenAI's embeddings, regardless of their specific application needs.
query_request = SemanticEmbeddingRequest(**query_params)

    query_response = await client.semantic_embed(

        request=query_request, model=model

    )

    

    qdrant_client.upsert(

        collection_name=""MyCollection"",

        points=Batch(

            ids=[1],

            vectors=[query_response.embedding],

        )

    )

```



If we wanted to create text embeddings with the same model, we wouldn't use `ImagePrompt.from_file`, but simply provide the input
```python

qdrant_client.search(

    collection_name=""GeminiCollection"",

    query=genai.embed_content(

        model=""models/embedding-001"",

        content=""What is the best vector database to use with Gemini?"",

        task_type=""retrieval_query"",

    )[""embedding""],

)

```



## Using Gemini Embedding Models with Binary Quantization
## Common Use Cases:



Qdrant is ideal for deploying applications based on the matching of embeddings produced by neural network encoders. Check out the [Examples](examples/) section to learn more about common use cases. Also, you can visit the [Tutorials](tutorials/) page to learn how to work with Qdrant in different ways. 



| Use Case              | Description                                  | Stack  |   

|-----------------------|----------------------------------------------|--------|","['Selecting the right embedding model for your use case is crucial to your application performance. Qdrant makes it easier by offering seamless integration with the best selection of embedding APIs, including [Cohere](https://qdrant.tech/documentation/embeddings/cohere/), [Gemini](https://qdrant.tech/documentation/embeddings/gemini/), [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/), [OpenAI](https://qdrant.tech/documentation/embeddings/openai/), [Aleph Alpha](https://qdrant', ""Through this detailed setup, our experiment sought to shed light on the nuanced interplay between Binary Quantization and the high-quality embeddings produced by OpenAI's models. By meticulously adjusting and observing the outcomes under different conditions, we aimed to uncover actionable insights that could empower users to harness the full potential of Qdrant in combination with OpenAI's embeddings, regardless of their specific application needs."", 'query_request = SemanticEmbeddingRequest(**query_params)\n\n    query_response = await client.semantic_embed(\n\n        request=query_request, model=model\n\n    )\n\n    \n\n    qdrant_client.upsert(\n\n        collection_name=""MyCollection"",\n\n        points=Batch(\n\n            ids=[1],\n\n            vectors=[query_response.embedding],\n\n        )\n\n    )\n\n```\n\n\n\nIf we wanted to create text embeddings with the same model, we wouldn\'t use `ImagePrompt.from_file`, but simply provide the input', '```python\n\nqdrant_client.search(\n\n    collection_name=""GeminiCollection"",\n\n    query=genai.embed_content(\n\n        model=""models/embedding-001"",\n\n        content=""What is the best vector database to use with Gemini?"",\n\n        task_type=""retrieval_query"",\n\n    )[""embedding""],\n\n)\n\n```\n\n\n\n## Using Gemini Embedding Models with Binary Quantization', '## Common Use Cases:\n\n\n\nQdrant is ideal for deploying applications based on the matching of embeddings produced by neural network encoders. Check out the [Examples](examples/) section to learn more about common use cases. Also, you can visit the [Tutorials](tutorials/) page to learn how to work with Qdrant in different ways. \n\n\n\n| Use Case              | Description                                  | Stack  |   \n\n|-----------------------|----------------------------------------------|--------|']","Question: What models does Qdrant support for embedding generation?\n\nContext: Selecting the right embedding model for your use case is crucial to your application performance. Qdrant makes it easier by offering seamless integration with the best selection of embedding APIs, including [Cohere](https://qdrant.tech/documentation/embeddings/cohere/), [Gemini](https://qdrant.tech/documentation/embeddings/gemini/), [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/), [OpenAI](https://qdrant.tech/documentation/embeddings/openai/), [Aleph Alpha](https://qdrant
Through this detailed setup, our experiment sought to shed light on the nuanced interplay between Binary Quantization and the high-quality embeddings produced by OpenAI's models. By meticulously adjusting and observing the outcomes under different conditions, we aimed to uncover actionable insights that could empower users to harness the full potential of Qdrant in combination with OpenAI's embeddings, regardless of their specific application needs.
query_request = SemanticEmbeddingRequest(**query_params)

query_response = await client.semantic_embed(

request=query_request, model=model

)



qdrant_client.upsert(

collection_name=""MyCollection"",

points=Batch(

ids=[1],

vectors=[query_response.embedding],

)

)

```



If we wanted to create text embeddings with the same model, we wouldn't use `ImagePrompt.from_file`, but simply provide the input
```python

qdrant_client.search(

collection_name=""GeminiCollection"",

query=genai.embed_content(

model=""models/embedding-001"",

content=""What is the best vector database to use with Gemini?"",

task_type=""retrieval_query"",

)[""embedding""],

)

```



## Using Gemini Embedding Models with Binary Quantization
## Common Use Cases:



Qdrant is ideal for deploying applications based on the matching of embeddings produced by neural network encoders. Check out the [Examples](examples/) section to learn more about common use cases. Also, you can visit the [Tutorials](tutorials/) page to learn how to work with Qdrant in different ways. 



| Use Case              | Description                                  | Stack  |   

|-----------------------|----------------------------------------------|--------|\n\nAnswer:",What models does Qdrant support for embedding generation?,"Qdrant supports a reasonable range of models, including a few multilingual ones.","['Selecting the right embedding model for your use case is crucial to your application performance. Qdrant makes it easier by offering seamless integration with the best selection of embedding APIs, including [Cohere](https://qdrant.tech/documentation/embeddings/cohere/), [Gemini](https://qdrant.tech/documentation/embeddings/gemini/), [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/), [OpenAI](https://qdrant.tech/documentation/embeddings/openai/), [Aleph Alpha](https://qdrant', ""Through this detailed setup, our experiment sought to shed light on the nuanced interplay between Binary Quantization and the high-quality embeddings produced by OpenAI's models. By meticulously adjusting and observing the outcomes under different conditions, we aimed to uncover actionable insights that could empower users to harness the full potential of Qdrant in combination with OpenAI's embeddings, regardless of their specific application needs."", 'query_request = SemanticEmbeddingRequest(**query_params)\n\n    query_response = await client.semantic_embed(\n\n        request=query_request, model=model\n\n    )\n\n    \n\n    qdrant_client.upsert(\n\n        collection_name=""MyCollection"",\n\n        points=Batch(\n\n            ids=[1],\n\n            vectors=[query_response.embedding],\n\n        )\n\n    )\n\n```\n\n\n\nIf we wanted to create text embeddings with the same model, we wouldn\'t use `ImagePrompt.from_file`, but simply provide the input', '```python\n\nqdrant_client.search(\n\n    collection_name=""GeminiCollection"",\n\n    query=genai.embed_content(\n\n        model=""models/embedding-001"",\n\n        content=""What is the best vector database to use with Gemini?"",\n\n        task_type=""retrieval_query"",\n\n    )[""embedding""],\n\n)\n\n```\n\n\n\n## Using Gemini Embedding Models with Binary Quantization', '## Common Use Cases:\n\n\n\nQdrant is ideal for deploying applications based on the matching of embeddings produced by neural network encoders. Check out the [Examples](examples/) section to learn more about common use cases. Also, you can visit the [Tutorials](tutorials/) page to learn how to work with Qdrant in different ways. \n\n\n\n| Use Case              | Description                                  | Stack  |   \n\n|-----------------------|----------------------------------------------|--------|']","Selecting the right embedding model for your use case is crucial to your application performance. Qdrant makes it easier by offering seamless integration with the best selection of embedding APIs, including [Cohere](https://qdrant.tech/documentation/embeddings/cohere/), [Gemini](https://qdrant.tech/documentation/embeddings/gemini/), [Jina Embeddings](https://qdrant.tech/documentation/embeddings/jina-embeddings/), [OpenAI](https://qdrant.tech/documentation/embeddings/openai/), [Aleph Alpha](https://qdrant
Through this detailed setup, our experiment sought to shed light on the nuanced interplay between Binary Quantization and the high-quality embeddings produced by OpenAI's models. By meticulously adjusting and observing the outcomes under different conditions, we aimed to uncover actionable insights that could empower users to harness the full potential of Qdrant in combination with OpenAI's embeddings, regardless of their specific application needs.
query_request = SemanticEmbeddingRequest(**query_params)

    query_response = await client.semantic_embed(

        request=query_request, model=model

    )

    

    qdrant_client.upsert(

        collection_name=""MyCollection"",

        points=Batch(

            ids=[1],

            vectors=[query_response.embedding],

        )

    )

```



If we wanted to create text embeddings with the same model, we wouldn't use `ImagePrompt.from_file`, but simply provide the input
```python

qdrant_client.search(

    collection_name=""GeminiCollection"",

    query=genai.embed_content(

        model=""models/embedding-001"",

        content=""What is the best vector database to use with Gemini?"",

        task_type=""retrieval_query"",

    )[""embedding""],

)

```



## Using Gemini Embedding Models with Binary Quantization
## Common Use Cases:



Qdrant is ideal for deploying applications based on the matching of embeddings produced by neural network encoders. Check out the [Examples](examples/) section to learn more about common use cases. Also, you can visit the [Tutorials](tutorials/) page to learn how to work with Qdrant in different ways. 



| Use Case              | Description                                  | Stack  |   

|-----------------------|----------------------------------------------|--------|",0.08333333333333333,0.3,0.13043478260869565,0.02857142857142857,0.1111111111111111,0.04545454545454545,0.08333333333333333,0.3,0.13043478260869565,0.08333333333333333,0.3,0.13043478260869565,0.6829841732978821,0.5128712058067322,0.5843667984008789,0.5462896823883057,0.968040082603693,0.6333140134811401,37,3.0833333333333335,0.54297261110468
True,,6053.12,a63e3b2f,How can you parallelize the upload of a large dataset using shards in Qdrant?,"By creating multiple shards in Qdrant, you can parallelize the upload of a large dataset. It is recommended to have 2 to 4 shards per machine for efficient processing. When creating a collection in Qdrant, you can specify the number of shards to use for that collection. ","To parallelize the upload of a large dataset using shards in Qdrant, you can follow these steps:

1. Define the number of shards you want to create for the collection by specifying the ""shard_number"" parameter in the PUT request. For example, in the provided HTTP request snippet, ""shard_number"": 2 indicates that the collection will be split into 2 shards.

2. Use the Rust client library for Qdrant, as it is the fastest client library available. You can find the Rust client library here: [Rust client library](https://github.com/qdrant/rust-client).

3. If you are not using Rust, consider parallelizing your upload process by utilizing parallel processing capabilities in your programming language or framework.

4. Consider disabling indexing during the upload process, especially for initial uploads of large datasets. This can help avoid unnecessary indexing of vectors that will be overwritten by subsequent batches.

5. Use parallel processing techniques in your code to take advantage of multi-core CPUs. For example, you can set the ""parallel"" parameter to utilize all available CPU cores for encoding and uploading data simultaneously.

6. Optimize your approach based on your specific requirements. If you want to minimize latency, configure Qdrant to use as many cores as possible for a single request by setting the number of segments in the collection equal to the number of cores in the system. This allows each segment to be processed in parallel for faster results.

7. If you prioritize throughput, configure Qdrant to use the minimal number of segments, typically 2. Large segments benefit from index size and require fewer vector comparisons, but they take longer to build.

By following these steps and utilizing shards in Qdrant, you can effectively parallelize the upload of a large dataset for efficient processing and indexing.","## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""shard_number"": 2

}

```



```python
We recommend using our [Rust client library](https://github.com/qdrant/rust-client) for this purpose, as it is the fastest client library available for Qdrant.



If you are not using Rust, you might want to consider parallelizing your upload process.



## Disable indexing during upload



In case you are doing an initial upload of a large dataset, you might want to disable indexing during upload.

It will enable to avoid unnecessary indexing of vectors, which will be overwritten by the next batch.
collection_name=""startups"",

    documents=documents,

    metadata=metadata,

    parallel=0,  # Use all available CPU cores to encode data

)

```



The `add` method will encode all documents and upload them to Qdrant.

This is one of two fastembed-specific methods, that combines encoding and uploading into a single step.



The `parallel` parameter controls the number of CPU cores used to encode data.
Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
);

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,","['## Parallel upload into multiple shards\n\n\n\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\n\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python', 'We recommend using our [Rust client library](https://github.com/qdrant/rust-client) for this purpose, as it is the fastest client library available for Qdrant.\n\n\n\nIf you are not using Rust, you might want to consider parallelizing your upload process.\n\n\n\n## Disable indexing during upload\n\n\n\nIn case you are doing an initial upload of a large dataset, you might want to disable indexing during upload.\n\nIt will enable to avoid unnecessary indexing of vectors, which will be overwritten by the next batch.', 'collection_name=""startups"",\n\n    documents=documents,\n\n    metadata=metadata,\n\n    parallel=0,  # Use all available CPU cores to encode data\n\n)\n\n```\n\n\n\nThe `add` method will encode all documents and upload them to Qdrant.\n\nThis is one of two fastembed-specific methods, that combines encoding and uploading into a single step.\n\n\n\nThe `parallel` parameter controls the number of CPU cores used to encode data.', 'Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.\n\n\n\nTo prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{', ');\n\n```\n\n\n\nTo prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.\n\nTo do that, you can configure qdrant to use minimal number of segments, which is usually 2.\n\nLarge segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,']","Question: How can you parallelize the upload of a large dataset using shards in Qdrant?\n\nContext: ## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""shard_number"": 2

}

```



```python
We recommend using our [Rust client library](https://github.com/qdrant/rust-client) for this purpose, as it is the fastest client library available for Qdrant.



If you are not using Rust, you might want to consider parallelizing your upload process.



## Disable indexing during upload



In case you are doing an initial upload of a large dataset, you might want to disable indexing during upload.

It will enable to avoid unnecessary indexing of vectors, which will be overwritten by the next batch.
collection_name=""startups"",

documents=documents,

metadata=metadata,

parallel=0,  # Use all available CPU cores to encode data

)

```



The `add` method will encode all documents and upload them to Qdrant.

This is one of two fastembed-specific methods, that combines encoding and uploading into a single step.



The `parallel` parameter controls the number of CPU cores used to encode data.
Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
);

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,\n\nAnswer:",How can you parallelize the upload of a large dataset using shards in Qdrant?,"By creating multiple shards in Qdrant, you can parallelize the upload of a large dataset. It is recommended to have 2 to 4 shards per machine for efficient processing. When creating a collection in Qdrant, you can specify the number of shards to use for that collection. ","['## Parallel upload into multiple shards\n\n\n\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\n\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python', 'We recommend using our [Rust client library](https://github.com/qdrant/rust-client) for this purpose, as it is the fastest client library available for Qdrant.\n\n\n\nIf you are not using Rust, you might want to consider parallelizing your upload process.\n\n\n\n## Disable indexing during upload\n\n\n\nIn case you are doing an initial upload of a large dataset, you might want to disable indexing during upload.\n\nIt will enable to avoid unnecessary indexing of vectors, which will be overwritten by the next batch.', 'collection_name=""startups"",\n\n    documents=documents,\n\n    metadata=metadata,\n\n    parallel=0,  # Use all available CPU cores to encode data\n\n)\n\n```\n\n\n\nThe `add` method will encode all documents and upload them to Qdrant.\n\nThis is one of two fastembed-specific methods, that combines encoding and uploading into a single step.\n\n\n\nThe `parallel` parameter controls the number of CPU cores used to encode data.', 'Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.\n\n\n\nTo prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{', ');\n\n```\n\n\n\nTo prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.\n\nTo do that, you can configure qdrant to use minimal number of segments, which is usually 2.\n\nLarge segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,']","## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""shard_number"": 2

}

```



```python
We recommend using our [Rust client library](https://github.com/qdrant/rust-client) for this purpose, as it is the fastest client library available for Qdrant.



If you are not using Rust, you might want to consider parallelizing your upload process.



## Disable indexing during upload



In case you are doing an initial upload of a large dataset, you might want to disable indexing during upload.

It will enable to avoid unnecessary indexing of vectors, which will be overwritten by the next batch.
collection_name=""startups"",

    documents=documents,

    metadata=metadata,

    parallel=0,  # Use all available CPU cores to encode data

)

```



The `add` method will encode all documents and upload them to Qdrant.

This is one of two fastembed-specific methods, that combines encoding and uploading into a single step.



The `parallel` parameter controls the number of CPU cores used to encode data.
Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
);

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,",0.13043478260869565,0.8372093023255814,0.225705329153605,0.06545454545454546,0.42857142857142855,0.11356466876971609,0.07608695652173914,0.4883720930232558,0.13166144200626959,0.11956521739130435,0.7674418604651163,0.20689655172413793,0.9106304049491882,0.5263031125068665,0.6469945907592773,0.580441415309906,0.6734669537815665,0.9164428685326129,290,6.170212765957447,0.44214561501559235
True,,3069.871,9bff4372,What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?,"Maximizing the distance between all points in the response when utilizing vector similarity for diversity search is crucial as it allows for the algorithm to sequentially output dissimilar results. By doing so, a diverse selection of the collection can be made possible without the need for any labeling efforts. This approach ensures that the algorithm can effectively identify and retrieve a wide range of diverse items or data points.","Maximizing the distance between all points in the response when utilizing vector similarity for diversity search is significant because it allows for the selection of dissimilar results in a sequential manner. By maximizing the distance between points, the algorithm can ensure that the results it outputs are diverse and not focused on non-important features or irrelevant elements. This approach helps in finding genuine errors and avoiding biased selections that might occur with random sampling or traditional search methods.

In essence, maximizing distance in vector similarity for diversity search helps in identifying the most distinctive examples in the data by focusing on dissimilarity rather than similarity. It enables the identification of points that are different from a reference set, thereby enhancing the overall selection of items from the dataset.","{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=""Example of similarity-based sampling"" >}}





The power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.

By maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.
You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. 

To mitigate this issue, you can perform a diversity search.



Diversity search is a method for finding the most distinctive examples in the data.

As similarity search, it also operates on embeddings and measures the distances between them.

The difference lies in deciding which point should be extracted next.
Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.





## Diversity Search



Even with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.



The naive approach is to do random sampling. 

However, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.
Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. 

It doesn’t rely on keywords or other metadata. 

With vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.



The dissimilarity search can find items in areas where previously no other search could be used.

Let’s look at a few examples.



### Case: Mislabeling Detection
Let's imagine how to get 3 points with similarity search and then with diversity search.



Similarity:

1. Calculate distance matrix

2. Choose your anchor

3. Get a vector corresponding to the distances from the selected anchor from the distance matrix

4. Sort fetched vector

5. Get top-3 embeddings

 

Diversity:

1. Calculate distance matrix

2. Initialize starting point (randomly or according to the certain conditions)","['{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=""Example of similarity-based sampling"" >}}\n\n\n\n\n\nThe power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.\n\nBy maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.', 'You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. \n\nTo mitigate this issue, you can perform a diversity search.\n\n\n\nDiversity search is a method for finding the most distinctive examples in the data.\n\nAs similarity search, it also operates on embeddings and measures the distances between them.\n\nThe difference lies in deciding which point should be extracted next.', 'Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.\n\n\n\n\n\n## Diversity Search\n\n\n\nEven with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.\n\n\n\nThe naive approach is to do random sampling. \n\nHowever, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.', 'Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. \n\nIt doesn’t rely on keywords or other metadata. \n\nWith vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.\n\n\n\nThe dissimilarity search can find items in areas where previously no other search could be used.\n\nLet’s look at a few examples.\n\n\n\n### Case: Mislabeling Detection', ""Let's imagine how to get 3 points with similarity search and then with diversity search.\n\n\n\nSimilarity:\n\n1. Calculate distance matrix\n\n2. Choose your anchor\n\n3. Get a vector corresponding to the distances from the selected anchor from the distance matrix\n\n4. Sort fetched vector\n\n5. Get top-3 embeddings\n\n \n\nDiversity:\n\n1. Calculate distance matrix\n\n2. Initialize starting point (randomly or according to the certain conditions)""]","Question: What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?\n\nContext: {{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=""Example of similarity-based sampling"" >}}





The power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.

By maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.
You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. 

To mitigate this issue, you can perform a diversity search.



Diversity search is a method for finding the most distinctive examples in the data.

As similarity search, it also operates on embeddings and measures the distances between them.

The difference lies in deciding which point should be extracted next.
Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.





## Diversity Search



Even with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.



The naive approach is to do random sampling. 

However, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.
Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. 

It doesn’t rely on keywords or other metadata. 

With vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.



The dissimilarity search can find items in areas where previously no other search could be used.

Let’s look at a few examples.



### Case: Mislabeling Detection
Let's imagine how to get 3 points with similarity search and then with diversity search.



Similarity:

1. Calculate distance matrix

2. Choose your anchor

3. Get a vector corresponding to the distances from the selected anchor from the distance matrix

4. Sort fetched vector

5. Get top-3 embeddings



Diversity:

1. Calculate distance matrix

2. Initialize starting point (randomly or according to the certain conditions)\n\nAnswer:",What is the significance of maximizing the distance between all points in the response when utilizing vector similarity for diversity search?,"Maximizing the distance between all points in the response when utilizing vector similarity for diversity search is crucial as it allows for the algorithm to sequentially output dissimilar results. By doing so, a diverse selection of the collection can be made possible without the need for any labeling efforts. This approach ensures that the algorithm can effectively identify and retrieve a wide range of diverse items or data points.","['{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=""Example of similarity-based sampling"" >}}\n\n\n\n\n\nThe power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.\n\nBy maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.', 'You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. \n\nTo mitigate this issue, you can perform a diversity search.\n\n\n\nDiversity search is a method for finding the most distinctive examples in the data.\n\nAs similarity search, it also operates on embeddings and measures the distances between them.\n\nThe difference lies in deciding which point should be extracted next.', 'Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.\n\n\n\n\n\n## Diversity Search\n\n\n\nEven with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.\n\n\n\nThe naive approach is to do random sampling. \n\nHowever, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.', 'Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. \n\nIt doesn’t rely on keywords or other metadata. \n\nWith vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.\n\n\n\nThe dissimilarity search can find items in areas where previously no other search could be used.\n\nLet’s look at a few examples.\n\n\n\n### Case: Mislabeling Detection', ""Let's imagine how to get 3 points with similarity search and then with diversity search.\n\n\n\nSimilarity:\n\n1. Calculate distance matrix\n\n2. Choose your anchor\n\n3. Get a vector corresponding to the distances from the selected anchor from the distance matrix\n\n4. Sort fetched vector\n\n5. Get top-3 embeddings\n\n \n\nDiversity:\n\n1. Calculate distance matrix\n\n2. Initialize starting point (randomly or according to the certain conditions)""]","{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/diversity-force.png caption=""Example of similarity-based sampling"" >}}





The power of vector similarity, in the context of being able to compare any two points, allows making a diverse selection of the collection possible without any labeling efforts.

By maximizing the distance between all points in the response, we can have an algorithm that will sequentially output dissimilar results.
You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. 

To mitigate this issue, you can perform a diversity search.



Diversity search is a method for finding the most distinctive examples in the data.

As similarity search, it also operates on embeddings and measures the distances between them.

The difference lies in deciding which point should be extracted next.
Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.





## Diversity Search



Even with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.



The naive approach is to do random sampling. 

However, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.
Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. 

It doesn’t rely on keywords or other metadata. 

With vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.



The dissimilarity search can find items in areas where previously no other search could be used.

Let’s look at a few examples.



### Case: Mislabeling Detection
Let's imagine how to get 3 points with similarity search and then with diversity search.



Similarity:

1. Calculate distance matrix

2. Choose your anchor

3. Get a vector corresponding to the distances from the selected anchor from the distance matrix

4. Sort fetched vector

5. Get top-3 embeddings

 

Diversity:

1. Calculate distance matrix

2. Initialize starting point (randomly or according to the certain conditions)",0.35344827586206895,0.6721311475409836,0.463276836158192,0.1826086956521739,0.35,0.24000000000000002,0.2413793103448276,0.45901639344262296,0.3163841807909605,0.3017241379310345,0.5737704918032787,0.39548022598870053,0.936796247959137,0.6380550861358643,0.6786271333694458,0.6577160358428955,0.9989265283657005,0.9959319630870596,128,1.855072463768116,0.5574917136099895
True,,2796.451,d9dc5401,How can you ensure that collection shards are replicated in Qdrant after adding a new node to the cluster?,"To ensure that collection shards are replicated in Qdrant after adding a new node to the cluster, you can use the Replicate Shard Operation. This operation allows you to create another copy of the shard on the newly connected node. It's important to note that Qdrant does not automatically balance shards as it is considered an expensive operation.","To ensure that collection shards are replicated in Qdrant after adding a new node to the cluster, you can follow these steps:

1. Use the Replicate Shard Operation to create another copy of the shard on the newly connected node. This operation will help replicate the shards and ensure data redundancy.

2. Check the state of the consensus using the cluster REST API to ensure that the cluster is not in a transition state. Collection update operations may be denied if the cluster is in an unstable state.

3. Remember that Qdrant does not automatically rebalance shards if the cluster size or replication factor changes, as it is a costly operation. Therefore, manual intervention may be required to ensure proper replication of shards.

By following these steps, you can ensure that collection shards are replicated and maintained properly in Qdrant after adding a new node to the cluster.","Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.

Use the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.
Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.



### Moving shards



*Available as of v0.9.0*
Currently, the replication factor of a collection can only be configured at creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",
Practically, it means that if the cluster is in a transition state - either electing a new leader after a failure or starting up, the collection update operations will be denied.



You may use the cluster [REST API](https://qdrant.github.io/qdrant/redoc/index.html?v=master#tag/cluster) to check the state of the consensus.



## Sharding



A Collection in Qdrant is made of one or more shards.

A shard is an independent store of points which is able to perform all operations provided by collections.
Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.



When you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 300,","['Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.\n\nUse the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.', 'Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.\n\n\n\n### Moving shards\n\n\n\n*Available as of v0.9.0*', 'Currently, the replication factor of a collection can only be configured at creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",', 'Practically, it means that if the cluster is in a transition state - either electing a new leader after a failure or starting up, the collection update operations will be denied.\n\n\n\nYou may use the cluster [REST API](https://qdrant.github.io/qdrant/redoc/index.html?v=master#tag/cluster) to check the state of the consensus.\n\n\n\n## Sharding\n\n\n\nA Collection in Qdrant is made of one or more shards.\n\nA shard is an independent store of points which is able to perform all operations provided by collections.', 'Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.\n\n\n\nWhen you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 300,']","Question: How can you ensure that collection shards are replicated in Qdrant after adding a new node to the cluster?\n\nContext: Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.

Use the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.
Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.



### Moving shards



*Available as of v0.9.0*
Currently, the replication factor of a collection can only be configured at creation time.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 300,

""distance"": ""Cosine""

},

""shard_number"": 6,

""replication_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",
Practically, it means that if the cluster is in a transition state - either electing a new leader after a failure or starting up, the collection update operations will be denied.



You may use the cluster [REST API](https://qdrant.github.io/qdrant/redoc/index.html?v=master#tag/cluster) to check the state of the consensus.



## Sharding



A Collection in Qdrant is made of one or more shards.

A shard is an independent store of points which is able to perform all operations provided by collections.
Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.



When you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 300,\n\nAnswer:",How can you ensure that collection shards are replicated in Qdrant after adding a new node to the cluster?,"To ensure that collection shards are replicated in Qdrant after adding a new node to the cluster, you can use the Replicate Shard Operation. This operation allows you to create another copy of the shard on the newly connected node. It's important to note that Qdrant does not automatically balance shards as it is considered an expensive operation.","['Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.\n\nUse the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.', 'Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.\n\n\n\n### Moving shards\n\n\n\n*Available as of v0.9.0*', 'Currently, the replication factor of a collection can only be configured at creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",', 'Practically, it means that if the cluster is in a transition state - either electing a new leader after a failure or starting up, the collection update operations will be denied.\n\n\n\nYou may use the cluster [REST API](https://qdrant.github.io/qdrant/redoc/index.html?v=master#tag/cluster) to check the state of the consensus.\n\n\n\n## Sharding\n\n\n\nA Collection in Qdrant is made of one or more shards.\n\nA shard is an independent store of points which is able to perform all operations provided by collections.', 'Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.\n\n\n\nWhen you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 300,']","Once the new node is ready and synchronized with the cluster, you might want to ensure that the collection shards are replicated enough. Remember that Qdrant will not automatically balance shards since this is an expensive operation.

Use the [Replicate Shard Operation](https://qdrant.github.io/qdrant/redoc/index.html#tag/cluster/operation/update_collection_cluster) to create another copy of the shard on the newly connected node.
Shards are evenly distributed across all existing nodes when a collection is first created, but Qdrant does not automatically rebalance shards if your cluster size or replication factor changes (since this is an expensive operation on large clusters). See the next section for how to move shards after scaling operations.



### Moving shards



*Available as of v0.9.0*
Currently, the replication factor of a collection can only be configured at creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",
Practically, it means that if the cluster is in a transition state - either electing a new leader after a failure or starting up, the collection update operations will be denied.



You may use the cluster [REST API](https://qdrant.github.io/qdrant/redoc/index.html?v=master#tag/cluster) to check the state of the consensus.



## Sharding



A Collection in Qdrant is made of one or more shards.

A shard is an independent store of points which is able to perform all operations provided by collections.
Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.



When you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 300,",0.3435114503816794,0.8490566037735849,0.48913043478260876,0.2692307692307692,0.6730769230769231,0.38461538461538464,0.32061068702290074,0.7924528301886793,0.45652173913043476,0.3435114503816794,0.8490566037735849,0.48913043478260876,0.9622147679328918,0.6528387069702148,0.7950096130371094,0.7169439792633057,0.891039852576796,0.9966677215707023,148,2.5084745762711864,0.5135182880980773
True,,1684.8890000000001,69455a40,Why would someone use a vector database?,"A vector database is used for various reasons, primarily for its efficiency in handling complex data structures and enabling advanced search capabilities. In the context of search and discovery, a vector database allows for state-of-the-art vector-search functionalities, making it ideal for applications requiring similarity search, recommendation systems, and content-based search. By leveraging vectors to represent data points, a vector database can efficiently compute similarities between vectors, enabling quick and accurate retrieval of relevant information","Vector databases are particularly useful for handling high-dimensional vector data efficiently. They offer benefits such as efficient storage and indexing of large-scale datasets with billions of data points, support for real-time analytics and queries, and the ability to retrieve unstructured data represented as vectors quickly and accurately. These advantages make vector databases essential for applications requiring advanced data analysis and similarity-search operations beyond what traditional databases can offer.","aliases: [ /blog/what-is-a-vector-database/ ]

---



> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.





## Why use a Vector Database?



The data flood is real.
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query
Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.





### Starting Your First Vector Database Project
When a query is made, the system can use **both** the vector data and the metadata to process the query. In other words, the database doesn’t just look for the closest vectors. It also considers the additional criteria set by the metadata filters, creating a more customizable search experience.





![](/articles_data/what-is-a-vector-database/metadata.jpg)







### Data Security and Access Control","['aliases: [ /blog/what-is-a-vector-database/ ]\n\n---\n\n\n\n> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.\n\n\n\n\n\n## Why use a Vector Database?\n\n\n\nThe data flood is real.', 'advantage of their efficient indexing and searching techniques, vector databases enable faster \n\nand more accurate retrieval of unstructured data already represented as vectors, which can \n\nhelp put in front of users the most relevant results to their queries.\n\n\n\nIn addition, other benefits of using vector databases include:\n\n1. Efficient storage and indexing of high-dimensional data.\n\n3. Ability to handle large-scale datasets with billions of data points.\n\n4. Support for real-time analytics and queries.', ""And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query"", 'Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.\n\n\n\n\n\n### Starting Your First Vector Database Project', 'When a query is made, the system can use **both** the vector data and the metadata to process the query. In other words, the database doesn’t just look for the closest vectors. It also considers the additional criteria set by the metadata filters, creating a more customizable search experience.\n\n\n\n\n\n![](/articles_data/what-is-a-vector-database/metadata.jpg)\n\n\n\n\n\n\n\n### Data Security and Access Control']","Question: Why would someone use a vector database?\n\nContext: aliases: [ /blog/what-is-a-vector-database/ ]

---



> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.





## Why use a Vector Database?



The data flood is real.
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query
Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.





### Starting Your First Vector Database Project
When a query is made, the system can use **both** the vector data and the metadata to process the query. In other words, the database doesn’t just look for the closest vectors. It also considers the additional criteria set by the metadata filters, creating a more customizable search experience.





![](/articles_data/what-is-a-vector-database/metadata.jpg)







### Data Security and Access Control\n\nAnswer:",Why would someone use a vector database?,"A vector database is used for various reasons, primarily for its efficiency in handling complex data structures and enabling advanced search capabilities. In the context of search and discovery, a vector database allows for state-of-the-art vector-search functionalities, making it ideal for applications requiring similarity search, recommendation systems, and content-based search. By leveraging vectors to represent data points, a vector database can efficiently compute similarities between vectors, enabling quick and accurate retrieval of relevant information","['aliases: [ /blog/what-is-a-vector-database/ ]\n\n---\n\n\n\n> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.\n\n\n\n\n\n## Why use a Vector Database?\n\n\n\nThe data flood is real.', 'advantage of their efficient indexing and searching techniques, vector databases enable faster \n\nand more accurate retrieval of unstructured data already represented as vectors, which can \n\nhelp put in front of users the most relevant results to their queries.\n\n\n\nIn addition, other benefits of using vector databases include:\n\n1. Efficient storage and indexing of high-dimensional data.\n\n3. Ability to handle large-scale datasets with billions of data points.\n\n4. Support for real-time analytics and queries.', ""And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query"", 'Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.\n\n\n\n\n\n### Starting Your First Vector Database Project', 'When a query is made, the system can use **both** the vector data and the metadata to process the query. In other words, the database doesn’t just look for the closest vectors. It also considers the additional criteria set by the metadata filters, creating a more customizable search experience.\n\n\n\n\n\n![](/articles_data/what-is-a-vector-database/metadata.jpg)\n\n\n\n\n\n\n\n### Data Security and Access Control']","aliases: [ /blog/what-is-a-vector-database/ ]

---



> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.





## Why use a Vector Database?



The data flood is real.
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query
Those are just a few examples. The ability of vector databases to “match” data with queries makes them essential for multiple types of applications. Here are some more [use cases examples](https://qdrant.tech/use-cases/) you can take a look at.





### Starting Your First Vector Database Project
When a query is made, the system can use **both** the vector data and the metadata to process the query. In other words, the database doesn’t just look for the closest vectors. It also considers the additional criteria set by the metadata filters, creating a more customizable search experience.





![](/articles_data/what-is-a-vector-database/metadata.jpg)







### Data Security and Access Control",0.4788732394366197,0.4594594594594595,0.46896551724137936,0.12857142857142856,0.1232876712328767,0.12587412587412586,0.2535211267605634,0.24324324324324326,0.2482758620689655,0.39436619718309857,0.3783783783783784,0.3862068965517241,0.8636921644210815,0.6767686009407043,0.686835527420044,0.6817649602890015,0.9994419222930446,0.9953065038813899,68,0.918918918918919,0.5003376617174984
True,,4512.729,d553291d,What benefits does Qdrant Cloud on Microsoft Azure offer for rapid application development?,"Qdrant Cloud on Microsoft Azure offers the benefit of rapid application development by allowing users to deploy their own cluster through the Qdrant Cloud Console within seconds. This means that users can set up their environment on Azure quickly, reducing deployment time and enabling them to scale their resources as needed. This rapid deployment capability enables users to hit the ground running with their development projects, facilitating faster development cycles and improved scalability.","Qdrant Cloud on Microsoft Azure offers several benefits for rapid application development, including:

1. **Efficient Deployment**: With Qdrant Cloud on Azure, you can deploy your own cluster within seconds through the Qdrant Cloud Console. This reduces deployment time and allows you to start working on your application quickly.

2. **Scalability**: Qdrant Cloud on Azure can handle large-scale datasets with billions of vectors. You can scale your resources as needed, leveraging features like horizontal scaling and binary quantization on Microsoft Azure's scalable infrastructure.

3. **Upgradeability**: The setup on Azure allows for easy upgrades to later versions of Qdrant as they are released. This ensures that you can access the latest features and improvements for your application development.

4. **Monitoring and Logging**: Qdrant Cloud on Azure is equipped with monitoring and logging tools to observe the health of each cluster. This helps in efficiently managing and maintaining your application environment.

5. **Accessible through Qdrant Cloud Console**: The Azure deployment of Qdrant Cloud is accessible through the Qdrant Cloud Console, providing a user-friendly interface for managing and monitoring your clusters.

Overall, the availability of Qdrant Cloud on Microsoft Azure offers a streamlined and scalable solution for rapid application development, enabling developers to focus on building their applications without worrying about infrastructure management.","What this means for you:



- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.

- **Billion vector scale**: Seamlessly grow and handle large-scale datasets with billions of vectors. Leverage Qdrant features like horizontal scaling and binary quantization with Microsoft Azure's scalable infrastructure.
- Information Retrieval

  - Cloud

  - Azure

---

Great news! We've expanded Qdrant's managed vector database offering — [Qdrant Cloud](https://cloud.qdrant.io/) — to be available on Microsoft Azure. 

You can now effortlessly set up your environment on Azure, which reduces deployment time, so you can hit the ground running.



[Get started](https://cloud.qdrant.io/)



What this means for you:
It provides horizontal and vertical scaling, one click installation and upgrades, monitoring, logging, as well as backup and disaster recovery. For more information, see the [Qdrant Cloud documentation](/documentation/cloud).



### Kubernetes



You can use a ready-made [Helm Chart](https://helm.sh/docs/) to run Qdrant in your Kubernetes cluster:



```bash

helm repo add qdrant https://qdrant.to/helm

helm install qdrant qdrant/qdrant

```
- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:
---

draft: false

title: Introducing Qdrant Cloud on Microsoft Azure

slug: qdrant-cloud-on-microsoft-azure

short_description: Qdrant Cloud is now available on Microsoft Azure

description: ""Learn the benefits of Qdrant Cloud on Azure.""

preview_image: /blog/from_cms/qdrant-azure-2-1.png

date: 2024-01-17T08:40:42Z

author: Manuel Meyer

featured: false

tags:

  - Data Science

  - Vector Database

  - Machine Learning

  - Information Retrieval

  - Cloud

  - Azure

---","[""What this means for you:\n\n\n\n- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.\n\n- **Billion vector scale**: Seamlessly grow and handle large-scale datasets with billions of vectors. Leverage Qdrant features like horizontal scaling and binary quantization with Microsoft Azure's scalable infrastructure."", ""- Information Retrieval\n\n  - Cloud\n\n  - Azure\n\n---\n\nGreat news! We've expanded Qdrant's managed vector database offering — [Qdrant Cloud](https://cloud.qdrant.io/) — to be available on Microsoft Azure. \n\nYou can now effortlessly set up your environment on Azure, which reduces deployment time, so you can hit the ground running.\n\n\n\n[Get started](https://cloud.qdrant.io/)\n\n\n\nWhat this means for you:"", 'It provides horizontal and vertical scaling, one click installation and upgrades, monitoring, logging, as well as backup and disaster recovery. For more information, see the [Qdrant Cloud documentation](/documentation/cloud).\n\n\n\n### Kubernetes\n\n\n\nYou can use a ready-made [Helm Chart](https://helm.sh/docs/) to run Qdrant in your Kubernetes cluster:\n\n\n\n```bash\n\nhelm repo add qdrant https://qdrant.to/helm\n\nhelm install qdrant qdrant/qdrant\n\n```', '- Upgradeable to later versions of Qdrant as they are released.\n\n- Equipped with monitoring and logging to observe the health of each cluster. \n\n- Accessible through the Qdrant Cloud Console.\n\n- Vertically scalable.\n\n- Offered on AWS and GCP, with Azure currently in development. \n\n\n\n### Getting started with Qdrant Cloud\n\n\n\nTo use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:', '---\n\ndraft: false\n\ntitle: Introducing Qdrant Cloud on Microsoft Azure\n\nslug: qdrant-cloud-on-microsoft-azure\n\nshort_description: Qdrant Cloud is now available on Microsoft Azure\n\ndescription: ""Learn the benefits of Qdrant Cloud on Azure.""\n\npreview_image: /blog/from_cms/qdrant-azure-2-1.png\n\ndate: 2024-01-17T08:40:42Z\n\nauthor: Manuel Meyer\n\nfeatured: false\n\ntags:\n\n  - Data Science\n\n  - Vector Database\n\n  - Machine Learning\n\n  - Information Retrieval\n\n  - Cloud\n\n  - Azure\n\n---']","Question: What benefits does Qdrant Cloud on Microsoft Azure offer for rapid application development?\n\nContext: What this means for you:



- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.

- **Billion vector scale**: Seamlessly grow and handle large-scale datasets with billions of vectors. Leverage Qdrant features like horizontal scaling and binary quantization with Microsoft Azure's scalable infrastructure.
- Information Retrieval

- Cloud

- Azure

---

Great news! We've expanded Qdrant's managed vector database offering — [Qdrant Cloud](https://cloud.qdrant.io/) — to be available on Microsoft Azure. 

You can now effortlessly set up your environment on Azure, which reduces deployment time, so you can hit the ground running.



[Get started](https://cloud.qdrant.io/)



What this means for you:
It provides horizontal and vertical scaling, one click installation and upgrades, monitoring, logging, as well as backup and disaster recovery. For more information, see the [Qdrant Cloud documentation](/documentation/cloud).



### Kubernetes



You can use a ready-made [Helm Chart](https://helm.sh/docs/) to run Qdrant in your Kubernetes cluster:



```bash

helm repo add qdrant https://qdrant.to/helm

helm install qdrant qdrant/qdrant

```
- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:
---

draft: false

title: Introducing Qdrant Cloud on Microsoft Azure

slug: qdrant-cloud-on-microsoft-azure

short_description: Qdrant Cloud is now available on Microsoft Azure

description: ""Learn the benefits of Qdrant Cloud on Azure.""

preview_image: /blog/from_cms/qdrant-azure-2-1.png

date: 2024-01-17T08:40:42Z

author: Manuel Meyer

featured: false

tags:

- Data Science

- Vector Database

- Machine Learning

- Information Retrieval

- Cloud

- Azure

---\n\nAnswer:",What benefits does Qdrant Cloud on Microsoft Azure offer for rapid application development?,"Qdrant Cloud on Microsoft Azure offers the benefit of rapid application development by allowing users to deploy their own cluster through the Qdrant Cloud Console within seconds. This means that users can set up their environment on Azure quickly, reducing deployment time and enabling them to scale their resources as needed. This rapid deployment capability enables users to hit the ground running with their development projects, facilitating faster development cycles and improved scalability.","[""What this means for you:\n\n\n\n- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.\n\n- **Billion vector scale**: Seamlessly grow and handle large-scale datasets with billions of vectors. Leverage Qdrant features like horizontal scaling and binary quantization with Microsoft Azure's scalable infrastructure."", ""- Information Retrieval\n\n  - Cloud\n\n  - Azure\n\n---\n\nGreat news! We've expanded Qdrant's managed vector database offering — [Qdrant Cloud](https://cloud.qdrant.io/) — to be available on Microsoft Azure. \n\nYou can now effortlessly set up your environment on Azure, which reduces deployment time, so you can hit the ground running.\n\n\n\n[Get started](https://cloud.qdrant.io/)\n\n\n\nWhat this means for you:"", 'It provides horizontal and vertical scaling, one click installation and upgrades, monitoring, logging, as well as backup and disaster recovery. For more information, see the [Qdrant Cloud documentation](/documentation/cloud).\n\n\n\n### Kubernetes\n\n\n\nYou can use a ready-made [Helm Chart](https://helm.sh/docs/) to run Qdrant in your Kubernetes cluster:\n\n\n\n```bash\n\nhelm repo add qdrant https://qdrant.to/helm\n\nhelm install qdrant qdrant/qdrant\n\n```', '- Upgradeable to later versions of Qdrant as they are released.\n\n- Equipped with monitoring and logging to observe the health of each cluster. \n\n- Accessible through the Qdrant Cloud Console.\n\n- Vertically scalable.\n\n- Offered on AWS and GCP, with Azure currently in development. \n\n\n\n### Getting started with Qdrant Cloud\n\n\n\nTo use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:', '---\n\ndraft: false\n\ntitle: Introducing Qdrant Cloud on Microsoft Azure\n\nslug: qdrant-cloud-on-microsoft-azure\n\nshort_description: Qdrant Cloud is now available on Microsoft Azure\n\ndescription: ""Learn the benefits of Qdrant Cloud on Azure.""\n\npreview_image: /blog/from_cms/qdrant-azure-2-1.png\n\ndate: 2024-01-17T08:40:42Z\n\nauthor: Manuel Meyer\n\nfeatured: false\n\ntags:\n\n  - Data Science\n\n  - Vector Database\n\n  - Machine Learning\n\n  - Information Retrieval\n\n  - Cloud\n\n  - Azure\n\n---']","What this means for you:



- **Rapid application development**: Deploy your own cluster through the Qdrant Cloud Console within seconds and scale your resources as needed.

- **Billion vector scale**: Seamlessly grow and handle large-scale datasets with billions of vectors. Leverage Qdrant features like horizontal scaling and binary quantization with Microsoft Azure's scalable infrastructure.
- Information Retrieval

  - Cloud

  - Azure

---

Great news! We've expanded Qdrant's managed vector database offering — [Qdrant Cloud](https://cloud.qdrant.io/) — to be available on Microsoft Azure. 

You can now effortlessly set up your environment on Azure, which reduces deployment time, so you can hit the ground running.



[Get started](https://cloud.qdrant.io/)



What this means for you:
It provides horizontal and vertical scaling, one click installation and upgrades, monitoring, logging, as well as backup and disaster recovery. For more information, see the [Qdrant Cloud documentation](/documentation/cloud).



### Kubernetes



You can use a ready-made [Helm Chart](https://helm.sh/docs/) to run Qdrant in your Kubernetes cluster:



```bash

helm repo add qdrant https://qdrant.to/helm

helm install qdrant qdrant/qdrant

```
- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:
---

draft: false

title: Introducing Qdrant Cloud on Microsoft Azure

slug: qdrant-cloud-on-microsoft-azure

short_description: Qdrant Cloud is now available on Microsoft Azure

description: ""Learn the benefits of Qdrant Cloud on Azure.""

preview_image: /blog/from_cms/qdrant-azure-2-1.png

date: 2024-01-17T08:40:42Z

author: Manuel Meyer

featured: false

tags:

  - Data Science

  - Vector Database

  - Machine Learning

  - Information Retrieval

  - Cloud

  - Azure

---",0.2512315270935961,0.7285714285714285,0.3736263736263737,0.09405940594059406,0.2753623188405797,0.14022140221402216,0.15763546798029557,0.45714285714285713,0.23443223443223443,0.2315270935960591,0.6714285714285714,0.3443223443223444,0.937808632850647,0.6335249543190002,0.7266429662704468,0.6768964529037476,0.6997262325681125,0.9974867807468399,210,2.8767123287671232,0.49974109227486296
True,,1104.921,5f86164d,What is the purpose of the `groupBy` parameter?,"The `groupBy` parameter is used to specify how the results should be grouped based on a specific field, like ""document_id"". By setting the `groupBy` parameter to ""document_id"", the results are organized into groups where each group contains all the hits associated with a particular document_id.",The `groupBy` parameter is used in the grouping API request to specify the field by which the search results should be grouped. This parameter helps in organizing the search results into distinct groups based on the specified field values.,"#### Adding the parameter to grouping API request:



When using the grouping API, add the `with_lookup` parameter to bring the information from those points into each group:



```http

POST /collections/chunks/points/search/groups

{

    // Same as in the regular search API

    ""vector"": [1.1],

    ...,



    // Grouping parameters

    ""group_by"": ""document_id"",  

    ""limit"": 2,                 

    ""group_size"": 2,            



    // Lookup parameters

    ""with_lookup"": {
{

    // Same as in the regular search API

    ""vector"": [1.1],



    // Grouping parameters

    ""group_by"": ""document_id"",

    ""limit"": 2,

    ""group_size"": 2,



    // Lookup parameters

    ""with_lookup"": {

        // Name of the collection to look up points in

        ""collection"": ""documents"",



        // Options for specifying what to bring from the payload 

        // of the looked up point, true by default

        ""with_payload"": [""title"", ""text""],
""with_payload"": [""title"", ""text""],



        // Options for specifying what to bring from the vector(s) 

        // of the looked up point, true by default

        ""with_vectors: false

    }

}

```



```python

client.search_groups(

    collection_name=""chunks"",

    # Same as in the regular search() API

    query_vector=[1.1],

    # Grouping parameters

    group_by=""document_id"",  # Path of the field to group by

    limit=2,  # Max amount of groups
vector: new float[] { 1.1f },

	groupBy: ""document_id"",

	limit: 4,

	groupSize: 2

);

```



The output of a ***groups*** call looks like this:



```json

{

    ""result"": {

        ""groups"": [

            {

                ""id"": ""a"",

                ""hits"": [

                    { ""id"": 0, ""score"": 0.91 },

                    { ""id"": 1, ""score"": 0.85 }

                ]

            },

            {

                ""id"": ""b"",

                ""hits"": [
client

    .search_groups(&SearchPointGroups {

        collection_name: ""{collection_name}"".to_string(),

        vector: vec![1.1],

        group_by: ""document_id"".to_string(),

        limit: 2,

        group_size: 2,

        with_lookup: Some(WithLookup {

            collection: ""documents"".to_string(),

            with_payload: Some(vec![""title"", ""text""].into()),

            with_vectors: Some(false.into()),

        }),

        ..Default::default()

    })

    .await?;

```



```java","['#### Adding the parameter to grouping API request:\n\n\n\nWhen using the grouping API, add the `with_lookup` parameter to bring the information from those points into each group:\n\n\n\n```http\n\nPOST /collections/chunks/points/search/groups\n\n{\n\n    // Same as in the regular search API\n\n    ""vector"": [1.1],\n\n    ...,\n\n\n\n    // Grouping parameters\n\n    ""group_by"": ""document_id"",  \n\n    ""limit"": 2,                 \n\n    ""group_size"": 2,            \n\n\n\n    // Lookup parameters\n\n    ""with_lookup"": {', '{\n\n    // Same as in the regular search API\n\n    ""vector"": [1.1],\n\n\n\n    // Grouping parameters\n\n    ""group_by"": ""document_id"",\n\n    ""limit"": 2,\n\n    ""group_size"": 2,\n\n\n\n    // Lookup parameters\n\n    ""with_lookup"": {\n\n        // Name of the collection to look up points in\n\n        ""collection"": ""documents"",\n\n\n\n        // Options for specifying what to bring from the payload \n\n        // of the looked up point, true by default\n\n        ""with_payload"": [""title"", ""text""],', '""with_payload"": [""title"", ""text""],\n\n\n\n        // Options for specifying what to bring from the vector(s) \n\n        // of the looked up point, true by default\n\n        ""with_vectors: false\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.search_groups(\n\n    collection_name=""chunks"",\n\n    # Same as in the regular search() API\n\n    query_vector=[1.1],\n\n    # Grouping parameters\n\n    group_by=""document_id"",  # Path of the field to group by\n\n    limit=2,  # Max amount of groups', 'vector: new float[] { 1.1f },\n\n\tgroupBy: ""document_id"",\n\n\tlimit: 4,\n\n\tgroupSize: 2\n\n);\n\n```\n\n\n\nThe output of a ***groups*** call looks like this:\n\n\n\n```json\n\n{\n\n    ""result"": {\n\n        ""groups"": [\n\n            {\n\n                ""id"": ""a"",\n\n                ""hits"": [\n\n                    { ""id"": 0, ""score"": 0.91 },\n\n                    { ""id"": 1, ""score"": 0.85 }\n\n                ]\n\n            },\n\n            {\n\n                ""id"": ""b"",\n\n                ""hits"": [', 'client\n\n    .search_groups(&SearchPointGroups {\n\n        collection_name: ""{collection_name}"".to_string(),\n\n        vector: vec![1.1],\n\n        group_by: ""document_id"".to_string(),\n\n        limit: 2,\n\n        group_size: 2,\n\n        with_lookup: Some(WithLookup {\n\n            collection: ""documents"".to_string(),\n\n            with_payload: Some(vec![""title"", ""text""].into()),\n\n            with_vectors: Some(false.into()),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java']","Question: What is the purpose of the `groupBy` parameter?\n\nContext: #### Adding the parameter to grouping API request:



When using the grouping API, add the `with_lookup` parameter to bring the information from those points into each group:



```http

POST /collections/chunks/points/search/groups

{

// Same as in the regular search API

""vector"": [1.1],

...,



// Grouping parameters

""group_by"": ""document_id"",  

""limit"": 2,                 

""group_size"": 2,            



// Lookup parameters

""with_lookup"": {
{

// Same as in the regular search API

""vector"": [1.1],



// Grouping parameters

""group_by"": ""document_id"",

""limit"": 2,

""group_size"": 2,



// Lookup parameters

""with_lookup"": {

// Name of the collection to look up points in

""collection"": ""documents"",



// Options for specifying what to bring from the payload 

// of the looked up point, true by default

""with_payload"": [""title"", ""text""],
""with_payload"": [""title"", ""text""],



// Options for specifying what to bring from the vector(s) 

// of the looked up point, true by default

""with_vectors: false

}

}

```



```python

client.search_groups(

collection_name=""chunks"",

# Same as in the regular search() API

query_vector=[1.1],

# Grouping parameters

group_by=""document_id"",  # Path of the field to group by

limit=2,  # Max amount of groups
vector: new float[] { 1.1f },

groupBy: ""document_id"",

limit: 4,

groupSize: 2

);

```



The output of a ***groups*** call looks like this:



```json

{

""result"": {

""groups"": [

{

""id"": ""a"",

""hits"": [

{ ""id"": 0, ""score"": 0.91 },

{ ""id"": 1, ""score"": 0.85 }

]

},

{

""id"": ""b"",

""hits"": [
client

.search_groups(&SearchPointGroups {

collection_name: ""{collection_name}"".to_string(),

vector: vec![1.1],

group_by: ""document_id"".to_string(),

limit: 2,

group_size: 2,

with_lookup: Some(WithLookup {

collection: ""documents"".to_string(),

with_payload: Some(vec![""title"", ""text""].into()),

with_vectors: Some(false.into()),

}),

..Default::default()

})

.await?;

```



```java\n\nAnswer:",What is the purpose of the `groupBy` parameter?,"The `groupBy` parameter is used to specify how the results should be grouped based on a specific field, like ""document_id"". By setting the `groupBy` parameter to ""document_id"", the results are organized into groups where each group contains all the hits associated with a particular document_id.","['#### Adding the parameter to grouping API request:\n\n\n\nWhen using the grouping API, add the `with_lookup` parameter to bring the information from those points into each group:\n\n\n\n```http\n\nPOST /collections/chunks/points/search/groups\n\n{\n\n    // Same as in the regular search API\n\n    ""vector"": [1.1],\n\n    ...,\n\n\n\n    // Grouping parameters\n\n    ""group_by"": ""document_id"",  \n\n    ""limit"": 2,                 \n\n    ""group_size"": 2,            \n\n\n\n    // Lookup parameters\n\n    ""with_lookup"": {', '{\n\n    // Same as in the regular search API\n\n    ""vector"": [1.1],\n\n\n\n    // Grouping parameters\n\n    ""group_by"": ""document_id"",\n\n    ""limit"": 2,\n\n    ""group_size"": 2,\n\n\n\n    // Lookup parameters\n\n    ""with_lookup"": {\n\n        // Name of the collection to look up points in\n\n        ""collection"": ""documents"",\n\n\n\n        // Options for specifying what to bring from the payload \n\n        // of the looked up point, true by default\n\n        ""with_payload"": [""title"", ""text""],', '""with_payload"": [""title"", ""text""],\n\n\n\n        // Options for specifying what to bring from the vector(s) \n\n        // of the looked up point, true by default\n\n        ""with_vectors: false\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.search_groups(\n\n    collection_name=""chunks"",\n\n    # Same as in the regular search() API\n\n    query_vector=[1.1],\n\n    # Grouping parameters\n\n    group_by=""document_id"",  # Path of the field to group by\n\n    limit=2,  # Max amount of groups', 'vector: new float[] { 1.1f },\n\n\tgroupBy: ""document_id"",\n\n\tlimit: 4,\n\n\tgroupSize: 2\n\n);\n\n```\n\n\n\nThe output of a ***groups*** call looks like this:\n\n\n\n```json\n\n{\n\n    ""result"": {\n\n        ""groups"": [\n\n            {\n\n                ""id"": ""a"",\n\n                ""hits"": [\n\n                    { ""id"": 0, ""score"": 0.91 },\n\n                    { ""id"": 1, ""score"": 0.85 }\n\n                ]\n\n            },\n\n            {\n\n                ""id"": ""b"",\n\n                ""hits"": [', 'client\n\n    .search_groups(&SearchPointGroups {\n\n        collection_name: ""{collection_name}"".to_string(),\n\n        vector: vec![1.1],\n\n        group_by: ""document_id"".to_string(),\n\n        limit: 2,\n\n        group_size: 2,\n\n        with_lookup: Some(WithLookup {\n\n            collection: ""documents"".to_string(),\n\n            with_payload: Some(vec![""title"", ""text""].into()),\n\n            with_vectors: Some(false.into()),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java']","#### Adding the parameter to grouping API request:



When using the grouping API, add the `with_lookup` parameter to bring the information from those points into each group:



```http

POST /collections/chunks/points/search/groups

{

    // Same as in the regular search API

    ""vector"": [1.1],

    ...,



    // Grouping parameters

    ""group_by"": ""document_id"",  

    ""limit"": 2,                 

    ""group_size"": 2,            



    // Lookup parameters

    ""with_lookup"": {
{

    // Same as in the regular search API

    ""vector"": [1.1],



    // Grouping parameters

    ""group_by"": ""document_id"",

    ""limit"": 2,

    ""group_size"": 2,



    // Lookup parameters

    ""with_lookup"": {

        // Name of the collection to look up points in

        ""collection"": ""documents"",



        // Options for specifying what to bring from the payload 

        // of the looked up point, true by default

        ""with_payload"": [""title"", ""text""],
""with_payload"": [""title"", ""text""],



        // Options for specifying what to bring from the vector(s) 

        // of the looked up point, true by default

        ""with_vectors: false

    }

}

```



```python

client.search_groups(

    collection_name=""chunks"",

    # Same as in the regular search() API

    query_vector=[1.1],

    # Grouping parameters

    group_by=""document_id"",  # Path of the field to group by

    limit=2,  # Max amount of groups
vector: new float[] { 1.1f },

	groupBy: ""document_id"",

	limit: 4,

	groupSize: 2

);

```



The output of a ***groups*** call looks like this:



```json

{

    ""result"": {

        ""groups"": [

            {

                ""id"": ""a"",

                ""hits"": [

                    { ""id"": 0, ""score"": 0.91 },

                    { ""id"": 1, ""score"": 0.85 }

                ]

            },

            {

                ""id"": ""b"",

                ""hits"": [
client

    .search_groups(&SearchPointGroups {

        collection_name: ""{collection_name}"".to_string(),

        vector: vec![1.1],

        group_by: ""document_id"".to_string(),

        limit: 2,

        group_size: 2,

        with_lookup: Some(WithLookup {

            collection: ""documents"".to_string(),

            with_payload: Some(vec![""title"", ""text""].into()),

            with_vectors: Some(false.into()),

        }),

        ..Default::default()

    })

    .await?;

```



```java",0.6060606060606061,0.4878048780487805,0.5405405405405406,0.28125,0.225,0.25,0.42424242424242425,0.34146341463414637,0.37837837837837834,0.5454545454545454,0.43902439024390244,0.48648648648648646,0.7982648015022278,0.6992229223251343,0.6689828634262085,0.6837687492370605,0.9085712456144392,0.9116517528891563,39,0.8666666666666667,0.6177086690116638
True,,3216.1879999999996,4ab823ba,How can one change or correct Qdrant's behavior and default collection settings using configuration files?,"To change or correct Qdrant's behavior and default collection settings, one can utilize configuration files. The default configuration file for Qdrant is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml). If you wish to modify the default configuration, you can add a new configuration file and specify the path using `--config-path path/to/custom_config.yaml","To change or correct Qdrant's behavior and default collection settings using configuration files, you can follow these steps:

1. Locate the default configuration file at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).

2. Add a new configuration file with your desired settings. You can specify different parameters in this file to customize Qdrant's behavior.

3. If you are using the `QdrantDocumentStore`, you can customize the default configuration of the collection being used by providing settings when you create an instance of the `QdrantDocumentStore`. For example, to enable Scalar Quantization, you can do it as follows:

```python
import qdrant_client

from haystack.document_store.qdrant import QdrantDocumentStore

qdrant_document_store = QdrantDocumentStore(
    qdrant_client=qdrant_client,
    host=""localhost"",
    port=6334,
    vector_size=768,
    collection_name=""your_collection_name"",
    enable_scalar_quantization=True
)
```

4. Access the `config.yaml` file and set `storage.temp_path` to another directory location if needed.

By following these steps and customizing the configuration files, you can change and correct Qdrant's behavior and default collection settings according to your requirements.","---

title: Configuration

weight: 160

aliases:

  - ../configuration

---



# Configuration



To change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.



The default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).



To change the default configuration, add a new configuration file and specify
```bash

pip install qdrant-haystack

```



`QdrantDocumentStore` supports [all the configuration properties](/documentation/collections/#create-collection) available in 

the Qdrant Python client. If you want to customize the default configuration of the collection used under the hood, you can

provide that settings when you create an instance of the `QdrantDocumentStore`. For example, if you'd like to enable the

Scalar Quantization, you'd make that in the following way:



```python
To change it, access `config.yaml` and set `storage.temp_path` to another directory location.



## Important changes



The latest release focuses not only on the new features but also introduces some changes making 

Qdrant even more reliable.



### Optimizing group requests
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()

                    .setParams(

                        VectorParams.newBuilder()

                            .setSize(768)
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()

                    .setParams(

                        VectorParams.newBuilder()

                            .setSize(768)","[""---\n\ntitle: Configuration\n\nweight: 160\n\naliases:\n\n  - ../configuration\n\n---\n\n\n\n# Configuration\n\n\n\nTo change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.\n\n\n\nThe default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\n\n\n\nTo change the default configuration, add a new configuration file and specify"", ""```bash\n\npip install qdrant-haystack\n\n```\n\n\n\n`QdrantDocumentStore` supports [all the configuration properties](/documentation/collections/#create-collection) available in \n\nthe Qdrant Python client. If you want to customize the default configuration of the collection used under the hood, you can\n\nprovide that settings when you create an instance of the `QdrantDocumentStore`. For example, if you'd like to enable the\n\nScalar Quantization, you'd make that in the following way:\n\n\n\n```python"", 'To change it, access `config.yaml` and set `storage.temp_path` to another directory location.\n\n\n\n## Important changes\n\n\n\nThe latest release focuses not only on the new features but also introduces some changes making \n\nQdrant even more reliable.\n\n\n\n### Optimizing group requests', 'import io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()\n\n                    .setParams(\n\n                        VectorParams.newBuilder()\n\n                            .setSize(768)', 'import io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()\n\n                    .setParams(\n\n                        VectorParams.newBuilder()\n\n                            .setSize(768)']","Question: How can one change or correct Qdrant's behavior and default collection settings using configuration files?\n\nContext: ---

title: Configuration

weight: 160

aliases:

- ../configuration

---



# Configuration



To change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.



The default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).



To change the default configuration, add a new configuration file and specify
```bash

pip install qdrant-haystack

```



`QdrantDocumentStore` supports [all the configuration properties](/documentation/collections/#create-collection) available in 

the Qdrant Python client. If you want to customize the default configuration of the collection used under the hood, you can

provide that settings when you create an instance of the `QdrantDocumentStore`. For example, if you'd like to enable the

Scalar Quantization, you'd make that in the following way:



```python
To change it, access `config.yaml` and set `storage.temp_path` to another directory location.



## Important changes



The latest release focuses not only on the new features but also introduces some changes making 

Qdrant even more reliable.



### Optimizing group requests
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createCollectionAsync(

CreateCollection.newBuilder()

.setCollectionName(""{collection_name}"")

.setVectorsConfig(

VectorsConfig.newBuilder()

.setParams(

VectorParams.newBuilder()

.setSize(768)
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createCollectionAsync(

CreateCollection.newBuilder()

.setCollectionName(""{collection_name}"")

.setVectorsConfig(

VectorsConfig.newBuilder()

.setParams(

VectorParams.newBuilder()

.setSize(768)\n\nAnswer:",How can one change or correct Qdrant's behavior and default collection settings using configuration files?,"To change or correct Qdrant's behavior and default collection settings, one can utilize configuration files. The default configuration file for Qdrant is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml). If you wish to modify the default configuration, you can add a new configuration file and specify the path using `--config-path path/to/custom_config.yaml","[""---\n\ntitle: Configuration\n\nweight: 160\n\naliases:\n\n  - ../configuration\n\n---\n\n\n\n# Configuration\n\n\n\nTo change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.\n\n\n\nThe default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).\n\n\n\nTo change the default configuration, add a new configuration file and specify"", ""```bash\n\npip install qdrant-haystack\n\n```\n\n\n\n`QdrantDocumentStore` supports [all the configuration properties](/documentation/collections/#create-collection) available in \n\nthe Qdrant Python client. If you want to customize the default configuration of the collection used under the hood, you can\n\nprovide that settings when you create an instance of the `QdrantDocumentStore`. For example, if you'd like to enable the\n\nScalar Quantization, you'd make that in the following way:\n\n\n\n```python"", 'To change it, access `config.yaml` and set `storage.temp_path` to another directory location.\n\n\n\n## Important changes\n\n\n\nThe latest release focuses not only on the new features but also introduces some changes making \n\nQdrant even more reliable.\n\n\n\n### Optimizing group requests', 'import io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()\n\n                    .setParams(\n\n                        VectorParams.newBuilder()\n\n                            .setSize(768)', 'import io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()\n\n                    .setParams(\n\n                        VectorParams.newBuilder()\n\n                            .setSize(768)']","---

title: Configuration

weight: 160

aliases:

  - ../configuration

---



# Configuration



To change or correct Qdrant's behavior, default collection settings, and network interface parameters, you can use configuration files.



The default configuration file is located at [config/config.yaml](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).



To change the default configuration, add a new configuration file and specify
```bash

pip install qdrant-haystack

```



`QdrantDocumentStore` supports [all the configuration properties](/documentation/collections/#create-collection) available in 

the Qdrant Python client. If you want to customize the default configuration of the collection used under the hood, you can

provide that settings when you create an instance of the `QdrantDocumentStore`. For example, if you'd like to enable the

Scalar Quantization, you'd make that in the following way:



```python
To change it, access `config.yaml` and set `storage.temp_path` to another directory location.



## Important changes



The latest release focuses not only on the new features but also introduces some changes making 

Qdrant even more reliable.



### Optimizing group requests
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()

                    .setParams(

                        VectorParams.newBuilder()

                            .setSize(768)
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()

                    .setParams(

                        VectorParams.newBuilder()

                            .setSize(768)",0.30994152046783624,0.8688524590163934,0.4568965517241379,0.21176470588235294,0.6,0.31304347826086953,0.23976608187134502,0.6721311475409836,0.35344827586206895,0.30409356725146197,0.8524590163934426,0.4482758620689655,0.9533815383911133,0.6287376880645752,0.7468288540840149,0.6827142834663391,0.9492031734165115,0.9901619399800742,149,2.98,0.7211102442130934
True,,1908.285,727c6a75,What are the two ways of creating batches supported by the Qdrant API and how do they differ internally?,"The Qdrant API supports two ways of creating batches - record-oriented and column-oriented. Internally, these options do not differ and are made only for the convenience of interaction. This means that both record-oriented and column-oriented batch creation methods achieve the same result in terms of processing and storing data within the Qdrant system.","The two ways of creating batches supported by the Qdrant API are record-oriented and column-oriented. 

Record-oriented batching involves grouping multiple data points together and sending them as a batch to the service. Each batch contains multiple records (data points).

Column-oriented batching involves organizing the data points by columns and sending them as a batch to the service. Each batch contains data for a specific attribute or column.

Internally, these two batching methods do not differ in terms of how the data is stored or processed by the Qdrant service. They are provided for the convenience of interaction based on the user's preference and use case.","}

	}

);



```



are both possible.



## Upload points



To optimize performance, Qdrant supports batch loading of points. I.e., you can load several points into the service in one API call.

Batching allows you to minimize the overhead of creating a network connection.



The Qdrant API supports two ways of creating batches - record-oriented and column-oriented.

Internally, these options do not differ and are made only for the convenience of interaction.



Create points with batch:



```http
## Summary



Batch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!
Now, thanks to the new batch search, you don’t need to worry about that. Qdrant will handle multiple search requests in just one API call and will perform those requests in the most optimal way.



## An example of using the batch search



We’ve used the official Python client to show how the batch search might be integrated with your application. Since there have been some changes in the interfaces of Qdrant 0.10.0, we’ll go step by step.



## Creating the collection
The results you may achieve on a specific setup may vary depending on the hardware, however, at the first glance, it seems that batch searching may save you quite a lot of time.



Additional improvements could be achieved in the case of distributed deployment, as Qdrant won’t need to make extensive inter-cluster requests. Moreover, if your requests share the same filtering condition, the query optimizer would be able to reuse it among batch requests.



## Summary
batch by batch, into Qdrant:



```python

batch_size = 100



for batch in batched(dataset, batch_size):

    ids = [point.pop(""id"") for point in batch]

    vectors = [point.pop(""vector"") for point in batch]



    client.upsert(

        collection_name=""arxiv-titles-instructorxl-embeddings"",

        points=models.Batch(

            ids=ids,

            vectors=vectors,

            payloads=batch,

        ),

    )

```","['}\n\n\t}\n\n);\n\n\n\n```\n\n\n\nare both possible.\n\n\n\n## Upload points\n\n\n\nTo optimize performance, Qdrant supports batch loading of points. I.e., you can load several points into the service in one API call.\n\nBatching allows you to minimize the overhead of creating a network connection.\n\n\n\nThe Qdrant API supports two ways of creating batches - record-oriented and column-oriented.\n\nInternally, these options do not differ and are made only for the convenience of interaction.\n\n\n\nCreate points with batch:\n\n\n\n```http', '## Summary\n\n\n\nBatch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!', 'Now, thanks to the new batch search, you don’t need to worry about that. Qdrant will handle multiple search requests in just one API call and will perform those requests in the most optimal way.\n\n\n\n## An example of using the batch search\n\n\n\nWe’ve used the official Python client to show how the batch search might be integrated with your application. Since there have been some changes in the interfaces of Qdrant 0.10.0, we’ll go step by step.\n\n\n\n## Creating the collection', 'The results you may achieve on a specific setup may vary depending on the hardware, however, at the first glance, it seems that batch searching may save you quite a lot of time.\n\n\n\nAdditional improvements could be achieved in the case of distributed deployment, as Qdrant won’t need to make extensive inter-cluster requests. Moreover, if your requests share the same filtering condition, the query optimizer would be able to reuse it among batch requests.\n\n\n\n## Summary', 'batch by batch, into Qdrant:\n\n\n\n```python\n\nbatch_size = 100\n\n\n\nfor batch in batched(dataset, batch_size):\n\n    ids = [point.pop(""id"") for point in batch]\n\n    vectors = [point.pop(""vector"") for point in batch]\n\n\n\n    client.upsert(\n\n        collection_name=""arxiv-titles-instructorxl-embeddings"",\n\n        points=models.Batch(\n\n            ids=ids,\n\n            vectors=vectors,\n\n            payloads=batch,\n\n        ),\n\n    )\n\n```']","Question: What are the two ways of creating batches supported by the Qdrant API and how do they differ internally?\n\nContext: }

}

);



```



are both possible.



## Upload points



To optimize performance, Qdrant supports batch loading of points. I.e., you can load several points into the service in one API call.

Batching allows you to minimize the overhead of creating a network connection.



The Qdrant API supports two ways of creating batches - record-oriented and column-oriented.

Internally, these options do not differ and are made only for the convenience of interaction.



Create points with batch:



```http
## Summary



Batch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!
Now, thanks to the new batch search, you don’t need to worry about that. Qdrant will handle multiple search requests in just one API call and will perform those requests in the most optimal way.



## An example of using the batch search



We’ve used the official Python client to show how the batch search might be integrated with your application. Since there have been some changes in the interfaces of Qdrant 0.10.0, we’ll go step by step.



## Creating the collection
The results you may achieve on a specific setup may vary depending on the hardware, however, at the first glance, it seems that batch searching may save you quite a lot of time.



Additional improvements could be achieved in the case of distributed deployment, as Qdrant won’t need to make extensive inter-cluster requests. Moreover, if your requests share the same filtering condition, the query optimizer would be able to reuse it among batch requests.



## Summary
batch by batch, into Qdrant:



```python

batch_size = 100



for batch in batched(dataset, batch_size):

ids = [point.pop(""id"") for point in batch]

vectors = [point.pop(""vector"") for point in batch]



client.upsert(

collection_name=""arxiv-titles-instructorxl-embeddings"",

points=models.Batch(

ids=ids,

vectors=vectors,

payloads=batch,

),

)

```\n\nAnswer:",What are the two ways of creating batches supported by the Qdrant API and how do they differ internally?,"The Qdrant API supports two ways of creating batches - record-oriented and column-oriented. Internally, these options do not differ and are made only for the convenience of interaction. This means that both record-oriented and column-oriented batch creation methods achieve the same result in terms of processing and storing data within the Qdrant system.","['}\n\n\t}\n\n);\n\n\n\n```\n\n\n\nare both possible.\n\n\n\n## Upload points\n\n\n\nTo optimize performance, Qdrant supports batch loading of points. I.e., you can load several points into the service in one API call.\n\nBatching allows you to minimize the overhead of creating a network connection.\n\n\n\nThe Qdrant API supports two ways of creating batches - record-oriented and column-oriented.\n\nInternally, these options do not differ and are made only for the convenience of interaction.\n\n\n\nCreate points with batch:\n\n\n\n```http', '## Summary\n\n\n\nBatch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!', 'Now, thanks to the new batch search, you don’t need to worry about that. Qdrant will handle multiple search requests in just one API call and will perform those requests in the most optimal way.\n\n\n\n## An example of using the batch search\n\n\n\nWe’ve used the official Python client to show how the batch search might be integrated with your application. Since there have been some changes in the interfaces of Qdrant 0.10.0, we’ll go step by step.\n\n\n\n## Creating the collection', 'The results you may achieve on a specific setup may vary depending on the hardware, however, at the first glance, it seems that batch searching may save you quite a lot of time.\n\n\n\nAdditional improvements could be achieved in the case of distributed deployment, as Qdrant won’t need to make extensive inter-cluster requests. Moreover, if your requests share the same filtering condition, the query optimizer would be able to reuse it among batch requests.\n\n\n\n## Summary', 'batch by batch, into Qdrant:\n\n\n\n```python\n\nbatch_size = 100\n\n\n\nfor batch in batched(dataset, batch_size):\n\n    ids = [point.pop(""id"") for point in batch]\n\n    vectors = [point.pop(""vector"") for point in batch]\n\n\n\n    client.upsert(\n\n        collection_name=""arxiv-titles-instructorxl-embeddings"",\n\n        points=models.Batch(\n\n            ids=ids,\n\n            vectors=vectors,\n\n            payloads=batch,\n\n        ),\n\n    )\n\n```']","}

	}

);



```



are both possible.



## Upload points



To optimize performance, Qdrant supports batch loading of points. I.e., you can load several points into the service in one API call.

Batching allows you to minimize the overhead of creating a network connection.



The Qdrant API supports two ways of creating batches - record-oriented and column-oriented.

Internally, these options do not differ and are made only for the convenience of interaction.



Create points with batch:



```http
## Summary



Batch search allows packing different queries into a single API call and retrieving the results in a single response. If you ever struggled with sending several consecutive queries into Qdrant, then you can easily switch to the new batch search method and simplify your application code. As shown in the benchmarks, that may almost effortlessly speed up your interactions with Qdrant even by over 30%, even not considering the spare network overhead and possible reuse of filters!
Now, thanks to the new batch search, you don’t need to worry about that. Qdrant will handle multiple search requests in just one API call and will perform those requests in the most optimal way.



## An example of using the batch search



We’ve used the official Python client to show how the batch search might be integrated with your application. Since there have been some changes in the interfaces of Qdrant 0.10.0, we’ll go step by step.



## Creating the collection
The results you may achieve on a specific setup may vary depending on the hardware, however, at the first glance, it seems that batch searching may save you quite a lot of time.



Additional improvements could be achieved in the case of distributed deployment, as Qdrant won’t need to make extensive inter-cluster requests. Moreover, if your requests share the same filtering condition, the query optimizer would be able to reuse it among batch requests.



## Summary
batch by batch, into Qdrant:



```python

batch_size = 100



for batch in batched(dataset, batch_size):

    ids = [point.pop(""id"") for point in batch]

    vectors = [point.pop(""vector"") for point in batch]



    client.upsert(

        collection_name=""arxiv-titles-instructorxl-embeddings"",

        points=models.Batch(

            ids=ids,

            vectors=vectors,

            payloads=batch,

        ),

    )

```",0.3979591836734694,0.75,0.5199999999999999,0.20618556701030927,0.39215686274509803,0.27027027027027023,0.22448979591836735,0.4230769230769231,0.29333333333333333,0.35714285714285715,0.6730769230769231,0.4666666666666667,0.9220096468925476,0.6482504606246948,0.7275190949440002,0.6856011152267456,0.9786347003454077,0.9727242654189467,106,2.0384615384615383,0.5058404769393615
True,,6603.183999999999,caa27746,How can you create an index for a specific field in a payload using the Qdrant library?,"To create an index for a specific field in a payload using the Qdrant library, you can utilize the `CreatePayloadIndexAsync` method provided by the client. This method takes in the parameters `collectionName` and `fieldName`. ","To create an index for a specific field in a payload using the Qdrant library, you can refer to the code snippets provided in different programming languages. Here is a general guide on how to achieve this:

1. Import necessary modules or libraries related to the Qdrant client.
2. Connect to the Qdrant client by specifying the host and port.
3. Use the appropriate method to create a payload index for the desired field in the specified collection.

Below is a summarized guide for different programming languages based on the provided code snippets:

Python:
```python
client.create_payload_index(
    collection_name=""{collection_name}"",
    field_name=""name_of_the_field_to_index"",
    field_schema=""keyword"",
)
```

C#:
```csharp
await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");
```

Javascript (Node.js):
```javascript
client.createPayloadIndex(""{collection_name}"", {
    field_name: ""name_of_the_field_to_index"",
    field_schema: ""keyword"",
});
```

Rust:
```rust
client.create_payload_index(
    collection_name=""{collection_name}"",
    field_name=""name_of_the_field_to_index"",
    field_schema=models.TextIndexParams(
        type=""text"",
        tokenizer=models.TokenizerType.WORD,
        min_token_len=2,
        max_token_len=15,
        lowercase=True,
    ),
)
```

Typescript:
```typescript
client.createPayloadIndex(""{collection_name}"", {
    field_name: ""name_of_the_field_to_index"",
    field_schema: ""keyword"",
});
```

Each programming language may have specific syntax and method names, but the general idea remains the same: create a payload index for a specific field in the collection using the Qdrant library.","to set up the Qdrant [payload index](/documentation/concepts/indexing/#payload-index), so the search 

is more efficient. 



```python

from qdrant_client import models



client.create_payload_index(

    collection_name=""my_collection"",

    field_name=""metadata.library"",

    field_type=models.PayloadSchemaType.KEYWORD,

)

```



The payload index is not the only thing we want to change. Since none of the search
client

    .createPayloadIndexAsync(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        PayloadSchemaType.Keyword,

        null,

        null,

        null,

        null)

    .get();

```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");

```



Available field types are:
const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createPayloadIndex(""{collection_name}"", {

  field_name: ""name_of_the_field_to_index"",

  field_schema: {

    type: ""text"",

    tokenizer: ""word"",

    min_token_len: 2,

    max_token_len: 15,

    lowercase: true,

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,

        TokenizerType,
client = QdrantClient(host=""localhost"", port=6333)



client.create_payload_index(

    collection_name=""{collection_name}"",

    field_name=""name_of_the_field_to_index"",

    field_schema=models.TextIndexParams(

        type=""text"",

        tokenizer=models.TokenizerType.WORD,

        min_token_len=2,

        max_token_len=15,

        lowercase=True,

    ),

)

```



```typescript

import { QdrantClient, Schemas } from ""@qdrant/js-client-rest"";
```http

PUT /collections/{collection_name}/index

{

    ""field_name"": ""name_of_the_field_to_index"",

    ""field_schema"": ""keyword""

}

```



```python

client.create_payload_index(

    collection_name=""{collection_name}"",

    field_name=""name_of_the_field_to_index"",

    field_schema=""keyword"",

)

```



```typescript

client.createPayloadIndex(""{collection_name}"", {

  field_name: ""name_of_the_field_to_index"",

  field_schema: ""keyword"",

});

```



```rust

use qdrant_client::qdrant::FieldType;","['to set up the Qdrant [payload index](/documentation/concepts/indexing/#payload-index), so the search \n\nis more efficient. \n\n\n\n```python\n\nfrom qdrant_client import models\n\n\n\nclient.create_payload_index(\n\n    collection_name=""my_collection"",\n\n    field_name=""metadata.library"",\n\n    field_type=models.PayloadSchemaType.KEYWORD,\n\n)\n\n```\n\n\n\nThe payload index is not the only thing we want to change. Since none of the search', 'client\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        PayloadSchemaType.Keyword,\n\n        null,\n\n        null,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");\n\n```\n\n\n\nAvailable field types are:', 'const client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createPayloadIndex(""{collection_name}"", {\n\n  field_name: ""name_of_the_field_to_index"",\n\n  field_schema: {\n\n    type: ""text"",\n\n    tokenizer: ""word"",\n\n    min_token_len: 2,\n\n    max_token_len: 15,\n\n    lowercase: true,\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\n\n        TokenizerType,', 'client = QdrantClient(host=""localhost"", port=6333)\n\n\n\nclient.create_payload_index(\n\n    collection_name=""{collection_name}"",\n\n    field_name=""name_of_the_field_to_index"",\n\n    field_schema=models.TextIndexParams(\n\n        type=""text"",\n\n        tokenizer=models.TokenizerType.WORD,\n\n        min_token_len=2,\n\n        max_token_len=15,\n\n        lowercase=True,\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient, Schemas } from ""@qdrant/js-client-rest"";', '```http\n\nPUT /collections/{collection_name}/index\n\n{\n\n    ""field_name"": ""name_of_the_field_to_index"",\n\n    ""field_schema"": ""keyword""\n\n}\n\n```\n\n\n\n```python\n\nclient.create_payload_index(\n\n    collection_name=""{collection_name}"",\n\n    field_name=""name_of_the_field_to_index"",\n\n    field_schema=""keyword"",\n\n)\n\n```\n\n\n\n```typescript\n\nclient.createPayloadIndex(""{collection_name}"", {\n\n  field_name: ""name_of_the_field_to_index"",\n\n  field_schema: ""keyword"",\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::qdrant::FieldType;']","Question: How can you create an index for a specific field in a payload using the Qdrant library?\n\nContext: to set up the Qdrant [payload index](/documentation/concepts/indexing/#payload-index), so the search 

is more efficient. 



```python

from qdrant_client import models



client.create_payload_index(

collection_name=""my_collection"",

field_name=""metadata.library"",

field_type=models.PayloadSchemaType.KEYWORD,

)

```



The payload index is not the only thing we want to change. Since none of the search
client

.createPayloadIndexAsync(

""{collection_name}"",

""name_of_the_field_to_index"",

PayloadSchemaType.Keyword,

null,

null,

null,

null)

.get();

```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");

```



Available field types are:
const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createPayloadIndex(""{collection_name}"", {

field_name: ""name_of_the_field_to_index"",

field_schema: {

type: ""text"",

tokenizer: ""word"",

min_token_len: 2,

max_token_len: 15,

lowercase: true,

},

});

```



```rust

use qdrant_client::{

client::QdrantClient,

qdrant::{

payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,

TokenizerType,
client = QdrantClient(host=""localhost"", port=6333)



client.create_payload_index(

collection_name=""{collection_name}"",

field_name=""name_of_the_field_to_index"",

field_schema=models.TextIndexParams(

type=""text"",

tokenizer=models.TokenizerType.WORD,

min_token_len=2,

max_token_len=15,

lowercase=True,

),

)

```



```typescript

import { QdrantClient, Schemas } from ""@qdrant/js-client-rest"";
```http

PUT /collections/{collection_name}/index

{

""field_name"": ""name_of_the_field_to_index"",

""field_schema"": ""keyword""

}

```



```python

client.create_payload_index(

collection_name=""{collection_name}"",

field_name=""name_of_the_field_to_index"",

field_schema=""keyword"",

)

```



```typescript

client.createPayloadIndex(""{collection_name}"", {

field_name: ""name_of_the_field_to_index"",

field_schema: ""keyword"",

});

```



```rust

use qdrant_client::qdrant::FieldType;\n\nAnswer:",How can you create an index for a specific field in a payload using the Qdrant library?,"To create an index for a specific field in a payload using the Qdrant library, you can utilize the `CreatePayloadIndexAsync` method provided by the client. This method takes in the parameters `collectionName` and `fieldName`. ","['to set up the Qdrant [payload index](/documentation/concepts/indexing/#payload-index), so the search \n\nis more efficient. \n\n\n\n```python\n\nfrom qdrant_client import models\n\n\n\nclient.create_payload_index(\n\n    collection_name=""my_collection"",\n\n    field_name=""metadata.library"",\n\n    field_type=models.PayloadSchemaType.KEYWORD,\n\n)\n\n```\n\n\n\nThe payload index is not the only thing we want to change. Since none of the search', 'client\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"",\n\n        ""name_of_the_field_to_index"",\n\n        PayloadSchemaType.Keyword,\n\n        null,\n\n        null,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");\n\n```\n\n\n\nAvailable field types are:', 'const client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n\n\nclient.createPayloadIndex(""{collection_name}"", {\n\n  field_name: ""name_of_the_field_to_index"",\n\n  field_schema: {\n\n    type: ""text"",\n\n    tokenizer: ""word"",\n\n    min_token_len: 2,\n\n    max_token_len: 15,\n\n    lowercase: true,\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\n\n        TokenizerType,', 'client = QdrantClient(host=""localhost"", port=6333)\n\n\n\nclient.create_payload_index(\n\n    collection_name=""{collection_name}"",\n\n    field_name=""name_of_the_field_to_index"",\n\n    field_schema=models.TextIndexParams(\n\n        type=""text"",\n\n        tokenizer=models.TokenizerType.WORD,\n\n        min_token_len=2,\n\n        max_token_len=15,\n\n        lowercase=True,\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient, Schemas } from ""@qdrant/js-client-rest"";', '```http\n\nPUT /collections/{collection_name}/index\n\n{\n\n    ""field_name"": ""name_of_the_field_to_index"",\n\n    ""field_schema"": ""keyword""\n\n}\n\n```\n\n\n\n```python\n\nclient.create_payload_index(\n\n    collection_name=""{collection_name}"",\n\n    field_name=""name_of_the_field_to_index"",\n\n    field_schema=""keyword"",\n\n)\n\n```\n\n\n\n```typescript\n\nclient.createPayloadIndex(""{collection_name}"", {\n\n  field_name: ""name_of_the_field_to_index"",\n\n  field_schema: ""keyword"",\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::qdrant::FieldType;']","to set up the Qdrant [payload index](/documentation/concepts/indexing/#payload-index), so the search 

is more efficient. 



```python

from qdrant_client import models



client.create_payload_index(

    collection_name=""my_collection"",

    field_name=""metadata.library"",

    field_type=models.PayloadSchemaType.KEYWORD,

)

```



The payload index is not the only thing we want to change. Since none of the search
client

    .createPayloadIndexAsync(

        ""{collection_name}"",

        ""name_of_the_field_to_index"",

        PayloadSchemaType.Keyword,

        null,

        null,

        null,

        null)

    .get();

```



```csharp

using Qdrant.Client;



var client = new QdrantClient(""localhost"", 6334);



await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""name_of_the_field_to_index"");

```



Available field types are:
const client = new QdrantClient({ host: ""localhost"", port: 6333 });



client.createPayloadIndex(""{collection_name}"", {

  field_name: ""name_of_the_field_to_index"",

  field_schema: {

    type: ""text"",

    tokenizer: ""word"",

    min_token_len: 2,

    max_token_len: 15,

    lowercase: true,

  },

});

```



```rust

use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,

        TokenizerType,
client = QdrantClient(host=""localhost"", port=6333)



client.create_payload_index(

    collection_name=""{collection_name}"",

    field_name=""name_of_the_field_to_index"",

    field_schema=models.TextIndexParams(

        type=""text"",

        tokenizer=models.TokenizerType.WORD,

        min_token_len=2,

        max_token_len=15,

        lowercase=True,

    ),

)

```



```typescript

import { QdrantClient, Schemas } from ""@qdrant/js-client-rest"";
```http

PUT /collections/{collection_name}/index

{

    ""field_name"": ""name_of_the_field_to_index"",

    ""field_schema"": ""keyword""

}

```



```python

client.create_payload_index(

    collection_name=""{collection_name}"",

    field_name=""name_of_the_field_to_index"",

    field_schema=""keyword"",

)

```



```typescript

client.createPayloadIndex(""{collection_name}"", {

  field_name: ""name_of_the_field_to_index"",

  field_schema: ""keyword"",

});

```



```rust

use qdrant_client::qdrant::FieldType;",0.11214953271028037,0.8888888888888888,0.19917012448132781,0.056338028169014086,0.46153846153846156,0.10041841004184099,0.08878504672897196,0.7037037037037037,0.15767634854771784,0.09345794392523364,0.7407407407407407,0.16597510373443983,0.9369155764579773,0.46836429834365845,0.6249905824661255,0.5354588031768799,0.7894893392920495,0.8580935597419739,173,5.088235294117647,0.3907683314691218
True,,3417.51,91d27868,What is the purpose of the Quantization?,"Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces. In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.","Quantization plays a crucial role in various fields, including quantum mechanics and information theory. In quantum physics, quantization refers to the process of discretizing certain properties such as energy levels, angular momentum, and electric charge. This discrete nature of quantized values is fundamental to understanding the behavior of particles at the quantum level.

Quantum entanglement, another concept in quantum physics, is closely related to quantization. Entanglement occurs when two or more particles become correlated in such a way that the state of one particle is dependent on the state of the other, regardless of the distance between them. This phenomenon defies classical intuition and is a key aspect of quantum mechanics.

In summary, quantization is essential in quantum physics for understanding the discrete nature of certain properties, while entanglement showcases the interconnectedness of quantum particles beyond classical boundaries.","Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.

In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.



There are tradeoffs associated with quantization.

On the one hand, quantization allows for significant reductions in storage requirements and faster search times.
Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear
Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever
. So without further ado, let's jump into this, Andrey, we're talking about binary quantization, but let's maybe start a step back. Why do we need any quantization at all? Why not just use original vectors?
## Quantum Quantization and Entanglement","['Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\n\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\n\n\n\nThere are tradeoffs associated with quantization.\n\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.', ""Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear"", ""Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever"", "". So without further ado, let's jump into this, Andrey, we're talking about binary quantization, but let's maybe start a step back. Why do we need any quantization at all? Why not just use original vectors?"", '## Quantum Quantization and Entanglement']","Question: What is the purpose of the Quantization?\n\nContext: Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.

In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.



There are tradeoffs associated with quantization.

On the one hand, quantization allows for significant reductions in storage requirements and faster search times.
Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear
Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever
. So without further ado, let's jump into this, Andrey, we're talking about binary quantization, but let's maybe start a step back. Why do we need any quantization at all? Why not just use original vectors?
## Quantum Quantization and Entanglement\n\nAnswer:",What is the purpose of the Quantization?,"Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces. In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.","['Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.\n\nIn the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.\n\n\n\nThere are tradeoffs associated with quantization.\n\nOn the one hand, quantization allows for significant reductions in storage requirements and faster search times.', ""Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear"", ""Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever"", "". So without further ado, let's jump into this, Andrey, we're talking about binary quantization, but let's maybe start a step back. Why do we need any quantization at all? Why not just use original vectors?"", '## Quantum Quantization and Entanglement']","Quantization is primarily used to reduce the memory footprint and accelerate the search process in high-dimensional vector spaces.

In the context of the Qdrant, quantization allows you to optimize the search engine for specific use cases, striking a balance between accuracy, storage efficiency, and search speed.



There are tradeoffs associated with quantization.

On the one hand, quantization allows for significant reductions in storage requirements and faster search times.
Right, so our choice of quantization is mostly defined by available CPU instructions we can apply to perform those computations. In case of binary quantization, it's straightforward and very simple. That's why we like binary quantization so much. In case of, for example, four bit quantization, it is not as clear which operation we should use. It's not yet clear
Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever
. So without further ado, let's jump into this, Andrey, we're talking about binary quantization, but let's maybe start a step back. Why do we need any quantization at all? Why not just use original vectors?
## Quantum Quantization and Entanglement",0.10317460317460317,0.3170731707317073,0.155688622754491,0.008,0.025,0.012121212121212121,0.06349206349206349,0.1951219512195122,0.09580838323353293,0.09523809523809523,0.2926829268292683,0.1437125748502994,0.5849984884262085,0.5081318616867065,0.5470848679542542,0.5268893837928772,0.6843710669449398,0.9647080234717578,138,3.0,0.6027992055433053
True,,5644.289,62d5aeef,How can the retrieval quality of an approximation be measured in the context of semantic search?,"The retrieval quality of an approximation in semantic search can be measured using various quality metrics. Some of these metrics include Precision@k, Mean Reciprocal Rank (MRR), and DCG and NDCG.","In the context of semantic search utilizing Approximate Nearest Neighbors (ANN) algorithms, the retrieval quality of an approximation can be measured using various metrics. Some common quality metrics to assess the performance of semantic search algorithms include:

1. **Precision@k**: Precision@k measures the proportion of relevant documents among the top-k search results. It indicates how many of the retrieved items are actually relevant to the query at different levels of recall.

2. **Mean Reciprocal Rank (MRR)**: MRR calculates the average of the reciprocal ranks of the first relevant document. It provides a single metric that considers the position of the relevant items in the ranked list of search results.

3. **Recall**: Recall measures the proportion of relevant documents that were retrieved compared to the total number of relevant documents in the dataset. It helps evaluate the completeness of the retrieval process.

4. **F1 Score**: The F1 Score is the harmonic mean of precision and recall. It provides a balance between precision and recall, offering a single metric to evaluate the quality of the retrieval results.

5. **Mean Average Precision (MAP)**: MAP calculates the average precision across multiple queries. It considers the precision at various recall levels and provides a comprehensive evaluation measure for the retrieval quality.

By using these metrics, you can quantitatively evaluate the performance of the semantic search algorithm and assess the quality of retrieval achieved through the ANN approximation. These metrics help in understanding how well the algorithm retrieves relevant information and can guide improvements in the search quality.","perform pure kNN search. Instead, they use **Approximate Nearest Neighbors** (ANN) algorithms, which are much faster than the exact search, 

but can return suboptimal results. We can also **measure the retrieval quality of that approximation** which also contributes to the overall

search quality.



### Quality metrics
to do it. 



## Wrapping up



Assessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.

your search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, 

even in an automated way, as part of your CI/CD pipeline.
search quality.



### Quality metrics



There are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), 

are based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),
---

title: Measure retrieval quality

weight: 21

---



# Measure retrieval quality



| Time: 30 min | Level: Intermediate |  |    |

|--------------|---------------------|--|----|



Semantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might

be far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another
them with the ground truth. In that setup, **finding the most similar documents is implemented as full kNN search, without any approximation**.

As a result, we can measure the quality of the embeddings themselves, without the influence of the ANN algorithm.



## Retrieval quality



Embeddings quality is indeed the most important factor in the semantic search quality. However, vector search engines, such as Qdrant, do not","['perform pure kNN search. Instead, they use **Approximate Nearest Neighbors** (ANN) algorithms, which are much faster than the exact search, \n\nbut can return suboptimal results. We can also **measure the retrieval quality of that approximation** which also contributes to the overall\n\nsearch quality.\n\n\n\n### Quality metrics', 'to do it. \n\n\n\n## Wrapping up\n\n\n\nAssessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.\n\nyour search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, \n\neven in an automated way, as part of your CI/CD pipeline.', 'search quality.\n\n\n\n### Quality metrics\n\n\n\nThere are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), \n\nare based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),', '---\n\ntitle: Measure retrieval quality\n\nweight: 21\n\n---\n\n\n\n# Measure retrieval quality\n\n\n\n| Time: 30 min | Level: Intermediate |  |    |\n\n|--------------|---------------------|--|----|\n\n\n\nSemantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might\n\nbe far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another', 'them with the ground truth. In that setup, **finding the most similar documents is implemented as full kNN search, without any approximation**.\n\nAs a result, we can measure the quality of the embeddings themselves, without the influence of the ANN algorithm.\n\n\n\n## Retrieval quality\n\n\n\nEmbeddings quality is indeed the most important factor in the semantic search quality. However, vector search engines, such as Qdrant, do not']","Question: How can the retrieval quality of an approximation be measured in the context of semantic search?\n\nContext: perform pure kNN search. Instead, they use **Approximate Nearest Neighbors** (ANN) algorithms, which are much faster than the exact search, 

but can return suboptimal results. We can also **measure the retrieval quality of that approximation** which also contributes to the overall

search quality.



### Quality metrics
to do it. 



## Wrapping up



Assessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.

your search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, 

even in an automated way, as part of your CI/CD pipeline.
search quality.



### Quality metrics



There are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), 

are based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),
---

title: Measure retrieval quality

weight: 21

---



# Measure retrieval quality



| Time: 30 min | Level: Intermediate |  |    |

|--------------|---------------------|--|----|



Semantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might

be far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another
them with the ground truth. In that setup, **finding the most similar documents is implemented as full kNN search, without any approximation**.

As a result, we can measure the quality of the embeddings themselves, without the influence of the ANN algorithm.



## Retrieval quality



Embeddings quality is indeed the most important factor in the semantic search quality. However, vector search engines, such as Qdrant, do not\n\nAnswer:",How can the retrieval quality of an approximation be measured in the context of semantic search?,"The retrieval quality of an approximation in semantic search can be measured using various quality metrics. Some of these metrics include Precision@k, Mean Reciprocal Rank (MRR), and DCG and NDCG.","['perform pure kNN search. Instead, they use **Approximate Nearest Neighbors** (ANN) algorithms, which are much faster than the exact search, \n\nbut can return suboptimal results. We can also **measure the retrieval quality of that approximation** which also contributes to the overall\n\nsearch quality.\n\n\n\n### Quality metrics', 'to do it. \n\n\n\n## Wrapping up\n\n\n\nAssessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.\n\nyour search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, \n\neven in an automated way, as part of your CI/CD pipeline.', 'search quality.\n\n\n\n### Quality metrics\n\n\n\nThere are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), \n\nare based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),', '---\n\ntitle: Measure retrieval quality\n\nweight: 21\n\n---\n\n\n\n# Measure retrieval quality\n\n\n\n| Time: 30 min | Level: Intermediate |  |    |\n\n|--------------|---------------------|--|----|\n\n\n\nSemantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might\n\nbe far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another', 'them with the ground truth. In that setup, **finding the most similar documents is implemented as full kNN search, without any approximation**.\n\nAs a result, we can measure the quality of the embeddings themselves, without the influence of the ANN algorithm.\n\n\n\n## Retrieval quality\n\n\n\nEmbeddings quality is indeed the most important factor in the semantic search quality. However, vector search engines, such as Qdrant, do not']","perform pure kNN search. Instead, they use **Approximate Nearest Neighbors** (ANN) algorithms, which are much faster than the exact search, 

but can return suboptimal results. We can also **measure the retrieval quality of that approximation** which also contributes to the overall

search quality.



### Quality metrics
to do it. 



## Wrapping up



Assessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.

your search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, 

even in an automated way, as part of your CI/CD pipeline.
search quality.



### Quality metrics



There are various ways of how quantify the quality of semantic search. Some of them, such as [Precision@k](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_k), 

are based on the number of relevant documents in the top-k search results. Others, such as [Mean Reciprocal Rank (MRR)](https://en.wikipedia.org/wiki/Mean_reciprocal_rank),
---

title: Measure retrieval quality

weight: 21

---



# Measure retrieval quality



| Time: 30 min | Level: Intermediate |  |    |

|--------------|---------------------|--|----|



Semantic search pipelines are as good as the embeddings they use. If your model cannot properly represent input data, similar objects might

be far away from each other in the vector space. No surprise, that the search results will be poor in this case. There is, however, another
them with the ground truth. In that setup, **finding the most similar documents is implemented as full kNN search, without any approximation**.

As a result, we can measure the quality of the embeddings themselves, without the influence of the ANN algorithm.



## Retrieval quality



Embeddings quality is indeed the most important factor in the semantic search quality. However, vector search engines, such as Qdrant, do not",0.12385321100917432,0.9310344827586207,0.21862348178137656,0.06912442396313365,0.5357142857142857,0.12244897959183673,0.0963302752293578,0.7241379310344828,0.17004048582995954,0.11926605504587157,0.896551724137931,0.2105263157894737,0.8867661952972412,0.5365023612976074,0.6625738143920898,0.5929104685783386,0.5109593061885487,0.8348561995662749,253,8.161290322580646,0.5588481235671164
True,,2493.308,1885023d,Why does Qdrant deliberately exclude libraries or algorithm implementations in their benchmark comparisons?,"Qdrant excludes libraries or algorithm implementations in their benchmark comparisons because their primary focus is on vector databases. By limiting their comparisons to open-source solutions and avoiding external cloud components, Qdrant ensures hardware parity and minimizes biases. This allows them to provide accurate and unbiased benchmarks specifically tailored to the performance of vector databases, enabling users to make informed decisions based on the data provided.","Qdrant deliberately excludes libraries or algorithm implementations in their benchmark comparisons because they want to focus solely on testing open-source vector databases. By doing so, they aim to ensure that their benchmarks are accessible and reproducible by everyone, allowing users to run them on their own hardware. This approach helps in maintaining hardware parity and avoiding biases from external cloud components. Additionally, by concentrating on open-source solutions, Qdrant can gather valuable input from the community, encourage contributions, and share best practices. Overall, this strategy allows Qdrant to improve its performance based on the collected information and feedback, ultimately benefitting all users of the platform.","That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.



This is not the final list, and we’ll continue benchmarking as many different engines as possible.



## How to reproduce the benchmark?



The source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.
We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.



In order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.



Additionally, Qdrant uses a bunch of internal heuristics to optimize the performance.

To better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.

With this information, we can make Qdrant faster for everyone.
Qdrant avoids all these problems and also benefits from the speed boost, as it implements an advanced [query planning strategy](/documentation/search/#query-planning).



<aside role=""status"">The Filtering Benchmark is all about changes in performance between filter and un-filtered queries. Please refer to the search benchmark for absolute speed comparison.</aside>
### What Hasn't Changed?



#### Our Principles of Benchmarking



At Qdrant all code stays open-source. We ensure our benchmarks are accessible for everyone, allowing you to run them on your own hardware. Your input matters to us, and contributions and sharing of best practices are welcome!





Our benchmarks are strictly limited to open-source solutions, ensuring hardware parity and avoiding biases from external cloud components.
. Then the benchmarks that I specified were only against PG vector, and we did not benchmark it any further because the speed and the accuracy that Qdrant provides, I think it is already covering our use case and it is way more faster than we thought the solution could be. So right now we did not benchmark against any other vector database or any other solution.","['That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.\n\n\n\nThis is not the final list, and we’ll continue benchmarking as many different engines as possible.\n\n\n\n## How to reproduce the benchmark?\n\n\n\nThe source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.', 'We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.\n\n\n\nIn order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.\n\n\n\nAdditionally, Qdrant uses a bunch of internal heuristics to optimize the performance.\n\nTo better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.\n\nWith this information, we can make Qdrant faster for everyone.', 'Qdrant avoids all these problems and also benefits from the speed boost, as it implements an advanced [query planning strategy](/documentation/search/#query-planning).\n\n\n\n<aside role=""status"">The Filtering Benchmark is all about changes in performance between filter and un-filtered queries. Please refer to the search benchmark for absolute speed comparison.</aside>', ""### What Hasn't Changed?\n\n\n\n#### Our Principles of Benchmarking\n\n\n\nAt Qdrant all code stays open-source. We ensure our benchmarks are accessible for everyone, allowing you to run them on your own hardware. Your input matters to us, and contributions and sharing of best practices are welcome!\n\n\n\n\n\nOur benchmarks are strictly limited to open-source solutions, ensuring hardware parity and avoiding biases from external cloud components."", '. Then the benchmarks that I specified were only against PG vector, and we did not benchmark it any further because the speed and the accuracy that Qdrant provides, I think it is already covering our use case and it is way more faster than we thought the solution could be. So right now we did not benchmark against any other vector database or any other solution.']","Question: Why does Qdrant deliberately exclude libraries or algorithm implementations in their benchmark comparisons?\n\nContext: That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.



This is not the final list, and we’ll continue benchmarking as many different engines as possible.



## How to reproduce the benchmark?



The source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.
We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.



In order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.



Additionally, Qdrant uses a bunch of internal heuristics to optimize the performance.

To better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.

With this information, we can make Qdrant faster for everyone.
Qdrant avoids all these problems and also benefits from the speed boost, as it implements an advanced [query planning strategy](/documentation/search/#query-planning).



<aside role=""status"">The Filtering Benchmark is all about changes in performance between filter and un-filtered queries. Please refer to the search benchmark for absolute speed comparison.</aside>
### What Hasn't Changed?



#### Our Principles of Benchmarking



At Qdrant all code stays open-source. We ensure our benchmarks are accessible for everyone, allowing you to run them on your own hardware. Your input matters to us, and contributions and sharing of best practices are welcome!





Our benchmarks are strictly limited to open-source solutions, ensuring hardware parity and avoiding biases from external cloud components.
. Then the benchmarks that I specified were only against PG vector, and we did not benchmark it any further because the speed and the accuracy that Qdrant provides, I think it is already covering our use case and it is way more faster than we thought the solution could be. So right now we did not benchmark against any other vector database or any other solution.\n\nAnswer:",Why does Qdrant deliberately exclude libraries or algorithm implementations in their benchmark comparisons?,"Qdrant excludes libraries or algorithm implementations in their benchmark comparisons because their primary focus is on vector databases. By limiting their comparisons to open-source solutions and avoiding external cloud components, Qdrant ensures hardware parity and minimizes biases. This allows them to provide accurate and unbiased benchmarks specifically tailored to the performance of vector databases, enabling users to make informed decisions based on the data provided.","['That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.\n\n\n\nThis is not the final list, and we’ll continue benchmarking as many different engines as possible.\n\n\n\n## How to reproduce the benchmark?\n\n\n\nThe source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.', 'We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.\n\n\n\nIn order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.\n\n\n\nAdditionally, Qdrant uses a bunch of internal heuristics to optimize the performance.\n\nTo better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.\n\nWith this information, we can make Qdrant faster for everyone.', 'Qdrant avoids all these problems and also benefits from the speed boost, as it implements an advanced [query planning strategy](/documentation/search/#query-planning).\n\n\n\n<aside role=""status"">The Filtering Benchmark is all about changes in performance between filter and un-filtered queries. Please refer to the search benchmark for absolute speed comparison.</aside>', ""### What Hasn't Changed?\n\n\n\n#### Our Principles of Benchmarking\n\n\n\nAt Qdrant all code stays open-source. We ensure our benchmarks are accessible for everyone, allowing you to run them on your own hardware. Your input matters to us, and contributions and sharing of best practices are welcome!\n\n\n\n\n\nOur benchmarks are strictly limited to open-source solutions, ensuring hardware parity and avoiding biases from external cloud components."", '. Then the benchmarks that I specified were only against PG vector, and we did not benchmark it any further because the speed and the accuracy that Qdrant provides, I think it is already covering our use case and it is way more faster than we thought the solution could be. So right now we did not benchmark against any other vector database or any other solution.']","That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.



This is not the final list, and we’ll continue benchmarking as many different engines as possible.



## How to reproduce the benchmark?



The source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.
We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.



In order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.



Additionally, Qdrant uses a bunch of internal heuristics to optimize the performance.

To better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.

With this information, we can make Qdrant faster for everyone.
Qdrant avoids all these problems and also benefits from the speed boost, as it implements an advanced [query planning strategy](/documentation/search/#query-planning).



<aside role=""status"">The Filtering Benchmark is all about changes in performance between filter and un-filtered queries. Please refer to the search benchmark for absolute speed comparison.</aside>
### What Hasn't Changed?



#### Our Principles of Benchmarking



At Qdrant all code stays open-source. We ensure our benchmarks are accessible for everyone, allowing you to run them on your own hardware. Your input matters to us, and contributions and sharing of best practices are welcome!





Our benchmarks are strictly limited to open-source solutions, ensuring hardware parity and avoiding biases from external cloud components.
. Then the benchmarks that I specified were only against PG vector, and we did not benchmark it any further because the speed and the accuracy that Qdrant provides, I think it is already covering our use case and it is way more faster than we thought the solution could be. So right now we did not benchmark against any other vector database or any other solution.",0.4563106796116505,0.734375,0.562874251497006,0.19607843137254902,0.31746031746031744,0.24242424242424243,0.30097087378640774,0.484375,0.3712574850299401,0.39805825242718446,0.640625,0.49101796407185627,0.9630480408668518,0.6878765821456909,0.7554425597190857,0.7200781106948853,0.9855358225526288,0.8572337270403901,104,1.6,0.3294554654957347
True,,4767.228,56497eee,What is the primary purpose of a Vector Database and why would someone choose to use it over traditional databases?,"A Vector Database is a specialized database system that is specifically designed for efficiently indexing, querying, and retrieving high-dimensional vector data. The primary purpose of a Vector Database is to enable advanced data analysis and similarity-search operations that go beyond the capabilities of traditional, structured query approaches used in conventional databases.","The primary purpose of a Vector Database is to efficiently index, query, and retrieve high-dimensional vector data, enabling advanced data analysis and similarity-search operations that go beyond traditional structured query approaches of conventional databases. 

Someone might choose to use a Vector Database over traditional databases for several reasons:

1. **Efficient storage and indexing of high-dimensional data**: Vector databases are optimized for handling data represented as vectors, making them more efficient for storing and indexing high-dimensional data.

2. **Handling large-scale datasets**: Vector databases are capable of managing large-scale datasets with billions of data points, providing scalability for growing data needs.

3. **Support for real-time analytics and queries**: Vector databases allow for real-time analytics and queries, enabling quick access to insights and results.

4. **Handling complex data types**: Vector databases can handle vectors derived from complex data types such as images, videos, and natural language text, making them suitable for diverse data sources.

5. **Improved performance in machine learning and AI applications**: Vector databases offer improved performance and reduced latency in applications related to machine learning and artificial intelligence, enhancing the efficiency of these processes.

6. **Reduced development and deployment time and cost**: Using a pre-built vector database can save time and resources compared to building a custom solution, as they come with specialized features and optimizations for vector data handling.

Overall, the specialized nature of vector databases, their efficiency in handling high-dimensional data, support for real-time analytics, and ability to work with complex data types make them a valuable choice for certain use cases over traditional databases.","aliases: [ /blog/what-is-a-vector-database/ ]

---



> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.





## Why use a Vector Database?



The data flood is real.
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
4. Support for real-time analytics and queries.

5. Ability to handle vectors derived from complex data types such as images, videos, and natural language text.

6. Improved performance and reduced latency in machine learning and AI applications.

7. Reduced development and deployment time and cost compared to building a custom solution.



Keep in mind that the specific benefits of using a vector database may vary depending on the
And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query
Those priorities lead to different architectural decisions that are not reproducible in a general-purpose database, even if it has vector index support.





###### Having a dedicated vector database requires duplication of data.



By their very nature, vector embeddings are derivatives of the primary source data.","['aliases: [ /blog/what-is-a-vector-database/ ]\n\n---\n\n\n\n> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.\n\n\n\n\n\n## Why use a Vector Database?\n\n\n\nThe data flood is real.', 'advantage of their efficient indexing and searching techniques, vector databases enable faster \n\nand more accurate retrieval of unstructured data already represented as vectors, which can \n\nhelp put in front of users the most relevant results to their queries.\n\n\n\nIn addition, other benefits of using vector databases include:\n\n1. Efficient storage and indexing of high-dimensional data.\n\n3. Ability to handle large-scale datasets with billions of data points.\n\n4. Support for real-time analytics and queries.', '4. Support for real-time analytics and queries.\n\n5. Ability to handle vectors derived from complex data types such as images, videos, and natural language text.\n\n6. Improved performance and reduced latency in machine learning and AI applications.\n\n7. Reduced development and deployment time and cost compared to building a custom solution.\n\n\n\nKeep in mind that the specific benefits of using a vector database may vary depending on the', ""And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query"", 'Those priorities lead to different architectural decisions that are not reproducible in a general-purpose database, even if it has vector index support.\n\n\n\n\n\n###### Having a dedicated vector database requires duplication of data.\n\n\n\nBy their very nature, vector embeddings are derivatives of the primary source data.']","Question: What is the primary purpose of a Vector Database and why would someone choose to use it over traditional databases?\n\nContext: aliases: [ /blog/what-is-a-vector-database/ ]

---



> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.





## Why use a Vector Database?



The data flood is real.
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
4. Support for real-time analytics and queries.

5. Ability to handle vectors derived from complex data types such as images, videos, and natural language text.

6. Improved performance and reduced latency in machine learning and AI applications.

7. Reduced development and deployment time and cost compared to building a custom solution.



Keep in mind that the specific benefits of using a vector database may vary depending on the
And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query
Those priorities lead to different architectural decisions that are not reproducible in a general-purpose database, even if it has vector index support.





###### Having a dedicated vector database requires duplication of data.



By their very nature, vector embeddings are derivatives of the primary source data.\n\nAnswer:",What is the primary purpose of a Vector Database and why would someone choose to use it over traditional databases?,"A Vector Database is a specialized database system that is specifically designed for efficiently indexing, querying, and retrieving high-dimensional vector data. The primary purpose of a Vector Database is to enable advanced data analysis and similarity-search operations that go beyond the capabilities of traditional, structured query approaches used in conventional databases.","['aliases: [ /blog/what-is-a-vector-database/ ]\n\n---\n\n\n\n> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.\n\n\n\n\n\n## Why use a Vector Database?\n\n\n\nThe data flood is real.', 'advantage of their efficient indexing and searching techniques, vector databases enable faster \n\nand more accurate retrieval of unstructured data already represented as vectors, which can \n\nhelp put in front of users the most relevant results to their queries.\n\n\n\nIn addition, other benefits of using vector databases include:\n\n1. Efficient storage and indexing of high-dimensional data.\n\n3. Ability to handle large-scale datasets with billions of data points.\n\n4. Support for real-time analytics and queries.', '4. Support for real-time analytics and queries.\n\n5. Ability to handle vectors derived from complex data types such as images, videos, and natural language text.\n\n6. Improved performance and reduced latency in machine learning and AI applications.\n\n7. Reduced development and deployment time and cost compared to building a custom solution.\n\n\n\nKeep in mind that the specific benefits of using a vector database may vary depending on the', ""And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query"", 'Those priorities lead to different architectural decisions that are not reproducible in a general-purpose database, even if it has vector index support.\n\n\n\n\n\n###### Having a dedicated vector database requires duplication of data.\n\n\n\nBy their very nature, vector embeddings are derivatives of the primary source data.']","aliases: [ /blog/what-is-a-vector-database/ ]

---



> A Vector Database is a specialized database system designed for efficiently indexing, querying, and retrieving high-dimensional vector data. Those systems enable advanced data analysis and similarity-search operations that extend well beyond the traditional, structured query approach of conventional databases.





## Why use a Vector Database?



The data flood is real.
advantage of their efficient indexing and searching techniques, vector databases enable faster 

and more accurate retrieval of unstructured data already represented as vectors, which can 

help put in front of users the most relevant results to their queries.



In addition, other benefits of using vector databases include:

1. Efficient storage and indexing of high-dimensional data.

3. Ability to handle large-scale datasets with billions of data points.

4. Support for real-time analytics and queries.
4. Support for real-time analytics and queries.

5. Ability to handle vectors derived from complex data types such as images, videos, and natural language text.

6. Improved performance and reduced latency in machine learning and AI applications.

7. Reduced development and deployment time and cost compared to building a custom solution.



Keep in mind that the specific benefits of using a vector database may vary depending on the
And if we look at this system on the bottom, you see the typical data ingestion. So the user gives a document, we slice it to small chunks, and we compute a numerical representation with vector embeddings and store those in a vector database. Why a vector database? Because it's really efficient to retrieve vectors from it when we get users query
Those priorities lead to different architectural decisions that are not reproducible in a general-purpose database, even if it has vector index support.





###### Having a dedicated vector database requires duplication of data.



By their very nature, vector embeddings are derivatives of the primary source data.",0.16216216216216217,0.875,0.27361563517915305,0.11627906976744186,0.6382978723404256,0.19672131147540986,0.11583011583011583,0.625,0.1954397394136808,0.15444015444015444,0.8333333333333334,0.26058631921824105,0.9311972856521606,0.579971969127655,0.7470473647117615,0.6529920101165771,0.7510990067696963,0.9984589008963667,256,5.019607843137255,0.3459712703570583
True,,6813.75,58a67c3c,"How does oversampling impact the performance of machine learning models, especially in the context of imbalanced datasets?","Oversampling is a technique used in machine learning to address imbalances in datasets, where one class significantly outnumbers others. This imbalance can lead to skewed model performance, as the model may favor the majority class at the expense of minority classes. By generating additional samples from the minority classes, oversampling helps to equalize the representation of different classes in the training dataset.","Oversampling can have a significant impact on the performance of machine learning models, especially in the context of imbalanced datasets. Here are some key points to consider:

1. **Improvement in Model Performance:** By oversampling the minority class, the model gets exposed to more examples of that class during training. This helps the model learn a more robust representation of the minority class, leading to improved performance metrics such as precision, recall, and F1-score.

2. **Balanced Representation:** Oversampling helps in creating a more balanced dataset where all classes are represented more equally. This balanced representation can prevent the model from being biased towards the majority class and can lead to better generalization on unseen data.

3. **Mitigating Bias:** Imbalanced datasets can introduce bias in the model towards the majority class. Oversampling helps in mitigating this bias by providing the model with sufficient examples from all classes to learn from.

4. **Impact on Model Fairness:** Oversampling plays a crucial role in ensuring model fairness by giving equal importance to all classes in the dataset. This can be especially important in sensitive applications where fairness and equity are paramount.

5. **Considerations:** While oversampling generally leads to improved performance, it is essential to monitor the impact on the overall model performance and avoid overfitting. Additionally, selecting the right oversampling technique and tuning the parameters appropriately are crucial for achieving optimal results.

In summary, oversampling is a powerful technique to address imbalances in datasets and can significantly enhance the performance and fairness of machine learning models, especially when dealing with imbalanced data.","The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one.
acc = average_accuracy.pivot(

        index=""limit"", columns=[""oversampling"", ""rescore""], values=""accuracy""

    )

    print(acc)

```



#### Impact of Oversampling



You can use oversampling in machine learning to counteract imbalances in datasets.

It works well when one class significantly outnumbers others. This imbalance

can skew the performance of models, which favors the majority class at the

expense of others. By creating additional samples from the minority classes,
oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
We see that ""correct"" results i.e. recall increases as the number of potential ""candidates"" increase (limit x oversampling). To highlight the impact of changing the `limit`, different limit values are broken apart into different curves. For example, we see that the lowest recall for limit 50 is around 94 correct, with 100 candidates. This also implies we used an oversampling of 2.0



As oversampling increases, we see a general improvement in results – but that does not hold in every case.
Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects.



![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png)



### Leveraging Binary Quantization: Best Practices","[""The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one."", 'acc = average_accuracy.pivot(\n\n        index=""limit"", columns=[""oversampling"", ""rescore""], values=""accuracy""\n\n    )\n\n    print(acc)\n\n```\n\n\n\n#### Impact of Oversampling\n\n\n\nYou can use oversampling in machine learning to counteract imbalances in datasets.\n\nIt works well when one class significantly outnumbers others. This imbalance\n\ncan skew the performance of models, which favors the majority class at the\n\nexpense of others. By creating additional samples from the minority classes,', 'oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.', 'We see that ""correct"" results i.e. recall increases as the number of potential ""candidates"" increase (limit x oversampling). To highlight the impact of changing the `limit`, different limit values are broken apart into different curves. For example, we see that the lowest recall for limit 50 is around 94 correct, with 100 candidates. This also implies we used an oversampling of 2.0\n\n\n\nAs oversampling increases, we see a general improvement in results – but that does not hold in every case.', 'Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects.\n\n\n\n![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png)\n\n\n\n### Leveraging Binary Quantization: Best Practices']","Question: How does oversampling impact the performance of machine learning models, especially in the context of imbalanced datasets?\n\nContext: The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one.
acc = average_accuracy.pivot(

index=""limit"", columns=[""oversampling"", ""rescore""], values=""accuracy""

)

print(acc)

```



#### Impact of Oversampling



You can use oversampling in machine learning to counteract imbalances in datasets.

It works well when one class significantly outnumbers others. This imbalance

can skew the performance of models, which favors the majority class at the

expense of others. By creating additional samples from the minority classes,
oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
We see that ""correct"" results i.e. recall increases as the number of potential ""candidates"" increase (limit x oversampling). To highlight the impact of changing the `limit`, different limit values are broken apart into different curves. For example, we see that the lowest recall for limit 50 is around 94 correct, with 100 candidates. This also implies we used an oversampling of 2.0



As oversampling increases, we see a general improvement in results – but that does not hold in every case.
Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects.



![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png)



### Leveraging Binary Quantization: Best Practices\n\nAnswer:","How does oversampling impact the performance of machine learning models, especially in the context of imbalanced datasets?","Oversampling is a technique used in machine learning to address imbalances in datasets, where one class significantly outnumbers others. This imbalance can lead to skewed model performance, as the model may favor the majority class at the expense of minority classes. By generating additional samples from the minority classes, oversampling helps to equalize the representation of different classes in the training dataset.","[""The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one."", 'acc = average_accuracy.pivot(\n\n        index=""limit"", columns=[""oversampling"", ""rescore""], values=""accuracy""\n\n    )\n\n    print(acc)\n\n```\n\n\n\n#### Impact of Oversampling\n\n\n\nYou can use oversampling in machine learning to counteract imbalances in datasets.\n\nIt works well when one class significantly outnumbers others. This imbalance\n\ncan skew the performance of models, which favors the majority class at the\n\nexpense of others. By creating additional samples from the minority classes,', 'oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.', 'We see that ""correct"" results i.e. recall increases as the number of potential ""candidates"" increase (limit x oversampling). To highlight the impact of changing the `limit`, different limit values are broken apart into different curves. For example, we see that the lowest recall for limit 50 is around 94 correct, with 100 candidates. This also implies we used an oversampling of 2.0\n\n\n\nAs oversampling increases, we see a general improvement in results – but that does not hold in every case.', 'Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects.\n\n\n\n![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png)\n\n\n\n### Leveraging Binary Quantization: Best Practices']","The screenshot showcases the effect of oversampling on model performance metrics. While the actual metrics aren't shown, we expect to see improvements in measures such as precision, recall, or F1-score. These improvements illustrate the effectiveness of oversampling in creating a more balanced dataset. It allows the model to learn a better representation of all classes, not just the dominant one.
acc = average_accuracy.pivot(

        index=""limit"", columns=[""oversampling"", ""rescore""], values=""accuracy""

    )

    print(acc)

```



#### Impact of Oversampling



You can use oversampling in machine learning to counteract imbalances in datasets.

It works well when one class significantly outnumbers others. This imbalance

can skew the performance of models, which favors the majority class at the

expense of others. By creating additional samples from the minority classes,
oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.
We see that ""correct"" results i.e. recall increases as the number of potential ""candidates"" increase (limit x oversampling). To highlight the impact of changing the `limit`, different limit values are broken apart into different curves. For example, we see that the lowest recall for limit 50 is around 94 correct, with 100 candidates. This also implies we used an oversampling of 2.0



As oversampling increases, we see a general improvement in results – but that does not hold in every case.
Without an explicit code snippet or output, we focus on the role of oversampling in model fairness and performance. Through graphical representation, you can set up before-and-after comparisons. These comparisons illustrate the contribution to machine learning projects.



![Measuring the impact of oversampling](/blog/openai/Oversampling_Impact.png)



### Leveraging Binary Quantization: Best Practices",0.18220338983050846,0.7818181818181819,0.29553264604810997,0.07659574468085106,0.3333333333333333,0.1245674740484429,0.09745762711864407,0.41818181818181815,0.15807560137457044,0.17372881355932204,0.7454545454545455,0.28178694158075607,0.8355451822280884,0.5633835196495056,0.6421827673912048,0.6002078652381897,0.7238924450425924,0.9992672425578348,257,4.145161290322581,0.6338081854431905
True,,3760.389,a85fa67a,How does binary quantization work in the context of vector comparisons?,"Binary quantization is a method used in indexing and data compression, particularly by Qdrant, that involves splitting a data point's vector in half at a certain point. This process essentially divides the vector into two parts, marking everything above the split point as ""1"" and everything below as ""0"". The result is a string of bits that represents the original vector in a compressed form. This quantized code is much smaller and easier to compare. Especially for OpenAI embeddings, this type of quantization has proven to achieve a massive performance improvement at a lower cost of accuracy.","Binary quantization in the context of vector comparisons involves converting floating point elements of a vector into binary representation, where each element is represented as either zero or one based on its sign. This process allows for the approximation of vector operations such as dot product or cosine similarity using Hamming distance, which is efficient to compute and requires only two optimized CPU instructions.

The main advantage of binary quantization is its speed, as it can achieve up to a 40x speedup compared to using original vectors. This speedup is particularly noticeable in high-dimensional vectors with a centered distribution of components.

The process of binary quantization involves splitting the data points in half at a certain threshold, marking values above the threshold as ""1"" and those below as ""0"". This results in a binary representation of the original vector, which can streamline vector comparisons and indexing processes.

While binary quantization offers enhanced performance and efficiency, it may not be compatible with all models. However, for models where it is applicable, binary quantization can significantly reduce storage size and boost speed, making it a valuable technique for optimizing vector operations and proximity graph searches.","Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever
This is the fastest quantization method, since it lets you perform a vector comparison with a few CPU instructions.



Binary quantization can achieve up to a **40x** speedup compared to the original vectors.



However, binary quantization is only efficient for high-dimensional vectors and require a centered distribution of vector components. 



At the moment, binary quantization shows good accuracy results with the following models:
Think of each data point as a ruler. Binary quantization splits this ruler in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This [binarization](https://deepai.org/machine-learning-glossary-and-terms/binarization) process results in a string of bits, representing the original vector.







![](/articles_data/what-is-a-vector-database/Binary-Quant.png)
> -- Andrey Vasnetsov



Ever wonder why we need quantization for vector indexes? Andrey Vasnetsov explains the complexities and challenges of searching through proximity graphs. Binary quantization reduces storage size and boosts speed by 30x, but not all models are compatible.
Here, we show how the accuracy of binary quantization is quite good across different dimensions -- for both the models. 



## Enhanced Performance and Efficiency with Binary Quantization","[""Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever"", 'This is the fastest quantization method, since it lets you perform a vector comparison with a few CPU instructions.\n\n\n\nBinary quantization can achieve up to a **40x** speedup compared to the original vectors.\n\n\n\nHowever, binary quantization is only efficient for high-dimensional vectors and require a centered distribution of vector components. \n\n\n\nAt the moment, binary quantization shows good accuracy results with the following models:', 'Think of each data point as a ruler. Binary quantization splits this ruler in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This [binarization](https://deepai.org/machine-learning-glossary-and-terms/binarization) process results in a string of bits, representing the original vector.\n\n\n\n\n\n\n\n![](/articles_data/what-is-a-vector-database/Binary-Quant.png)', '> -- Andrey Vasnetsov\n\n\n\nEver wonder why we need quantization for vector indexes? Andrey Vasnetsov explains the complexities and challenges of searching through proximity graphs. Binary quantization reduces storage size and boosts speed by 30x, but not all models are compatible.', 'Here, we show how the accuracy of binary quantization is quite good across different dimensions -- for both the models. \n\n\n\n## Enhanced Performance and Efficiency with Binary Quantization']","Question: How does binary quantization work in the context of vector comparisons?\n\nContext: Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever
This is the fastest quantization method, since it lets you perform a vector comparison with a few CPU instructions.



Binary quantization can achieve up to a **40x** speedup compared to the original vectors.



However, binary quantization is only efficient for high-dimensional vectors and require a centered distribution of vector components. 



At the moment, binary quantization shows good accuracy results with the following models:
Think of each data point as a ruler. Binary quantization splits this ruler in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This [binarization](https://deepai.org/machine-learning-glossary-and-terms/binarization) process results in a string of bits, representing the original vector.







![](/articles_data/what-is-a-vector-database/Binary-Quant.png)
> -- Andrey Vasnetsov



Ever wonder why we need quantization for vector indexes? Andrey Vasnetsov explains the complexities and challenges of searching through proximity graphs. Binary quantization reduces storage size and boosts speed by 30x, but not all models are compatible.
Here, we show how the accuracy of binary quantization is quite good across different dimensions -- for both the models. 



## Enhanced Performance and Efficiency with Binary Quantization\n\nAnswer:",How does binary quantization work in the context of vector comparisons?,"Binary quantization is a method used in indexing and data compression, particularly by Qdrant, that involves splitting a data point's vector in half at a certain point. This process essentially divides the vector into two parts, marking everything above the split point as ""1"" and everything below as ""0"". The result is a string of bits that represents the original vector in a compressed form. This quantized code is much smaller and easier to compare. Especially for OpenAI embeddings, this type of quantization has proven to achieve a massive performance improvement at a lower cost of accuracy.","[""Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever"", 'This is the fastest quantization method, since it lets you perform a vector comparison with a few CPU instructions.\n\n\n\nBinary quantization can achieve up to a **40x** speedup compared to the original vectors.\n\n\n\nHowever, binary quantization is only efficient for high-dimensional vectors and require a centered distribution of vector components. \n\n\n\nAt the moment, binary quantization shows good accuracy results with the following models:', 'Think of each data point as a ruler. Binary quantization splits this ruler in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This [binarization](https://deepai.org/machine-learning-glossary-and-terms/binarization) process results in a string of bits, representing the original vector.\n\n\n\n\n\n\n\n![](/articles_data/what-is-a-vector-database/Binary-Quant.png)', '> -- Andrey Vasnetsov\n\n\n\nEver wonder why we need quantization for vector indexes? Andrey Vasnetsov explains the complexities and challenges of searching through proximity graphs. Binary quantization reduces storage size and boosts speed by 30x, but not all models are compatible.', 'Here, we show how the accuracy of binary quantization is quite good across different dimensions -- for both the models. \n\n\n\n## Enhanced Performance and Efficiency with Binary Quantization']","Right, so binary quantization is actually very simple. The main idea that we convert the float point elements of the vector into binary representation. So it's either zero or one, depending if the original element is positive or negative. And by doing this we can approximate dot production or cosine similarity, whatever metric you use to compare vectors with just hemming distance, and hemming distance is turned to be very simple to compute. It uses only two most optimized CPU instructions ever
This is the fastest quantization method, since it lets you perform a vector comparison with a few CPU instructions.



Binary quantization can achieve up to a **40x** speedup compared to the original vectors.



However, binary quantization is only efficient for high-dimensional vectors and require a centered distribution of vector components. 



At the moment, binary quantization shows good accuracy results with the following models:
Think of each data point as a ruler. Binary quantization splits this ruler in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This [binarization](https://deepai.org/machine-learning-glossary-and-terms/binarization) process results in a string of bits, representing the original vector.







![](/articles_data/what-is-a-vector-database/Binary-Quant.png)
> -- Andrey Vasnetsov



Ever wonder why we need quantization for vector indexes? Andrey Vasnetsov explains the complexities and challenges of searching through proximity graphs. Binary quantization reduces storage size and boosts speed by 30x, but not all models are compatible.
Here, we show how the accuracy of binary quantization is quite good across different dimensions -- for both the models. 



## Enhanced Performance and Efficiency with Binary Quantization",0.287292817679558,0.5977011494252874,0.38805970149253727,0.08333333333333333,0.1744186046511628,0.11278195488721804,0.16574585635359115,0.3448275862068966,0.22388059701492535,0.26519337016574585,0.5517241379310345,0.3582089552238806,0.7381978034973145,0.6213537454605103,0.675815224647522,0.6474412083625793,0.9987223120078852,0.9321483617182821,193,1.969387755102041,0.42189291106325816
True,,2803.575,e5b26fdb,What is the significance of the 'always_ram' parameter in the context of vector quantization in Qdrant?,"In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times.","The significance of the 'always_ram' parameter in the context of vector quantization in Qdrant is related to how quantized vectors are handled in memory. When 'always_ram' is set to true, quantized vectors are always cached in RAM, which can lead to faster search processes due to the reduced memory footprint. 

On the other hand, when 'always_ram' is set to false, quantized vectors are loaded in the same way as the original vectors, potentially affecting the precision and search quality. This parameter allows users to tune the trade-off between memory usage and search quality based on their specific needs and requirements.","Using quantiles lower than `1.0` might be useful if there are outliers in your vector components.

This parameter only affects the resulting precision and not the memory footprint.

It might be worth tuning this parameter if you experience a significant decrease in search quality.



`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": false

        }

    }

}

```



```python

from qdrant_client import QdrantClient, models
""size"": 1536,

      ""distance"": ""Cosine""

    },

    ""quantization_config"": {

        ""binary"": {

            ""always_ram"": true

        }

    }

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
..Default::default()

        }),

        quantization_config: Some(QuantizationConfig {

            quantization: Some(Quantization::Scalar(ScalarQuantization {

                r#type: QuantizationType::Int8.into(),

                always_ram: Some(true),

                ..Default::default()

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;","['Using quantiles lower than `1.0` might be useful if there are outliers in your vector components.\n\nThis parameter only affects the resulting precision and not the memory footprint.\n\nIt might be worth tuning this parameter if you experience a significant decrease in search quality.\n\n\n\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.', 'This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models', '""size"": 1536,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""quantization_config"": {\n\n        ""binary"": {\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),', 'Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.\n\n\n\nIn addition to reducing the memory footprint, scalar quantization also speeds up the search process.\n\nQdrant uses a special SIMD CPU instruction to perform fast vector comparison.\n\nThis instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.', '..Default::default()\n\n        }),\n\n        quantization_config: Some(QuantizationConfig {\n\n            quantization: Some(Quantization::Scalar(ScalarQuantization {\n\n                r#type: QuantizationType::Int8.into(),\n\n                always_ram: Some(true),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;']","Question: What is the significance of the 'always_ram' parameter in the context of vector quantization in Qdrant?\n\nContext: Using quantiles lower than `1.0` might be useful if there are outliers in your vector components.

This parameter only affects the resulting precision and not the memory footprint.

It might be worth tuning this parameter if you experience a significant decrease in search quality.



`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""optimizers_config"": {

""memmap_threshold"": 20000

},

""quantization_config"": {

""scalar"": {

""type"": ""int8"",

""always_ram"": false

}

}

}

```



```python

from qdrant_client import QdrantClient, models
""size"": 1536,

""distance"": ""Cosine""

},

""quantization_config"": {

""binary"": {

""always_ram"": true

}

}

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
..Default::default()

}),

quantization_config: Some(QuantizationConfig {

quantization: Some(Quantization::Scalar(ScalarQuantization {

r#type: QuantizationType::Int8.into(),

always_ram: Some(true),

..Default::default()

})),

}),

..Default::default()

})

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;\n\nAnswer:",What is the significance of the 'always_ram' parameter in the context of vector quantization in Qdrant?,"In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times.","['Using quantiles lower than `1.0` might be useful if there are outliers in your vector components.\n\nThis parameter only affects the resulting precision and not the memory footprint.\n\nIt might be worth tuning this parameter if you experience a significant decrease in search quality.\n\n\n\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.', 'This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models', '""size"": 1536,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""quantization_config"": {\n\n        ""binary"": {\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),', 'Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.\n\n\n\nIn addition to reducing the memory footprint, scalar quantization also speeds up the search process.\n\nQdrant uses a special SIMD CPU instruction to perform fast vector comparison.\n\nThis instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.', '..Default::default()\n\n        }),\n\n        quantization_config: Some(QuantizationConfig {\n\n            quantization: Some(Quantization::Scalar(ScalarQuantization {\n\n                r#type: QuantizationType::Int8.into(),\n\n                always_ram: Some(true),\n\n                ..Default::default()\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;']","Using quantiles lower than `1.0` might be useful if there are outliers in your vector components.

This parameter only affects the resulting precision and not the memory footprint.

It might be worth tuning this parameter if you experience a significant decrease in search quality.



`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": false

        }

    }

}

```



```python

from qdrant_client import QdrantClient, models
""size"": 1536,

      ""distance"": ""Cosine""

    },

    ""quantization_config"": {

        ""binary"": {

            ""always_ram"": true

        }

    }

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE),
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
..Default::default()

        }),

        quantization_config: Some(QuantizationConfig {

            quantization: Some(Quantization::Scalar(ScalarQuantization {

                r#type: QuantizationType::Int8.into(),

                always_ram: Some(true),

                ..Default::default()

            })),

        }),

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;",0.37894736842105264,0.6792452830188679,0.48648648648648657,0.24468085106382978,0.4423076923076923,0.3150684931506849,0.2631578947368421,0.4716981132075472,0.3378378378378378,0.3684210526315789,0.660377358490566,0.47297297297297297,0.9550440311431885,0.6309033632278442,0.7091563940048218,0.667745053768158,0.5738921463489532,0.7486315419276555,100,1.7857142857142858,0.6932862722387968
True,,4208.616,4a76f661,How can automatic backups be set up for clusters using the Cloud UI?,"Automatic backups for clusters can be set up using the Cloud UI by following the procedures listed on the page. These procedures allow you to configure snapshots on a daily, weekly, or monthly basis. You have the flexibility to keep as many snapshots as needed and can restore a cluster from the snapshot of your choice. It is important to note that during the restoration of a snapshot, the affected cluster will not be available.","To set up automatic backups for clusters using the Cloud UI, follow these steps:

1. Sign in to the Qdrant Cloud Dashboard at https://cloud.qdrant.io.
2. Select the ""Clusters"" option.
3. Choose the specific cluster that you want to back up.
4. Navigate to the ""Backups"" tab.
5. Set up a backup schedule by specifying the frequency of snapshots (daily, weekly, monthly) and the number of days for retention.
6. Alternatively, you can select ""Backup now"" to take an immediate snapshot.
7. To restore a backup, go to the list of ""Available Backups"" and choose the backup you want to restore or delete.

Ensure that you have set up your cluster and configured it according to the prerequisites mentioned in the Cloud UI documentation. This includes using performant persistent storage, configuring security settings, setting up Qdrant on multiple nodes for high availability, implementing a load balancer, and establishing a backup and disaster recovery strategy for your data.","- [Create a cluster](/documentation/cloud/create-cluster/)

- Set up [Authentication](/documentation/cloud/authentication/)

- Configure one or more [Collections](/documentation/concepts/collections/)



## Automatic backups



You can set up automatic backups of your clusters with our Cloud UI. With the

procedures listed in this page, you can set up

snapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you

need. You can restore a cluster from the snapshot of your choice.
recommend replication. It guarantees the proper cluster functionality as long as

at least one replica is running.



For other use-cases such as disaster recovery, you can set up automatic or

self-service backups.



## Prerequisites



You can back up your Qdrant clusters though the Qdrant Cloud

Dashboard at https://cloud.qdrant.io. This section assumes that you've already

set up your cluster, as described in the following sections:



- [Create a cluster](/documentation/cloud/create-cluster/)
[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:



1. Sign in to the dashboard

1. Select Clusters.

1. Select the cluster that you want to back up.

   ![Select a cluster](/documentation/cloud/select-cluster.png)

1. Find and select the **Backups** tab.

1. Now you can set up a backup schedule.

   The **Days of Retention** is the number of days after a backup snapshot is

   deleted.

1. Alternatively, you can select **Backup now** to take an immediate snapshot.
![Configure a cluster backup](/documentation/cloud/backup-schedule.png)



### Restore a backup



If you have a backup, it appears in the list of **Available Backups**. You can

choose to restore or delete the backups of your choice.



![Restore or delete a cluster backup](/documentation/cloud/restore-delete.png)



<!-- I think we should move this to the Snapshot page, but I'll do it later -->



## Backups with a snapshot



Qdrant also offers a snapshot API which allows you to create a snapshot
In addition, you have to make sure:



* To use a performant [persistent storage](#storage) for your data

* To configure the [security settings](/documentation/guides/security/) for your deployment

* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)

* To set up a load balancer for your Qdrant cluster

* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data","['- [Create a cluster](/documentation/cloud/create-cluster/)\n\n- Set up [Authentication](/documentation/cloud/authentication/)\n\n- Configure one or more [Collections](/documentation/concepts/collections/)\n\n\n\n## Automatic backups\n\n\n\nYou can set up automatic backups of your clusters with our Cloud UI. With the\n\nprocedures listed in this page, you can set up\n\nsnapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you\n\nneed. You can restore a cluster from the snapshot of your choice.', ""recommend replication. It guarantees the proper cluster functionality as long as\n\nat least one replica is running.\n\n\n\nFor other use-cases such as disaster recovery, you can set up automatic or\n\nself-service backups.\n\n\n\n## Prerequisites\n\n\n\nYou can back up your Qdrant clusters though the Qdrant Cloud\n\nDashboard at https://cloud.qdrant.io. This section assumes that you've already\n\nset up your cluster, as described in the following sections:\n\n\n\n- [Create a cluster](/documentation/cloud/create-cluster/)"", '[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:\n\n\n\n1. Sign in to the dashboard\n\n1. Select Clusters.\n\n1. Select the cluster that you want to back up.\n\n   ![Select a cluster](/documentation/cloud/select-cluster.png)\n\n1. Find and select the **Backups** tab.\n\n1. Now you can set up a backup schedule.\n\n   The **Days of Retention** is the number of days after a backup snapshot is\n\n   deleted.\n\n1. Alternatively, you can select **Backup now** to take an immediate snapshot.', ""![Configure a cluster backup](/documentation/cloud/backup-schedule.png)\n\n\n\n### Restore a backup\n\n\n\nIf you have a backup, it appears in the list of **Available Backups**. You can\n\nchoose to restore or delete the backups of your choice.\n\n\n\n![Restore or delete a cluster backup](/documentation/cloud/restore-delete.png)\n\n\n\n<!-- I think we should move this to the Snapshot page, but I'll do it later -->\n\n\n\n## Backups with a snapshot\n\n\n\nQdrant also offers a snapshot API which allows you to create a snapshot"", 'In addition, you have to make sure:\n\n\n\n* To use a performant [persistent storage](#storage) for your data\n\n* To configure the [security settings](/documentation/guides/security/) for your deployment\n\n* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)\n\n* To set up a load balancer for your Qdrant cluster\n\n* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data']","Question: How can automatic backups be set up for clusters using the Cloud UI?\n\nContext: - [Create a cluster](/documentation/cloud/create-cluster/)

- Set up [Authentication](/documentation/cloud/authentication/)

- Configure one or more [Collections](/documentation/concepts/collections/)



## Automatic backups



You can set up automatic backups of your clusters with our Cloud UI. With the

procedures listed in this page, you can set up

snapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you

need. You can restore a cluster from the snapshot of your choice.
recommend replication. It guarantees the proper cluster functionality as long as

at least one replica is running.



For other use-cases such as disaster recovery, you can set up automatic or

self-service backups.



## Prerequisites



You can back up your Qdrant clusters though the Qdrant Cloud

Dashboard at https://cloud.qdrant.io. This section assumes that you've already

set up your cluster, as described in the following sections:



- [Create a cluster](/documentation/cloud/create-cluster/)
[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:



1. Sign in to the dashboard

1. Select Clusters.

1. Select the cluster that you want to back up.

![Select a cluster](/documentation/cloud/select-cluster.png)

1. Find and select the **Backups** tab.

1. Now you can set up a backup schedule.

The **Days of Retention** is the number of days after a backup snapshot is

deleted.

1. Alternatively, you can select **Backup now** to take an immediate snapshot.
![Configure a cluster backup](/documentation/cloud/backup-schedule.png)



### Restore a backup



If you have a backup, it appears in the list of **Available Backups**. You can

choose to restore or delete the backups of your choice.



![Restore or delete a cluster backup](/documentation/cloud/restore-delete.png)



<!-- I think we should move this to the Snapshot page, but I'll do it later -->



## Backups with a snapshot



Qdrant also offers a snapshot API which allows you to create a snapshot
In addition, you have to make sure:



* To use a performant [persistent storage](#storage) for your data

* To configure the [security settings](/documentation/guides/security/) for your deployment

* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)

* To set up a load balancer for your Qdrant cluster

* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data\n\nAnswer:",How can automatic backups be set up for clusters using the Cloud UI?,"Automatic backups for clusters can be set up using the Cloud UI by following the procedures listed on the page. These procedures allow you to configure snapshots on a daily, weekly, or monthly basis. You have the flexibility to keep as many snapshots as needed and can restore a cluster from the snapshot of your choice. It is important to note that during the restoration of a snapshot, the affected cluster will not be available.","['- [Create a cluster](/documentation/cloud/create-cluster/)\n\n- Set up [Authentication](/documentation/cloud/authentication/)\n\n- Configure one or more [Collections](/documentation/concepts/collections/)\n\n\n\n## Automatic backups\n\n\n\nYou can set up automatic backups of your clusters with our Cloud UI. With the\n\nprocedures listed in this page, you can set up\n\nsnapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you\n\nneed. You can restore a cluster from the snapshot of your choice.', ""recommend replication. It guarantees the proper cluster functionality as long as\n\nat least one replica is running.\n\n\n\nFor other use-cases such as disaster recovery, you can set up automatic or\n\nself-service backups.\n\n\n\n## Prerequisites\n\n\n\nYou can back up your Qdrant clusters though the Qdrant Cloud\n\nDashboard at https://cloud.qdrant.io. This section assumes that you've already\n\nset up your cluster, as described in the following sections:\n\n\n\n- [Create a cluster](/documentation/cloud/create-cluster/)"", '[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:\n\n\n\n1. Sign in to the dashboard\n\n1. Select Clusters.\n\n1. Select the cluster that you want to back up.\n\n   ![Select a cluster](/documentation/cloud/select-cluster.png)\n\n1. Find and select the **Backups** tab.\n\n1. Now you can set up a backup schedule.\n\n   The **Days of Retention** is the number of days after a backup snapshot is\n\n   deleted.\n\n1. Alternatively, you can select **Backup now** to take an immediate snapshot.', ""![Configure a cluster backup](/documentation/cloud/backup-schedule.png)\n\n\n\n### Restore a backup\n\n\n\nIf you have a backup, it appears in the list of **Available Backups**. You can\n\nchoose to restore or delete the backups of your choice.\n\n\n\n![Restore or delete a cluster backup](/documentation/cloud/restore-delete.png)\n\n\n\n<!-- I think we should move this to the Snapshot page, but I'll do it later -->\n\n\n\n## Backups with a snapshot\n\n\n\nQdrant also offers a snapshot API which allows you to create a snapshot"", 'In addition, you have to make sure:\n\n\n\n* To use a performant [persistent storage](#storage) for your data\n\n* To configure the [security settings](/documentation/guides/security/) for your deployment\n\n* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)\n\n* To set up a load balancer for your Qdrant cluster\n\n* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data']","- [Create a cluster](/documentation/cloud/create-cluster/)

- Set up [Authentication](/documentation/cloud/authentication/)

- Configure one or more [Collections](/documentation/concepts/collections/)



## Automatic backups



You can set up automatic backups of your clusters with our Cloud UI. With the

procedures listed in this page, you can set up

snapshots on a daily/weekly/monthly basis. You can keep as many snapshots as you

need. You can restore a cluster from the snapshot of your choice.
recommend replication. It guarantees the proper cluster functionality as long as

at least one replica is running.



For other use-cases such as disaster recovery, you can set up automatic or

self-service backups.



## Prerequisites



You can back up your Qdrant clusters though the Qdrant Cloud

Dashboard at https://cloud.qdrant.io. This section assumes that you've already

set up your cluster, as described in the following sections:



- [Create a cluster](/documentation/cloud/create-cluster/)
[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:



1. Sign in to the dashboard

1. Select Clusters.

1. Select the cluster that you want to back up.

   ![Select a cluster](/documentation/cloud/select-cluster.png)

1. Find and select the **Backups** tab.

1. Now you can set up a backup schedule.

   The **Days of Retention** is the number of days after a backup snapshot is

   deleted.

1. Alternatively, you can select **Backup now** to take an immediate snapshot.
![Configure a cluster backup](/documentation/cloud/backup-schedule.png)



### Restore a backup



If you have a backup, it appears in the list of **Available Backups**. You can

choose to restore or delete the backups of your choice.



![Restore or delete a cluster backup](/documentation/cloud/restore-delete.png)



<!-- I think we should move this to the Snapshot page, but I'll do it later -->



## Backups with a snapshot



Qdrant also offers a snapshot API which allows you to create a snapshot
In addition, you have to make sure:



* To use a performant [persistent storage](#storage) for your data

* To configure the [security settings](/documentation/guides/security/) for your deployment

* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)

* To set up a load balancer for your Qdrant cluster

* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data",0.2727272727272727,0.6,0.37499999999999994,0.06338028169014084,0.140625,0.08737864077669903,0.16783216783216784,0.36923076923076925,0.2307692307692308,0.25874125874125875,0.5692307692307692,0.3557692307692308,0.7881677150726318,0.5820438861846924,0.6485121250152588,0.6134828329086304,0.5394245057573749,0.7627697453426663,157,2.0933333333333333,0.5084572806909132
True,,3788.418,1bc99bb9,What are snapshots in the context of Qdrant Cloud and how are they used in a distributed setup?,"Snapshots in Qdrant Cloud are `tar` archive files that contain data and configuration of a specific collection on a specific node at a specific time. In a distributed setup with multiple nodes in a cluster, snapshots must be created for each node separately when dealing with a single collection. These snapshots can be used to archive data or easily replicate an existing deployment.","In the context of Qdrant Cloud, snapshots are a way to create point-in-time copies of the data stored in the Qdrant Cloud cluster. These snapshots can be used for archiving data, replicating deployments, and disaster recovery purposes.

In a distributed setup, such as one with multiple nodes, creating a snapshot involves calling the endpoint on each node to create a snapshot on each node. This can be done using the Qdrant Python SDK, with separate client instances for each node.

Snapshots are useful for recovery scenarios, where even if there are no copies of data in the cluster, it is possible to recover from a snapshot. This feature provides a way to restore the data in case of any data loss or corruption within the cluster.

It's important to note that while snapshots can be created and downloaded from Qdrant Cloud, restoring a Qdrant Cloud cluster from a whole storage snapshot requires the use of the Qdrant CLI. For disaster recovery, users may prefer to use Backups, which are physical disk-level copies of the data stored in the cluster.","<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>



### Create full storage snapshot



```http

POST /snapshots

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.create_full_snapshot()

```
This feature can be used to archive data or easily replicate an existing deployment. For disaster recovery, Qdrant Cloud users may prefer to use [Backups](/documentation/cloud/backups/) instead, which are physical disk-level copies of your data.



For a step-by-step guide on how to use snapshots, see our [tutorial](/documentation/tutorials/create-snapshot/).



## Store snapshots
</details>



If you already have a collection, you can skip the previous step and start by [creating a snapshot](#create-and-download-snapshots).



## Create and download snapshots



Qdrant exposes an HTTP endpoint to request creating a snapshot, but we can also call it with the Python SDK.

Our setup consists of 3 nodes, so we need to call the endpoint **on each of them** and create a snapshot on each node. While using Python SDK, that means creating a separate client instance for each node.
It's worth mentioning that Qdrant only provides the necessary building blocks to create an automated failure recovery.

Building a completely automatic process of collection scaling would require control over the cluster machines themself.

Check out our [cloud solution](https://qdrant.to/cloud), where we made exactly that.





**Recover from snapshot**



If there are no copies of data in the cluster, it is still possible to recover from a snapshot.
[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:



1. Sign in to the dashboard

1. Select Clusters.

1. Select the cluster that you want to back up.

   ![Select a cluster](/documentation/cloud/select-cluster.png)

1. Find and select the **Backups** tab.

1. Now you can set up a backup schedule.

   The **Days of Retention** is the number of days after a backup snapshot is

   deleted.

1. Alternatively, you can select **Backup now** to take an immediate snapshot.","['<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>\n\n\n\n### Create full storage snapshot\n\n\n\n```http\n\nPOST /snapshots\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_full_snapshot()\n\n```', 'This feature can be used to archive data or easily replicate an existing deployment. For disaster recovery, Qdrant Cloud users may prefer to use [Backups](/documentation/cloud/backups/) instead, which are physical disk-level copies of your data.\n\n\n\nFor a step-by-step guide on how to use snapshots, see our [tutorial](/documentation/tutorials/create-snapshot/).\n\n\n\n## Store snapshots', '</details>\n\n\n\nIf you already have a collection, you can skip the previous step and start by [creating a snapshot](#create-and-download-snapshots).\n\n\n\n## Create and download snapshots\n\n\n\nQdrant exposes an HTTP endpoint to request creating a snapshot, but we can also call it with the Python SDK.\n\nOur setup consists of 3 nodes, so we need to call the endpoint **on each of them** and create a snapshot on each node. While using Python SDK, that means creating a separate client instance for each node.', ""It's worth mentioning that Qdrant only provides the necessary building blocks to create an automated failure recovery.\n\nBuilding a completely automatic process of collection scaling would require control over the cluster machines themself.\n\nCheck out our [cloud solution](https://qdrant.to/cloud), where we made exactly that.\n\n\n\n\n\n**Recover from snapshot**\n\n\n\nIf there are no copies of data in the cluster, it is still possible to recover from a snapshot."", '[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:\n\n\n\n1. Sign in to the dashboard\n\n1. Select Clusters.\n\n1. Select the cluster that you want to back up.\n\n   ![Select a cluster](/documentation/cloud/select-cluster.png)\n\n1. Find and select the **Backups** tab.\n\n1. Now you can set up a backup schedule.\n\n   The **Days of Retention** is the number of days after a backup snapshot is\n\n   deleted.\n\n1. Alternatively, you can select **Backup now** to take an immediate snapshot.']","Question: What are snapshots in the context of Qdrant Cloud and how are they used in a distributed setup?\n\nContext: <aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>



### Create full storage snapshot



```http

POST /snapshots

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.create_full_snapshot()

```
This feature can be used to archive data or easily replicate an existing deployment. For disaster recovery, Qdrant Cloud users may prefer to use [Backups](/documentation/cloud/backups/) instead, which are physical disk-level copies of your data.



For a step-by-step guide on how to use snapshots, see our [tutorial](/documentation/tutorials/create-snapshot/).



## Store snapshots
</details>



If you already have a collection, you can skip the previous step and start by [creating a snapshot](#create-and-download-snapshots).



## Create and download snapshots



Qdrant exposes an HTTP endpoint to request creating a snapshot, but we can also call it with the Python SDK.

Our setup consists of 3 nodes, so we need to call the endpoint **on each of them** and create a snapshot on each node. While using Python SDK, that means creating a separate client instance for each node.
It's worth mentioning that Qdrant only provides the necessary building blocks to create an automated failure recovery.

Building a completely automatic process of collection scaling would require control over the cluster machines themself.

Check out our [cloud solution](https://qdrant.to/cloud), where we made exactly that.





**Recover from snapshot**



If there are no copies of data in the cluster, it is still possible to recover from a snapshot.
[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:



1. Sign in to the dashboard

1. Select Clusters.

1. Select the cluster that you want to back up.

![Select a cluster](/documentation/cloud/select-cluster.png)

1. Find and select the **Backups** tab.

1. Now you can set up a backup schedule.

The **Days of Retention** is the number of days after a backup snapshot is

deleted.

1. Alternatively, you can select **Backup now** to take an immediate snapshot.\n\nAnswer:",What are snapshots in the context of Qdrant Cloud and how are they used in a distributed setup?,"Snapshots in Qdrant Cloud are `tar` archive files that contain data and configuration of a specific collection on a specific node at a specific time. In a distributed setup with multiple nodes in a cluster, snapshots must be created for each node separately when dealing with a single collection. These snapshots can be used to archive data or easily replicate an existing deployment.","['<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>\n\n\n\n### Create full storage snapshot\n\n\n\n```http\n\nPOST /snapshots\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_full_snapshot()\n\n```', 'This feature can be used to archive data or easily replicate an existing deployment. For disaster recovery, Qdrant Cloud users may prefer to use [Backups](/documentation/cloud/backups/) instead, which are physical disk-level copies of your data.\n\n\n\nFor a step-by-step guide on how to use snapshots, see our [tutorial](/documentation/tutorials/create-snapshot/).\n\n\n\n## Store snapshots', '</details>\n\n\n\nIf you already have a collection, you can skip the previous step and start by [creating a snapshot](#create-and-download-snapshots).\n\n\n\n## Create and download snapshots\n\n\n\nQdrant exposes an HTTP endpoint to request creating a snapshot, but we can also call it with the Python SDK.\n\nOur setup consists of 3 nodes, so we need to call the endpoint **on each of them** and create a snapshot on each node. While using Python SDK, that means creating a separate client instance for each node.', ""It's worth mentioning that Qdrant only provides the necessary building blocks to create an automated failure recovery.\n\nBuilding a completely automatic process of collection scaling would require control over the cluster machines themself.\n\nCheck out our [cloud solution](https://qdrant.to/cloud), where we made exactly that.\n\n\n\n\n\n**Recover from snapshot**\n\n\n\nIf there are no copies of data in the cluster, it is still possible to recover from a snapshot."", '[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:\n\n\n\n1. Sign in to the dashboard\n\n1. Select Clusters.\n\n1. Select the cluster that you want to back up.\n\n   ![Select a cluster](/documentation/cloud/select-cluster.png)\n\n1. Find and select the **Backups** tab.\n\n1. Now you can set up a backup schedule.\n\n   The **Days of Retention** is the number of days after a backup snapshot is\n\n   deleted.\n\n1. Alternatively, you can select **Backup now** to take an immediate snapshot.']","<aside role=""status"">Whole storage snapshots can be created and downloaded from Qdrant Cloud, but you cannot restore a Qdrant Cloud cluster from a whole storage snapshot since that requires use of the Qdrant CLI. You can use <a href=""/documentation/cloud/backups/"">Backups</a> instead.</aside>



### Create full storage snapshot



```http

POST /snapshots

```



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)



client.create_full_snapshot()

```
This feature can be used to archive data or easily replicate an existing deployment. For disaster recovery, Qdrant Cloud users may prefer to use [Backups](/documentation/cloud/backups/) instead, which are physical disk-level copies of your data.



For a step-by-step guide on how to use snapshots, see our [tutorial](/documentation/tutorials/create-snapshot/).



## Store snapshots
</details>



If you already have a collection, you can skip the previous step and start by [creating a snapshot](#create-and-download-snapshots).



## Create and download snapshots



Qdrant exposes an HTTP endpoint to request creating a snapshot, but we can also call it with the Python SDK.

Our setup consists of 3 nodes, so we need to call the endpoint **on each of them** and create a snapshot on each node. While using Python SDK, that means creating a separate client instance for each node.
It's worth mentioning that Qdrant only provides the necessary building blocks to create an automated failure recovery.

Building a completely automatic process of collection scaling would require control over the cluster machines themself.

Check out our [cloud solution](https://qdrant.to/cloud), where we made exactly that.





**Recover from snapshot**



If there are no copies of data in the cluster, it is still possible to recover from a snapshot.
[Qdrant Cloud Dashboard](https://cloud.qdrant.io). To do so, take these steps:



1. Sign in to the dashboard

1. Select Clusters.

1. Select the cluster that you want to back up.

   ![Select a cluster](/documentation/cloud/select-cluster.png)

1. Find and select the **Backups** tab.

1. Now you can set up a backup schedule.

   The **Days of Retention** is the number of days after a backup snapshot is

   deleted.

1. Alternatively, you can select **Backup now** to take an immediate snapshot.",0.2392638036809816,0.6964285714285714,0.35616438356164387,0.09259259259259259,0.2727272727272727,0.1382488479262673,0.147239263803681,0.42857142857142855,0.2191780821917808,0.2147239263803681,0.625,0.31963470319634707,0.9044822454452515,0.602074146270752,0.6733448505401611,0.6357181668281555,0.9436153977294452,0.6884030451377232,180,2.857142857142857,0.27850628182347803
True,,2104.431,c28ac2d7,What is the significance of the lowercase parameter in the context of text filters in Qdrant?,"The lowercase parameter in Qdrant is used to specify whether the index should be case-insensitive or not. When set to true, Qdrant will convert all the texts to lowercase before indexing them. This means that during searches, the case of the letters in the query will not affect the results.","In the context of text filters in Qdrant, the significance of the `lowercase` parameter is that if it is set to `true`, then the index will be case-insensitive. This means that Qdrant will convert all the texts to lowercase before performing any comparison or search operations. This ensures that the search results are not affected by the case (upper or lower) of the text, making the search process more robust and accurate.","* **min_token_len** — minimal length of the token

* **max_token_len** — maximal length of the token

* **lowercase** — if set to *true*, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase



## Using text filters in practice
## Using text filters in practice



![](/blog/from_cms/1_pbtd2tzqtjqqlbi61r8czg.webp ""There are also some additional parameters you can provide, such as  min_token_len — minimal length of the token max_token_len — maximal length of the token lowercase — if set to true, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase Using text filters in practice"")
This feature is implemented as additional filters during the search and will enable you to incorporate custom logic on top of semantic similarity.



During the filtering, Qdrant will check the conditions over those values that match the type of the filtering condition. If the stored value type does not fit the filtering condition - it will be considered not satisfied.



For example, you will get an empty output if you apply the [range condition](../filtering/#range) on the string data.
using static Qdrant.Client.Grpc.Conditions;



var client = new QdrantClient(""localhost"", 6334);



// The ! operator negates the condition(must not)

await client.ScrollAsync(

	collectionName: ""{collection_name}"",

	filter: !(MatchKeyword(""city"", ""London"") & MatchKeyword(""color"", ""red""))

);

```



Filtered points would be:



```json

[

  { ""id"": 5, ""city"": ""Moscow"", ""color"": ""green"" },

  { ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }

]

```
using static Qdrant.Client.Grpc.Conditions;



var client = new QdrantClient(""localhost"", 6334);



// | operator combines two conditions in an OR disjunction(should)

await client.ScrollAsync(

	collectionName: ""{collection_name}"",

	filter: MatchKeyword(""city"", ""London"") | MatchKeyword(""color"", ""red"")

);

```



Filtered points would be:



```json

[

  { ""id"": 1, ""city"": ""London"", ""color"": ""green"" },

  { ""id"": 2, ""city"": ""London"", ""color"": ""red"" },

  { ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },","['* **min_token_len**\xa0— minimal length of the token\n\n* **max_token_len**\xa0— maximal length of the token\n\n* **lowercase**\xa0— if set to\xa0*true*, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase\n\n\n\n## Using text filters in practice', '## Using text filters in practice\n\n\n\n![](/blog/from_cms/1_pbtd2tzqtjqqlbi61r8czg.webp ""There are also some additional parameters you can provide, such as  min_token_len — minimal length of the token max_token_len — maximal length of the token lowercase — if set to true, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase Using text filters in practice"")', 'This feature is implemented as additional filters during the search and will enable you to incorporate custom logic on top of semantic similarity.\n\n\n\nDuring the filtering, Qdrant will check the conditions over those values that match the type of the filtering condition. If the stored value type does not fit the filtering condition - it will be considered not satisfied.\n\n\n\nFor example, you will get an empty output if you apply the [range condition](../filtering/#range) on the string data.', 'using static Qdrant.Client.Grpc.Conditions;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\n// The ! operator negates the condition(must not)\n\nawait client.ScrollAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tfilter: !(MatchKeyword(""city"", ""London"") & MatchKeyword(""color"", ""red""))\n\n);\n\n```\n\n\n\nFiltered points would be:\n\n\n\n```json\n\n[\n\n  { ""id"": 5, ""city"": ""Moscow"", ""color"": ""green"" },\n\n  { ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }\n\n]\n\n```', 'using static Qdrant.Client.Grpc.Conditions;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\n// | operator combines two conditions in an OR disjunction(should)\n\nawait client.ScrollAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tfilter: MatchKeyword(""city"", ""London"") | MatchKeyword(""color"", ""red"")\n\n);\n\n```\n\n\n\nFiltered points would be:\n\n\n\n```json\n\n[\n\n  { ""id"": 1, ""city"": ""London"", ""color"": ""green"" },\n\n  { ""id"": 2, ""city"": ""London"", ""color"": ""red"" },\n\n  { ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },']","Question: What is the significance of the lowercase parameter in the context of text filters in Qdrant?\n\nContext: * **min_token_len** — minimal length of the token

* **max_token_len** — maximal length of the token

* **lowercase** — if set to *true*, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase



## Using text filters in practice
## Using text filters in practice



![](/blog/from_cms/1_pbtd2tzqtjqqlbi61r8czg.webp ""There are also some additional parameters you can provide, such as  min_token_len — minimal length of the token max_token_len — maximal length of the token lowercase — if set to true, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase Using text filters in practice"")
This feature is implemented as additional filters during the search and will enable you to incorporate custom logic on top of semantic similarity.



During the filtering, Qdrant will check the conditions over those values that match the type of the filtering condition. If the stored value type does not fit the filtering condition - it will be considered not satisfied.



For example, you will get an empty output if you apply the [range condition](../filtering/#range) on the string data.
using static Qdrant.Client.Grpc.Conditions;



var client = new QdrantClient(""localhost"", 6334);



// The ! operator negates the condition(must not)

await client.ScrollAsync(

collectionName: ""{collection_name}"",

filter: !(MatchKeyword(""city"", ""London"") & MatchKeyword(""color"", ""red""))

);

```



Filtered points would be:



```json

[

{ ""id"": 5, ""city"": ""Moscow"", ""color"": ""green"" },

{ ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }

]

```
using static Qdrant.Client.Grpc.Conditions;



var client = new QdrantClient(""localhost"", 6334);



// | operator combines two conditions in an OR disjunction(should)

await client.ScrollAsync(

collectionName: ""{collection_name}"",

filter: MatchKeyword(""city"", ""London"") | MatchKeyword(""color"", ""red"")

);

```



Filtered points would be:



```json

[

{ ""id"": 1, ""city"": ""London"", ""color"": ""green"" },

{ ""id"": 2, ""city"": ""London"", ""color"": ""red"" },

{ ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },\n\nAnswer:",What is the significance of the lowercase parameter in the context of text filters in Qdrant?,"The lowercase parameter in Qdrant is used to specify whether the index should be case-insensitive or not. When set to true, Qdrant will convert all the texts to lowercase before indexing them. This means that during searches, the case of the letters in the query will not affect the results.","['* **min_token_len**\xa0— minimal length of the token\n\n* **max_token_len**\xa0— maximal length of the token\n\n* **lowercase**\xa0— if set to\xa0*true*, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase\n\n\n\n## Using text filters in practice', '## Using text filters in practice\n\n\n\n![](/blog/from_cms/1_pbtd2tzqtjqqlbi61r8czg.webp ""There are also some additional parameters you can provide, such as  min_token_len — minimal length of the token max_token_len — maximal length of the token lowercase — if set to true, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase Using text filters in practice"")', 'This feature is implemented as additional filters during the search and will enable you to incorporate custom logic on top of semantic similarity.\n\n\n\nDuring the filtering, Qdrant will check the conditions over those values that match the type of the filtering condition. If the stored value type does not fit the filtering condition - it will be considered not satisfied.\n\n\n\nFor example, you will get an empty output if you apply the [range condition](../filtering/#range) on the string data.', 'using static Qdrant.Client.Grpc.Conditions;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\n// The ! operator negates the condition(must not)\n\nawait client.ScrollAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tfilter: !(MatchKeyword(""city"", ""London"") & MatchKeyword(""color"", ""red""))\n\n);\n\n```\n\n\n\nFiltered points would be:\n\n\n\n```json\n\n[\n\n  { ""id"": 5, ""city"": ""Moscow"", ""color"": ""green"" },\n\n  { ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }\n\n]\n\n```', 'using static Qdrant.Client.Grpc.Conditions;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\n// | operator combines two conditions in an OR disjunction(should)\n\nawait client.ScrollAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tfilter: MatchKeyword(""city"", ""London"") | MatchKeyword(""color"", ""red"")\n\n);\n\n```\n\n\n\nFiltered points would be:\n\n\n\n```json\n\n[\n\n  { ""id"": 1, ""city"": ""London"", ""color"": ""green"" },\n\n  { ""id"": 2, ""city"": ""London"", ""color"": ""red"" },\n\n  { ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },']","* **min_token_len** — minimal length of the token

* **max_token_len** — maximal length of the token

* **lowercase** — if set to *true*, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase



## Using text filters in practice
## Using text filters in practice



![](/blog/from_cms/1_pbtd2tzqtjqqlbi61r8czg.webp ""There are also some additional parameters you can provide, such as  min_token_len — minimal length of the token max_token_len — maximal length of the token lowercase — if set to true, then the index will be case-insensitive, as Qdrant will convert all the texts to lowercase Using text filters in practice"")
This feature is implemented as additional filters during the search and will enable you to incorporate custom logic on top of semantic similarity.



During the filtering, Qdrant will check the conditions over those values that match the type of the filtering condition. If the stored value type does not fit the filtering condition - it will be considered not satisfied.



For example, you will get an empty output if you apply the [range condition](../filtering/#range) on the string data.
using static Qdrant.Client.Grpc.Conditions;



var client = new QdrantClient(""localhost"", 6334);



// The ! operator negates the condition(must not)

await client.ScrollAsync(

	collectionName: ""{collection_name}"",

	filter: !(MatchKeyword(""city"", ""London"") & MatchKeyword(""color"", ""red""))

);

```



Filtered points would be:



```json

[

  { ""id"": 5, ""city"": ""Moscow"", ""color"": ""green"" },

  { ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }

]

```
using static Qdrant.Client.Grpc.Conditions;



var client = new QdrantClient(""localhost"", 6334);



// | operator combines two conditions in an OR disjunction(should)

await client.ScrollAsync(

	collectionName: ""{collection_name}"",

	filter: MatchKeyword(""city"", ""London"") | MatchKeyword(""color"", ""red"")

);

```



Filtered points would be:



```json

[

  { ""id"": 1, ""city"": ""London"", ""color"": ""green"" },

  { ""id"": 2, ""city"": ""London"", ""color"": ""red"" },

  { ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },",0.5,0.7272727272727273,0.5925925925925926,0.25396825396825395,0.37209302325581395,0.3018867924528302,0.328125,0.4772727272727273,0.3888888888888889,0.4375,0.6363636363636364,0.5185185185185185,0.9177958965301514,0.7261381149291992,0.7608590722084045,0.7430932521820068,0.7530167251825333,0.9050355739891529,72,1.44,0.30101854228820324
True,,5717.739,2fae3cd8,How does adjusting the `write_consistency_factor` parameter impact write operations in a distributed deployment using Qdrant?,"The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully.","Adjusting the `write_consistency_factor` parameter in Qdrant impacts the write operations in a distributed deployment as follows:

1. **Number of Replicas Acknowledging Write Operations**: The `write_consistency_factor` determines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value makes the write operations more tolerant to network partitions in the cluster. It ensures that a specific number of replicas have successfully written the data before acknowledging the write operation to the client.

2. **Impact on Write Operations**: When the `write_consistency_factor` is increased, it requires a higher number of replicas to be active in the cluster to perform write operations successfully. This means that if the number of active replicas is less than the `write_consistency_factor`, write operations will fail. On the other hand, setting a lower `write_consistency_factor` may lead to faster responses but at the cost of reduced fault tolerance.

3. **Configuration at Collection Creation Time**: The `write_consistency_factor` can be configured at the time of creating a collection in Qdrant. By specifying this parameter during collection creation, you can tailor the level of consistency and fault tolerance required for write operations in your distributed deployment.

In summary, adjusting the `write_consistency_factor` parameter in Qdrant allows you to control the level of fault tolerance and consistency for write operations in a distributed deployment, balancing between performance and reliability based on your specific requirements.","- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

	shardNumber: 6,

	replicationFactor: 2,

	writeConsistencyFactor: 2

);

```



Write operations will fail if the number of active replicas is less than the `write_consistency_factor`.



### Read consistency



Read `consistency` can be specified for most read requests and will ensure that the returned result

is consistent across cluster nodes.
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

    ""write_consistency_factor"": 2,

}

```



```python
""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

    shard_number=6,

    replication_factor=2,

    write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
* `params` - other collection parameters, including `write_consistency_factor` and `on_disk_payload`. 



Full API specification is available in [schema definitions](https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/update_collection).



Calls to this endpoint may be blocking as it waits for existing optimizers to

finish. We recommended against using this in a production database as it may

introduce huge overhead due to the rebuilding of the index.","['- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.', 'collectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\n\tshardNumber: 6,\n\n\treplicationFactor: 2,\n\n\twriteConsistencyFactor: 2\n\n);\n\n```\n\n\n\nWrite operations will fail if the number of active replicas is less than the `write_consistency_factor`.\n\n\n\n### Read consistency\n\n\n\nRead `consistency` can be specified for most read requests and will ensure that the returned result\n\nis consistent across cluster nodes.', '### Write consistency factor\n\n\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\n\nIt can be configured at the collection\'s creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python', '""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n\n    shard_number=6,\n\n    replication_factor=2,\n\n    write_consistency_factor=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', '* `params` - other collection parameters, including `write_consistency_factor` and `on_disk_payload`. \n\n\n\nFull API specification is available in [schema definitions](https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/update_collection).\n\n\n\nCalls to this endpoint may be blocking as it waits for existing optimizers to\n\nfinish. We recommended against using this in a production database as it may\n\nintroduce huge overhead due to the rebuilding of the index.']","Question: How does adjusting the `write_consistency_factor` parameter impact write operations in a distributed deployment using Qdrant?\n\nContext: - `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
collectionName: ""{collection_name}"",

vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

shardNumber: 6,

replicationFactor: 2,

writeConsistencyFactor: 2

);

```



Write operations will fail if the number of active replicas is less than the `write_consistency_factor`.



### Read consistency



Read `consistency` can be specified for most read requests and will ensure that the returned result

is consistent across cluster nodes.
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 300,

""distance"": ""Cosine""

},

""shard_number"": 6,

""replication_factor"": 2,

""write_consistency_factor"": 2,

}

```



```python
""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

shard_number=6,

replication_factor=2,

write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
* `params` - other collection parameters, including `write_consistency_factor` and `on_disk_payload`. 



Full API specification is available in [schema definitions](https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/update_collection).



Calls to this endpoint may be blocking as it waits for existing optimizers to

finish. We recommended against using this in a production database as it may

introduce huge overhead due to the rebuilding of the index.\n\nAnswer:",How does adjusting the `write_consistency_factor` parameter impact write operations in a distributed deployment using Qdrant?,"The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully.","['- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.', 'collectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\n\tshardNumber: 6,\n\n\treplicationFactor: 2,\n\n\twriteConsistencyFactor: 2\n\n);\n\n```\n\n\n\nWrite operations will fail if the number of active replicas is less than the `write_consistency_factor`.\n\n\n\n### Read consistency\n\n\n\nRead `consistency` can be specified for most read requests and will ensure that the returned result\n\nis consistent across cluster nodes.', '### Write consistency factor\n\n\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\n\nIt can be configured at the collection\'s creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python', '""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n\n    shard_number=6,\n\n    replication_factor=2,\n\n    write_consistency_factor=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', '* `params` - other collection parameters, including `write_consistency_factor` and `on_disk_payload`. \n\n\n\nFull API specification is available in [schema definitions](https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/update_collection).\n\n\n\nCalls to this endpoint may be blocking as it waits for existing optimizers to\n\nfinish. We recommended against using this in a production database as it may\n\nintroduce huge overhead due to the rebuilding of the index.']","- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.
collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

	shardNumber: 6,

	replicationFactor: 2,

	writeConsistencyFactor: 2

);

```



Write operations will fail if the number of active replicas is less than the `write_consistency_factor`.



### Read consistency



Read `consistency` can be specified for most read requests and will ensure that the returned result

is consistent across cluster nodes.
### Write consistency factor



The `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.

It can be configured at the collection's creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

    ""write_consistency_factor"": 2,

}

```



```python
""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

    shard_number=6,

    replication_factor=2,

    write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
* `params` - other collection parameters, including `write_consistency_factor` and `on_disk_payload`. 



Full API specification is available in [schema definitions](https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/update_collection).



Calls to this endpoint may be blocking as it waits for existing optimizers to

finish. We recommended against using this in a production database as it may

introduce huge overhead due to the rebuilding of the index.",0.22748815165876776,0.8571428571428571,0.3595505617977528,0.16666666666666666,0.6363636363636364,0.2641509433962264,0.20853080568720378,0.7857142857142857,0.3295880149812734,0.22748815165876776,0.8571428571428571,0.3595505617977528,0.9277154207229614,0.6200878024101257,0.7625842094421387,0.6839932203292847,0.5983297089890887,0.8210333913642293,225,3.629032258064516,0.7178990230412916
True,,1844.953,560079d0,What are some of the features and support services that come pre-configured with each instance in Qdrant Cloud?,"Each instance in Qdrant Cloud comes pre-configured with the following tools, features, and support services:1. Automatically created with the latest available version of Qdrant.
2. Upgradeable to later versions of Qdrant as they are released.
3. Equipped with monitoring and logging to observe the health of each cluster.
4. Accessible through the Qdrant Cloud Console.
5. Vertically scalable.
6. Offered on AWS and GCP, with Azure currently in development.
","Some of the features and support services that come pre-configured with each instance in Qdrant Cloud include:

- Upgradeable to later versions of Qdrant as they are released.
- Equipped with monitoring and logging to observe the health of each cluster.
- Accessible through the Qdrant Cloud Console.
- Vertically scalable.
- Offered on AWS and GCP, with Azure currently in development.","It provides horizontal and vertical scaling, one click installation and upgrades, monitoring, logging, as well as backup and disaster recovery. For more information, see the [Qdrant Cloud documentation](/documentation/cloud).



### Kubernetes



You can use a ready-made [Helm Chart](https://helm.sh/docs/) to run Qdrant in your Kubernetes cluster:



```bash

helm repo add qdrant https://qdrant.to/helm

helm install qdrant qdrant/qdrant

```
- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:
## Configuration



Qdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.



Example: 

```yaml

qdrant:

    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""

    api_key: ""<your-api-key>""

```



The available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:

| Field        | Description |
Transitioning from on-premise to the cloud version of Qdrant does not require changing anything in the way you interact with the service. All you have to do is [create a Qdrant Cloud account](https://qdrant.to/cloud) and [provide a new API key]({{< ref ""/documentation/cloud/authentication"" >}}) to each request.
## Prerequisites



Before you start, make sure you have the following:



1. A Qdrant instance to connect to. You can get free cloud instance [cloud.qdrant.io](https://cloud.qdrant.io/). 

2. An account at Make.com. You can register yourself [here](https://www.make.com/en/register).



## Setting up a connection



Navigate to your scenario on the Make dashboard and select a Qdrant app module to start a connection.

![Qdrant Make connection](/documentation/frameworks/make/connection.png)","['It provides horizontal and vertical scaling, one click installation and upgrades, monitoring, logging, as well as backup and disaster recovery. For more information, see the [Qdrant Cloud documentation](/documentation/cloud).\n\n\n\n### Kubernetes\n\n\n\nYou can use a ready-made [Helm Chart](https://helm.sh/docs/) to run Qdrant in your Kubernetes cluster:\n\n\n\n```bash\n\nhelm repo add qdrant https://qdrant.to/helm\n\nhelm install qdrant qdrant/qdrant\n\n```', '- Upgradeable to later versions of Qdrant as they are released.\n\n- Equipped with monitoring and logging to observe the health of each cluster. \n\n- Accessible through the Qdrant Cloud Console.\n\n- Vertically scalable.\n\n- Offered on AWS and GCP, with Azure currently in development. \n\n\n\n### Getting started with Qdrant Cloud\n\n\n\nTo use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:', '## Configuration\n\n\n\nQdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.\n\n\n\nExample: \n\n```yaml\n\nqdrant:\n\n    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""\n\n    api_key: ""<your-api-key>""\n\n```\n\n\n\nThe available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:\n\n| Field        | Description |', 'Transitioning from on-premise to the cloud version of Qdrant does not require changing anything in the way you interact with the service. All you have to do is [create a Qdrant Cloud account](https://qdrant.to/cloud) and [provide a new API key]({{< ref ""/documentation/cloud/authentication"" >}}) to each request.', '## Prerequisites\n\n\n\nBefore you start, make sure you have the following:\n\n\n\n1. A Qdrant instance to connect to. You can get free cloud instance [cloud.qdrant.io](https://cloud.qdrant.io/). \n\n2. An account at Make.com. You can register yourself [here](https://www.make.com/en/register).\n\n\n\n## Setting up a connection\n\n\n\nNavigate to your scenario on the Make dashboard and select a Qdrant app module to start a connection.\n\n![Qdrant Make connection](/documentation/frameworks/make/connection.png)']","Question: What are some of the features and support services that come pre-configured with each instance in Qdrant Cloud?\n\nContext: It provides horizontal and vertical scaling, one click installation and upgrades, monitoring, logging, as well as backup and disaster recovery. For more information, see the [Qdrant Cloud documentation](/documentation/cloud).



### Kubernetes



You can use a ready-made [Helm Chart](https://helm.sh/docs/) to run Qdrant in your Kubernetes cluster:



```bash

helm repo add qdrant https://qdrant.to/helm

helm install qdrant qdrant/qdrant

```
- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:
## Configuration



Qdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.



Example: 

```yaml

qdrant:

url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""

api_key: ""<your-api-key>""

```



The available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:

| Field        | Description |
Transitioning from on-premise to the cloud version of Qdrant does not require changing anything in the way you interact with the service. All you have to do is [create a Qdrant Cloud account](https://qdrant.to/cloud) and [provide a new API key]({{< ref ""/documentation/cloud/authentication"" >}}) to each request.
## Prerequisites



Before you start, make sure you have the following:



1. A Qdrant instance to connect to. You can get free cloud instance [cloud.qdrant.io](https://cloud.qdrant.io/). 

2. An account at Make.com. You can register yourself [here](https://www.make.com/en/register).



## Setting up a connection



Navigate to your scenario on the Make dashboard and select a Qdrant app module to start a connection.

![Qdrant Make connection](/documentation/frameworks/make/connection.png)\n\nAnswer:",What are some of the features and support services that come pre-configured with each instance in Qdrant Cloud?,"Each instance in Qdrant Cloud comes pre-configured with the following tools, features, and support services:1. Automatically created with the latest available version of Qdrant.
2. Upgradeable to later versions of Qdrant as they are released.
3. Equipped with monitoring and logging to observe the health of each cluster.
4. Accessible through the Qdrant Cloud Console.
5. Vertically scalable.
6. Offered on AWS and GCP, with Azure currently in development.
","['It provides horizontal and vertical scaling, one click installation and upgrades, monitoring, logging, as well as backup and disaster recovery. For more information, see the [Qdrant Cloud documentation](/documentation/cloud).\n\n\n\n### Kubernetes\n\n\n\nYou can use a ready-made [Helm Chart](https://helm.sh/docs/) to run Qdrant in your Kubernetes cluster:\n\n\n\n```bash\n\nhelm repo add qdrant https://qdrant.to/helm\n\nhelm install qdrant qdrant/qdrant\n\n```', '- Upgradeable to later versions of Qdrant as they are released.\n\n- Equipped with monitoring and logging to observe the health of each cluster. \n\n- Accessible through the Qdrant Cloud Console.\n\n- Vertically scalable.\n\n- Offered on AWS and GCP, with Azure currently in development. \n\n\n\n### Getting started with Qdrant Cloud\n\n\n\nTo use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:', '## Configuration\n\n\n\nQdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.\n\n\n\nExample: \n\n```yaml\n\nqdrant:\n\n    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""\n\n    api_key: ""<your-api-key>""\n\n```\n\n\n\nThe available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:\n\n| Field        | Description |', 'Transitioning from on-premise to the cloud version of Qdrant does not require changing anything in the way you interact with the service. All you have to do is [create a Qdrant Cloud account](https://qdrant.to/cloud) and [provide a new API key]({{< ref ""/documentation/cloud/authentication"" >}}) to each request.', '## Prerequisites\n\n\n\nBefore you start, make sure you have the following:\n\n\n\n1. A Qdrant instance to connect to. You can get free cloud instance [cloud.qdrant.io](https://cloud.qdrant.io/). \n\n2. An account at Make.com. You can register yourself [here](https://www.make.com/en/register).\n\n\n\n## Setting up a connection\n\n\n\nNavigate to your scenario on the Make dashboard and select a Qdrant app module to start a connection.\n\n![Qdrant Make connection](/documentation/frameworks/make/connection.png)']","It provides horizontal and vertical scaling, one click installation and upgrades, monitoring, logging, as well as backup and disaster recovery. For more information, see the [Qdrant Cloud documentation](/documentation/cloud).



### Kubernetes



You can use a ready-made [Helm Chart](https://helm.sh/docs/) to run Qdrant in your Kubernetes cluster:



```bash

helm repo add qdrant https://qdrant.to/helm

helm install qdrant qdrant/qdrant

```
- Upgradeable to later versions of Qdrant as they are released.

- Equipped with monitoring and logging to observe the health of each cluster. 

- Accessible through the Qdrant Cloud Console.

- Vertically scalable.

- Offered on AWS and GCP, with Azure currently in development. 



### Getting started with Qdrant Cloud



To use Qdrant Cloud, you will need to create at least one cluster. There are two ways to start:
## Configuration



Qdrant settings can be configured by setting values to the qdrant property in the `settings.yaml` file. By default, Qdrant tries to connect to an instance at http://localhost:3000.



Example: 

```yaml

qdrant:

    url: ""https://xyz-example.eu-central.aws.cloud.qdrant.io:6333""

    api_key: ""<your-api-key>""

```



The available [configuration options](https://docs.privategpt.dev/manual/storage/vector-stores#qdrant-configuration) are:

| Field        | Description |
Transitioning from on-premise to the cloud version of Qdrant does not require changing anything in the way you interact with the service. All you have to do is [create a Qdrant Cloud account](https://qdrant.to/cloud) and [provide a new API key]({{< ref ""/documentation/cloud/authentication"" >}}) to each request.
## Prerequisites



Before you start, make sure you have the following:



1. A Qdrant instance to connect to. You can get free cloud instance [cloud.qdrant.io](https://cloud.qdrant.io/). 

2. An account at Make.com. You can register yourself [here](https://www.make.com/en/register).



## Setting up a connection



Navigate to your scenario on the Make dashboard and select a Qdrant app module to start a connection.

![Qdrant Make connection](/documentation/frameworks/make/connection.png)",0.9454545454545454,0.7761194029850746,0.8524590163934428,0.7962962962962963,0.6515151515151515,0.7166666666666667,0.8,0.6567164179104478,0.7213114754098361,0.8363636363636363,0.6865671641791045,0.7540983606557378,0.9616585969924927,0.8756068348884583,0.8163191676139832,0.84492427110672,0.6856086701154709,0.3312761001288891,57,0.8260869565217391,0.5907355612160425
True,,3959.251,c12193ce,How is the `must_not` clause used in conjunction with other clauses in filtering data?,"The `must_not` clause in filtering data is used to exclude documents that meet specific criteria. The `must_not` clause can be used in combination with the `must` clause to create a more refined filter. Specifically, the `must_not` clause is used to specify conditions that must not be met for a document to be included in the results.","The `must_not` clause is used in conjunction with other clauses to filter data by excluding certain conditions. When using `must_not`, the clause evaluates to `true` if the specified conditions are not satisfied. It is equivalent to the logical expression `(NOT A) AND (NOT B) AND (NOT C)`.

In the provided examples, the `must_not` clause is used alongside the `must` clause to filter data based on specific criteria. For instance, in the HTTP, Python, TypeScript, Rust, and Java examples provided, the `must_not` clause is used to exclude data where the city is ""London"" and the color is ""red"".

By combining `must_not` with other clauses like `must`, you can create complex filtering conditions to retrieve the desired data from a dataset.","{ ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }

]

```



When using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.

In this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.



### Clauses combination



It is also possible to use several clauses simultaneously:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""must"": [
{

    ""filter"": {

        ""must"": [

            { ""key"": ""city"", ""match"": { ""value"": ""London"" } }

        ],

        ""must_not"": [

            { ""key"": ""color"", ""match"": { ""value"": ""red"" } }

        ]

    }

}

```



```python

client.scroll(

    collection_name=""{collection_name}"",

    scroll_filter=models.Filter(

        must=[

            models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")),

        ],

        must_not=[
{ ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },

  { ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" }

]

```



When using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied.

In this sense, `should` is equivalent to the operator `OR`.



### Must Not



Example:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""must_not"": [

            { ""key"": ""city"", ""match"": { ""value"": ""London"" } },
],

        must_not=[

            models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")),

        ],

    ),

)

```



```typescript

client.scroll(""{collection_name}"", {

  filter: {

    must: [

      {

        key: ""city"",

        match: { value: ""London"" },

      },

    ],

    must_not: [

      {

        key: ""color"",

        match: { value: ""red"" },

      },

    ],

  },

});

```



```rust

use qdrant_client::qdrant::{Condition, Filter, ScrollPoints};
filter: Some(Filter::must_not([

            Condition::matches(""city"", ""london"".to_string()),

            Condition::matches(""color"", ""red"".to_string()),

        ])),

        ..Default::default()

    })

    .await?;

```



```java

import java.util.List;



import static io.qdrant.client.ConditionFactory.matchKeyword;



import io.qdrant.client.grpc.Points.Filter;

import io.qdrant.client.grpc.Points.ScrollPoints;



client

    .scrollAsync(

        ScrollPoints.newBuilder()","['{ ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }\n\n]\n\n```\n\n\n\nWhen using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.\n\nIn this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.\n\n\n\n### Clauses combination\n\n\n\nIt is also possible to use several clauses simultaneously:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""must"": [', '{\n\n    ""filter"": {\n\n        ""must"": [\n\n            { ""key"": ""city"", ""match"": { ""value"": ""London"" } }\n\n        ],\n\n        ""must_not"": [\n\n            { ""key"": ""color"", ""match"": { ""value"": ""red"" } }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.scroll(\n\n    collection_name=""{collection_name}"",\n\n    scroll_filter=models.Filter(\n\n        must=[\n\n            models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")),\n\n        ],\n\n        must_not=[', '{ ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },\n\n  { ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" }\n\n]\n\n```\n\n\n\nWhen using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied.\n\nIn this sense, `should` is equivalent to the operator `OR`.\n\n\n\n### Must Not\n\n\n\nExample:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""must_not"": [\n\n            { ""key"": ""city"", ""match"": { ""value"": ""London"" } },', '],\n\n        must_not=[\n\n            models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")),\n\n        ],\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nclient.scroll(""{collection_name}"", {\n\n  filter: {\n\n    must: [\n\n      {\n\n        key: ""city"",\n\n        match: { value: ""London"" },\n\n      },\n\n    ],\n\n    must_not: [\n\n      {\n\n        key: ""color"",\n\n        match: { value: ""red"" },\n\n      },\n\n    ],\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::qdrant::{Condition, Filter, ScrollPoints};', 'filter: Some(Filter::must_not([\n\n            Condition::matches(""city"", ""london"".to_string()),\n\n            Condition::matches(""color"", ""red"".to_string()),\n\n        ])),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport java.util.List;\n\n\n\nimport static io.qdrant.client.ConditionFactory.matchKeyword;\n\n\n\nimport io.qdrant.client.grpc.Points.Filter;\n\nimport io.qdrant.client.grpc.Points.ScrollPoints;\n\n\n\nclient\n\n    .scrollAsync(\n\n        ScrollPoints.newBuilder()']","Question: How is the `must_not` clause used in conjunction with other clauses in filtering data?\n\nContext: { ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }

]

```



When using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.

In this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.



### Clauses combination



It is also possible to use several clauses simultaneously:



```http

POST /collections/{collection_name}/points/scroll

{

""filter"": {

""must"": [
{

""filter"": {

""must"": [

{ ""key"": ""city"", ""match"": { ""value"": ""London"" } }

],

""must_not"": [

{ ""key"": ""color"", ""match"": { ""value"": ""red"" } }

]

}

}

```



```python

client.scroll(

collection_name=""{collection_name}"",

scroll_filter=models.Filter(

must=[

models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")),

],

must_not=[
{ ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },

{ ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" }

]

```



When using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied.

In this sense, `should` is equivalent to the operator `OR`.



### Must Not



Example:



```http

POST /collections/{collection_name}/points/scroll

{

""filter"": {

""must_not"": [

{ ""key"": ""city"", ""match"": { ""value"": ""London"" } },
],

must_not=[

models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")),

],

),

)

```



```typescript

client.scroll(""{collection_name}"", {

filter: {

must: [

{

key: ""city"",

match: { value: ""London"" },

},

],

must_not: [

{

key: ""color"",

match: { value: ""red"" },

},

],

},

});

```



```rust

use qdrant_client::qdrant::{Condition, Filter, ScrollPoints};
filter: Some(Filter::must_not([

Condition::matches(""city"", ""london"".to_string()),

Condition::matches(""color"", ""red"".to_string()),

])),

..Default::default()

})

.await?;

```



```java

import java.util.List;



import static io.qdrant.client.ConditionFactory.matchKeyword;



import io.qdrant.client.grpc.Points.Filter;

import io.qdrant.client.grpc.Points.ScrollPoints;



client

.scrollAsync(

ScrollPoints.newBuilder()\n\nAnswer:",How is the `must_not` clause used in conjunction with other clauses in filtering data?,"The `must_not` clause in filtering data is used to exclude documents that meet specific criteria. The `must_not` clause can be used in combination with the `must` clause to create a more refined filter. Specifically, the `must_not` clause is used to specify conditions that must not be met for a document to be included in the results.","['{ ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }\n\n]\n\n```\n\n\n\nWhen using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.\n\nIn this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.\n\n\n\n### Clauses combination\n\n\n\nIt is also possible to use several clauses simultaneously:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""must"": [', '{\n\n    ""filter"": {\n\n        ""must"": [\n\n            { ""key"": ""city"", ""match"": { ""value"": ""London"" } }\n\n        ],\n\n        ""must_not"": [\n\n            { ""key"": ""color"", ""match"": { ""value"": ""red"" } }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.scroll(\n\n    collection_name=""{collection_name}"",\n\n    scroll_filter=models.Filter(\n\n        must=[\n\n            models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")),\n\n        ],\n\n        must_not=[', '{ ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },\n\n  { ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" }\n\n]\n\n```\n\n\n\nWhen using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied.\n\nIn this sense, `should` is equivalent to the operator `OR`.\n\n\n\n### Must Not\n\n\n\nExample:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/scroll\n\n{\n\n    ""filter"": {\n\n        ""must_not"": [\n\n            { ""key"": ""city"", ""match"": { ""value"": ""London"" } },', '],\n\n        must_not=[\n\n            models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")),\n\n        ],\n\n    ),\n\n)\n\n```\n\n\n\n```typescript\n\nclient.scroll(""{collection_name}"", {\n\n  filter: {\n\n    must: [\n\n      {\n\n        key: ""city"",\n\n        match: { value: ""London"" },\n\n      },\n\n    ],\n\n    must_not: [\n\n      {\n\n        key: ""color"",\n\n        match: { value: ""red"" },\n\n      },\n\n    ],\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::qdrant::{Condition, Filter, ScrollPoints};', 'filter: Some(Filter::must_not([\n\n            Condition::matches(""city"", ""london"".to_string()),\n\n            Condition::matches(""color"", ""red"".to_string()),\n\n        ])),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport java.util.List;\n\n\n\nimport static io.qdrant.client.ConditionFactory.matchKeyword;\n\n\n\nimport io.qdrant.client.grpc.Points.Filter;\n\nimport io.qdrant.client.grpc.Points.ScrollPoints;\n\n\n\nclient\n\n    .scrollAsync(\n\n        ScrollPoints.newBuilder()']","{ ""id"": 6, ""city"": ""Moscow"", ""color"": ""blue"" }

]

```



When using `must_not`, the clause becomes `true` if none if the conditions listed inside `should` is satisfied.

In this sense, `must_not` is equivalent to the expression `(NOT A) AND (NOT B) AND (NOT C)`.



### Clauses combination



It is also possible to use several clauses simultaneously:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""must"": [
{

    ""filter"": {

        ""must"": [

            { ""key"": ""city"", ""match"": { ""value"": ""London"" } }

        ],

        ""must_not"": [

            { ""key"": ""color"", ""match"": { ""value"": ""red"" } }

        ]

    }

}

```



```python

client.scroll(

    collection_name=""{collection_name}"",

    scroll_filter=models.Filter(

        must=[

            models.FieldCondition(key=""city"", match=models.MatchValue(value=""London"")),

        ],

        must_not=[
{ ""id"": 3, ""city"": ""London"", ""color"": ""blue"" },

  { ""id"": 4, ""city"": ""Berlin"", ""color"": ""red"" }

]

```



When using `should`, the clause becomes `true` if at least one condition listed inside `should` is satisfied.

In this sense, `should` is equivalent to the operator `OR`.



### Must Not



Example:



```http

POST /collections/{collection_name}/points/scroll

{

    ""filter"": {

        ""must_not"": [

            { ""key"": ""city"", ""match"": { ""value"": ""London"" } },
],

        must_not=[

            models.FieldCondition(key=""color"", match=models.MatchValue(value=""red"")),

        ],

    ),

)

```



```typescript

client.scroll(""{collection_name}"", {

  filter: {

    must: [

      {

        key: ""city"",

        match: { value: ""London"" },

      },

    ],

    must_not: [

      {

        key: ""color"",

        match: { value: ""red"" },

      },

    ],

  },

});

```



```rust

use qdrant_client::qdrant::{Condition, Filter, ScrollPoints};
filter: Some(Filter::must_not([

            Condition::matches(""city"", ""london"".to_string()),

            Condition::matches(""color"", ""red"".to_string()),

        ])),

        ..Default::default()

    })

    .await?;

```



```java

import java.util.List;



import static io.qdrant.client.ConditionFactory.matchKeyword;



import io.qdrant.client.grpc.Points.Filter;

import io.qdrant.client.grpc.Points.ScrollPoints;



client

    .scrollAsync(

        ScrollPoints.newBuilder()",0.34545454545454546,0.7307692307692307,0.4691358024691359,0.1651376146788991,0.35294117647058826,0.22500000000000003,0.23636363636363636,0.5,0.32098765432098764,0.3181818181818182,0.6730769230769231,0.4320987654320988,0.8033227324485779,0.6113802194595337,0.6948496699333191,0.6504480838775635,0.6275244100640218,0.8268666341900826,119,2.125,0.6077156051637455
True,,4327.776,8825dd72,What are some advantages of vector search over keyword-based search?,"Vector search has clear advantages over keyword-based search in various scenarios as outlined in the document chunk. Some of these advantages include:

1. Multi-lingual & multi-modal search: Vector search is effective in handling searches across multiple languages and modalities.
2. For short texts with typos and ambiguous content-dependent meanings: Vector search excels in situations where the search queries are short, contain typos, or have ambiguous meanings.
3. Specialized domains with tuned encoder models: Vector search is beneficial in specialized domains where encoder models can be fine-tuned for better search results.
4. Document-as-a-Query similarity search: Vector search allows for similarity searches where the entire document can be used as a query to find similar documents.

While vector search offers these advantages, it is important to note that keyword-based search still has its relevance in certain cases. For example, in out-of-domain search.","Some advantages of vector search over keyword-based search include:

1. **Multi-lingual & multi-modal search**: Vector search can handle multiple languages and modalities effectively, making it useful in diverse content environments.

2. **Handling short texts with typos and ambiguous content**: Vector search is more robust in dealing with short texts that may contain typos or have ambiguous meanings, improving search accuracy.

3. **Specialized domains with tuned encoder models**: Vector search can be tailored to specific domains by using encoder models that are fine-tuned for the specialized content, leading to more relevant search results.

4. **Document-as-a-Query similarity search**: Vector search excels in finding similarities between documents based on their content, enabling more nuanced and precise search results.

5. **High compression rate and over-parameterization for retrieval**: Vector search can achieve a high compression rate while maintaining a large amount of information encoded in the vectors, which is beneficial for ranking, clustering, and other similar use cases.

6. **Efficient retrieval in large datasets**: While keyword-based search may struggle with finding similar documents in a large set of objects, vector search can efficiently handle this task by avoiding the need to calculate distances to every document individually.

Overall, vector search offers improved accuracy, flexibility, and efficiency compared to traditional keyword-based search methods, especially in scenarios with complex or diverse content.","preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has

some clear advantages in the following scenarios:



- 🌍 Multi-lingual & multi-modal search

- 🤔 For short texts with typos and ambiguous content-dependent meanings

- 👨‍🔬 Specialized domains with tuned encoder models

- 📄 Document-as-a-Query similarity search



It doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method

might be useful:
## Next Steps



Vector search is an exciting alternative to sparse methods. It solves the issues we had with the keyword-based search without needing to maintain lots of heuristics manually. It requires an additional component, a neural encoder, to convert text into vectors. 



[**Tutorial 1 - Qdrant for Complete Beginners**](../../tutorials/search-beginners)
documents. There are also some cases in which your users use different terminology than the one you have in your database. 

Those problems are easily solvable with neural vector embeddings, and combining both approaches with an additional reranking 

step is possible. So you don't need to resign from your well-known full-text search mechanism but extend it with vector 

search to support the queries you haven't foreseen.
One of the reasons vector search still works with such a high compression rate is that these large vectors are over-parameterized for retrieval. This is because they are designed for ranking, clustering, and similar use cases, which typically need more information encoded in the vector.
The challenge with vector search arises when we need to find similar documents in a big set of objects. If we want to find the closest examples, the naive approach would require calculating the distance to every document. That might work with dozens or even hundreds of examples but may become a bottleneck if we have more than that. When we work with relational data, we set up database indexes to speed things up and avoid full table scans. And the same is true for vector search","[""preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has\n\nsome clear advantages in the following scenarios:\n\n\n\n- 🌍 Multi-lingual & multi-modal search\n\n- 🤔 For short texts with typos and ambiguous content-dependent meanings\n\n- 👨\u200d🔬 Specialized domains with tuned encoder models\n\n- 📄 Document-as-a-Query similarity search\n\n\n\nIt doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method\n\nmight be useful:"", '## Next Steps\n\n\n\nVector search is an exciting alternative to sparse methods. It solves the issues we had with the keyword-based search without needing to maintain lots of heuristics manually. It requires an additional component, a neural encoder, to convert text into vectors. \n\n\n\n[**Tutorial 1 - Qdrant for Complete Beginners**](../../tutorials/search-beginners)', ""documents. There are also some cases in which your users use different terminology than the one you have in your database. \n\nThose problems are easily solvable with neural vector embeddings, and combining both approaches with an additional reranking \n\nstep is possible. So you don't need to resign from your well-known full-text search mechanism but extend it with vector \n\nsearch to support the queries you haven't foreseen."", 'One of the reasons vector search still works with such a high compression rate is that these large vectors are over-parameterized for retrieval. This is because they are designed for ranking, clustering, and similar use cases, which typically need more information encoded in the vector.', 'The challenge with vector search arises when we need to find similar documents in a big set of objects. If we want to find the closest examples, the naive approach would require calculating the distance to every document. That might work with dozens or even hundreds of examples but may become a bottleneck if we have more than that. When we work with relational data, we set up database indexes to speed things up and avoid full table scans. And the same is true for vector search']","Question: What are some advantages of vector search over keyword-based search?\n\nContext: preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has

some clear advantages in the following scenarios:



- 🌍 Multi-lingual & multi-modal search

- 🤔 For short texts with typos and ambiguous content-dependent meanings

- 👨‍🔬 Specialized domains with tuned encoder models

- 📄 Document-as-a-Query similarity search



It doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method

might be useful:
## Next Steps



Vector search is an exciting alternative to sparse methods. It solves the issues we had with the keyword-based search without needing to maintain lots of heuristics manually. It requires an additional component, a neural encoder, to convert text into vectors. 



[**Tutorial 1 - Qdrant for Complete Beginners**](../../tutorials/search-beginners)
documents. There are also some cases in which your users use different terminology than the one you have in your database. 

Those problems are easily solvable with neural vector embeddings, and combining both approaches with an additional reranking 

step is possible. So you don't need to resign from your well-known full-text search mechanism but extend it with vector 

search to support the queries you haven't foreseen.
One of the reasons vector search still works with such a high compression rate is that these large vectors are over-parameterized for retrieval. This is because they are designed for ranking, clustering, and similar use cases, which typically need more information encoded in the vector.
The challenge with vector search arises when we need to find similar documents in a big set of objects. If we want to find the closest examples, the naive approach would require calculating the distance to every document. That might work with dozens or even hundreds of examples but may become a bottleneck if we have more than that. When we work with relational data, we set up database indexes to speed things up and avoid full table scans. And the same is true for vector search\n\nAnswer:",What are some advantages of vector search over keyword-based search?,"Vector search has clear advantages over keyword-based search in various scenarios as outlined in the document chunk. Some of these advantages include:

1. Multi-lingual & multi-modal search: Vector search is effective in handling searches across multiple languages and modalities.
2. For short texts with typos and ambiguous content-dependent meanings: Vector search excels in situations where the search queries are short, contain typos, or have ambiguous meanings.
3. Specialized domains with tuned encoder models: Vector search is beneficial in specialized domains where encoder models can be fine-tuned for better search results.
4. Document-as-a-Query similarity search: Vector search allows for similarity searches where the entire document can be used as a query to find similar documents.

While vector search offers these advantages, it is important to note that keyword-based search still has its relevance in certain cases. For example, in out-of-domain search.","[""preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has\n\nsome clear advantages in the following scenarios:\n\n\n\n- 🌍 Multi-lingual & multi-modal search\n\n- 🤔 For short texts with typos and ambiguous content-dependent meanings\n\n- 👨\u200d🔬 Specialized domains with tuned encoder models\n\n- 📄 Document-as-a-Query similarity search\n\n\n\nIt doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method\n\nmight be useful:"", '## Next Steps\n\n\n\nVector search is an exciting alternative to sparse methods. It solves the issues we had with the keyword-based search without needing to maintain lots of heuristics manually. It requires an additional component, a neural encoder, to convert text into vectors. \n\n\n\n[**Tutorial 1 - Qdrant for Complete Beginners**](../../tutorials/search-beginners)', ""documents. There are also some cases in which your users use different terminology than the one you have in your database. \n\nThose problems are easily solvable with neural vector embeddings, and combining both approaches with an additional reranking \n\nstep is possible. So you don't need to resign from your well-known full-text search mechanism but extend it with vector \n\nsearch to support the queries you haven't foreseen."", 'One of the reasons vector search still works with such a high compression rate is that these large vectors are over-parameterized for retrieval. This is because they are designed for ranking, clustering, and similar use cases, which typically need more information encoded in the vector.', 'The challenge with vector search arises when we need to find similar documents in a big set of objects. If we want to find the closest examples, the naive approach would require calculating the distance to every document. That might work with dozens or even hundreds of examples but may become a bottleneck if we have more than that. When we work with relational data, we set up database indexes to speed things up and avoid full table scans. And the same is true for vector search']","preprocessing of the documents and queries. Vector search turned out to be a breakthrough, as it has

some clear advantages in the following scenarios:



- 🌍 Multi-lingual & multi-modal search

- 🤔 For short texts with typos and ambiguous content-dependent meanings

- 👨‍🔬 Specialized domains with tuned encoder models

- 📄 Document-as-a-Query similarity search



It doesn't mean we do not keyword search anymore. There are also some cases in which this kind of method

might be useful:
## Next Steps



Vector search is an exciting alternative to sparse methods. It solves the issues we had with the keyword-based search without needing to maintain lots of heuristics manually. It requires an additional component, a neural encoder, to convert text into vectors. 



[**Tutorial 1 - Qdrant for Complete Beginners**](../../tutorials/search-beginners)
documents. There are also some cases in which your users use different terminology than the one you have in your database. 

Those problems are easily solvable with neural vector embeddings, and combining both approaches with an additional reranking 

step is possible. So you don't need to resign from your well-known full-text search mechanism but extend it with vector 

search to support the queries you haven't foreseen.
One of the reasons vector search still works with such a high compression rate is that these large vectors are over-parameterized for retrieval. This is because they are designed for ranking, clustering, and similar use cases, which typically need more information encoded in the vector.
The challenge with vector search arises when we need to find similar documents in a big set of objects. If we want to find the closest examples, the naive approach would require calculating the distance to every document. That might work with dozens or even hundreds of examples but may become a bottleneck if we have more than that. When we work with relational data, we set up database indexes to speed things up and avoid full table scans. And the same is true for vector search",0.5069124423963134,0.7586206896551724,0.6077348066298343,0.26851851851851855,0.4027777777777778,0.3222222222222223,0.35944700460829493,0.5379310344827586,0.430939226519337,0.46543778801843316,0.696551724137931,0.5580110497237569,0.9670776128768921,0.7033056020736694,0.7350832223892212,0.718843400478363,0.628698390792124,0.7524316164323994,214,1.539568345323741,0.4809296322424265
True,,2211.534,5209cb45,What is the purpose of the DocArray library and how does it relate to the Qdrant engine?,"The DocArray library is designed to define a structure for unstructured data, making it easier to process a collection of documents that may include various data types such as audio, video, and text. This library simplifies the handling of these diverse types of data within a single framework. On the other hand, the Qdrant engine focuses on enabling the scaling of vector search and storage capabilities. By integrating the DocArray library with the Qdrant engine, users can benefit from efficient processing of unstructured data and enhanced scalability for vector search operations. This integration streamlines the handling of different data types while also optimizing the performance of vector search and storage functions.","The purpose of the DocArray library is to provide a structured way to handle nested, unstructured data in transit, such as text, image, audio, video, 3D mesh, etc. It enables deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer data using a Pythonic API.

In relation to the Qdrant engine, the DocArray library integrates with Qdrant as a high-performance document store for scalable vector search. Qdrant engine serves as a storage backend for the DocArray solution, allowing users to benefit from the Pythonic API of DocArray and the performance of Qdrant for vector search and storage operations.","---

title: DocArray

weight: 300

aliases: [ ../integrations/docarray/ ]

---



# DocArray

You can use Qdrant natively in DocArray, where Qdrant serves as a high-performance document store to enable scalable vector search.



DocArray is a library from Jina AI for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc.

It allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API.
DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,

including audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.



Read more about the integration by this [link](/documentation/install/#docarray)
date: 2022-03-15T15:00:00+03:00

author: Alyona Kavyerina

featured: false

author_link: https://medium.com/@alyona.kavyerina

tags:

  - jina integration

  - docarray

categories:

  - News

---

We are happy to announce that [Jina.AI](https://jina.ai/) integrates Qdrant engine as a storage backend to their [DocArray](https://docarray.jina.ai/) solution.



Now you can experience the convenience of Pythonic API and Rust performance in a single workflow.
```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

    texts=[""Qdrant is the best vector database!""],

    model=""nomic-embed-text-v1"",

    task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

    collection_name=""my-collection"",

    points=models.Batch(

        ids=[1],

        vectors=output[""embeddings""],

    ),

)

```



To query the collection, set the `task_type` to `search_query`:



```python
WITH ENGINE = ""qdrant"",

PARAMETERS = {

    ""location"": "":memory:"",

    ""collection_config"": {

        ""size"": 386,

        ""distance"": ""Cosine""

    }

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table","['---\n\ntitle: DocArray\n\nweight: 300\n\naliases: [ ../integrations/docarray/ ]\n\n---\n\n\n\n# DocArray\n\nYou can use Qdrant natively in DocArray, where Qdrant serves as a high-performance document store to enable scalable vector search.\n\n\n\nDocArray is a library from Jina AI for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc.\n\nIt allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API.', 'DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,\n\nincluding audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.\n\n\n\nRead more about the integration by this [link](/documentation/install/#docarray)', 'date: 2022-03-15T15:00:00+03:00\n\nauthor: Alyona Kavyerina\n\nfeatured: false\n\nauthor_link: https://medium.com/@alyona.kavyerina\n\ntags:\n\n  - jina integration\n\n  - docarray\n\ncategories:\n\n  - News\n\n---\n\nWe are happy to announce that [Jina.AI](https://jina.ai/) integrates Qdrant engine as a storage backend to their [DocArray](https://docarray.jina.ai/) solution.\n\n\n\nNow you can experience the convenience of Pythonic API and Rust performance in a single workflow.', '```python\n\nfrom qdrant_client import QdrantClient, models\n\nfrom nomic import embed\n\n\n\noutput = embed.text(\n\n    texts=[""Qdrant is the best vector database!""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_document"",\n\n)\n\n\n\nqdrant_client = QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""my-collection"",\n\n    points=models.Batch(\n\n        ids=[1],\n\n        vectors=output[""embeddings""],\n\n    ),\n\n)\n\n```\n\n\n\nTo query the collection, set the `task_type` to `search_query`:\n\n\n\n```python', 'WITH ENGINE = ""qdrant"",\n\nPARAMETERS = {\n\n    ""location"": "":memory:"",\n\n    ""collection_config"": {\n\n        ""size"": 386,\n\n        ""distance"": ""Cosine""\n\n    }\n\n}\n\n```\n\n\n\nThe available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).\n\n\n\n## Creating a new table']","Question: What is the purpose of the DocArray library and how does it relate to the Qdrant engine?\n\nContext: ---

title: DocArray

weight: 300

aliases: [ ../integrations/docarray/ ]

---



# DocArray

You can use Qdrant natively in DocArray, where Qdrant serves as a high-performance document store to enable scalable vector search.



DocArray is a library from Jina AI for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc.

It allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API.
DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,

including audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.



Read more about the integration by this [link](/documentation/install/#docarray)
date: 2022-03-15T15:00:00+03:00

author: Alyona Kavyerina

featured: false

author_link: https://medium.com/@alyona.kavyerina

tags:

- jina integration

- docarray

categories:

- News

---

We are happy to announce that [Jina.AI](https://jina.ai/) integrates Qdrant engine as a storage backend to their [DocArray](https://docarray.jina.ai/) solution.



Now you can experience the convenience of Pythonic API and Rust performance in a single workflow.
```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

texts=[""Qdrant is the best vector database!""],

model=""nomic-embed-text-v1"",

task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

collection_name=""my-collection"",

points=models.Batch(

ids=[1],

vectors=output[""embeddings""],

),

)

```



To query the collection, set the `task_type` to `search_query`:



```python
WITH ENGINE = ""qdrant"",

PARAMETERS = {

""location"": "":memory:"",

""collection_config"": {

""size"": 386,

""distance"": ""Cosine""

}

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table\n\nAnswer:",What is the purpose of the DocArray library and how does it relate to the Qdrant engine?,"The DocArray library is designed to define a structure for unstructured data, making it easier to process a collection of documents that may include various data types such as audio, video, and text. This library simplifies the handling of these diverse types of data within a single framework. On the other hand, the Qdrant engine focuses on enabling the scaling of vector search and storage capabilities. By integrating the DocArray library with the Qdrant engine, users can benefit from efficient processing of unstructured data and enhanced scalability for vector search operations. This integration streamlines the handling of different data types while also optimizing the performance of vector search and storage functions.","['---\n\ntitle: DocArray\n\nweight: 300\n\naliases: [ ../integrations/docarray/ ]\n\n---\n\n\n\n# DocArray\n\nYou can use Qdrant natively in DocArray, where Qdrant serves as a high-performance document store to enable scalable vector search.\n\n\n\nDocArray is a library from Jina AI for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc.\n\nIt allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API.', 'DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,\n\nincluding audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.\n\n\n\nRead more about the integration by this [link](/documentation/install/#docarray)', 'date: 2022-03-15T15:00:00+03:00\n\nauthor: Alyona Kavyerina\n\nfeatured: false\n\nauthor_link: https://medium.com/@alyona.kavyerina\n\ntags:\n\n  - jina integration\n\n  - docarray\n\ncategories:\n\n  - News\n\n---\n\nWe are happy to announce that [Jina.AI](https://jina.ai/) integrates Qdrant engine as a storage backend to their [DocArray](https://docarray.jina.ai/) solution.\n\n\n\nNow you can experience the convenience of Pythonic API and Rust performance in a single workflow.', '```python\n\nfrom qdrant_client import QdrantClient, models\n\nfrom nomic import embed\n\n\n\noutput = embed.text(\n\n    texts=[""Qdrant is the best vector database!""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_document"",\n\n)\n\n\n\nqdrant_client = QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""my-collection"",\n\n    points=models.Batch(\n\n        ids=[1],\n\n        vectors=output[""embeddings""],\n\n    ),\n\n)\n\n```\n\n\n\nTo query the collection, set the `task_type` to `search_query`:\n\n\n\n```python', 'WITH ENGINE = ""qdrant"",\n\nPARAMETERS = {\n\n    ""location"": "":memory:"",\n\n    ""collection_config"": {\n\n        ""size"": 386,\n\n        ""distance"": ""Cosine""\n\n    }\n\n}\n\n```\n\n\n\nThe available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).\n\n\n\n## Creating a new table']","---

title: DocArray

weight: 300

aliases: [ ../integrations/docarray/ ]

---



# DocArray

You can use Qdrant natively in DocArray, where Qdrant serves as a high-performance document store to enable scalable vector search.



DocArray is a library from Jina AI for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc.

It allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API.
DocArray library defines a structure for the unstructured data and simplifies processing a collection of documents,

including audio, video, text, and other data types. Qdrant engine empowers scaling of its vector search and storage.



Read more about the integration by this [link](/documentation/install/#docarray)
date: 2022-03-15T15:00:00+03:00

author: Alyona Kavyerina

featured: false

author_link: https://medium.com/@alyona.kavyerina

tags:

  - jina integration

  - docarray

categories:

  - News

---

We are happy to announce that [Jina.AI](https://jina.ai/) integrates Qdrant engine as a storage backend to their [DocArray](https://docarray.jina.ai/) solution.



Now you can experience the convenience of Pythonic API and Rust performance in a single workflow.
```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

    texts=[""Qdrant is the best vector database!""],

    model=""nomic-embed-text-v1"",

    task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

    collection_name=""my-collection"",

    points=models.Batch(

        ids=[1],

        vectors=output[""embeddings""],

    ),

)

```



To query the collection, set the `task_type` to `search_query`:



```python
WITH ENGINE = ""qdrant"",

PARAMETERS = {

    ""location"": "":memory:"",

    ""collection_config"": {

        ""size"": 386,

        ""distance"": ""Cosine""

    }

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table",0.5444444444444444,0.494949494949495,0.5185185185185185,0.19101123595505617,0.17346938775510204,0.18181818181818182,0.34444444444444444,0.31313131313131315,0.3280423280423281,0.5111111111111111,0.46464646464646464,0.4867724867724868,0.8318887948989868,0.6759136319160461,0.6619017720222473,0.6688343286514282,0.9757358164060861,0.9842359165661037,99,0.8918918918918919,0.5632007755015705
True,,2253.4480000000003,c42ef405,What are the search-time parameters that can be used to tune the search accuracy and speed?,"The search-time parameters that can be used to tune the search accuracy and speed are ""hnsw_ef"" and ""exact"". The ""hnsw_ef"" parameter is set to 128, which controls the search accuracy by specifying the number of neighbors to inspect during the search process. The ""exact"" parameter is set to false, indicating that an approximate search method is used for faster retrieval of results. These parameters are essential for optimizing the search process in the context of the QdrantClient for efficient retrieval of relevant data points.","To tune the search accuracy and speed, you can use the following search-time parameters:

1. `exact`: When set to `true`, it performs an exact search which is slower but more accurate. This parameter can be used to compare results of the search with different `hnsw_ef` values against the ground truth.

By adjusting these parameters, you can control the accuracy of the search results without rebuilding the index or changing the stored data structures. This allows for real-time optimization of search accuracy and speed by simply modifying the search query parameters.","collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },

	optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 },

	quantizationConfig: new QuantizationConfig

	{

		Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }

	}

);

```



There are also some search-time parameters you can use to tune the search accuracy and speed:



```http

POST /collections/{collection_name}/points/search

{
. And that's how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself.
- `exact` - if set to `true`, will perform exact search, which will be slower, but more accurate. You can use it to compare results of the search with different `hnsw_ef` values versus the ground truth.



## Latency vs Throughput



- There are two main approaches to measure the speed of search:

  - latency of the request - the time from the moment request is submitted to the moment a response is received

  - throughput - the number of requests per second the system can handle
## The results



We launched the benchmark 5 times on 10000 test vectors and averaged the results. Presented numbers are the mean values of all the attempts:



1. Sequential search: 225.9 seconds

2. Batch search: 208.0 seconds

3. Multiprocessing search (8 processes): 194.2 seconds

4. Multiprocessing batch search (8 processes, batch size 10): 148.9 seconds
We can serve the search requests with 135mb of RAM, but the speed of the requests makes it impossible to use the system in production.



Let's see how we can improve the speed.





## How to speed up the search





<!--

We need to look into disk parameters and see how they affect the search speed.



Let's measure the disk speed with `fio`:



```

fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=fiotest --filename=testfio --bs=4k --iodepth=64 --size=8G --readwrite=randread

```","['collectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },\n\n\toptimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 },\n\n\tquantizationConfig: new QuantizationConfig\n\n\t{\n\n\t\tScalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }\n\n\t}\n\n);\n\n```\n\n\n\nThere are also some search-time parameters you can use to tune the search accuracy and speed:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{', "". And that's how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself."", '- `exact` - if set to `true`, will perform exact search, which will be slower, but more accurate. You can use it to compare results of the search with different `hnsw_ef` values versus the ground truth.\n\n\n\n## Latency vs Throughput\n\n\n\n- There are two main approaches to measure the speed of search:\n\n  - latency of the request - the time from the moment request is submitted to the moment a response is received\n\n  - throughput - the number of requests per second the system can handle', '## The results\n\n\n\nWe launched the benchmark 5 times on 10000 test vectors and averaged the results. Presented numbers are the mean values of all the attempts:\n\n\n\n1. Sequential search: 225.9 seconds\n\n2. Batch search: 208.0 seconds\n\n3. Multiprocessing search (8 processes): 194.2 seconds\n\n4. Multiprocessing batch search (8 processes, batch size 10): 148.9 seconds', ""We can serve the search requests with 135mb of RAM, but the speed of the requests makes it impossible to use the system in production.\n\n\n\nLet's see how we can improve the speed.\n\n\n\n\n\n## How to speed up the search\n\n\n\n\n\n<!--\n\nWe need to look into disk parameters and see how they affect the search speed.\n\n\n\nLet's measure the disk speed with `fio`:\n\n\n\n```\n\nfio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=fiotest --filename=testfio --bs=4k --iodepth=64 --size=8G --readwrite=randread\n\n```""]","Question: What are the search-time parameters that can be used to tune the search accuracy and speed?\n\nContext: collectionName: ""{collection_name}"",

vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },

optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 },

quantizationConfig: new QuantizationConfig

{

Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }

}

);

```



There are also some search-time parameters you can use to tune the search accuracy and speed:



```http

POST /collections/{collection_name}/points/search

{
. And that's how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself.
- `exact` - if set to `true`, will perform exact search, which will be slower, but more accurate. You can use it to compare results of the search with different `hnsw_ef` values versus the ground truth.



## Latency vs Throughput



- There are two main approaches to measure the speed of search:

- latency of the request - the time from the moment request is submitted to the moment a response is received

- throughput - the number of requests per second the system can handle
## The results



We launched the benchmark 5 times on 10000 test vectors and averaged the results. Presented numbers are the mean values of all the attempts:



1. Sequential search: 225.9 seconds

2. Batch search: 208.0 seconds

3. Multiprocessing search (8 processes): 194.2 seconds

4. Multiprocessing batch search (8 processes, batch size 10): 148.9 seconds
We can serve the search requests with 135mb of RAM, but the speed of the requests makes it impossible to use the system in production.



Let's see how we can improve the speed.





## How to speed up the search





<!--

We need to look into disk parameters and see how they affect the search speed.



Let's measure the disk speed with `fio`:



```

fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=fiotest --filename=testfio --bs=4k --iodepth=64 --size=8G --readwrite=randread

```\n\nAnswer:",What are the search-time parameters that can be used to tune the search accuracy and speed?,"The search-time parameters that can be used to tune the search accuracy and speed are ""hnsw_ef"" and ""exact"". The ""hnsw_ef"" parameter is set to 128, which controls the search accuracy by specifying the number of neighbors to inspect during the search process. The ""exact"" parameter is set to false, indicating that an approximate search method is used for faster retrieval of results. These parameters are essential for optimizing the search process in the context of the QdrantClient for efficient retrieval of relevant data points.","['collectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },\n\n\toptimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 },\n\n\tquantizationConfig: new QuantizationConfig\n\n\t{\n\n\t\tScalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }\n\n\t}\n\n);\n\n```\n\n\n\nThere are also some search-time parameters you can use to tune the search accuracy and speed:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{', "". And that's how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself."", '- `exact` - if set to `true`, will perform exact search, which will be slower, but more accurate. You can use it to compare results of the search with different `hnsw_ef` values versus the ground truth.\n\n\n\n## Latency vs Throughput\n\n\n\n- There are two main approaches to measure the speed of search:\n\n  - latency of the request - the time from the moment request is submitted to the moment a response is received\n\n  - throughput - the number of requests per second the system can handle', '## The results\n\n\n\nWe launched the benchmark 5 times on 10000 test vectors and averaged the results. Presented numbers are the mean values of all the attempts:\n\n\n\n1. Sequential search: 225.9 seconds\n\n2. Batch search: 208.0 seconds\n\n3. Multiprocessing search (8 processes): 194.2 seconds\n\n4. Multiprocessing batch search (8 processes, batch size 10): 148.9 seconds', ""We can serve the search requests with 135mb of RAM, but the speed of the requests makes it impossible to use the system in production.\n\n\n\nLet's see how we can improve the speed.\n\n\n\n\n\n## How to speed up the search\n\n\n\n\n\n<!--\n\nWe need to look into disk parameters and see how they affect the search speed.\n\n\n\nLet's measure the disk speed with `fio`:\n\n\n\n```\n\nfio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=fiotest --filename=testfio --bs=4k --iodepth=64 --size=8G --readwrite=randread\n\n```""]","collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 768, Distance = Distance.Cosine },

	optimizersConfig: new OptimizersConfigDiff { MemmapThreshold = 20000 },

	quantizationConfig: new QuantizationConfig

	{

		Scalar = new ScalarQuantization { Type = QuantizationType.Int8, AlwaysRam = true }

	}

);

```



There are also some search-time parameters you can use to tune the search accuracy and speed:



```http

POST /collections/{collection_name}/points/search

{
. And that's how we can basically control accuracy without rebuilding index, without changing any kind of parameters inside the stored data structures. But we can do it real time in just one parameter change of the search query itself.
- `exact` - if set to `true`, will perform exact search, which will be slower, but more accurate. You can use it to compare results of the search with different `hnsw_ef` values versus the ground truth.



## Latency vs Throughput



- There are two main approaches to measure the speed of search:

  - latency of the request - the time from the moment request is submitted to the moment a response is received

  - throughput - the number of requests per second the system can handle
## The results



We launched the benchmark 5 times on 10000 test vectors and averaged the results. Presented numbers are the mean values of all the attempts:



1. Sequential search: 225.9 seconds

2. Batch search: 208.0 seconds

3. Multiprocessing search (8 processes): 194.2 seconds

4. Multiprocessing batch search (8 processes, batch size 10): 148.9 seconds
We can serve the search requests with 135mb of RAM, but the speed of the requests makes it impossible to use the system in production.



Let's see how we can improve the speed.





## How to speed up the search





<!--

We need to look into disk parameters and see how they affect the search speed.



Let's measure the disk speed with `fio`:



```

fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=fiotest --filename=testfio --bs=4k --iodepth=64 --size=8G --readwrite=randread

```",0.4939759036144578,0.5394736842105263,0.5157232704402516,0.17073170731707318,0.18666666666666668,0.178343949044586,0.2289156626506024,0.25,0.2389937106918239,0.4457831325301205,0.4868421052631579,0.4654088050314465,0.6718049645423889,0.6607758402824402,0.6491098403930664,0.6548908352851868,0.9969548616441898,0.5080607305280864,90,1.0714285714285714,0.3949900672304256
True,,3123.619,4f2b2574,What is the significance of using named vectors in a collection within the Qdrant system?,"Named vectors in a collection within the Qdrant system allow for the inclusion of multiple vectors in a single point, with each vector having its own dimensionality and metric requirements. This feature enables more flexibility in organizing and structuring data within a collection, as different vectors can represent distinct aspects or features of the data points. By utilizing named vectors, users can better tailor the representation of their data to suit specific analysis or search requirements, enhancing the overall efficiency and effectiveness of the system.","Named vectors in a collection within the Qdrant system have significance in providing a more organized and structured way to store and retrieve data. By assigning names to vectors, users can easily reference and manipulate specific vectors within the collection. This not only improves the readability and usability of the data but also streamlines the process of working with multiple vectors associated with each object.

In the context of creating a collection with multiple vectors, using named vectors allows for clear identification and management of each vector type. This approach aligns with the existing named vectors API in Qdrant, making it compatible with existing codebases and facilitating seamless integration of sparse vectors. Additionally, named vectors enable the implementation of advanced features such as payload filters, shard keys, and distance functions for measuring point proximity.

Overall, the use of named vectors enhances data consistency, simplifies vector manipulation, and supports the efficient retrieval of information within the Qdrant system.","Even with such a system, Qdrant ensures data consistency.



[*Available as of v0.10.0*](#create-vector-name)



If the collection was created with multiple vectors, each vector data can be provided using the vector's name:



```http

PUT /collections/{collection_name}/points

{

    ""points"": [

        {

            ""id"": 1,

            ""vector"": {

                ""image"": [0.9, 0.1, 0.1, 0.2],

                ""text"": [0.4, 0.7, 0.1, 0.8, 0.1, 0.1, 0.9, 0.2]

            }

        },

        {
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
```



## Creating a collection



Adding new functionalities typically requires making some changes to the interfaces, so no surprise we had to do it to enable the multiple vectors support. Currently, if you want to create a collection, you need to define the configuration of all the vectors you want to store for each object. Each vector type has its own name and the distance function used to measure how far the points are.



```python

from qdrant_client import QdrantClient
. So collection are where all your vector are stored in Qdrant, it's like a table in SQL or an index in elasticsearch. And so we decided to split our collection between three nodes.
---

draft: false

title: Storing multiple vectors per object in Qdrant

slug: storing-multiple-vectors-per-object-in-qdrant

short_description: Qdrant's approach to storing multiple vectors per object,

  unraveling new possibilities in data representation and retrieval.

description: Discover how Qdrant continues to push the boundaries of data

  indexing, providing insights into the practical applications and benefits of

  this novel vector storage strategy.","['Even with such a system, Qdrant ensures data consistency.\n\n\n\n[*Available as of v0.10.0*](#create-vector-name)\n\n\n\nIf the collection was created with multiple vectors, each vector data can be provided using the vector\'s name:\n\n\n\n```http\n\nPUT /collections/{collection_name}/points\n\n{\n\n    ""points"": [\n\n        {\n\n            ""id"": 1,\n\n            ""vector"": {\n\n                ""image"": [0.9, 0.1, 0.1, 0.2],\n\n                ""text"": [0.4, 0.7, 0.1, 0.8, 0.1, 0.1, 0.9, 0.2]\n\n            }\n\n        },\n\n        {', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", '```\n\n\n\n## Creating a collection\n\n\n\nAdding new functionalities typically requires making some changes to the interfaces, so no surprise we had to do it to enable the multiple vectors support. Currently, if you want to create a collection, you need to define the configuration of all the vectors you want to store for each object. Each vector type has its own name and the distance function used to measure how far the points are.\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient', "". So collection are where all your vector are stored in Qdrant, it's like a table in SQL or an index in elasticsearch. And so we decided to split our collection between three nodes."", ""---\n\ndraft: false\n\ntitle: Storing multiple vectors per object in Qdrant\n\nslug: storing-multiple-vectors-per-object-in-qdrant\n\nshort_description: Qdrant's approach to storing multiple vectors per object,\n\n  unraveling new possibilities in data representation and retrieval.\n\ndescription: Discover how Qdrant continues to push the boundaries of data\n\n  indexing, providing insights into the practical applications and benefits of\n\n  this novel vector storage strategy.""]","Question: What is the significance of using named vectors in a collection within the Qdrant system?\n\nContext: Even with such a system, Qdrant ensures data consistency.



[*Available as of v0.10.0*](#create-vector-name)



If the collection was created with multiple vectors, each vector data can be provided using the vector's name:



```http

PUT /collections/{collection_name}/points

{

""points"": [

{

""id"": 1,

""vector"": {

""image"": [0.9, 0.1, 0.1, 0.2],

""text"": [0.4, 0.7, 0.1, 0.8, 0.1, 0.1, 0.9, 0.2]

}

},

{
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
```



## Creating a collection



Adding new functionalities typically requires making some changes to the interfaces, so no surprise we had to do it to enable the multiple vectors support. Currently, if you want to create a collection, you need to define the configuration of all the vectors you want to store for each object. Each vector type has its own name and the distance function used to measure how far the points are.



```python

from qdrant_client import QdrantClient
. So collection are where all your vector are stored in Qdrant, it's like a table in SQL or an index in elasticsearch. And so we decided to split our collection between three nodes.
---

draft: false

title: Storing multiple vectors per object in Qdrant

slug: storing-multiple-vectors-per-object-in-qdrant

short_description: Qdrant's approach to storing multiple vectors per object,

unraveling new possibilities in data representation and retrieval.

description: Discover how Qdrant continues to push the boundaries of data

indexing, providing insights into the practical applications and benefits of

this novel vector storage strategy.\n\nAnswer:",What is the significance of using named vectors in a collection within the Qdrant system?,"Named vectors in a collection within the Qdrant system allow for the inclusion of multiple vectors in a single point, with each vector having its own dimensionality and metric requirements. This feature enables more flexibility in organizing and structuring data within a collection, as different vectors can represent distinct aspects or features of the data points. By utilizing named vectors, users can better tailor the representation of their data to suit specific analysis or search requirements, enhancing the overall efficiency and effectiveness of the system.","['Even with such a system, Qdrant ensures data consistency.\n\n\n\n[*Available as of v0.10.0*](#create-vector-name)\n\n\n\nIf the collection was created with multiple vectors, each vector data can be provided using the vector\'s name:\n\n\n\n```http\n\nPUT /collections/{collection_name}/points\n\n{\n\n    ""points"": [\n\n        {\n\n            ""id"": 1,\n\n            ""vector"": {\n\n                ""image"": [0.9, 0.1, 0.1, 0.2],\n\n                ""text"": [0.4, 0.7, 0.1, 0.8, 0.1, 0.1, 0.9, 0.2]\n\n            }\n\n        },\n\n        {', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", '```\n\n\n\n## Creating a collection\n\n\n\nAdding new functionalities typically requires making some changes to the interfaces, so no surprise we had to do it to enable the multiple vectors support. Currently, if you want to create a collection, you need to define the configuration of all the vectors you want to store for each object. Each vector type has its own name and the distance function used to measure how far the points are.\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient', "". So collection are where all your vector are stored in Qdrant, it's like a table in SQL or an index in elasticsearch. And so we decided to split our collection between three nodes."", ""---\n\ndraft: false\n\ntitle: Storing multiple vectors per object in Qdrant\n\nslug: storing-multiple-vectors-per-object-in-qdrant\n\nshort_description: Qdrant's approach to storing multiple vectors per object,\n\n  unraveling new possibilities in data representation and retrieval.\n\ndescription: Discover how Qdrant continues to push the boundaries of data\n\n  indexing, providing insights into the practical applications and benefits of\n\n  this novel vector storage strategy.""]","Even with such a system, Qdrant ensures data consistency.



[*Available as of v0.10.0*](#create-vector-name)



If the collection was created with multiple vectors, each vector data can be provided using the vector's name:



```http

PUT /collections/{collection_name}/points

{

    ""points"": [

        {

            ""id"": 1,

            ""vector"": {

                ""image"": [0.9, 0.1, 0.1, 0.2],

                ""text"": [0.4, 0.7, 0.1, 0.8, 0.1, 0.1, 0.9, 0.2]

            }

        },

        {
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
```



## Creating a collection



Adding new functionalities typically requires making some changes to the interfaces, so no surprise we had to do it to enable the multiple vectors support. Currently, if you want to create a collection, you need to define the configuration of all the vectors you want to store for each object. Each vector type has its own name and the distance function used to measure how far the points are.



```python

from qdrant_client import QdrantClient
. So collection are where all your vector are stored in Qdrant, it's like a table in SQL or an index in elasticsearch. And so we decided to split our collection between three nodes.
---

draft: false

title: Storing multiple vectors per object in Qdrant

slug: storing-multiple-vectors-per-object-in-qdrant

short_description: Qdrant's approach to storing multiple vectors per object,

  unraveling new possibilities in data representation and retrieval.

description: Discover how Qdrant continues to push the boundaries of data

  indexing, providing insights into the practical applications and benefits of

  this novel vector storage strategy.",0.34265734265734266,0.6447368421052632,0.4474885844748859,0.11971830985915492,0.22666666666666666,0.1566820276497696,0.18181818181818182,0.34210526315789475,0.23744292237442924,0.3076923076923077,0.5789473684210527,0.4018264840182649,0.8518034815788269,0.6497730612754822,0.6857379674911499,0.6672712564468384,0.9893832058546,0.9782682408889135,157,1.8470588235294119,0.17302406508309978
True,,3473.8579999999997,b8ed82eb,What parameters can be configured in the configuration file to improve performance in HNSW indexing?,"In the context of HNSW indexing, the parameters that can be configured in the configuration file to enhance performance are `m`, `ef_construct`, and `ef`. 

- `m`: This parameter represents the number of edges per node in the index graph. A higher value for `m` leads to more accurate search results but also requires more space. By adjusting `m`, you can control the trade-off between search accuracy and space efficiency.

- `ef_construct`: When building the index, `ef_construct` specifies the number of neighbors to consider. Increasing the value of `ef_construct` improves the accuracy of the search but also increases the time required to build the index. 

- `ef`: This parameter is used when searching for targets. It determines the search range for finding nearest neighbors. By setting an appropriate value for `ef`, you can optimize the search process to balance efficiency and accuracy in finding the nearest neighbors within the specified search range.","To improve performance in HNSW indexing, the following parameters can be configured in the configuration file:

1. Increase the `ef` parameter: This parameter controls the number of neighbors to visit during the search. Increasing `ef` can improve the precision of the search.

2. Increase the `m` parameter: This parameter determines the number of edges per node in the hierarchical graph. Increasing `m` can also improve search performance.

3. Enable on-disk storage: Setting the `on_disk` parameter to true can help in scenarios with limited RAM by storing some data on disk.

4. Adjust disk IOPS: The speed of disk operations (IOPS) is crucial for search performance. Consider optimizing disk IOPS for faster search operations.

5. Configure other relevant parameters like `on_disk_payload`, `full_scan_threshold`, `max_indexing_threads`, etc., based on your specific requirements and system configuration.

By fine-tuning these parameters in the HNSW configuration file, you can enhance the performance of the HNSW indexing system.","hnswConfig: new HnswConfigDiff { OnDisk = true }

);

```



In this scenario you can increase the precision of the search by increasing the `ef` and `m` parameters of the HNSW index, even with limited RAM.



```json

...

""hnsw_config"": {

    ""m"": 64,

    ""ef_construct"": 512,

    ""on_disk"": true

}

...

```



The disk IOPS is a critical factor in this scenario, it will determine how fast you can perform search.
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
### Indexing vectors in HNSW



In some cases, you might be surprised the value of `indexed_vectors_count` is lower than `vectors_count`. This is an intended behaviour and

depends on the [optimizer configuration](../optimizer). A new index segment is built if the size of non-indexed vectors is higher than the

value of `indexing_threshold`(in kB).  If your collection is very small or the dimensionality of the vectors is low, there might be no HNSW segment
```text

avg(precision@5) = 0.9935999999999995

```



As we can see, the precision of the approximate search vs exact search is pretty high. There are, however, some scenarios when we

need higher precision and can accept higher latency. HNSW is pretty tunable, and we can increase the precision by changing its parameters.

  

## Tweaking the HNSW parameters



HNSW is a hierarchical graph, where each node has a set of links to other nodes. The number of edges per node is called the `m` parameter.
To implement this approach, you should:



1. Set `payload_m` in the HNSW configuration to a non-zero value, such as 16.

2. Set `m` in hnsw config to 0. This will disable building global index for the whole collection.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""hnsw_config"": {

        ""payload_m"": 16,

        ""m"": 0

    }

}

```



```python

from qdrant_client import QdrantClient, models","['hnswConfig: new HnswConfigDiff { OnDisk = true }\n\n);\n\n```\n\n\n\nIn this scenario you can increase the precision of the search by increasing the `ef` and `m` parameters of the HNSW index, even with limited RAM.\n\n\n\n```json\n\n...\n\n""hnsw_config"": {\n\n    ""m"": 64,\n\n    ""ef_construct"": 512,\n\n    ""on_disk"": true\n\n}\n\n...\n\n```\n\n\n\nThe disk IOPS is a critical factor in this scenario, it will determine how fast you can perform search.', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', '### Indexing vectors in HNSW\n\n\n\nIn some cases, you might be surprised the value of `indexed_vectors_count` is lower than `vectors_count`. This is an intended behaviour and\n\ndepends on the [optimizer configuration](../optimizer). A new index segment is built if the size of non-indexed vectors is higher than the\n\nvalue of `indexing_threshold`(in kB).  If your collection is very small or the dimensionality of the vectors is low, there might be no HNSW segment', '```text\n\navg(precision@5) = 0.9935999999999995\n\n```\n\n\n\nAs we can see, the precision of the approximate search vs exact search is pretty high. There are, however, some scenarios when we\n\nneed higher precision and can accept higher latency. HNSW is pretty tunable, and we can increase the precision by changing its parameters.\n\n  \n\n## Tweaking the HNSW parameters\n\n\n\nHNSW is a hierarchical graph, where each node has a set of links to other nodes. The number of edges per node is called the `m` parameter.', 'To implement this approach, you should:\n\n\n\n1. Set `payload_m` in the HNSW configuration to a non-zero value, such as 16.\n\n2. Set `m` in hnsw config to 0. This will disable building global index for the whole collection.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""hnsw_config"": {\n\n        ""payload_m"": 16,\n\n        ""m"": 0\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models']","Question: What parameters can be configured in the configuration file to improve performance in HNSW indexing?\n\nContext: hnswConfig: new HnswConfigDiff { OnDisk = true }

);

```



In this scenario you can increase the precision of the search by increasing the `ef` and `m` parameters of the HNSW index, even with limited RAM.



```json

...

""hnsw_config"": {

""m"": 64,

""ef_construct"": 512,

""on_disk"": true

}

...

```



The disk IOPS is a critical factor in this scenario, it will determine how fast you can perform search.
""on_disk_payload"": false

},

""hnsw_config"": {

""m"": 16,

""ef_construct"": 100,

""full_scan_threshold"": 10000,

""max_indexing_threads"": 0

},

""optimizer_config"": {

""deleted_threshold"": 0.2,

""vacuum_min_vector_number"": 1000,

""default_segment_number"": 0,

""max_segment_size"": null,
### Indexing vectors in HNSW



In some cases, you might be surprised the value of `indexed_vectors_count` is lower than `vectors_count`. This is an intended behaviour and

depends on the [optimizer configuration](../optimizer). A new index segment is built if the size of non-indexed vectors is higher than the

value of `indexing_threshold`(in kB).  If your collection is very small or the dimensionality of the vectors is low, there might be no HNSW segment
```text

avg(precision@5) = 0.9935999999999995

```



As we can see, the precision of the approximate search vs exact search is pretty high. There are, however, some scenarios when we

need higher precision and can accept higher latency. HNSW is pretty tunable, and we can increase the precision by changing its parameters.



## Tweaking the HNSW parameters



HNSW is a hierarchical graph, where each node has a set of links to other nodes. The number of edges per node is called the `m` parameter.
To implement this approach, you should:



1. Set `payload_m` in the HNSW configuration to a non-zero value, such as 16.

2. Set `m` in hnsw config to 0. This will disable building global index for the whole collection.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""hnsw_config"": {

""payload_m"": 16,

""m"": 0

}

}

```



```python

from qdrant_client import QdrantClient, models\n\nAnswer:",What parameters can be configured in the configuration file to improve performance in HNSW indexing?,"In the context of HNSW indexing, the parameters that can be configured in the configuration file to enhance performance are `m`, `ef_construct`, and `ef`. 

- `m`: This parameter represents the number of edges per node in the index graph. A higher value for `m` leads to more accurate search results but also requires more space. By adjusting `m`, you can control the trade-off between search accuracy and space efficiency.

- `ef_construct`: When building the index, `ef_construct` specifies the number of neighbors to consider. Increasing the value of `ef_construct` improves the accuracy of the search but also increases the time required to build the index. 

- `ef`: This parameter is used when searching for targets. It determines the search range for finding nearest neighbors. By setting an appropriate value for `ef`, you can optimize the search process to balance efficiency and accuracy in finding the nearest neighbors within the specified search range.","['hnswConfig: new HnswConfigDiff { OnDisk = true }\n\n);\n\n```\n\n\n\nIn this scenario you can increase the precision of the search by increasing the `ef` and `m` parameters of the HNSW index, even with limited RAM.\n\n\n\n```json\n\n...\n\n""hnsw_config"": {\n\n    ""m"": 64,\n\n    ""ef_construct"": 512,\n\n    ""on_disk"": true\n\n}\n\n...\n\n```\n\n\n\nThe disk IOPS is a critical factor in this scenario, it will determine how fast you can perform search.', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', '### Indexing vectors in HNSW\n\n\n\nIn some cases, you might be surprised the value of `indexed_vectors_count` is lower than `vectors_count`. This is an intended behaviour and\n\ndepends on the [optimizer configuration](../optimizer). A new index segment is built if the size of non-indexed vectors is higher than the\n\nvalue of `indexing_threshold`(in kB).  If your collection is very small or the dimensionality of the vectors is low, there might be no HNSW segment', '```text\n\navg(precision@5) = 0.9935999999999995\n\n```\n\n\n\nAs we can see, the precision of the approximate search vs exact search is pretty high. There are, however, some scenarios when we\n\nneed higher precision and can accept higher latency. HNSW is pretty tunable, and we can increase the precision by changing its parameters.\n\n  \n\n## Tweaking the HNSW parameters\n\n\n\nHNSW is a hierarchical graph, where each node has a set of links to other nodes. The number of edges per node is called the `m` parameter.', 'To implement this approach, you should:\n\n\n\n1. Set `payload_m` in the HNSW configuration to a non-zero value, such as 16.\n\n2. Set `m` in hnsw config to 0. This will disable building global index for the whole collection.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""hnsw_config"": {\n\n        ""payload_m"": 16,\n\n        ""m"": 0\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models']","hnswConfig: new HnswConfigDiff { OnDisk = true }

);

```



In this scenario you can increase the precision of the search by increasing the `ef` and `m` parameters of the HNSW index, even with limited RAM.



```json

...

""hnsw_config"": {

    ""m"": 64,

    ""ef_construct"": 512,

    ""on_disk"": true

}

...

```



The disk IOPS is a critical factor in this scenario, it will determine how fast you can perform search.
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
### Indexing vectors in HNSW



In some cases, you might be surprised the value of `indexed_vectors_count` is lower than `vectors_count`. This is an intended behaviour and

depends on the [optimizer configuration](../optimizer). A new index segment is built if the size of non-indexed vectors is higher than the

value of `indexing_threshold`(in kB).  If your collection is very small or the dimensionality of the vectors is low, there might be no HNSW segment
```text

avg(precision@5) = 0.9935999999999995

```



As we can see, the precision of the approximate search vs exact search is pretty high. There are, however, some scenarios when we

need higher precision and can accept higher latency. HNSW is pretty tunable, and we can increase the precision by changing its parameters.

  

## Tweaking the HNSW parameters



HNSW is a hierarchical graph, where each node has a set of links to other nodes. The number of edges per node is called the `m` parameter.
To implement this approach, you should:



1. Set `payload_m` in the HNSW configuration to a non-zero value, such as 16.

2. Set `m` in hnsw config to 0. This will disable building global index for the whole collection.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""hnsw_config"": {

        ""payload_m"": 16,

        ""m"": 0

    }

}

```



```python

from qdrant_client import QdrantClient, models",0.4444444444444444,0.47761194029850745,0.4604316546762589,0.13986013986013987,0.15037593984962405,0.14492753623188404,0.2361111111111111,0.2537313432835821,0.24460431654676257,0.4236111111111111,0.4552238805970149,0.4388489208633094,0.841134786605835,0.6116604208946228,0.6262691617012024,0.6188786029815674,0.5825136378407478,0.6116267650698621,150,1.0135135135135136,0.4084200435396675
True,,3954.574,1b4b2089,How does product quantization differ from scalar quantization in terms of compression and computational efficiency?,"Product quantization involves dividing data into chunks and quantizing each segment individually, approximating each chunk with a centroid index that represents the original vector component. The positions of these centroids are determined through clustering algorithms like k-means, with Qdrant currently utilizing 256 centroids, allowing each centroid index to be represented by a single byte. Product quantization can achieve higher compression factors compared to scalar quantization. However, there are tradeoffs to consider. Product quantization distance calculations are not SIMD-friendly, resulting in slower computational speed compared to scalar quantization. Additionally, product quantization introduces a loss of accuracy, making it more suitable for high-dimensional vectors. To optimize quantization parameters for specific use cases, it is recommended to refer to the ""Quantization Tips"" section for more information.","Product quantization differs from scalar quantization in terms of compression and computational efficiency by offering a higher compression rate but with trade-offs in accuracy and computational speed. Product quantization can provide a better compression ratio compared to scalar quantization, but it may suffer from a significant loss of accuracy and slower search speeds. While scalar quantization is faster and more accurate, it may not achieve the same level of compression as product quantization.

In terms of computational efficiency, scalar quantization is generally faster than product quantization as product quantization distance calculations are not SIMD-friendly, making it slower. Additionally, product quantization is recommended for high-dimensional vectors due to its loss of accuracy.

In summary, product quantization is favored when memory footprint is a top priority and search speed is not critical, despite its lower accuracy and slower computational speed. On the other hand, scalar quantization is faster and more accurate but may not achieve the same level of compression as product quantization.","<td>474 s</td>

      </tr>

   </tbody>

</table>



It turns out that in some cases, Product Quantization may not only reduce the memory usage, 

but also the search time.



## Good practices



Compared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.



Product Quantization tends to be favored in certain specific scenarios:
* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.



## Setting up Quantization in Qdrant



You can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.
But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method
. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
## How to choose the right quantization method



Here is a brief table of the pros and cons of each quantization method:



| Quantization method | Accuracy | Speed        | Compression |

|---------------------|----------|--------------|-------------|

| Scalar              | 0.99     | up to x2     | 4           |

| Product             | 0.7      | 0.5          | up to 64    |

| Binary              | 0.95*    | up to x40    | 32          |



`*` - for compatible models","['<td>474 s</td>\n\n      </tr>\n\n   </tbody>\n\n</table>\n\n\n\nIt turns out that in some cases, Product Quantization may not only reduce the memory usage, \n\nbut also the search time.\n\n\n\n## Good practices\n\n\n\nCompared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\n\n\n\nProduct Quantization tends to be favored in certain specific scenarios:', '* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.\n\n\n\n## Setting up Quantization in Qdrant\n\n\n\nYou can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.', 'But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\n\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\n\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n## How to choose the right quantization method', '. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.', '## How to choose the right quantization method\n\n\n\nHere is a brief table of the pros and cons of each quantization method:\n\n\n\n| Quantization method | Accuracy | Speed        | Compression |\n\n|---------------------|----------|--------------|-------------|\n\n| Scalar              | 0.99     | up to x2     | 4           |\n\n| Product             | 0.7      | 0.5          | up to 64    |\n\n| Binary              | 0.95*    | up to x40    | 32          |\n\n\n\n`*` - for compatible models']","Question: How does product quantization differ from scalar quantization in terms of compression and computational efficiency?\n\nContext: <td>474 s</td>

</tr>

</tbody>

</table>



It turns out that in some cases, Product Quantization may not only reduce the memory usage, 

but also the search time.



## Good practices



Compared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.



Product Quantization tends to be favored in certain specific scenarios:
* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.



## Setting up Quantization in Qdrant



You can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.
But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method
. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
## How to choose the right quantization method



Here is a brief table of the pros and cons of each quantization method:



| Quantization method | Accuracy | Speed        | Compression |

|---------------------|----------|--------------|-------------|

| Scalar              | 0.99     | up to x2     | 4           |

| Product             | 0.7      | 0.5          | up to 64    |

| Binary              | 0.95*    | up to x40    | 32          |



`*` - for compatible models\n\nAnswer:",How does product quantization differ from scalar quantization in terms of compression and computational efficiency?,"Product quantization involves dividing data into chunks and quantizing each segment individually, approximating each chunk with a centroid index that represents the original vector component. The positions of these centroids are determined through clustering algorithms like k-means, with Qdrant currently utilizing 256 centroids, allowing each centroid index to be represented by a single byte. Product quantization can achieve higher compression factors compared to scalar quantization. However, there are tradeoffs to consider. Product quantization distance calculations are not SIMD-friendly, resulting in slower computational speed compared to scalar quantization. Additionally, product quantization introduces a loss of accuracy, making it more suitable for high-dimensional vectors. To optimize quantization parameters for specific use cases, it is recommended to refer to the ""Quantization Tips"" section for more information.","['<td>474 s</td>\n\n      </tr>\n\n   </tbody>\n\n</table>\n\n\n\nIt turns out that in some cases, Product Quantization may not only reduce the memory usage, \n\nbut also the search time.\n\n\n\n## Good practices\n\n\n\nCompared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\n\n\n\nProduct Quantization tends to be favored in certain specific scenarios:', '* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.\n\n\n\n## Setting up Quantization in Qdrant\n\n\n\nYou can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.', 'But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\n\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\n\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n## How to choose the right quantization method', '. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.', '## How to choose the right quantization method\n\n\n\nHere is a brief table of the pros and cons of each quantization method:\n\n\n\n| Quantization method | Accuracy | Speed        | Compression |\n\n|---------------------|----------|--------------|-------------|\n\n| Scalar              | 0.99     | up to x2     | 4           |\n\n| Product             | 0.7      | 0.5          | up to 64    |\n\n| Binary              | 0.95*    | up to x40    | 32          |\n\n\n\n`*` - for compatible models']","<td>474 s</td>

      </tr>

   </tbody>

</table>



It turns out that in some cases, Product Quantization may not only reduce the memory usage, 

but also the search time.



## Good practices



Compared to Scalar Quantization, Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.



Product Quantization tends to be favored in certain specific scenarios:
* **Product Quantization** may provide a better compression ratio, but it has a significant loss of accuracy and is slower than scalar quantization. It is recommended if the memory footprint is the top priority and the search speed is not critical.



## Setting up Quantization in Qdrant



You can configure quantization for a collection by specifying the quantization parameters in the `quantization_config` section of the collection configuration.
But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.

Also, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.



Please refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.



## How to choose the right quantization method
. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.
## How to choose the right quantization method



Here is a brief table of the pros and cons of each quantization method:



| Quantization method | Accuracy | Speed        | Compression |

|---------------------|----------|--------------|-------------|

| Scalar              | 0.99     | up to x2     | 4           |

| Product             | 0.7      | 0.5          | up to 64    |

| Binary              | 0.95*    | up to x40    | 32          |



`*` - for compatible models",0.3248407643312102,0.425,0.36823104693140796,0.16666666666666666,0.2184873949579832,0.1890909090909091,0.21656050955414013,0.2833333333333333,0.24548736462093862,0.3057324840764331,0.4,0.34657039711191334,0.8234570622444153,0.6110963821411133,0.605880081653595,0.6084770560264587,0.9959095965542863,0.7977765673380678,161,1.3089430894308942,0.6561790027530411
True,,4971.202,81a69ff4,What is the significance of Qdrant's approach to storing multiple vectors per object in data indexing?,"Qdrant's approach to storing multiple vectors per object in data indexing is significant as it opens up new possibilities in data representation and retrieval. By allowing multiple vectors to be associated with a single object, Qdrant enables more nuanced and detailed analysis of the data. This approach can be particularly beneficial in applications such as data science, neural networks, database management, and similarity search. It provides a more comprehensive understanding of the relationships between objects and enhances the accuracy and efficiency of search and retrieval processes. Overall, Qdrant's innovative vector storage strategy contributes to pushing the boundaries of data indexing and offers practical applications and benefits in various fields.","Qdrant's approach to storing multiple vectors per object in data indexing is significant for several reasons:

1. **Flexibility**: By allowing multiple vectors per object, Qdrant provides flexibility in data representation. This is crucial in real-world scenarios where objects may be described using several vectors of different dimensionalities.

2. **Efficiency**: Storing multiple vectors per object can lead to reduced memory requirements, potentially up to 64 times less compared to other approaches. This efficiency can be a game-changer for large-scale data indexing and retrieval tasks.

3. **Improved Search Quality**: Having multiple vectors per object enables the use of different distance functions for calculating similarity. This can lead to more accurate and meaningful search results, especially in applications where multiple facets of an object need to be considered.

4. **Optimization**: The ability to store multiple vectors together and reuse payloads instead of creating separate collections can optimize the data retrieval process. This optimization measure is valuable for improving the overall performance of data indexing systems.

5. **Scalability**: Qdrant's support for features like Multitenancy and Sharding allows for easy scaling of the data indexing process. This scalability is essential for handling large volumes of data efficiently and effectively.

In essence, Qdrant's approach to storing multiple vectors per object opens up new possibilities for data representation, retrieval, and optimization in the field of data indexing.","---

draft: false

title: Storing multiple vectors per object in Qdrant

slug: storing-multiple-vectors-per-object-in-qdrant

short_description: Qdrant's approach to storing multiple vectors per object,

  unraveling new possibilities in data representation and retrieval.

description: Discover how Qdrant continues to push the boundaries of data

  indexing, providing insights into the practical applications and benefits of

  this novel vector storage strategy.
5. Qdrant allows storing multiple vectors per point, and those might be of a different dimensionality. Pinecone doesn't support anything similar.

6. Vectors in Pinecone are mandatory for each point. Qdrant supports optional vectors.



It is worth mentioning, that **Pinecone will automatically create metadata indexes for all the fields**. Qdrant assumes you know
detail. In a nutshell, you can **reduce the memory requirements by up to 64 times**!



### Optional named vectors



Qdrant has been supporting multiple named vectors per point for quite a long time. Those may have 

utterly different dimensionality and distance functions used to calculate similarity. Having multiple 

embeddings per item is an essential real-world scenario. For example, you might be encoding textual
It is not surprising that a method used for creating neural encoding plays an important role in the search process and its quality. If your data points might be described using several vectors, then the latest release of Qdrant gives you an opportunity to store them together and reuse the payloads, instead of creating several collections and querying them separately.
Conversely, the only up-front investment with vector databases is the indexing (which requires more compute). After this step, everything else is a breeze. Once setup, Qdrant easily scales via [features like Multitenancy and Sharding](https://qdrant.tech/articles/multitenancy/). This lets you scale up your reliance on the vector retrieval process and minimize your use of the compute-heavy LLMs. As an optimization  measure, Qdrant is irreplaceable. 



Julien Simon from HuggingFace says it best:","[""---\n\ndraft: false\n\ntitle: Storing multiple vectors per object in Qdrant\n\nslug: storing-multiple-vectors-per-object-in-qdrant\n\nshort_description: Qdrant's approach to storing multiple vectors per object,\n\n  unraveling new possibilities in data representation and retrieval.\n\ndescription: Discover how Qdrant continues to push the boundaries of data\n\n  indexing, providing insights into the practical applications and benefits of\n\n  this novel vector storage strategy."", ""5. Qdrant allows storing multiple vectors per point, and those might be of a different dimensionality. Pinecone doesn't support anything similar.\n\n6. Vectors in Pinecone are mandatory for each point. Qdrant supports optional vectors.\n\n\n\nIt is worth mentioning, that **Pinecone will automatically create metadata indexes for all the fields**. Qdrant assumes you know"", 'detail. In a nutshell, you can **reduce the memory requirements by up to 64 times**!\n\n\n\n### Optional named vectors\n\n\n\nQdrant has been supporting multiple named vectors per point for quite a long time. Those may have \n\nutterly different dimensionality and distance functions used to calculate similarity. Having multiple \n\nembeddings per item is an essential real-world scenario. For example, you might be encoding textual', 'It is not surprising that a method used for creating neural encoding plays an important role in the search process and its quality. If your data points might be described using several vectors, then the latest release of Qdrant gives you an opportunity to store them together and reuse the payloads, instead of creating several collections and querying them separately.', 'Conversely, the only up-front investment with vector databases is the indexing (which requires more compute). After this step, everything else is a breeze. Once setup, Qdrant easily scales via [features like Multitenancy and Sharding](https://qdrant.tech/articles/multitenancy/). This lets you scale up your reliance on the vector retrieval process and minimize your use of the compute-heavy LLMs. As an optimization  measure, Qdrant is irreplaceable. \n\n\n\nJulien Simon from HuggingFace says it best:']","Question: What is the significance of Qdrant's approach to storing multiple vectors per object in data indexing?\n\nContext: ---

draft: false

title: Storing multiple vectors per object in Qdrant

slug: storing-multiple-vectors-per-object-in-qdrant

short_description: Qdrant's approach to storing multiple vectors per object,

unraveling new possibilities in data representation and retrieval.

description: Discover how Qdrant continues to push the boundaries of data

indexing, providing insights into the practical applications and benefits of

this novel vector storage strategy.
5. Qdrant allows storing multiple vectors per point, and those might be of a different dimensionality. Pinecone doesn't support anything similar.

6. Vectors in Pinecone are mandatory for each point. Qdrant supports optional vectors.



It is worth mentioning, that **Pinecone will automatically create metadata indexes for all the fields**. Qdrant assumes you know
detail. In a nutshell, you can **reduce the memory requirements by up to 64 times**!



### Optional named vectors



Qdrant has been supporting multiple named vectors per point for quite a long time. Those may have 

utterly different dimensionality and distance functions used to calculate similarity. Having multiple 

embeddings per item is an essential real-world scenario. For example, you might be encoding textual
It is not surprising that a method used for creating neural encoding plays an important role in the search process and its quality. If your data points might be described using several vectors, then the latest release of Qdrant gives you an opportunity to store them together and reuse the payloads, instead of creating several collections and querying them separately.
Conversely, the only up-front investment with vector databases is the indexing (which requires more compute). After this step, everything else is a breeze. Once setup, Qdrant easily scales via [features like Multitenancy and Sharding](https://qdrant.tech/articles/multitenancy/). This lets you scale up your reliance on the vector retrieval process and minimize your use of the compute-heavy LLMs. As an optimization  measure, Qdrant is irreplaceable. 



Julien Simon from HuggingFace says it best:\n\nAnswer:",What is the significance of Qdrant's approach to storing multiple vectors per object in data indexing?,"Qdrant's approach to storing multiple vectors per object in data indexing is significant as it opens up new possibilities in data representation and retrieval. By allowing multiple vectors to be associated with a single object, Qdrant enables more nuanced and detailed analysis of the data. This approach can be particularly beneficial in applications such as data science, neural networks, database management, and similarity search. It provides a more comprehensive understanding of the relationships between objects and enhances the accuracy and efficiency of search and retrieval processes. Overall, Qdrant's innovative vector storage strategy contributes to pushing the boundaries of data indexing and offers practical applications and benefits in various fields.","[""---\n\ndraft: false\n\ntitle: Storing multiple vectors per object in Qdrant\n\nslug: storing-multiple-vectors-per-object-in-qdrant\n\nshort_description: Qdrant's approach to storing multiple vectors per object,\n\n  unraveling new possibilities in data representation and retrieval.\n\ndescription: Discover how Qdrant continues to push the boundaries of data\n\n  indexing, providing insights into the practical applications and benefits of\n\n  this novel vector storage strategy."", ""5. Qdrant allows storing multiple vectors per point, and those might be of a different dimensionality. Pinecone doesn't support anything similar.\n\n6. Vectors in Pinecone are mandatory for each point. Qdrant supports optional vectors.\n\n\n\nIt is worth mentioning, that **Pinecone will automatically create metadata indexes for all the fields**. Qdrant assumes you know"", 'detail. In a nutshell, you can **reduce the memory requirements by up to 64 times**!\n\n\n\n### Optional named vectors\n\n\n\nQdrant has been supporting multiple named vectors per point for quite a long time. Those may have \n\nutterly different dimensionality and distance functions used to calculate similarity. Having multiple \n\nembeddings per item is an essential real-world scenario. For example, you might be encoding textual', 'It is not surprising that a method used for creating neural encoding plays an important role in the search process and its quality. If your data points might be described using several vectors, then the latest release of Qdrant gives you an opportunity to store them together and reuse the payloads, instead of creating several collections and querying them separately.', 'Conversely, the only up-front investment with vector databases is the indexing (which requires more compute). After this step, everything else is a breeze. Once setup, Qdrant easily scales via [features like Multitenancy and Sharding](https://qdrant.tech/articles/multitenancy/). This lets you scale up your reliance on the vector retrieval process and minimize your use of the compute-heavy LLMs. As an optimization  measure, Qdrant is irreplaceable. \n\n\n\nJulien Simon from HuggingFace says it best:']","---

draft: false

title: Storing multiple vectors per object in Qdrant

slug: storing-multiple-vectors-per-object-in-qdrant

short_description: Qdrant's approach to storing multiple vectors per object,

  unraveling new possibilities in data representation and retrieval.

description: Discover how Qdrant continues to push the boundaries of data

  indexing, providing insights into the practical applications and benefits of

  this novel vector storage strategy.
5. Qdrant allows storing multiple vectors per point, and those might be of a different dimensionality. Pinecone doesn't support anything similar.

6. Vectors in Pinecone are mandatory for each point. Qdrant supports optional vectors.



It is worth mentioning, that **Pinecone will automatically create metadata indexes for all the fields**. Qdrant assumes you know
detail. In a nutshell, you can **reduce the memory requirements by up to 64 times**!



### Optional named vectors



Qdrant has been supporting multiple named vectors per point for quite a long time. Those may have 

utterly different dimensionality and distance functions used to calculate similarity. Having multiple 

embeddings per item is an essential real-world scenario. For example, you might be encoding textual
It is not surprising that a method used for creating neural encoding plays an important role in the search process and its quality. If your data points might be described using several vectors, then the latest release of Qdrant gives you an opportunity to store them together and reuse the payloads, instead of creating several collections and querying them separately.
Conversely, the only up-front investment with vector databases is the indexing (which requires more compute). After this step, everything else is a breeze. Once setup, Qdrant easily scales via [features like Multitenancy and Sharding](https://qdrant.tech/articles/multitenancy/). This lets you scale up your reliance on the vector retrieval process and minimize your use of the compute-heavy LLMs. As an optimization  measure, Qdrant is irreplaceable. 



Julien Simon from HuggingFace says it best:",0.3073394495412844,0.638095238095238,0.41486068111455104,0.14746543778801843,0.3076923076923077,0.1993769470404984,0.20642201834862386,0.42857142857142855,0.2786377708978328,0.2981651376146789,0.6190476190476191,0.4024767801857585,0.8947785496711731,0.6451908946037292,0.7168334126472473,0.6791279911994934,0.7442292492487468,0.9890640462748707,223,2.009009009009009,0.41992043366008136
True,,4965.295,6e259bec,What configuration is required to enable in-memory quantization with on-disk original vectors in Qdrant?,"To configure in-memory quantization with on-disk original vectors in Qdrant, you need to create a collection with the following configuration:

```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 768,

        ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": true

        }

    }

}
```

Additionally, in the Python code snippet provided, you can use the QdrantClient library to create the collection with the specified configuration parameters. This includes defining vector size, distance metric (such as Cosine similarity), memmap threshold for optimization, and scalar quantization configuration with type set to INT8","To enable in-memory quantization with on-disk original vectors in Qdrant, you need to configure the collection with the following settings:

```http
PUT /collections/{collection_name}

{
  ""vectors"": {
    ""size"": 768,
    ""distance"": ""Cosine""
  },
  ""optimizers_config"": {
    ""memmap_threshold"": 20000
  },
  ""quantization_config"": {
    ""scalar"": {
      ""type"": ""int8"",
      ""always_ram"": true
    }
  }
}
```

By setting `""always_ram"": true`, you are configuring Qdrant to use in-memory quantization with the original vectors stored on disk.

Alternatively, if you want to enable in-memory quantization with original vectors stored on disk using mmap storage, you can set `""always_ram"": false` in the quantization configuration as shown below:

```http
PUT /collections/{collection_name}

{
  ""vectors"": {
    ""size"": 768,
    ""distance"": ""Cosine""
  },
  ""optimizers_config"": {
    ""memmap_threshold"": 20000
  },
  ""quantization_config"": {
    ""scalar"": {
      ""type"": ""int8"",
      ""always_ram"": false
    }
  }
}
```

These configurations allow you to optimize memory usage by compressing embeddings and storing original vectors on disk while performing in-memory quantization.","To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 768,

        ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": true

        }

    }

}

```



```python
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": false

        }

    }

}

```



```python

from qdrant_client import QdrantClient, models
5. RAM: Store the full vectors and payload on disk. Limit what you load from memory to the binary quantization index. This helps reduce the memory footprint and improve the overall efficiency of the system. The incremental latency from the disk read is negligible compared to the latency savings from the binary scoring in Qdrant, which uses SIMD instructions where possible.



Want to discuss these findings and learn more about Binary Quantization? [Join our Discord community.](https://discord.gg/qdrant)
```



While Qdrant offers various options to store some parts of the data on disk, starting 

from version 1.1.0, you can also optimize your memory by compressing the embeddings. 

We've implemented the mechanism of **Scalar Quantization**! It turns out to have not 

only a positive impact on memory but also on the performance. 



## Scalar Quantization



Scalar quantization is a data compression technique that converts floating point values
Right? So unfortunately Qdrant architecture is not designed and not expecting this type of behavior for several reasons. First of all, removing of the original vectors will compromise some features like oversampling, like segment building. And actually removing of those original vectors will only be compatible with some types of quantization for example, it won't be compatible with scalar quantization because in this case we won't be able to rebuild index to do maintenance of the system","['To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 768,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python', 'This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models', '5. RAM: Store the full vectors and payload on disk. Limit what you load from memory to the binary quantization index. This helps reduce the memory footprint and improve the overall efficiency of the system. The incremental latency from the disk read is negligible compared to the latency savings from the binary scoring in Qdrant, which uses SIMD instructions where possible.\n\n\n\nWant to discuss these findings and learn more about Binary Quantization? [Join our Discord community.](https://discord.gg/qdrant)', ""```\n\n\n\nWhile Qdrant offers various options to store some parts of the data on disk, starting \n\nfrom version 1.1.0, you can also optimize your memory by compressing the embeddings. \n\nWe've implemented the mechanism of **Scalar Quantization**! It turns out to have not \n\nonly a positive impact on memory but also on the performance. \n\n\n\n## Scalar Quantization\n\n\n\nScalar quantization is a data compression technique that converts floating point values"", ""Right? So unfortunately Qdrant architecture is not designed and not expecting this type of behavior for several reasons. First of all, removing of the original vectors will compromise some features like oversampling, like segment building. And actually removing of those original vectors will only be compatible with some types of quantization for example, it won't be compatible with scalar quantization because in this case we won't be able to rebuild index to do maintenance of the system""]","Question: What configuration is required to enable in-memory quantization with on-disk original vectors in Qdrant?\n\nContext: To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration:



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""optimizers_config"": {

""memmap_threshold"": 20000

},

""quantization_config"": {

""scalar"": {

""type"": ""int8"",

""always_ram"": true

}

}

}

```



```python
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""optimizers_config"": {

""memmap_threshold"": 20000

},

""quantization_config"": {

""scalar"": {

""type"": ""int8"",

""always_ram"": false

}

}

}

```



```python

from qdrant_client import QdrantClient, models
5. RAM: Store the full vectors and payload on disk. Limit what you load from memory to the binary quantization index. This helps reduce the memory footprint and improve the overall efficiency of the system. The incremental latency from the disk read is negligible compared to the latency savings from the binary scoring in Qdrant, which uses SIMD instructions where possible.



Want to discuss these findings and learn more about Binary Quantization? [Join our Discord community.](https://discord.gg/qdrant)
```



While Qdrant offers various options to store some parts of the data on disk, starting 

from version 1.1.0, you can also optimize your memory by compressing the embeddings. 

We've implemented the mechanism of **Scalar Quantization**! It turns out to have not 

only a positive impact on memory but also on the performance. 



## Scalar Quantization



Scalar quantization is a data compression technique that converts floating point values
Right? So unfortunately Qdrant architecture is not designed and not expecting this type of behavior for several reasons. First of all, removing of the original vectors will compromise some features like oversampling, like segment building. And actually removing of those original vectors will only be compatible with some types of quantization for example, it won't be compatible with scalar quantization because in this case we won't be able to rebuild index to do maintenance of the system\n\nAnswer:",What configuration is required to enable in-memory quantization with on-disk original vectors in Qdrant?,"To configure in-memory quantization with on-disk original vectors in Qdrant, you need to create a collection with the following configuration:

```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 768,

        ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": true

        }

    }

}
```

Additionally, in the Python code snippet provided, you can use the QdrantClient library to create the collection with the specified configuration parameters. This includes defining vector size, distance metric (such as Cosine similarity), memmap threshold for optimization, and scalar quantization configuration with type set to INT8","['To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 768,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python', 'This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models', '5. RAM: Store the full vectors and payload on disk. Limit what you load from memory to the binary quantization index. This helps reduce the memory footprint and improve the overall efficiency of the system. The incremental latency from the disk read is negligible compared to the latency savings from the binary scoring in Qdrant, which uses SIMD instructions where possible.\n\n\n\nWant to discuss these findings and learn more about Binary Quantization? [Join our Discord community.](https://discord.gg/qdrant)', ""```\n\n\n\nWhile Qdrant offers various options to store some parts of the data on disk, starting \n\nfrom version 1.1.0, you can also optimize your memory by compressing the embeddings. \n\nWe've implemented the mechanism of **Scalar Quantization**! It turns out to have not \n\nonly a positive impact on memory but also on the performance. \n\n\n\n## Scalar Quantization\n\n\n\nScalar quantization is a data compression technique that converts floating point values"", ""Right? So unfortunately Qdrant architecture is not designed and not expecting this type of behavior for several reasons. First of all, removing of the original vectors will compromise some features like oversampling, like segment building. And actually removing of those original vectors will only be compatible with some types of quantization for example, it won't be compatible with scalar quantization because in this case we won't be able to rebuild index to do maintenance of the system""]","To configure in-memory quantization, with on-disk original vectors, you need to create a collection with the following configuration:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 768,

        ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": true

        }

    }

}

```



```python
This mode is enabled by setting `always_ram` to `false` in the quantization config while using mmap storage:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""quantization_config"": {

        ""scalar"": {

            ""type"": ""int8"",

            ""always_ram"": false

        }

    }

}

```



```python

from qdrant_client import QdrantClient, models
5. RAM: Store the full vectors and payload on disk. Limit what you load from memory to the binary quantization index. This helps reduce the memory footprint and improve the overall efficiency of the system. The incremental latency from the disk read is negligible compared to the latency savings from the binary scoring in Qdrant, which uses SIMD instructions where possible.



Want to discuss these findings and learn more about Binary Quantization? [Join our Discord community.](https://discord.gg/qdrant)
```



While Qdrant offers various options to store some parts of the data on disk, starting 

from version 1.1.0, you can also optimize your memory by compressing the embeddings. 

We've implemented the mechanism of **Scalar Quantization**! It turns out to have not 

only a positive impact on memory but also on the performance. 



## Scalar Quantization



Scalar quantization is a data compression technique that converts floating point values
Right? So unfortunately Qdrant architecture is not designed and not expecting this type of behavior for several reasons. First of all, removing of the original vectors will compromise some features like oversampling, like segment building. And actually removing of those original vectors will only be compatible with some types of quantization for example, it won't be compatible with scalar quantization because in this case we won't be able to rebuild index to do maintenance of the system",0.4855072463768116,0.788235294117647,0.600896860986547,0.29927007299270075,0.4880952380952381,0.37104072398190047,0.391304347826087,0.6352941176470588,0.484304932735426,0.42028985507246375,0.6823529411764706,0.5201793721973095,0.9888614416122437,0.647621214389801,0.6777667999267578,0.6623511910438538,0.20411893725395203,0.4114315062761307,125,1.4880952380952381,0.6488224468217381
True,,5451.503,4fdd8698,How can dissimilarity search be used for outlier detection in a dataset?,"Dissimilarity search can be utilized for outlier detection in a dataset by first establishing a set of reference points that are considered ""normal"". These reference points serve as a benchmark for comparison. Subsequently, the dissimilarity search algorithm is applied to identify the data points that are the most dissimilar to the reference set. These identified points are then flagged as potential outliers or anomalies in the dataset. This approach allows for the detection of abnormal data points even in cases where labels are not available, enabling the identification of outliers based on their deviation from the established ""normal"" reference points. This method can be a valuable tool in data analysis and anomaly detection tasks.","Dissimilarity search can be used for outlier detection in a dataset by following these steps:

1. **Define Normal Reference Points**: Start by selecting a set of reference points that are considered to be ""normal"" within the dataset. These reference points serve as a baseline for comparison.

2. **Calculate Dissimilarity**: Use vector similarity to compute the dissimilarity between each data point in the dataset and the normal reference points. This involves comparing the embeddings of data points to determine how different they are from the normal reference points.

3. **Identify Most Dissimilar Points**: Focus on finding the data points that are the most dissimilar to the normal reference points. These highly dissimilar points are potential outliers or anomalies in the dataset.

4. **Further Analysis**: Once the most dissimilar points are identified, they can be flagged as candidates for further analysis. This could involve investigating why these points deviate significantly from the normal reference points and determining if they are indeed outliers.

5. **Utilize Models**: To enhance the outlier detection process, consider training models such as autoencoders or classifiers on the dataset. These models can be used to automatically identify anomalies based on the learned patterns in the data, reducing the manual effort required for outlier detection.

By leveraging dissimilarity search and incorporating models, outlier detection in a dataset can be enhanced, even in cases where labels are not available.","### Case: Outlier Detection



In some cases, we might not even have labels, but it is still possible to try to detect anomalies in our dataset.

Dissimilarity search can be used for this purpose as well.



{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/anomaly-detection.png caption=""Anomaly Detection"" >}}



The only thing we need is a bunch of reference points that we consider ""normal"".
Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. 

It doesn’t rely on keywords or other metadata. 

With vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.



The dissimilarity search can find items in areas where previously no other search could be used.

Let’s look at a few examples.



### Case: Mislabeling Detection
Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.





## Diversity Search



Even with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.



The naive approach is to do random sampling. 

However, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.
You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. 

To mitigate this issue, you can perform a diversity search.



Diversity search is a method for finding the most distinctive examples in the data.

As similarity search, it also operates on embeddings and measures the distances between them.

The difference lies in deciding which point should be extracted next.
There is no silver bullet. You should validate your dataset thoroughly, and you need tools for this.



When you are sure that there are not many objects placed in the wrong category, they can be considered outliers or anomalies.

Thus, you can train a model or a bunch of models capable of looking for anomalies, e.g. autoencoder and a classifier on it.

However, this is again a resource-intensive task, both in terms of time and manual labour, since labels have to be provided for classification.","['### Case: Outlier Detection\n\n\n\nIn some cases, we might not even have labels, but it is still possible to try to detect anomalies in our dataset.\n\nDissimilarity search can be used for this purpose as well.\n\n\n\n{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/anomaly-detection.png caption=""Anomaly Detection"" >}}\n\n\n\nThe only thing we need is a bunch of reference points that we consider ""normal"".', 'Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. \n\nIt doesn’t rely on keywords or other metadata. \n\nWith vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.\n\n\n\nThe dissimilarity search can find items in areas where previously no other search could be used.\n\nLet’s look at a few examples.\n\n\n\n### Case: Mislabeling Detection', 'Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.\n\n\n\n\n\n## Diversity Search\n\n\n\nEven with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.\n\n\n\nThe naive approach is to do random sampling. \n\nHowever, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.', 'You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. \n\nTo mitigate this issue, you can perform a diversity search.\n\n\n\nDiversity search is a method for finding the most distinctive examples in the data.\n\nAs similarity search, it also operates on embeddings and measures the distances between them.\n\nThe difference lies in deciding which point should be extracted next.', 'There is no silver bullet. You should validate your dataset thoroughly, and you need tools for this.\n\n\n\nWhen you are sure that there are not many objects placed in the wrong category, they can be considered outliers or anomalies.\n\nThus, you can train a model or a bunch of models capable of looking for anomalies, e.g. autoencoder and a classifier on it.\n\nHowever, this is again a resource-intensive task, both in terms of time and manual labour, since labels have to be provided for classification.']","Question: How can dissimilarity search be used for outlier detection in a dataset?\n\nContext: ### Case: Outlier Detection



In some cases, we might not even have labels, but it is still possible to try to detect anomalies in our dataset.

Dissimilarity search can be used for this purpose as well.



{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/anomaly-detection.png caption=""Anomaly Detection"" >}}



The only thing we need is a bunch of reference points that we consider ""normal"".
Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. 

It doesn’t rely on keywords or other metadata. 

With vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.



The dissimilarity search can find items in areas where previously no other search could be used.

Let’s look at a few examples.



### Case: Mislabeling Detection
Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.





## Diversity Search



Even with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.



The naive approach is to do random sampling. 

However, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.
You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. 

To mitigate this issue, you can perform a diversity search.



Diversity search is a method for finding the most distinctive examples in the data.

As similarity search, it also operates on embeddings and measures the distances between them.

The difference lies in deciding which point should be extracted next.
There is no silver bullet. You should validate your dataset thoroughly, and you need tools for this.



When you are sure that there are not many objects placed in the wrong category, they can be considered outliers or anomalies.

Thus, you can train a model or a bunch of models capable of looking for anomalies, e.g. autoencoder and a classifier on it.

However, this is again a resource-intensive task, both in terms of time and manual labour, since labels have to be provided for classification.\n\nAnswer:",How can dissimilarity search be used for outlier detection in a dataset?,"Dissimilarity search can be utilized for outlier detection in a dataset by first establishing a set of reference points that are considered ""normal"". These reference points serve as a benchmark for comparison. Subsequently, the dissimilarity search algorithm is applied to identify the data points that are the most dissimilar to the reference set. These identified points are then flagged as potential outliers or anomalies in the dataset. This approach allows for the detection of abnormal data points even in cases where labels are not available, enabling the identification of outliers based on their deviation from the established ""normal"" reference points. This method can be a valuable tool in data analysis and anomaly detection tasks.","['### Case: Outlier Detection\n\n\n\nIn some cases, we might not even have labels, but it is still possible to try to detect anomalies in our dataset.\n\nDissimilarity search can be used for this purpose as well.\n\n\n\n{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/anomaly-detection.png caption=""Anomaly Detection"" >}}\n\n\n\nThe only thing we need is a bunch of reference points that we consider ""normal"".', 'Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. \n\nIt doesn’t rely on keywords or other metadata. \n\nWith vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.\n\n\n\nThe dissimilarity search can find items in areas where previously no other search could be used.\n\nLet’s look at a few examples.\n\n\n\n### Case: Mislabeling Detection', 'Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.\n\n\n\n\n\n## Diversity Search\n\n\n\nEven with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.\n\n\n\nThe naive approach is to do random sampling. \n\nHowever, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.', 'You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. \n\nTo mitigate this issue, you can perform a diversity search.\n\n\n\nDiversity search is a method for finding the most distinctive examples in the data.\n\nAs similarity search, it also operates on embeddings and measures the distances between them.\n\nThe difference lies in deciding which point should be extracted next.', 'There is no silver bullet. You should validate your dataset thoroughly, and you need tools for this.\n\n\n\nWhen you are sure that there are not many objects placed in the wrong category, they can be considered outliers or anomalies.\n\nThus, you can train a model or a bunch of models capable of looking for anomalies, e.g. autoencoder and a classifier on it.\n\nHowever, this is again a resource-intensive task, both in terms of time and manual labour, since labels have to be provided for classification.']","### Case: Outlier Detection



In some cases, we might not even have labels, but it is still possible to try to detect anomalies in our dataset.

Dissimilarity search can be used for this purpose as well.



{{< figure width=80% src=/articles_data/vector-similarity-beyond-search/anomaly-detection.png caption=""Anomaly Detection"" >}}



The only thing we need is a bunch of reference points that we consider ""normal"".
Unlike full-text match, Vector similarity can compare any pair of documents (or points) and assign a similarity score. 

It doesn’t rely on keywords or other metadata. 

With vector similarity, we can easily achieve a dissimilarity search by inverting the search objective from maximizing similarity to minimizing it.



The dissimilarity search can find items in areas where previously no other search could be used.

Let’s look at a few examples.



### Case: Mislabeling Detection
Then we can search for the most dissimilar points to this reference set and use them as candidates for further analysis.





## Diversity Search



Even with no input provided vector, (dis-)similarity can improve an overall selection of items from the dataset.



The naive approach is to do random sampling. 

However, unless our dataset has a uniform distribution, the results of such sampling might be biased toward more frequent types of items.
You might find yourself in a situation when the model focuses on non-important features, selects a lot of irrelevant elements, and fails to find genuine errors. 

To mitigate this issue, you can perform a diversity search.



Diversity search is a method for finding the most distinctive examples in the data.

As similarity search, it also operates on embeddings and measures the distances between them.

The difference lies in deciding which point should be extracted next.
There is no silver bullet. You should validate your dataset thoroughly, and you need tools for this.



When you are sure that there are not many objects placed in the wrong category, they can be considered outliers or anomalies.

Thus, you can train a model or a bunch of models capable of looking for anomalies, e.g. autoencoder and a classifier on it.

However, this is again a resource-intensive task, both in terms of time and manual labour, since labels have to be provided for classification.",0.391304347826087,0.7941176470588235,0.5242718446601942,0.22815533980582525,0.46534653465346537,0.3061889250814332,0.28019323671497587,0.5686274509803921,0.3754045307443366,0.38164251207729466,0.7745098039215687,0.5113268608414239,0.8834488391876221,0.671454906463623,0.7528845071792603,0.7098420262336731,0.7941727247798553,0.9929439077871697,228,2.0,0.414837131756288
True,,3966.79,2704ed39,How does binary quantization work and what benefits does it offer in terms of query processing times and data compression?,"Binary quantization is a method used by Qdrant for fast indexing and data compression. In this process, each data point is represented as a vector. The quantization splits the vector in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This results in a string of bits that represents the original vector, making it much smaller and easier to compare. By supporting vector comparisons, binary quantization can significantly speed up query processing times, up to 40 times faster. This method is especially effective for OpenAI embeddings, where it has been shown to achieve a massive performance improvement at a lower cost of accuracy.","Binary quantization is a technique used for compressing vectors into a more compact representation by converting continuous values into binary values. This compression method significantly reduces the storage footprint of the data while sacrificing some level of accuracy.

In terms of benefits, binary quantization offers several advantages in query processing times and data compression:

1. **Reduced storage footprint**: By converting vectors into binary representations, the storage requirements for the data are significantly reduced. This is especially beneficial for large-scale datasets where storage costs can be a limiting factor.

2. **Enhanced speed of data retrieval**: Smaller data sizes resulting from binary quantization generally lead to faster search times. This is because searching through compressed binary data is more efficient than searching through larger continuous values.

3. **Accelerated search process**: Binary quantization simplifies the distance calculations between vectors into bitwise operations. This simplification enables faster real-time querying, even in extensive databases. By reducing the complexity of calculations, the search process is accelerated, leading to quicker query responses.

Overall, binary quantization offers a balance between storage efficiency, query processing speed, and data compression, making it a valuable technique for optimizing query processing times and reducing storage costs in database systems.","#### What is Binary Quantization?



Quantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.



[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).
![](/blog/openai/Accuracy_Models.png)



The efficiency gains from Binary Quantization are as follows: 



- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. 

- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. 

- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.
By reducing storage needs, you can scale applications with lower costs. This addresses a critical challenge posed by the original embedding sizes. Binary Quantization also speeds the search process. It simplifies the complex distance calculations between vectors into more manageable bitwise operations, which supports potentially real-time searches across vast datasets.
5 Keys to Learning from the Episode:



1. The necessity of quantization and the complex challenges it helps to overcome.

2. The transformative effects of binary quantization on processing speed and storage size reduction.

3. A detailed exploration of oversampling and its real-time precision control in query search.

4. Understanding the simplicity and effectiveness of binary quantization, especially when compared to more intricate quantization methods.
As is the case with other quantization methods, these benefits come at the cost of recall degradation. However, our implementation lets you balance the tradeoff between speed and recall accuracy at time of search, rather than time of index creation. 



The rest of this article will cover:

1. The importance of binary quantization

2. Basic implementation using our Python client

3. Benchmark analysis and usage recommendations



## What is Binary Quantization?","['#### What is Binary Quantization?\n\n\n\nQuantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.\n\n\n\n[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).', '![](/blog/openai/Accuracy_Models.png)\n\n\n\nThe efficiency gains from Binary Quantization are as follows: \n\n\n\n- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. \n\n- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. \n\n- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.', 'By reducing storage needs, you can scale applications with lower costs. This addresses a critical challenge posed by the original embedding sizes. Binary Quantization also speeds the search process. It simplifies the complex distance calculations between vectors into more manageable bitwise operations, which supports potentially real-time searches across vast datasets.', '5 Keys to Learning from the Episode:\n\n\n\n1. The necessity of quantization and the complex challenges it helps to overcome.\n\n2. The transformative effects of binary quantization on processing speed and storage size reduction.\n\n3. A detailed exploration of oversampling and its real-time precision control in query search.\n\n4. Understanding the simplicity and effectiveness of binary quantization, especially when compared to more intricate quantization methods.', 'As is the case with other quantization methods, these benefits come at the cost of recall degradation. However, our implementation lets you balance the tradeoff between speed and recall accuracy at time of search, rather than time of index creation. \n\n\n\nThe rest of this article will cover:\n\n1. The importance of binary quantization\n\n2. Basic implementation using our Python client\n\n3. Benchmark analysis and usage recommendations\n\n\n\n## What is Binary Quantization?']","Question: How does binary quantization work and what benefits does it offer in terms of query processing times and data compression?\n\nContext: #### What is Binary Quantization?



Quantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.



[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).
![](/blog/openai/Accuracy_Models.png)



The efficiency gains from Binary Quantization are as follows: 



- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. 

- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. 

- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.
By reducing storage needs, you can scale applications with lower costs. This addresses a critical challenge posed by the original embedding sizes. Binary Quantization also speeds the search process. It simplifies the complex distance calculations between vectors into more manageable bitwise operations, which supports potentially real-time searches across vast datasets.
5 Keys to Learning from the Episode:



1. The necessity of quantization and the complex challenges it helps to overcome.

2. The transformative effects of binary quantization on processing speed and storage size reduction.

3. A detailed exploration of oversampling and its real-time precision control in query search.

4. Understanding the simplicity and effectiveness of binary quantization, especially when compared to more intricate quantization methods.
As is the case with other quantization methods, these benefits come at the cost of recall degradation. However, our implementation lets you balance the tradeoff between speed and recall accuracy at time of search, rather than time of index creation. 



The rest of this article will cover:

1. The importance of binary quantization

2. Basic implementation using our Python client

3. Benchmark analysis and usage recommendations



## What is Binary Quantization?\n\nAnswer:",How does binary quantization work and what benefits does it offer in terms of query processing times and data compression?,"Binary quantization is a method used by Qdrant for fast indexing and data compression. In this process, each data point is represented as a vector. The quantization splits the vector in half at a certain point, marking everything above as ""1"" and everything below as ""0"". This results in a string of bits that represents the original vector, making it much smaller and easier to compare. By supporting vector comparisons, binary quantization can significantly speed up query processing times, up to 40 times faster. This method is especially effective for OpenAI embeddings, where it has been shown to achieve a massive performance improvement at a lower cost of accuracy.","['#### What is Binary Quantization?\n\n\n\nQuantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.\n\n\n\n[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).', '![](/blog/openai/Accuracy_Models.png)\n\n\n\nThe efficiency gains from Binary Quantization are as follows: \n\n\n\n- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. \n\n- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. \n\n- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.', 'By reducing storage needs, you can scale applications with lower costs. This addresses a critical challenge posed by the original embedding sizes. Binary Quantization also speeds the search process. It simplifies the complex distance calculations between vectors into more manageable bitwise operations, which supports potentially real-time searches across vast datasets.', '5 Keys to Learning from the Episode:\n\n\n\n1. The necessity of quantization and the complex challenges it helps to overcome.\n\n2. The transformative effects of binary quantization on processing speed and storage size reduction.\n\n3. A detailed exploration of oversampling and its real-time precision control in query search.\n\n4. Understanding the simplicity and effectiveness of binary quantization, especially when compared to more intricate quantization methods.', 'As is the case with other quantization methods, these benefits come at the cost of recall degradation. However, our implementation lets you balance the tradeoff between speed and recall accuracy at time of search, rather than time of index creation. \n\n\n\nThe rest of this article will cover:\n\n1. The importance of binary quantization\n\n2. Basic implementation using our Python client\n\n3. Benchmark analysis and usage recommendations\n\n\n\n## What is Binary Quantization?']","#### What is Binary Quantization?



Quantization is a technique used for reducing the total size of the database. It works by compressing vectors into a more compact representation at the cost of accuracy.



[Binary Quantization](https://qdrant.tech/articles/binary-quantization/) is a fast indexing and data compression method used by Qdrant. It supports vector comparisons, which can dramatically speed up query processing times (up to 40x faster!).
![](/blog/openai/Accuracy_Models.png)



The efficiency gains from Binary Quantization are as follows: 



- Reduced storage footprint: It helps with large-scale datasets. It also saves on memory, and scales up to 30x at the same cost. 

- Enhanced speed of data retrieval: Smaller data sizes generally leads to faster searches. 

- Accelerated search process: It is based on simplified distance calculations between vectors to bitwise operations. This enables real-time querying even in extensive databases.
By reducing storage needs, you can scale applications with lower costs. This addresses a critical challenge posed by the original embedding sizes. Binary Quantization also speeds the search process. It simplifies the complex distance calculations between vectors into more manageable bitwise operations, which supports potentially real-time searches across vast datasets.
5 Keys to Learning from the Episode:



1. The necessity of quantization and the complex challenges it helps to overcome.

2. The transformative effects of binary quantization on processing speed and storage size reduction.

3. A detailed exploration of oversampling and its real-time precision control in query search.

4. Understanding the simplicity and effectiveness of binary quantization, especially when compared to more intricate quantization methods.
As is the case with other quantization methods, these benefits come at the cost of recall degradation. However, our implementation lets you balance the tradeoff between speed and recall accuracy at time of search, rather than time of index creation. 



The rest of this article will cover:

1. The importance of binary quantization

2. Basic implementation using our Python client

3. Benchmark analysis and usage recommendations



## What is Binary Quantization?",0.2727272727272727,0.51,0.35540069686411146,0.053763440860215055,0.10101010101010101,0.07017543859649122,0.12834224598930483,0.24,0.1672473867595819,0.25133689839572193,0.47,0.32752613240418116,0.8294735550880432,0.5879443883895874,0.624904990196228,0.605861485004425,0.9570958515844092,0.787096936677699,197,1.8073394495412844,0.5464417655230579
True,,2731.023,92844143,What is the primary function of vector embeddings in the context of machine learning and artificial intelligence?,"Vector embeddings play a crucial role in machine learning and artificial intelligence by converting complex data into a format that machines can easily understand and process. These embeddings represent data points in a multi-dimensional space, where similar data points are closer together and dissimilar data points are farther apart. By capturing the underlying relationships and similarities between data points, vector embeddings enable algorithms to perform tasks such as similarity search, recommendation systems, and natural language processing more effectively. In essence, vector embeddings help in transforming raw data into a structured and meaningful representation that can be utilized by machine learning models to make accurate predictions and decisions.","The primary function of vector embeddings in the context of machine learning and artificial intelligence is to translate high-dimensional data into a more manageable, lower-dimensional vector form that is more suitable for processing by machine learning algorithms. Embeddings capture the semantic meaning of complex data such as text, images, or audio into numerical representations, enabling algorithms to analyze and process the data more efficiently. They help in reducing storage requirements, improving computational efficiency, and making sense of unstructured data. The quality of the vector representations significantly impacts the performance of machine learning models, and the choice of embedding model depends on the specific use case.","The **creation** of vector data (so we can store this high-dimensional data on our vector database) is primarily done through **embeddings**.



![](/articles_data/what-is-a-vector-database/Vector-Data.jpg)



### How do Embeddings Work?



Embeddings translate this high-dimensional data into a more manageable, **lower-dimensional** vector form that's more suitable for machine learning and data processing applications, typically through **neural network models**.
![How embeddings are applied to perform recommendantions and other use cases](/articles_data/what-are-embeddings/Embeddings-Use-Case.jpg)





They make [high-dimensional](https://www.sciencedirect.com/topics/computer-science/high-dimensional-data) data more manageable. This reduces storage requirements, improves computational efficiency, and makes sense of a ton of **unstructured** data.





## Why Use Vector Embeddings?
- machine-learning

  - artificial intelligence



---



> **Embeddings** are numerical machine learning representations of the semantic of the input data. They capture the meaning of complex, high-dimensional data, like text, images, or audio, into vectors. Enabling algorithms to process and analyze the data more efficiently.
> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.





### Creating Vector Embeddings



Embeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.
## Why Use Vector Embeddings?



The **nuances** of natural language or the hidden **meaning** in large datasets of images, sounds, or user interactions are hard to fit into a table. Traditional relational databases can't efficiently query most types of data being currently used and produced, making the **retrieval** of this information very limited.","[""The **creation** of vector data (so we can store this high-dimensional data on our vector database) is primarily done through **embeddings**.\n\n\n\n![](/articles_data/what-is-a-vector-database/Vector-Data.jpg)\n\n\n\n### How do Embeddings Work?\n\n\n\nEmbeddings translate this high-dimensional data into a more manageable, **lower-dimensional** vector form that's more suitable for machine learning and data processing applications, typically through **neural network models**."", '![How embeddings are applied to perform recommendantions and other use cases](/articles_data/what-are-embeddings/Embeddings-Use-Case.jpg)\n\n\n\n\n\nThey make [high-dimensional](https://www.sciencedirect.com/topics/computer-science/high-dimensional-data) data more manageable. This reduces storage requirements, improves computational efficiency, and makes sense of a ton of **unstructured** data.\n\n\n\n\n\n## Why Use Vector Embeddings?', '- machine-learning\n\n  - artificial intelligence\n\n\n\n---\n\n\n\n> **Embeddings** are numerical machine learning representations of the semantic of the input data. They capture the meaning of complex, high-dimensional data, like text, images, or audio, into vectors. Enabling algorithms to process and analyze the data more efficiently.', '> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.\n\n\n\n\n\n### Creating Vector Embeddings\n\n\n\nEmbeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.', ""## Why Use Vector Embeddings?\n\n\n\nThe **nuances** of natural language or the hidden **meaning** in large datasets of images, sounds, or user interactions are hard to fit into a table. Traditional relational databases can't efficiently query most types of data being currently used and produced, making the **retrieval** of this information very limited.""]","Question: What is the primary function of vector embeddings in the context of machine learning and artificial intelligence?\n\nContext: The **creation** of vector data (so we can store this high-dimensional data on our vector database) is primarily done through **embeddings**.



![](/articles_data/what-is-a-vector-database/Vector-Data.jpg)



### How do Embeddings Work?



Embeddings translate this high-dimensional data into a more manageable, **lower-dimensional** vector form that's more suitable for machine learning and data processing applications, typically through **neural network models**.
![How embeddings are applied to perform recommendantions and other use cases](/articles_data/what-are-embeddings/Embeddings-Use-Case.jpg)





They make [high-dimensional](https://www.sciencedirect.com/topics/computer-science/high-dimensional-data) data more manageable. This reduces storage requirements, improves computational efficiency, and makes sense of a ton of **unstructured** data.





## Why Use Vector Embeddings?
- machine-learning

- artificial intelligence



---



> **Embeddings** are numerical machine learning representations of the semantic of the input data. They capture the meaning of complex, high-dimensional data, like text, images, or audio, into vectors. Enabling algorithms to process and analyze the data more efficiently.
> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.





### Creating Vector Embeddings



Embeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.
## Why Use Vector Embeddings?



The **nuances** of natural language or the hidden **meaning** in large datasets of images, sounds, or user interactions are hard to fit into a table. Traditional relational databases can't efficiently query most types of data being currently used and produced, making the **retrieval** of this information very limited.\n\nAnswer:",What is the primary function of vector embeddings in the context of machine learning and artificial intelligence?,"Vector embeddings play a crucial role in machine learning and artificial intelligence by converting complex data into a format that machines can easily understand and process. These embeddings represent data points in a multi-dimensional space, where similar data points are closer together and dissimilar data points are farther apart. By capturing the underlying relationships and similarities between data points, vector embeddings enable algorithms to perform tasks such as similarity search, recommendation systems, and natural language processing more effectively. In essence, vector embeddings help in transforming raw data into a structured and meaningful representation that can be utilized by machine learning models to make accurate predictions and decisions.","[""The **creation** of vector data (so we can store this high-dimensional data on our vector database) is primarily done through **embeddings**.\n\n\n\n![](/articles_data/what-is-a-vector-database/Vector-Data.jpg)\n\n\n\n### How do Embeddings Work?\n\n\n\nEmbeddings translate this high-dimensional data into a more manageable, **lower-dimensional** vector form that's more suitable for machine learning and data processing applications, typically through **neural network models**."", '![How embeddings are applied to perform recommendantions and other use cases](/articles_data/what-are-embeddings/Embeddings-Use-Case.jpg)\n\n\n\n\n\nThey make [high-dimensional](https://www.sciencedirect.com/topics/computer-science/high-dimensional-data) data more manageable. This reduces storage requirements, improves computational efficiency, and makes sense of a ton of **unstructured** data.\n\n\n\n\n\n## Why Use Vector Embeddings?', '- machine-learning\n\n  - artificial intelligence\n\n\n\n---\n\n\n\n> **Embeddings** are numerical machine learning representations of the semantic of the input data. They capture the meaning of complex, high-dimensional data, like text, images, or audio, into vectors. Enabling algorithms to process and analyze the data more efficiently.', '> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.\n\n\n\n\n\n### Creating Vector Embeddings\n\n\n\nEmbeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.', ""## Why Use Vector Embeddings?\n\n\n\nThe **nuances** of natural language or the hidden **meaning** in large datasets of images, sounds, or user interactions are hard to fit into a table. Traditional relational databases can't efficiently query most types of data being currently used and produced, making the **retrieval** of this information very limited.""]","The **creation** of vector data (so we can store this high-dimensional data on our vector database) is primarily done through **embeddings**.



![](/articles_data/what-is-a-vector-database/Vector-Data.jpg)



### How do Embeddings Work?



Embeddings translate this high-dimensional data into a more manageable, **lower-dimensional** vector form that's more suitable for machine learning and data processing applications, typically through **neural network models**.
![How embeddings are applied to perform recommendantions and other use cases](/articles_data/what-are-embeddings/Embeddings-Use-Case.jpg)





They make [high-dimensional](https://www.sciencedirect.com/topics/computer-science/high-dimensional-data) data more manageable. This reduces storage requirements, improves computational efficiency, and makes sense of a ton of **unstructured** data.





## Why Use Vector Embeddings?
- machine-learning

  - artificial intelligence



---



> **Embeddings** are numerical machine learning representations of the semantic of the input data. They capture the meaning of complex, high-dimensional data, like text, images, or audio, into vectors. Enabling algorithms to process and analyze the data more efficiently.
> The quality of the vector representations drives the performance. The embedding model that works best for you depends on your use case.





### Creating Vector Embeddings



Embeddings translate the complexities of human language to a format that computers can understand. It uses neural networks to assign **numerical values** to the input data, in a way that similar data has similar values.
## Why Use Vector Embeddings?



The **nuances** of natural language or the hidden **meaning** in large datasets of images, sounds, or user interactions are hard to fit into a table. Traditional relational databases can't efficiently query most types of data being currently used and produced, making the **retrieval** of this information very limited.",0.4536082474226804,0.42718446601941745,0.44,0.15625,0.14705882352941177,0.15151515151515152,0.29896907216494845,0.2815533980582524,0.29000000000000004,0.4329896907216495,0.4077669902912621,0.41999999999999993,0.7827973365783691,0.6765779256820679,0.6928789615631104,0.6846314072608948,0.9984818425655249,0.9923815218207892,105,0.9813084112149533,0.576028666498147
True,,2175.292,f369b77b,What is the concept of the `best_score` strategy introduced in version 1.6.0 of the recommendation system?,"The `best_score` strategy, introduced in version 1.6.0 of the recommendation system, is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided.","The `best_score` strategy introduced in version 1.6.0 of the recommendation system is based on finding vectors that are closer to a positive example while avoiding those closer to a negative example. This strategy takes into account multiple examples separately, providing a richer variety of results compared to the old strategy where only a single vector was considered. By using `best_score`, the recommendation engine can provide more diverse and tailored recommendations based on the specific positive and negative examples provided.","This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
##### One positive and one negative example



<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>



The more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there 

is just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making 

the variety richer.
Qdrant has provided the [Recommendation API](https://qdrant.tech/documentation/concepts/search/#recommendation-api) for a while, and with the latest release, [Qdrant 1.6](https://github.com/qdrant/qdrant/releases/tag/v1.6.0), 

we're glad to give you more flexibility and control over the Recommendation API. 

Here, we'll discuss some internals and show how they may be used in practice.



### Recap of the old recommendations API
The `recommend` endpoint works roughly the same as `search_points`, but instead of searching for a vector, Qdrant searches for one or more points (you can also give negative example points the search engine will try to avoid in the results). It was built to help drive recommendation engines, saving the round-trip of sending the current point's vector back to Qdrant to find more similar ones
```



Example result of this API would be



```json

{

  ""result"": [

    { ""id"": 10, ""score"": 0.81 },

    { ""id"": 14, ""score"": 0.75 },

    { ""id"": 11, ""score"": 0.73 }

  ],

  ""status"": ""ok"",

  ""time"": 0.001

}

```



The algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.



### Average vector strategy","['This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.\n\n\n\n### Best score strategy\n\n\n\n*Available as of v1.6.0*\n\n\n\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.', '##### One positive and one negative example\n\n\n\n<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>\n\n\n\nThe more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there \n\nis just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making \n\nthe variety richer.', ""Qdrant has provided the [Recommendation API](https://qdrant.tech/documentation/concepts/search/#recommendation-api) for a while, and with the latest release, [Qdrant 1.6](https://github.com/qdrant/qdrant/releases/tag/v1.6.0), \n\nwe're glad to give you more flexibility and control over the Recommendation API. \n\nHere, we'll discuss some internals and show how they may be used in practice.\n\n\n\n### Recap of the old recommendations API"", ""The `recommend` endpoint works roughly the same as `search_points`, but instead of searching for a vector, Qdrant searches for one or more points (you can also give negative example points the search engine will try to avoid in the results). It was built to help drive recommendation engines, saving the round-trip of sending the current point's vector back to Qdrant to find more similar ones"", '```\n\n\n\nExample result of this API would be\n\n\n\n```json\n\n{\n\n  ""result"": [\n\n    { ""id"": 10, ""score"": 0.81 },\n\n    { ""id"": 14, ""score"": 0.75 },\n\n    { ""id"": 11, ""score"": 0.73 }\n\n  ],\n\n  ""status"": ""ok"",\n\n  ""time"": 0.001\n\n}\n\n```\n\n\n\nThe algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.\n\n\n\n### Average vector strategy']","Question: What is the concept of the `best_score` strategy introduced in version 1.6.0 of the recommendation system?\n\nContext: This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
##### One positive and one negative example



<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>



The more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there 

is just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making 

the variety richer.
Qdrant has provided the [Recommendation API](https://qdrant.tech/documentation/concepts/search/#recommendation-api) for a while, and with the latest release, [Qdrant 1.6](https://github.com/qdrant/qdrant/releases/tag/v1.6.0), 

we're glad to give you more flexibility and control over the Recommendation API. 

Here, we'll discuss some internals and show how they may be used in practice.



### Recap of the old recommendations API
The `recommend` endpoint works roughly the same as `search_points`, but instead of searching for a vector, Qdrant searches for one or more points (you can also give negative example points the search engine will try to avoid in the results). It was built to help drive recommendation engines, saving the round-trip of sending the current point's vector back to Qdrant to find more similar ones
```



Example result of this API would be



```json

{

""result"": [

{ ""id"": 10, ""score"": 0.81 },

{ ""id"": 14, ""score"": 0.75 },

{ ""id"": 11, ""score"": 0.73 }

],

""status"": ""ok"",

""time"": 0.001

}

```



The algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.



### Average vector strategy\n\nAnswer:",What is the concept of the `best_score` strategy introduced in version 1.6.0 of the recommendation system?,"The `best_score` strategy, introduced in version 1.6.0 of the recommendation system, is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided.","['This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.\n\n\n\n### Best score strategy\n\n\n\n*Available as of v1.6.0*\n\n\n\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.', '##### One positive and one negative example\n\n\n\n<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>\n\n\n\nThe more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there \n\nis just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making \n\nthe variety richer.', ""Qdrant has provided the [Recommendation API](https://qdrant.tech/documentation/concepts/search/#recommendation-api) for a while, and with the latest release, [Qdrant 1.6](https://github.com/qdrant/qdrant/releases/tag/v1.6.0), \n\nwe're glad to give you more flexibility and control over the Recommendation API. \n\nHere, we'll discuss some internals and show how they may be used in practice.\n\n\n\n### Recap of the old recommendations API"", ""The `recommend` endpoint works roughly the same as `search_points`, but instead of searching for a vector, Qdrant searches for one or more points (you can also give negative example points the search engine will try to avoid in the results). It was built to help drive recommendation engines, saving the round-trip of sending the current point's vector back to Qdrant to find more similar ones"", '```\n\n\n\nExample result of this API would be\n\n\n\n```json\n\n{\n\n  ""result"": [\n\n    { ""id"": 10, ""score"": 0.81 },\n\n    { ""id"": 14, ""score"": 0.75 },\n\n    { ""id"": 11, ""score"": 0.73 }\n\n  ],\n\n  ""status"": ""ok"",\n\n  ""time"": 0.001\n\n}\n\n```\n\n\n\nThe algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.\n\n\n\n### Average vector strategy']","This is the default strategy that's going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.



### Best score strategy



*Available as of v1.6.0*



A new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.
##### One positive and one negative example



<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>



The more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there 

is just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making 

the variety richer.
Qdrant has provided the [Recommendation API](https://qdrant.tech/documentation/concepts/search/#recommendation-api) for a while, and with the latest release, [Qdrant 1.6](https://github.com/qdrant/qdrant/releases/tag/v1.6.0), 

we're glad to give you more flexibility and control over the Recommendation API. 

Here, we'll discuss some internals and show how they may be used in practice.



### Recap of the old recommendations API
The `recommend` endpoint works roughly the same as `search_points`, but instead of searching for a vector, Qdrant searches for one or more points (you can also give negative example points the search engine will try to avoid in the results). It was built to help drive recommendation engines, saving the round-trip of sending the current point's vector back to Qdrant to find more similar ones
```



Example result of this API would be



```json

{

  ""result"": [

    { ""id"": 10, ""score"": 0.81 },

    { ""id"": 14, ""score"": 0.75 },

    { ""id"": 11, ""score"": 0.73 }

  ],

  ""status"": ""ok"",

  ""time"": 0.001

}

```



The algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.



### Average vector strategy",0.6216216216216216,0.4144144144144144,0.49729729729729716,0.3835616438356164,0.2545454545454545,0.3060109289617486,0.527027027027027,0.35135135135135137,0.42162162162162165,0.581081081081081,0.38738738738738737,0.4648648648648649,0.8970018625259399,0.6910831332206726,0.6557198762893677,0.672937273979187,0.9849535539591064,0.6615269497036934,79,0.626984126984127,0.305458267958452
True,,3312.201,5a64efd0,How does Qdrant handle sparse vectors and what changes have been made to accommodate them?,"Qdrant, until recently, was not able to handle sparse vectors natively. The majority of dimensions in sparse vectors are usually zeros, making them theoretically high dimensional. However, Qdrant stores sparse vectors differently by only keeping track of the non-zero dimensions. Previously, some users attempted to convert sparse vectors to dense vectors as a workaround, but this was not considered the best solution. In response to user demand for a single tool that can handle both sparse and dense vectors, Qdrant has now introduced support for sparse vectors. This change allows users to work with both sparse and dense vectors seamlessly within the Qdrant tool, eliminating the need for alternative solutions or tools for keyword lookup.","Qdrant handles sparse vectors by introducing a Sparse Vector Index, which is available as of version 1.7.0. This index supports sparse vectors, which are vectors characterized by a high proportion of zeroes. The key features of the Sparse Vector Index include efficient indexing using an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.

In terms of search mechanism, the index identifies vectors with non-zero values in query dimensions during a search. Users familiar with the Qdrant API will notice that extra care has been taken to ensure consistency with the existing named vectors API, making it easier to use sparse vectors in existing codebases. Additionally, users can apply payload filters, shard keys, and other advanced features expected from Qdrant. Notably, indices and values do not have to be sorted before upsert, as Qdrant will handle sorting when the index is persisted, for example, on disk.

The introduction of sparse vector support in Qdrant enhances its capabilities and provides users with more flexibility in handling and searching sparse vectors efficiently.","performance.



## Sparse Vector Index



*Available as of v1.7.0*



### Key Features of Sparse Vector Index

- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.

- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.



### Search Mechanism

- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
- hybrid search

  - bm25

  - tfidf

  - splade

---



Please welcome the long-awaited [Qdrant 1.7.0 release](https://github.com/qdrant/qdrant/releases/tag/v1.7.0). Except for a handful of minor fixes and improvements, this release brings some cool brand-new features that we are excited to share! 

The latest version of your favorite vector search engine finally supports **sparse vectors**. That's the feature many of you requested, so why should we ignore it?
Check out the [sparse vectors article](../sparse-vectors/) and [sparse vectors index docs](/documentation/concepts/indexing/#sparse-vector-index) for more details on what this new index means for Qdrant users.



### Discovery API
Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.","['performance.\n\n\n\n## Sparse Vector Index\n\n\n\n*Available as of v1.7.0*\n\n\n\n### Key Features of Sparse Vector Index\n\n- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.\n\n- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.\n\n\n\n### Search Mechanism\n\n- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", ""- hybrid search\n\n  - bm25\n\n  - tfidf\n\n  - splade\n\n---\n\n\n\nPlease welcome the long-awaited [Qdrant 1.7.0 release](https://github.com/qdrant/qdrant/releases/tag/v1.7.0). Except for a handful of minor fixes and improvements, this release brings some cool brand-new features that we are excited to share! \n\nThe latest version of your favorite vector search engine finally supports **sparse vectors**. That's the feature many of you requested, so why should we ignore it?"", 'Check out the [sparse vectors article](../sparse-vectors/) and [sparse vectors index docs](/documentation/concepts/indexing/#sparse-vector-index) for more details on what this new index means for Qdrant users.\n\n\n\n### Discovery API', 'Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.']","Question: How does Qdrant handle sparse vectors and what changes have been made to accommodate them?\n\nContext: performance.



## Sparse Vector Index



*Available as of v1.7.0*



### Key Features of Sparse Vector Index

- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.

- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.



### Search Mechanism

- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
- hybrid search

- bm25

- tfidf

- splade

---



Please welcome the long-awaited [Qdrant 1.7.0 release](https://github.com/qdrant/qdrant/releases/tag/v1.7.0). Except for a handful of minor fixes and improvements, this release brings some cool brand-new features that we are excited to share! 

The latest version of your favorite vector search engine finally supports **sparse vectors**. That's the feature many of you requested, so why should we ignore it?
Check out the [sparse vectors article](../sparse-vectors/) and [sparse vectors index docs](/documentation/concepts/indexing/#sparse-vector-index) for more details on what this new index means for Qdrant users.



### Discovery API
Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.\n\nAnswer:",How does Qdrant handle sparse vectors and what changes have been made to accommodate them?,"Qdrant, until recently, was not able to handle sparse vectors natively. The majority of dimensions in sparse vectors are usually zeros, making them theoretically high dimensional. However, Qdrant stores sparse vectors differently by only keeping track of the non-zero dimensions. Previously, some users attempted to convert sparse vectors to dense vectors as a workaround, but this was not considered the best solution. In response to user demand for a single tool that can handle both sparse and dense vectors, Qdrant has now introduced support for sparse vectors. This change allows users to work with both sparse and dense vectors seamlessly within the Qdrant tool, eliminating the need for alternative solutions or tools for keyword lookup.","['performance.\n\n\n\n## Sparse Vector Index\n\n\n\n*Available as of v1.7.0*\n\n\n\n### Key Features of Sparse Vector Index\n\n- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.\n\n- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.\n\n\n\n### Search Mechanism\n\n- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", ""- hybrid search\n\n  - bm25\n\n  - tfidf\n\n  - splade\n\n---\n\n\n\nPlease welcome the long-awaited [Qdrant 1.7.0 release](https://github.com/qdrant/qdrant/releases/tag/v1.7.0). Except for a handful of minor fixes and improvements, this release brings some cool brand-new features that we are excited to share! \n\nThe latest version of your favorite vector search engine finally supports **sparse vectors**. That's the feature many of you requested, so why should we ignore it?"", 'Check out the [sparse vectors article](../sparse-vectors/) and [sparse vectors index docs](/documentation/concepts/indexing/#sparse-vector-index) for more details on what this new index means for Qdrant users.\n\n\n\n### Discovery API', 'Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.']","performance.



## Sparse Vector Index



*Available as of v1.7.0*



### Key Features of Sparse Vector Index

- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.

- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.



### Search Mechanism

- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
- hybrid search

  - bm25

  - tfidf

  - splade

---



Please welcome the long-awaited [Qdrant 1.7.0 release](https://github.com/qdrant/qdrant/releases/tag/v1.7.0). Except for a handful of minor fixes and improvements, this release brings some cool brand-new features that we are excited to share! 

The latest version of your favorite vector search engine finally supports **sparse vectors**. That's the feature many of you requested, so why should we ignore it?
Check out the [sparse vectors article](../sparse-vectors/) and [sparse vectors index docs](/documentation/concepts/indexing/#sparse-vector-index) for more details on what this new index means for Qdrant users.



### Discovery API
Until now, Qdrant has not been able to handle sparse vectors natively. Some were trying to convert them to dense vectors, but that was not the best solution or a suggested way. We even wrote a piece with [our thoughts on building a hybrid search](/articles/hybrid-search/), and we encouraged you to use a different tool for keyword lookup.",0.3254437869822485,0.5045871559633027,0.39568345323741005,0.047619047619047616,0.07407407407407407,0.057971014492753624,0.1893491124260355,0.29357798165137616,0.2302158273381295,0.30177514792899407,0.46788990825688076,0.3669064748201438,0.868706464767456,0.5782734155654907,0.6038317680358887,0.5907763242721558,0.9655333953310868,0.7482927324235789,176,1.5304347826086957,0.4514728485929845
True,,3977.944,5be69cca,What is the purpose of the `prefix` parameter in the configuration options for the Qdrant instance?,"The `prefix` parameter in the configuration options for the Qdrant instance serves the purpose of adding a specific prefix to the REST URL path. For example, setting the `prefix` parameter to `service/v1` would result in the REST API endpoint being accessed at `http://localhost:6333/service/v1/{qdrant-endpoint}`. This allows for customization and organization of the API endpoints under a specific path, making it easier to manage and access different functionalities within the Qdrant instance.","The `prefix` parameter in the configuration options for the Qdrant instance serves as a way to group and organize environment variables. In the provided example, all environment variables related to Qdrant are prefixed with `QDRANT__`. This helps in distinguishing and organizing Qdrant-specific configurations from other environment variables that may be present.

By using a prefix like `QDRANT__`, it becomes easier to manage and identify Qdrant-related configuration options within a larger set of environment variables. This prefixing convention ensures that the variables are structured in a standardized and consistent manner, making it clear which variables are associated with Qdrant and avoiding potential conflicts with variables used by other components or applications.","(see [ordering](#order-and-priority)).



All environment variables are prefixed with `QDRANT__` and are separated with

`__`.



These variables:



```bash

QDRANT__LOG_LEVEL=INFO

QDRANT__SERVICE__HTTP_PORT=6333

QDRANT__SERVICE__ENABLE_TLS=1

QDRANT__TLS__CERT=./tls/cert.pem

QDRANT__TLS__CERT_TTL=3600

```



result in this configuration:



```yaml

log_level: INFO

service:

  http_port: 6333

  enable_tls: true

tls:

  cert: ./tls/cert.pem

  cert_ttl: 3600

```
WITH ENGINE = ""qdrant"",

PARAMETERS = {

    ""location"": "":memory:"",

    ""collection_config"": {

        ""size"": 386,

        ""distance"": ""Cosine""

    }

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
import io.qdrant.client.grpc.Collections.QuantizationConfig;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()

                    .setParams(

                        VectorParams.newBuilder()

                            .setSize(768)
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()

                    .setParams(

                        VectorParams.newBuilder()

                            .setSize(768)","['(see [ordering](#order-and-priority)).\n\n\n\nAll environment variables are prefixed with `QDRANT__` and are separated with\n\n`__`.\n\n\n\nThese variables:\n\n\n\n```bash\n\nQDRANT__LOG_LEVEL=INFO\n\nQDRANT__SERVICE__HTTP_PORT=6333\n\nQDRANT__SERVICE__ENABLE_TLS=1\n\nQDRANT__TLS__CERT=./tls/cert.pem\n\nQDRANT__TLS__CERT_TTL=3600\n\n```\n\n\n\nresult in this configuration:\n\n\n\n```yaml\n\nlog_level: INFO\n\nservice:\n\n  http_port: 6333\n\n  enable_tls: true\n\ntls:\n\n  cert: ./tls/cert.pem\n\n  cert_ttl: 3600\n\n```', 'WITH ENGINE = ""qdrant"",\n\nPARAMETERS = {\n\n    ""location"": "":memory:"",\n\n    ""collection_config"": {\n\n        ""size"": 386,\n\n        ""distance"": ""Cosine""\n\n    }\n\n}\n\n```\n\n\n\nThe available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).\n\n\n\n## Creating a new table', 'import io.qdrant.client.grpc.Collections.QuantizationConfig;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()', 'import io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()\n\n                    .setParams(\n\n                        VectorParams.newBuilder()\n\n                            .setSize(768)', 'import io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()\n\n                    .setParams(\n\n                        VectorParams.newBuilder()\n\n                            .setSize(768)']","Question: What is the purpose of the `prefix` parameter in the configuration options for the Qdrant instance?\n\nContext: (see [ordering](#order-and-priority)).



All environment variables are prefixed with `QDRANT__` and are separated with

`__`.



These variables:



```bash

QDRANT__LOG_LEVEL=INFO

QDRANT__SERVICE__HTTP_PORT=6333

QDRANT__SERVICE__ENABLE_TLS=1

QDRANT__TLS__CERT=./tls/cert.pem

QDRANT__TLS__CERT_TTL=3600

```



result in this configuration:



```yaml

log_level: INFO

service:

http_port: 6333

enable_tls: true

tls:

cert: ./tls/cert.pem

cert_ttl: 3600

```
WITH ENGINE = ""qdrant"",

PARAMETERS = {

""location"": "":memory:"",

""collection_config"": {

""size"": 386,

""distance"": ""Cosine""

}

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
import io.qdrant.client.grpc.Collections.QuantizationConfig;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createCollectionAsync(

CreateCollection.newBuilder()

.setCollectionName(""{collection_name}"")

.setVectorsConfig(

VectorsConfig.newBuilder()
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createCollectionAsync(

CreateCollection.newBuilder()

.setCollectionName(""{collection_name}"")

.setVectorsConfig(

VectorsConfig.newBuilder()

.setParams(

VectorParams.newBuilder()

.setSize(768)
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createCollectionAsync(

CreateCollection.newBuilder()

.setCollectionName(""{collection_name}"")

.setVectorsConfig(

VectorsConfig.newBuilder()

.setParams(

VectorParams.newBuilder()

.setSize(768)\n\nAnswer:",What is the purpose of the `prefix` parameter in the configuration options for the Qdrant instance?,"The `prefix` parameter in the configuration options for the Qdrant instance serves the purpose of adding a specific prefix to the REST URL path. For example, setting the `prefix` parameter to `service/v1` would result in the REST API endpoint being accessed at `http://localhost:6333/service/v1/{qdrant-endpoint}`. This allows for customization and organization of the API endpoints under a specific path, making it easier to manage and access different functionalities within the Qdrant instance.","['(see [ordering](#order-and-priority)).\n\n\n\nAll environment variables are prefixed with `QDRANT__` and are separated with\n\n`__`.\n\n\n\nThese variables:\n\n\n\n```bash\n\nQDRANT__LOG_LEVEL=INFO\n\nQDRANT__SERVICE__HTTP_PORT=6333\n\nQDRANT__SERVICE__ENABLE_TLS=1\n\nQDRANT__TLS__CERT=./tls/cert.pem\n\nQDRANT__TLS__CERT_TTL=3600\n\n```\n\n\n\nresult in this configuration:\n\n\n\n```yaml\n\nlog_level: INFO\n\nservice:\n\n  http_port: 6333\n\n  enable_tls: true\n\ntls:\n\n  cert: ./tls/cert.pem\n\n  cert_ttl: 3600\n\n```', 'WITH ENGINE = ""qdrant"",\n\nPARAMETERS = {\n\n    ""location"": "":memory:"",\n\n    ""collection_config"": {\n\n        ""size"": 386,\n\n        ""distance"": ""Cosine""\n\n    }\n\n}\n\n```\n\n\n\nThe available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).\n\n\n\n## Creating a new table', 'import io.qdrant.client.grpc.Collections.QuantizationConfig;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()', 'import io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()\n\n                    .setParams(\n\n                        VectorParams.newBuilder()\n\n                            .setSize(768)', 'import io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(\n\n                VectorsConfig.newBuilder()\n\n                    .setParams(\n\n                        VectorParams.newBuilder()\n\n                            .setSize(768)']","(see [ordering](#order-and-priority)).



All environment variables are prefixed with `QDRANT__` and are separated with

`__`.



These variables:



```bash

QDRANT__LOG_LEVEL=INFO

QDRANT__SERVICE__HTTP_PORT=6333

QDRANT__SERVICE__ENABLE_TLS=1

QDRANT__TLS__CERT=./tls/cert.pem

QDRANT__TLS__CERT_TTL=3600

```



result in this configuration:



```yaml

log_level: INFO

service:

  http_port: 6333

  enable_tls: true

tls:

  cert: ./tls/cert.pem

  cert_ttl: 3600

```
WITH ENGINE = ""qdrant"",

PARAMETERS = {

    ""location"": "":memory:"",

    ""collection_config"": {

        ""size"": 386,

        ""distance"": ""Cosine""

    }

}

```



The available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).



## Creating a new table
import io.qdrant.client.grpc.Collections.QuantizationConfig;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()

                    .setParams(

                        VectorParams.newBuilder()

                            .setSize(768)
import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(

                VectorsConfig.newBuilder()

                    .setParams(

                        VectorParams.newBuilder()

                            .setSize(768)",0.30097087378640774,0.4696969696969697,0.3668639053254438,0.12745098039215685,0.2,0.15568862275449102,0.23300970873786409,0.36363636363636365,0.28402366863905326,0.2912621359223301,0.45454545454545453,0.35502958579881655,0.7755497694015503,0.6444997191429138,0.6360569000244141,0.6402504444122314,0.9769948904402554,0.7358430524667103,110,1.5277777777777777,0.501109818924532
True,,688.772,e10e9159,What is the default scoring metric used in Qdrant for search operations?,"The default scoring metric used in Qdrant for search operations is `Dot product`. It is not necessary to specify this metric explicitly as it is the default setting. However, Qdrant also supports other distance metrics such as Cosine for scoring. When performing a search operation in Qdrant, if no specific metric is mentioned, the system will automatically use the Dot product as the scoring metric.",The default scoring metric used in Qdrant for search operations is the Dot product.,"| Index| Sparse Query | Dense Query |

| --- | --- | --- | 

| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |

| Search Type | Always exact in Qdrant | HNSW is an approximate NN |

| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |



In general, the speed of the search is proportional to the number of non-zero values in the query vector.



```http
to do it. 



## Wrapping up



Assessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.

your search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, 

even in an automated way, as part of your CI/CD pipeline.
such as `precision@k`. It is calculated as the number of relevant documents in the top-k search results divided by `k`. In case of testing

just the ANN algorithm, we can use the exact kNN search as a ground truth, with `k` being fixed. It will be a measure on **how well the ANN

algorithm approximates the exact search**.



## Measure the quality of the search results



Let's build a quality evaluation of the ANN algorithm in Qdrant. We will, first, call the search endpoint in a standard way to obtain
qdrant: &QdrantClient,

) -> Result<Vec<ScoredPoint>> {

    Ok(qdrant.search_points(&SearchPoints {

        collection_name,

        limit: 5, // use what fits your use case here

        with_payload: Some(true.into()),

        vector: embed(client, text, api_key)?,

        ..Default::default()

    }).await?.result)

}

```
let search_result = client

    .search_points(&SearchPoints {

        collection_name: ""test_collection"".to_string(),

        vector: vec![0.2, 0.1, 0.9, 0.7],

        filter: Some(Filter::all([Condition::matches(

            ""city"",

            ""London"".to_string(),

        )])),

        limit: 2,

        ..Default::default()

    })

    .await?;



dbg!(search_result);

```



```java

import static io.qdrant.client.ConditionFactory.matchKeyword;



List<ScoredPoint> searchResult =

    client","['| Index| Sparse Query | Dense Query |\n\n| --- | --- | --- | \n\n| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |\n\n| Search Type | Always exact in Qdrant | HNSW is an approximate NN |\n\n| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |\n\n\n\nIn general, the speed of the search is proportional to the number of non-zero values in the query vector.\n\n\n\n```http', 'to do it. \n\n\n\n## Wrapping up\n\n\n\nAssessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.\n\nyour search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, \n\neven in an automated way, as part of your CI/CD pipeline.', ""such as `precision@k`. It is calculated as the number of relevant documents in the top-k search results divided by `k`. In case of testing\n\njust the ANN algorithm, we can use the exact kNN search as a ground truth, with `k` being fixed. It will be a measure on **how well the ANN\n\nalgorithm approximates the exact search**.\n\n\n\n## Measure the quality of the search results\n\n\n\nLet's build a quality evaluation of the ANN algorithm in Qdrant. We will, first, call the search endpoint in a standard way to obtain"", 'qdrant: &QdrantClient,\n\n) -> Result<Vec<ScoredPoint>> {\n\n    Ok(qdrant.search_points(&SearchPoints {\n\n        collection_name,\n\n        limit: 5, // use what fits your use case here\n\n        with_payload: Some(true.into()),\n\n        vector: embed(client, text, api_key)?,\n\n        ..Default::default()\n\n    }).await?.result)\n\n}\n\n```', 'let search_result = client\n\n    .search_points(&SearchPoints {\n\n        collection_name: ""test_collection"".to_string(),\n\n        vector: vec![0.2, 0.1, 0.9, 0.7],\n\n        filter: Some(Filter::all([Condition::matches(\n\n            ""city"",\n\n            ""London"".to_string(),\n\n        )])),\n\n        limit: 2,\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n\n\ndbg!(search_result);\n\n```\n\n\n\n```java\n\nimport static io.qdrant.client.ConditionFactory.matchKeyword;\n\n\n\nList<ScoredPoint> searchResult =\n\n    client']","Question: What is the default scoring metric used in Qdrant for search operations?\n\nContext: | Index| Sparse Query | Dense Query |

| --- | --- | --- | 

| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |

| Search Type | Always exact in Qdrant | HNSW is an approximate NN |

| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |



In general, the speed of the search is proportional to the number of non-zero values in the query vector.



```http
to do it. 



## Wrapping up



Assessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.

your search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, 

even in an automated way, as part of your CI/CD pipeline.
such as `precision@k`. It is calculated as the number of relevant documents in the top-k search results divided by `k`. In case of testing

just the ANN algorithm, we can use the exact kNN search as a ground truth, with `k` being fixed. It will be a measure on **how well the ANN

algorithm approximates the exact search**.



## Measure the quality of the search results



Let's build a quality evaluation of the ANN algorithm in Qdrant. We will, first, call the search endpoint in a standard way to obtain
qdrant: &QdrantClient,

) -> Result<Vec<ScoredPoint>> {

Ok(qdrant.search_points(&SearchPoints {

collection_name,

limit: 5, // use what fits your use case here

with_payload: Some(true.into()),

vector: embed(client, text, api_key)?,

..Default::default()

}).await?.result)

}

```
let search_result = client

.search_points(&SearchPoints {

collection_name: ""test_collection"".to_string(),

vector: vec![0.2, 0.1, 0.9, 0.7],

filter: Some(Filter::all([Condition::matches(

""city"",

""London"".to_string(),

)])),

limit: 2,

..Default::default()

})

.await?;



dbg!(search_result);

```



```java

import static io.qdrant.client.ConditionFactory.matchKeyword;



List<ScoredPoint> searchResult =

client\n\nAnswer:",What is the default scoring metric used in Qdrant for search operations?,"The default scoring metric used in Qdrant for search operations is `Dot product`. It is not necessary to specify this metric explicitly as it is the default setting. However, Qdrant also supports other distance metrics such as Cosine for scoring. When performing a search operation in Qdrant, if no specific metric is mentioned, the system will automatically use the Dot product as the scoring metric.","['| Index| Sparse Query | Dense Query |\n\n| --- | --- | --- | \n\n| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |\n\n| Search Type | Always exact in Qdrant | HNSW is an approximate NN |\n\n| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |\n\n\n\nIn general, the speed of the search is proportional to the number of non-zero values in the query vector.\n\n\n\n```http', 'to do it. \n\n\n\n## Wrapping up\n\n\n\nAssessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.\n\nyour search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, \n\neven in an automated way, as part of your CI/CD pipeline.', ""such as `precision@k`. It is calculated as the number of relevant documents in the top-k search results divided by `k`. In case of testing\n\njust the ANN algorithm, we can use the exact kNN search as a ground truth, with `k` being fixed. It will be a measure on **how well the ANN\n\nalgorithm approximates the exact search**.\n\n\n\n## Measure the quality of the search results\n\n\n\nLet's build a quality evaluation of the ANN algorithm in Qdrant. We will, first, call the search endpoint in a standard way to obtain"", 'qdrant: &QdrantClient,\n\n) -> Result<Vec<ScoredPoint>> {\n\n    Ok(qdrant.search_points(&SearchPoints {\n\n        collection_name,\n\n        limit: 5, // use what fits your use case here\n\n        with_payload: Some(true.into()),\n\n        vector: embed(client, text, api_key)?,\n\n        ..Default::default()\n\n    }).await?.result)\n\n}\n\n```', 'let search_result = client\n\n    .search_points(&SearchPoints {\n\n        collection_name: ""test_collection"".to_string(),\n\n        vector: vec![0.2, 0.1, 0.9, 0.7],\n\n        filter: Some(Filter::all([Condition::matches(\n\n            ""city"",\n\n            ""London"".to_string(),\n\n        )])),\n\n        limit: 2,\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n\n\ndbg!(search_result);\n\n```\n\n\n\n```java\n\nimport static io.qdrant.client.ConditionFactory.matchKeyword;\n\n\n\nList<ScoredPoint> searchResult =\n\n    client']","| Index| Sparse Query | Dense Query |

| --- | --- | --- | 

| Scoring Metric | Default is `Dot product`, no need to specify it | `Distance` has supported metrics e.g. Dot, Cosine |

| Search Type | Always exact in Qdrant | HNSW is an approximate NN |

| Return Behaviour | Returns only vectors with non-zero values in the same indices as the query vector | Returns `limit` vectors |



In general, the speed of the search is proportional to the number of non-zero values in the query vector.



```http
to do it. 



## Wrapping up



Assessing the quality of retrieval is a critical aspect of evaluating semantic search performance. It is imperative to measure retrieval quality when aiming for optimal quality of.

your search results. Qdrant provides a built-in exact search mode, which can be used to measure the quality of the ANN algorithm itself, 

even in an automated way, as part of your CI/CD pipeline.
such as `precision@k`. It is calculated as the number of relevant documents in the top-k search results divided by `k`. In case of testing

just the ANN algorithm, we can use the exact kNN search as a ground truth, with `k` being fixed. It will be a measure on **how well the ANN

algorithm approximates the exact search**.



## Measure the quality of the search results



Let's build a quality evaluation of the ANN algorithm in Qdrant. We will, first, call the search endpoint in a standard way to obtain
qdrant: &QdrantClient,

) -> Result<Vec<ScoredPoint>> {

    Ok(qdrant.search_points(&SearchPoints {

        collection_name,

        limit: 5, // use what fits your use case here

        with_payload: Some(true.into()),

        vector: embed(client, text, api_key)?,

        ..Default::default()

    }).await?.result)

}

```
let search_result = client

    .search_points(&SearchPoints {

        collection_name: ""test_collection"".to_string(),

        vector: vec![0.2, 0.1, 0.9, 0.7],

        filter: Some(Filter::all([Condition::matches(

            ""city"",

            ""London"".to_string(),

        )])),

        limit: 2,

        ..Default::default()

    })

    .await?;



dbg!(search_result);

```



```java

import static io.qdrant.client.ConditionFactory.matchKeyword;



List<ScoredPoint> searchResult =

    client",1.0,0.2033898305084746,0.33802816901408456,1.0,0.1896551724137931,0.3188405797101449,1.0,0.2033898305084746,0.33802816901408456,1.0,0.2033898305084746,0.33802816901408456,0.9470928907394409,0.7628515958786011,0.5855770707130432,0.6625614166259766,0.9927468625828624,0.9631758623436326,14,0.2153846153846154,0.30784215472293763
True,,3630.833,1d8e8e41,What options are available for using Qdrant if the official clients do not support the language you are using?,"If the official clients for Qdrant do not support the language you are using, you have the option to either use the REST API directly or generate a client for your language using OpenAPI definitions or protobuf definitions. By utilizing the OpenAPI definitions available at [this link](https://github.com/qdrant/qdrant/blob/master/docs/redoc/master/openapi.json) or the protobuf definitions at [this link](https://github.com/qdrant/qdrant/tree/master/lib/api/src/grpc/proto), you can interact with Qdrant's functionalities even if there is no official client available for your specific programming language. This flexibility allows developers to integrate Qdrant into a wide range of applications and environments.","If the official clients of Qdrant do not support the language you are using, you still have a few options available to interact with Qdrant:

1. **HTTP API**: Qdrant provides a flexible HTTP API that allows you to interact with the database using HTTP requests. This means you can directly send HTTP requests to the Qdrant server to perform operations like searching, adding data, updating data, etc.

2. **Custom Client Development**: If you are comfortable with developing your own client, you can create a custom client in the language of your choice that communicates with Qdrant using the HTTP API. This allows you to have full control over the interactions and customization options.

3. **Proxy Service**: Another option is to set up a proxy service that translates your requests from your preferred language to the supported languages of Qdrant. This proxy service can act as an intermediary between your application and Qdrant, handling the translation of requests and responses.

By utilizing these options, you can still effectively interact with Qdrant even if the official clients do not directly support your preferred programming language.","**Clients**: Qdrant supports various programming languages for client interaction, such as Python, Go, Rust, and Typescript. This way developers can connect to and interact with Qdrant using the programming language they prefer.





### Vector Database Use Cases
. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.
of code, thanks to the recent integration with Qdrant. We're not even going to work directly with `QdrantClient`, as everything is already done in the background

by LangChain. If you want to get into the source code right away, all the processing is available as a 

[Google Colab notebook](https://colab.research.google.com/drive/19RxxkZdnq_YqBH5kBV10Rt0Rax-kminD?usp=sharing).



## Implementing Question Answering with LangChain and Qdrant



### Configuration
So one thing which I mean, one is definitely try the different models which we support. We support a reasonable range of models, including a few multilingual ones. Second is while we take care of this when you do use with Qdrants. So, for instance, let's say this is how you would have to manually specify, let's say, passage or query. When you do this, let's say add and query. What we do, we add the passage and query keys while creating the Embeddings for you. So this is taken care of
However, if the only information you had was that your user dislikes some items, you couldn't use it directly.



Qdrant 1.6 brings a more flexible API. You can now provide both IDs and vectors of positive and negative examples. You can even 

combine them within a single request. That makes the new implementation backward compatible, so you can easily upgrade an existing

Qdrant instance without any changes in your code. And the default behaviour of the API is still the same as before. However, we","['**Clients**: Qdrant supports various programming languages for client interaction, such as Python, Go, Rust, and Typescript. This way developers can connect to and interact with Qdrant using the programming language they prefer.\n\n\n\n\n\n### Vector Database Use Cases', '. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.', ""of code, thanks to the recent integration with Qdrant. We're not even going to work directly with `QdrantClient`, as everything is already done in the background\n\nby LangChain. If you want to get into the source code right away, all the processing is available as a \n\n[Google Colab notebook](https://colab.research.google.com/drive/19RxxkZdnq_YqBH5kBV10Rt0Rax-kminD?usp=sharing).\n\n\n\n## Implementing Question Answering with LangChain and Qdrant\n\n\n\n### Configuration"", ""So one thing which I mean, one is definitely try the different models which we support. We support a reasonable range of models, including a few multilingual ones. Second is while we take care of this when you do use with Qdrants. So, for instance, let's say this is how you would have to manually specify, let's say, passage or query. When you do this, let's say add and query. What we do, we add the passage and query keys while creating the Embeddings for you. So this is taken care of"", ""However, if the only information you had was that your user dislikes some items, you couldn't use it directly.\n\n\n\nQdrant 1.6 brings a more flexible API. You can now provide both IDs and vectors of positive and negative examples. You can even \n\ncombine them within a single request. That makes the new implementation backward compatible, so you can easily upgrade an existing\n\nQdrant instance without any changes in your code. And the default behaviour of the API is still the same as before. However, we""]","Question: What options are available for using Qdrant if the official clients do not support the language you are using?\n\nContext: **Clients**: Qdrant supports various programming languages for client interaction, such as Python, Go, Rust, and Typescript. This way developers can connect to and interact with Qdrant using the programming language they prefer.





### Vector Database Use Cases
. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.
of code, thanks to the recent integration with Qdrant. We're not even going to work directly with `QdrantClient`, as everything is already done in the background

by LangChain. If you want to get into the source code right away, all the processing is available as a 

[Google Colab notebook](https://colab.research.google.com/drive/19RxxkZdnq_YqBH5kBV10Rt0Rax-kminD?usp=sharing).



## Implementing Question Answering with LangChain and Qdrant



### Configuration
So one thing which I mean, one is definitely try the different models which we support. We support a reasonable range of models, including a few multilingual ones. Second is while we take care of this when you do use with Qdrants. So, for instance, let's say this is how you would have to manually specify, let's say, passage or query. When you do this, let's say add and query. What we do, we add the passage and query keys while creating the Embeddings for you. So this is taken care of
However, if the only information you had was that your user dislikes some items, you couldn't use it directly.



Qdrant 1.6 brings a more flexible API. You can now provide both IDs and vectors of positive and negative examples. You can even 

combine them within a single request. That makes the new implementation backward compatible, so you can easily upgrade an existing

Qdrant instance without any changes in your code. And the default behaviour of the API is still the same as before. However, we\n\nAnswer:",What options are available for using Qdrant if the official clients do not support the language you are using?,"If the official clients for Qdrant do not support the language you are using, you have the option to either use the REST API directly or generate a client for your language using OpenAPI definitions or protobuf definitions. By utilizing the OpenAPI definitions available at [this link](https://github.com/qdrant/qdrant/blob/master/docs/redoc/master/openapi.json) or the protobuf definitions at [this link](https://github.com/qdrant/qdrant/tree/master/lib/api/src/grpc/proto), you can interact with Qdrant's functionalities even if there is no official client available for your specific programming language. This flexibility allows developers to integrate Qdrant into a wide range of applications and environments.","['**Clients**: Qdrant supports various programming languages for client interaction, such as Python, Go, Rust, and Typescript. This way developers can connect to and interact with Qdrant using the programming language they prefer.\n\n\n\n\n\n### Vector Database Use Cases', '. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.', ""of code, thanks to the recent integration with Qdrant. We're not even going to work directly with `QdrantClient`, as everything is already done in the background\n\nby LangChain. If you want to get into the source code right away, all the processing is available as a \n\n[Google Colab notebook](https://colab.research.google.com/drive/19RxxkZdnq_YqBH5kBV10Rt0Rax-kminD?usp=sharing).\n\n\n\n## Implementing Question Answering with LangChain and Qdrant\n\n\n\n### Configuration"", ""So one thing which I mean, one is definitely try the different models which we support. We support a reasonable range of models, including a few multilingual ones. Second is while we take care of this when you do use with Qdrants. So, for instance, let's say this is how you would have to manually specify, let's say, passage or query. When you do this, let's say add and query. What we do, we add the passage and query keys while creating the Embeddings for you. So this is taken care of"", ""However, if the only information you had was that your user dislikes some items, you couldn't use it directly.\n\n\n\nQdrant 1.6 brings a more flexible API. You can now provide both IDs and vectors of positive and negative examples. You can even \n\ncombine them within a single request. That makes the new implementation backward compatible, so you can easily upgrade an existing\n\nQdrant instance without any changes in your code. And the default behaviour of the API is still the same as before. However, we""]","**Clients**: Qdrant supports various programming languages for client interaction, such as Python, Go, Rust, and Typescript. This way developers can connect to and interact with Qdrant using the programming language they prefer.





### Vector Database Use Cases
. In the default build configuration, qdrant does not include support for all languages, due to the increasing size of the resulting binary. Chinese, Japanese and Korean languages are not enabled by default, but can be enabled by building qdrant from source with `--features multiling-chinese,multiling-japanese,multiling-korean` flags.
of code, thanks to the recent integration with Qdrant. We're not even going to work directly with `QdrantClient`, as everything is already done in the background

by LangChain. If you want to get into the source code right away, all the processing is available as a 

[Google Colab notebook](https://colab.research.google.com/drive/19RxxkZdnq_YqBH5kBV10Rt0Rax-kminD?usp=sharing).



## Implementing Question Answering with LangChain and Qdrant



### Configuration
So one thing which I mean, one is definitely try the different models which we support. We support a reasonable range of models, including a few multilingual ones. Second is while we take care of this when you do use with Qdrants. So, for instance, let's say this is how you would have to manually specify, let's say, passage or query. When you do this, let's say add and query. What we do, we add the passage and query keys while creating the Embeddings for you. So this is taken care of
However, if the only information you had was that your user dislikes some items, you couldn't use it directly.



Qdrant 1.6 brings a more flexible API. You can now provide both IDs and vectors of positive and negative examples. You can even 

combine them within a single request. That makes the new implementation backward compatible, so you can easily upgrade an existing

Qdrant instance without any changes in your code. And the default behaviour of the API is still the same as before. However, we",0.3333333333333333,0.5333333333333333,0.4102564102564102,0.10778443113772455,0.17307692307692307,0.1328413284132841,0.22023809523809523,0.3523809523809524,0.2710622710622711,0.2857142857142857,0.45714285714285713,0.3516483516483516,0.8466659784317017,0.6315863132476807,0.6029600501060486,0.6169412732124329,0.656872326740995,0.6419758759438992,183,1.967741935483871,0.44686468212471897
True,,3439.3570000000004,cd28890e,What are the two main interfaces available for interacting with Qdrant and where can the documentation for each be found?,"The two main interfaces available for interacting with Qdrant are REST API and gRPC API. The documentation for REST API can be found in the [OpenAPI Specification](https://qdrant.github.io/qdrant/redoc/index.html), while the documentation for gRPC API can be found [here](https://github.com/qdrant/qdrant/blob/master/docs/grpc/docs.md). The gRPC methods in Qdrant follow the same principles as REST, with each REST endpoint having a corresponding gRPC method. The gRPC interface in Qdrant is available on a specified port as outlined in the configuration file, with the default port being 6334. If choosing to use gRPC, it is necessary to expose the port when starting Qdrant, such as when running the service inside a Docker container.","The two main interfaces available for interacting with Qdrant are:

1. **Console**: You can use the REST API to interact with Qdrant in the Console interface.

2. **Collections**: In Collections, you can manage all the collections and upload Snapshots.

The documentation for each interface can be found at the following locations:

1. **REST API Documentation**: You can find the documentation for interacting with Qdrant via the REST API [here](https://qdrant.to/docs/rest-api).

2. **Qdrant Web UI Documentation**: For managing collections and interacting with Qdrant through the Web UI, you can refer to the documentation provided at the GUI available at `http://localhost:6333/dashboard`.

Additionally, for specific programming language interfaces, you can refer to the following:

- **Java Client**: Documentation for the Java client interface can be found [here](https://github.com/qdrant/java-client).","In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. 



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Accessing the Web UI



First, run the Docker container:



```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```



The GUI is available at `http://localhost:6333/dashboard`
|![java](/docs/misc/java.webp)|**[Java](https://github.com/qdrant/java-client)**|[Available on Maven Central](https://central.sonatype.com/artifact/io.qdrant/client)|[Latest Release](https://github.com/qdrant/java-client/releases)|





## API Reference



All interaction with Qdrant takes place via the REST API. We recommend using REST API if you are using Qdrant for the first time or if you are working on a prototype.



|API|Documentation|

|-|-|
You can get started using Qdrant with the Python `qdrant-client`, by pulling the latest docker 

image of `qdrant` and connecting to it locally, or by trying out [Qdrant's Cloud](https://cloud.qdrant.io/) 

free tier option until you are ready to make the full switch.



With that out of the way, let's talk about what are vector databases.



## What Are Vector Databases?



![dbs](https://raw.githubusercontent.com/ramonpzg/mlops-sydney-2023/main/images/databases.png)
```python

QDRANT_MAIN_URL = ""https://my-cluster.com:6333""

QDRANT_NODES = (

    ""https://node-0.my-cluster.com:6333"",

    ""https://node-1.my-cluster.com:6333"",

    ""https://node-2.my-cluster.com:6333"",

)

QDRANT_API_KEY = ""my-api-key""

```



<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>



We can now create a client instance:



```python

from qdrant_client import QdrantClient
technology, jump to the section most relevant to your use case.



As you go through the tutorials, please let us know if any questions come up in our 

[Discord channel here](https://qdrant.to/discord). 😎","['In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. \n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Accessing the Web UI\n\n\n\nFirst, run the Docker container:\n\n\n\n```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```\n\n\n\nThe GUI is available at `http://localhost:6333/dashboard`', '|![java](/docs/misc/java.webp)|**[Java](https://github.com/qdrant/java-client)**|[Available on Maven Central](https://central.sonatype.com/artifact/io.qdrant/client)|[Latest Release](https://github.com/qdrant/java-client/releases)|\n\n\n\n\n\n## API Reference\n\n\n\nAll interaction with Qdrant takes place via the REST API. We recommend using REST API if you are using Qdrant for the first time or if you are working on a prototype.\n\n\n\n|API|Documentation|\n\n|-|-|', ""You can get started using Qdrant with the Python `qdrant-client`, by pulling the latest docker \n\nimage of `qdrant` and connecting to it locally, or by trying out [Qdrant's Cloud](https://cloud.qdrant.io/) \n\nfree tier option until you are ready to make the full switch.\n\n\n\nWith that out of the way, let's talk about what are vector databases.\n\n\n\n## What Are Vector Databases?\n\n\n\n![dbs](https://raw.githubusercontent.com/ramonpzg/mlops-sydney-2023/main/images/databases.png)"", '```python\n\nQDRANT_MAIN_URL = ""https://my-cluster.com:6333""\n\nQDRANT_NODES = (\n\n    ""https://node-0.my-cluster.com:6333"",\n\n    ""https://node-1.my-cluster.com:6333"",\n\n    ""https://node-2.my-cluster.com:6333"",\n\n)\n\nQDRANT_API_KEY = ""my-api-key""\n\n```\n\n\n\n<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>\n\n\n\nWe can now create a client instance:\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient', 'technology, jump to the section most relevant to your use case.\n\n\n\nAs you go through the tutorials, please let us know if any questions come up in our \n\n[Discord channel here](https://qdrant.to/discord). 😎']","Question: What are the two main interfaces available for interacting with Qdrant and where can the documentation for each be found?\n\nContext: In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. 



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Accessing the Web UI



First, run the Docker container:



```bash

docker run -p 6333:6333 -p 6334:6334 \

-v $(pwd)/qdrant_storage:/qdrant/storage:z \

qdrant/qdrant

```



The GUI is available at `http://localhost:6333/dashboard`
|![java](/docs/misc/java.webp)|**[Java](https://github.com/qdrant/java-client)**|[Available on Maven Central](https://central.sonatype.com/artifact/io.qdrant/client)|[Latest Release](https://github.com/qdrant/java-client/releases)|





## API Reference



All interaction with Qdrant takes place via the REST API. We recommend using REST API if you are using Qdrant for the first time or if you are working on a prototype.



|API|Documentation|

|-|-|
You can get started using Qdrant with the Python `qdrant-client`, by pulling the latest docker 

image of `qdrant` and connecting to it locally, or by trying out [Qdrant's Cloud](https://cloud.qdrant.io/) 

free tier option until you are ready to make the full switch.



With that out of the way, let's talk about what are vector databases.



## What Are Vector Databases?



![dbs](https://raw.githubusercontent.com/ramonpzg/mlops-sydney-2023/main/images/databases.png)
```python

QDRANT_MAIN_URL = ""https://my-cluster.com:6333""

QDRANT_NODES = (

""https://node-0.my-cluster.com:6333"",

""https://node-1.my-cluster.com:6333"",

""https://node-2.my-cluster.com:6333"",

)

QDRANT_API_KEY = ""my-api-key""

```



<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>



We can now create a client instance:



```python

from qdrant_client import QdrantClient
technology, jump to the section most relevant to your use case.



As you go through the tutorials, please let us know if any questions come up in our 

[Discord channel here](https://qdrant.to/discord). 😎\n\nAnswer:",What are the two main interfaces available for interacting with Qdrant and where can the documentation for each be found?,"The two main interfaces available for interacting with Qdrant are REST API and gRPC API. The documentation for REST API can be found in the [OpenAPI Specification](https://qdrant.github.io/qdrant/redoc/index.html), while the documentation for gRPC API can be found [here](https://github.com/qdrant/qdrant/blob/master/docs/grpc/docs.md). The gRPC methods in Qdrant follow the same principles as REST, with each REST endpoint having a corresponding gRPC method. The gRPC interface in Qdrant is available on a specified port as outlined in the configuration file, with the default port being 6334. If choosing to use gRPC, it is necessary to expose the port when starting Qdrant, such as when running the service inside a Docker container.","['In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. \n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Accessing the Web UI\n\n\n\nFirst, run the Docker container:\n\n\n\n```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```\n\n\n\nThe GUI is available at `http://localhost:6333/dashboard`', '|![java](/docs/misc/java.webp)|**[Java](https://github.com/qdrant/java-client)**|[Available on Maven Central](https://central.sonatype.com/artifact/io.qdrant/client)|[Latest Release](https://github.com/qdrant/java-client/releases)|\n\n\n\n\n\n## API Reference\n\n\n\nAll interaction with Qdrant takes place via the REST API. We recommend using REST API if you are using Qdrant for the first time or if you are working on a prototype.\n\n\n\n|API|Documentation|\n\n|-|-|', ""You can get started using Qdrant with the Python `qdrant-client`, by pulling the latest docker \n\nimage of `qdrant` and connecting to it locally, or by trying out [Qdrant's Cloud](https://cloud.qdrant.io/) \n\nfree tier option until you are ready to make the full switch.\n\n\n\nWith that out of the way, let's talk about what are vector databases.\n\n\n\n## What Are Vector Databases?\n\n\n\n![dbs](https://raw.githubusercontent.com/ramonpzg/mlops-sydney-2023/main/images/databases.png)"", '```python\n\nQDRANT_MAIN_URL = ""https://my-cluster.com:6333""\n\nQDRANT_NODES = (\n\n    ""https://node-0.my-cluster.com:6333"",\n\n    ""https://node-1.my-cluster.com:6333"",\n\n    ""https://node-2.my-cluster.com:6333"",\n\n)\n\nQDRANT_API_KEY = ""my-api-key""\n\n```\n\n\n\n<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>\n\n\n\nWe can now create a client instance:\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient', 'technology, jump to the section most relevant to your use case.\n\n\n\nAs you go through the tutorials, please let us know if any questions come up in our \n\n[Discord channel here](https://qdrant.to/discord). 😎']","In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. 



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Accessing the Web UI



First, run the Docker container:



```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```



The GUI is available at `http://localhost:6333/dashboard`
|![java](/docs/misc/java.webp)|**[Java](https://github.com/qdrant/java-client)**|[Available on Maven Central](https://central.sonatype.com/artifact/io.qdrant/client)|[Latest Release](https://github.com/qdrant/java-client/releases)|





## API Reference



All interaction with Qdrant takes place via the REST API. We recommend using REST API if you are using Qdrant for the first time or if you are working on a prototype.



|API|Documentation|

|-|-|
You can get started using Qdrant with the Python `qdrant-client`, by pulling the latest docker 

image of `qdrant` and connecting to it locally, or by trying out [Qdrant's Cloud](https://cloud.qdrant.io/) 

free tier option until you are ready to make the full switch.



With that out of the way, let's talk about what are vector databases.



## What Are Vector Databases?



![dbs](https://raw.githubusercontent.com/ramonpzg/mlops-sydney-2023/main/images/databases.png)
```python

QDRANT_MAIN_URL = ""https://my-cluster.com:6333""

QDRANT_NODES = (

    ""https://node-0.my-cluster.com:6333"",

    ""https://node-1.my-cluster.com:6333"",

    ""https://node-2.my-cluster.com:6333"",

)

QDRANT_API_KEY = ""my-api-key""

```



<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>



We can now create a client instance:



```python

from qdrant_client import QdrantClient
technology, jump to the section most relevant to your use case.



As you go through the tutorials, please let us know if any questions come up in our 

[Discord channel here](https://qdrant.to/discord). 😎",0.41935483870967744,0.4727272727272727,0.4444444444444444,0.18699186991869918,0.21100917431192662,0.19827586206896555,0.24193548387096775,0.2727272727272727,0.2564102564102564,0.3870967741935484,0.43636363636363634,0.41025641025641024,0.9097216129302979,0.6653099656105042,0.6557916402816772,0.6605165004730225,0.6122461358706157,0.5903670899569988,127,1.165137614678899,0.4009884625642516
True,,5274.775,b9287a62,When should one use gRPC with Qdrant and what factors should be considered in making this choice?,"gRPC is recommended for use with Qdrant when optimizing the performance of an application is a priority and when the user is already familiar with Qdrant. The decision to use gRPC over the REST API is a trade-off between convenience and speed. gRPC is a binary protocol that offers higher performance compared to REST API, but it can be more challenging to debug. Therefore, if the goal is to maximize performance and the user is comfortable with the potential debugging complexities associated with gRPC, it is advisable to opt for gRPC when interacting with Qdrant.","When considering whether to use gRPC with Qdrant, several factors should be taken into account:

1. **Performance Optimization**: gRPC is a high-performance, low-latency communication protocol that can significantly improve the speed of data exchange between services. If you are looking to optimize the performance of your application and prioritize speed, using gRPC with Qdrant can be beneficial.

2. **Familiarity and Expertise**: If you or your team are already familiar with gRPC and have expertise in working with binary protocols, using gRPC with Qdrant might be a natural choice. However, if you are more comfortable with REST APIs or if your team lacks experience with gRPC, you may need to weigh the learning curve against the potential performance gains.

3. **Debugging and Maintenance**: It's important to consider the ease of debugging and maintaining your application when choosing between gRPC and REST API. gRPC can be more challenging to debug compared to REST due to its binary nature. If easy debugging and maintenance are crucial for your project, you may want to stick with the REST API.

4. **Compatibility and Ecosystem**: Consider the compatibility of gRPC with your existing ecosystem and services. If your infrastructure and tools are already optimized for gRPC communication, integrating Qdrant with gRPC may streamline your operations. However, if your ecosystem is primarily based on REST APIs, sticking with REST might be more convenient.

In summary, the decision to use gRPC with Qdrant should be based on factors such as performance requirements, team expertise, debugging considerations, compatibility with existing systems, and long-term maintenance needs. Assessing these factors will help you determine whether gRPC is the right choice for your specific use case.","qdrant/qdrant

```



**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.



## Qdrant Web UI



Qdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points.
As per the [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml), the gRPC interface is available on the specified port.



```yaml

service:

  grpc_port: 6334

```

<aside role=""status"">If you decide to use gRPC, you must expose the port when starting Qdrant.</aside>

 

Running the service inside of Docker will look like this:



```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```
* `6334` - For the [gRPC](/documentation/interfaces/#grpc-interface) API

* `6335` - For [Distributed deployment](/documentation/guides/distributed_deployment/)



All Qdrant instances in a cluster must be able to:



- Communicate with each other over these ports

- Allow incoming connections to ports `6333` and `6334` from clients that use Qdrant.



## Installation options



Qdrant can be installed in different ways depending on your needs:
![REST API vs gRPC upload time, sec](/blog/qdrant-v-0-6-0-engine-with-grpc-released/upload_time.png)



Read more about the gRPC interface and whether you should use it by this [link](/documentation/quick_start/#grpc).



The release v0.6.0 includes several bug fixes. More information is available in a [changelog](https://github.com/qdrant/qdrant/releases/tag/v0.6.0).



New version was provided in addition to the REST API that the company keeps supporting due to its easy debugging.
- GRPC API: [localhost:6334](http://localhost:6334)



## Initialize the client



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });

```



```rust

use qdrant_client::client::QdrantClient;



// The Rust client uses Qdrant's GRPC interface","[""qdrant/qdrant\n\n```\n\n\n\n**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.\n\n\n\n## Qdrant Web UI\n\n\n\nQdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points."", 'As per the [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml), the gRPC interface is available on the specified port.\n\n\n\n```yaml\n\nservice:\n\n  grpc_port: 6334\n\n```\n\n<aside role=""status"">If you decide to use gRPC, you must expose the port when starting Qdrant.</aside>\n\n \n\nRunning the service inside of Docker will look like this:\n\n\n\n```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```', '* `6334` - For the [gRPC](/documentation/interfaces/#grpc-interface) API\n\n* `6335` - For [Distributed deployment](/documentation/guides/distributed_deployment/)\n\n\n\nAll Qdrant instances in a cluster must be able to:\n\n\n\n- Communicate with each other over these ports\n\n- Allow incoming connections to ports `6333` and `6334` from clients that use Qdrant.\n\n\n\n## Installation options\n\n\n\nQdrant can be installed in different ways depending on your needs:', '![REST API vs gRPC upload time, sec](/blog/qdrant-v-0-6-0-engine-with-grpc-released/upload_time.png)\n\n\n\nRead more about the gRPC interface and whether you should use it by this [link](/documentation/quick_start/#grpc).\n\n\n\nThe release v0.6.0 includes several bug fixes. More information is available in a [changelog](https://github.com/qdrant/qdrant/releases/tag/v0.6.0).\n\n\n\nNew version was provided in addition to the REST API that the company keeps supporting due to its easy debugging.', '- GRPC API: [localhost:6334](http://localhost:6334)\n\n\n\n## Initialize the client\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n```\n\n\n\n```rust\n\nuse qdrant_client::client::QdrantClient;\n\n\n\n// The Rust client uses Qdrant\'s GRPC interface']","Question: When should one use gRPC with Qdrant and what factors should be considered in making this choice?\n\nContext: qdrant/qdrant

```



**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.



## Qdrant Web UI



Qdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points.
As per the [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml), the gRPC interface is available on the specified port.



```yaml

service:

grpc_port: 6334

```

<aside role=""status"">If you decide to use gRPC, you must expose the port when starting Qdrant.</aside>



Running the service inside of Docker will look like this:



```bash

docker run -p 6333:6333 -p 6334:6334 \

-v $(pwd)/qdrant_storage:/qdrant/storage:z \

qdrant/qdrant

```
* `6334` - For the [gRPC](/documentation/interfaces/#grpc-interface) API

* `6335` - For [Distributed deployment](/documentation/guides/distributed_deployment/)



All Qdrant instances in a cluster must be able to:



- Communicate with each other over these ports

- Allow incoming connections to ports `6333` and `6334` from clients that use Qdrant.



## Installation options



Qdrant can be installed in different ways depending on your needs:
![REST API vs gRPC upload time, sec](/blog/qdrant-v-0-6-0-engine-with-grpc-released/upload_time.png)



Read more about the gRPC interface and whether you should use it by this [link](/documentation/quick_start/#grpc).



The release v0.6.0 includes several bug fixes. More information is available in a [changelog](https://github.com/qdrant/qdrant/releases/tag/v0.6.0).



New version was provided in addition to the REST API that the company keeps supporting due to its easy debugging.
- GRPC API: [localhost:6334](http://localhost:6334)



## Initialize the client



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });

```



```rust

use qdrant_client::client::QdrantClient;



// The Rust client uses Qdrant's GRPC interface\n\nAnswer:",When should one use gRPC with Qdrant and what factors should be considered in making this choice?,"gRPC is recommended for use with Qdrant when optimizing the performance of an application is a priority and when the user is already familiar with Qdrant. The decision to use gRPC over the REST API is a trade-off between convenience and speed. gRPC is a binary protocol that offers higher performance compared to REST API, but it can be more challenging to debug. Therefore, if the goal is to maximize performance and the user is comfortable with the potential debugging complexities associated with gRPC, it is advisable to opt for gRPC when interacting with Qdrant.","[""qdrant/qdrant\n\n```\n\n\n\n**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.\n\n\n\n## Qdrant Web UI\n\n\n\nQdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points."", 'As per the [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml), the gRPC interface is available on the specified port.\n\n\n\n```yaml\n\nservice:\n\n  grpc_port: 6334\n\n```\n\n<aside role=""status"">If you decide to use gRPC, you must expose the port when starting Qdrant.</aside>\n\n \n\nRunning the service inside of Docker will look like this:\n\n\n\n```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```', '* `6334` - For the [gRPC](/documentation/interfaces/#grpc-interface) API\n\n* `6335` - For [Distributed deployment](/documentation/guides/distributed_deployment/)\n\n\n\nAll Qdrant instances in a cluster must be able to:\n\n\n\n- Communicate with each other over these ports\n\n- Allow incoming connections to ports `6333` and `6334` from clients that use Qdrant.\n\n\n\n## Installation options\n\n\n\nQdrant can be installed in different ways depending on your needs:', '![REST API vs gRPC upload time, sec](/blog/qdrant-v-0-6-0-engine-with-grpc-released/upload_time.png)\n\n\n\nRead more about the gRPC interface and whether you should use it by this [link](/documentation/quick_start/#grpc).\n\n\n\nThe release v0.6.0 includes several bug fixes. More information is available in a [changelog](https://github.com/qdrant/qdrant/releases/tag/v0.6.0).\n\n\n\nNew version was provided in addition to the REST API that the company keeps supporting due to its easy debugging.', '- GRPC API: [localhost:6334](http://localhost:6334)\n\n\n\n## Initialize the client\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";\n\n\n\nconst client = new QdrantClient({ host: ""localhost"", port: 6333 });\n\n```\n\n\n\n```rust\n\nuse qdrant_client::client::QdrantClient;\n\n\n\n// The Rust client uses Qdrant\'s GRPC interface']","qdrant/qdrant

```



**When to use gRPC:** The choice between gRPC and the REST API is a trade-off between convenience and speed. gRPC is a binary protocol and can be more challenging to debug. We recommend using gRPC if you are already familiar with Qdrant and are trying to optimize the performance of your application.



## Qdrant Web UI



Qdrant's Web UI is an intuitive and efficient graphic interface for your Qdrant Collections, REST API and data points.
As per the [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml), the gRPC interface is available on the specified port.



```yaml

service:

  grpc_port: 6334

```

<aside role=""status"">If you decide to use gRPC, you must expose the port when starting Qdrant.</aside>

 

Running the service inside of Docker will look like this:



```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```
* `6334` - For the [gRPC](/documentation/interfaces/#grpc-interface) API

* `6335` - For [Distributed deployment](/documentation/guides/distributed_deployment/)



All Qdrant instances in a cluster must be able to:



- Communicate with each other over these ports

- Allow incoming connections to ports `6333` and `6334` from clients that use Qdrant.



## Installation options



Qdrant can be installed in different ways depending on your needs:
![REST API vs gRPC upload time, sec](/blog/qdrant-v-0-6-0-engine-with-grpc-released/upload_time.png)



Read more about the gRPC interface and whether you should use it by this [link](/documentation/quick_start/#grpc).



The release v0.6.0 includes several bug fixes. More information is available in a [changelog](https://github.com/qdrant/qdrant/releases/tag/v0.6.0).



New version was provided in addition to the REST API that the company keeps supporting due to its easy debugging.
- GRPC API: [localhost:6334](http://localhost:6334)



## Initialize the client



```python

from qdrant_client import QdrantClient



client = QdrantClient(""localhost"", port=6333)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";



const client = new QdrantClient({ host: ""localhost"", port: 6333 });

```



```rust

use qdrant_client::client::QdrantClient;



// The Rust client uses Qdrant's GRPC interface",0.2247191011235955,0.7058823529411765,0.34090909090909094,0.09774436090225563,0.30952380952380953,0.14857142857142858,0.13857677902621723,0.43529411764705883,0.21022727272727273,0.20599250936329588,0.6470588235294118,0.3125,0.8707091808319092,0.5950604677200317,0.6801292300224304,0.634757399559021,0.5973666978534311,0.9726652358658612,275,2.8947368421052633,0.7141019729482067
True,,1344.7169999999999,07830708,Where can the GUI for Qdrant be accessed?,"The GUI for Qdrant can be accessed at `http://localhost:6333/dashboard`. This web address will lead you to the graphical user interface (GUI) for Qdrant, providing a visual interface for interacting with the Qdrant system. The GUI allows users to perform various operations and tasks related to Qdrant, such as managing indexes, querying data, and configuring settings. By accessing the GUI at the specified URL, users can easily navigate and interact with the Qdrant system in a user-friendly and intuitive manner.",The GUI for Qdrant can be accessed at `http://localhost:6333/dashboard` after running the Docker container as shown in the code snippet provided. You can access the Qdrant Dashboard locally by visiting [http://localhost:6333/dashboard](http://localhost:6333/dashboard).,"In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. 



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Accessing the Web UI



First, run the Docker container:



```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```



The GUI is available at `http://localhost:6333/dashboard`
Try it out now! If you have Docker running, you can [quickstart Qdrant](https://qdrant.tech/documentation/quick-start/) and access the Dashboard locally from [http://localhost:6333/dashboard](http://localhost:6333/dashboard). You should see this simple access point to Qdrant:



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Temporary directory for Snapshots
```python

QDRANT_MAIN_URL = ""https://my-cluster.com:6333""

QDRANT_NODES = (

    ""https://node-0.my-cluster.com:6333"",

    ""https://node-1.my-cluster.com:6333"",

    ""https://node-2.my-cluster.com:6333"",

)

QDRANT_API_KEY = ""my-api-key""

```



<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>



We can now create a client instance:



```python

from qdrant_client import QdrantClient
technology, jump to the section most relevant to your use case.



As you go through the tutorials, please let us know if any questions come up in our 

[Discord channel here](https://qdrant.to/discord). 😎
```bash

unstructured-ingest \

  local \

  --input-path ""discord-output"" \

  --embedding-provider ""langchain-huggingface"" \

  qdrant \

  --collection-name ""<COLLECTION_NAME>"" \

  --api-key ""<QDRANT_API_KEY>"" \

  --location ""<QDRANT_URL>""

```","['In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. \n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Accessing the Web UI\n\n\n\nFirst, run the Docker container:\n\n\n\n```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```\n\n\n\nThe GUI is available at `http://localhost:6333/dashboard`', 'Try it out now! If you have Docker running, you can [quickstart Qdrant](https://qdrant.tech/documentation/quick-start/) and access the Dashboard locally from [http://localhost:6333/dashboard](http://localhost:6333/dashboard). You should see this simple access point to Qdrant:\n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Temporary directory for Snapshots', '```python\n\nQDRANT_MAIN_URL = ""https://my-cluster.com:6333""\n\nQDRANT_NODES = (\n\n    ""https://node-0.my-cluster.com:6333"",\n\n    ""https://node-1.my-cluster.com:6333"",\n\n    ""https://node-2.my-cluster.com:6333"",\n\n)\n\nQDRANT_API_KEY = ""my-api-key""\n\n```\n\n\n\n<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>\n\n\n\nWe can now create a client instance:\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient', 'technology, jump to the section most relevant to your use case.\n\n\n\nAs you go through the tutorials, please let us know if any questions come up in our \n\n[Discord channel here](https://qdrant.to/discord). 😎', '```bash\n\nunstructured-ingest \\\n\n  local \\\n\n  --input-path ""discord-output"" \\\n\n  --embedding-provider ""langchain-huggingface"" \\\n\n  qdrant \\\n\n  --collection-name ""<COLLECTION_NAME>"" \\\n\n  --api-key ""<QDRANT_API_KEY>"" \\\n\n  --location ""<QDRANT_URL>""\n\n```']","Question: Where can the GUI for Qdrant be accessed?\n\nContext: In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. 



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Accessing the Web UI



First, run the Docker container:



```bash

docker run -p 6333:6333 -p 6334:6334 \

-v $(pwd)/qdrant_storage:/qdrant/storage:z \

qdrant/qdrant

```



The GUI is available at `http://localhost:6333/dashboard`
Try it out now! If you have Docker running, you can [quickstart Qdrant](https://qdrant.tech/documentation/quick-start/) and access the Dashboard locally from [http://localhost:6333/dashboard](http://localhost:6333/dashboard). You should see this simple access point to Qdrant:



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Temporary directory for Snapshots
```python

QDRANT_MAIN_URL = ""https://my-cluster.com:6333""

QDRANT_NODES = (

""https://node-0.my-cluster.com:6333"",

""https://node-1.my-cluster.com:6333"",

""https://node-2.my-cluster.com:6333"",

)

QDRANT_API_KEY = ""my-api-key""

```



<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>



We can now create a client instance:



```python

from qdrant_client import QdrantClient
technology, jump to the section most relevant to your use case.



As you go through the tutorials, please let us know if any questions come up in our 

[Discord channel here](https://qdrant.to/discord). 😎
```bash

unstructured-ingest \

local \

--input-path ""discord-output"" \

--embedding-provider ""langchain-huggingface"" \

qdrant \

--collection-name ""<COLLECTION_NAME>"" \

--api-key ""<QDRANT_API_KEY>"" \

--location ""<QDRANT_URL>""

```\n\nAnswer:",Where can the GUI for Qdrant be accessed?,"The GUI for Qdrant can be accessed at `http://localhost:6333/dashboard`. This web address will lead you to the graphical user interface (GUI) for Qdrant, providing a visual interface for interacting with the Qdrant system. The GUI allows users to perform various operations and tasks related to Qdrant, such as managing indexes, querying data, and configuring settings. By accessing the GUI at the specified URL, users can easily navigate and interact with the Qdrant system in a user-friendly and intuitive manner.","['In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. \n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Accessing the Web UI\n\n\n\nFirst, run the Docker container:\n\n\n\n```bash\n\ndocker run -p 6333:6333 -p 6334:6334 \\\n\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n\n    qdrant/qdrant\n\n```\n\n\n\nThe GUI is available at `http://localhost:6333/dashboard`', 'Try it out now! If you have Docker running, you can [quickstart Qdrant](https://qdrant.tech/documentation/quick-start/) and access the Dashboard locally from [http://localhost:6333/dashboard](http://localhost:6333/dashboard). You should see this simple access point to Qdrant:\n\n\n\n![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)\n\n\n\n### Temporary directory for Snapshots', '```python\n\nQDRANT_MAIN_URL = ""https://my-cluster.com:6333""\n\nQDRANT_NODES = (\n\n    ""https://node-0.my-cluster.com:6333"",\n\n    ""https://node-1.my-cluster.com:6333"",\n\n    ""https://node-2.my-cluster.com:6333"",\n\n)\n\nQDRANT_API_KEY = ""my-api-key""\n\n```\n\n\n\n<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>\n\n\n\nWe can now create a client instance:\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient', 'technology, jump to the section most relevant to your use case.\n\n\n\nAs you go through the tutorials, please let us know if any questions come up in our \n\n[Discord channel here](https://qdrant.to/discord). 😎', '```bash\n\nunstructured-ingest \\\n\n  local \\\n\n  --input-path ""discord-output"" \\\n\n  --embedding-provider ""langchain-huggingface"" \\\n\n  qdrant \\\n\n  --collection-name ""<COLLECTION_NAME>"" \\\n\n  --api-key ""<QDRANT_API_KEY>"" \\\n\n  --location ""<QDRANT_URL>""\n\n```']","In the **Console**, you may use the REST API to interact with Qdrant, while in **Collections**, you can manage all the collections and upload Snapshots. 



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Accessing the Web UI



First, run the Docker container:



```bash

docker run -p 6333:6333 -p 6334:6334 \

    -v $(pwd)/qdrant_storage:/qdrant/storage:z \

    qdrant/qdrant

```



The GUI is available at `http://localhost:6333/dashboard`
Try it out now! If you have Docker running, you can [quickstart Qdrant](https://qdrant.tech/documentation/quick-start/) and access the Dashboard locally from [http://localhost:6333/dashboard](http://localhost:6333/dashboard). You should see this simple access point to Qdrant:



![Qdrant Web UI](/articles_data/qdrant-1.3.x/web-ui.png)



### Temporary directory for Snapshots
```python

QDRANT_MAIN_URL = ""https://my-cluster.com:6333""

QDRANT_NODES = (

    ""https://node-0.my-cluster.com:6333"",

    ""https://node-1.my-cluster.com:6333"",

    ""https://node-2.my-cluster.com:6333"",

)

QDRANT_API_KEY = ""my-api-key""

```



<aside role=""status"">If you are using Qdrant Cloud, you can find the URL and API key in the <a href=""https://cloud.qdrant.io/"">Qdrant Cloud dashboard</a>.</aside>



We can now create a client instance:



```python

from qdrant_client import QdrantClient
technology, jump to the section most relevant to your use case.



As you go through the tutorials, please let us know if any questions come up in our 

[Discord channel here](https://qdrant.to/discord). 😎
```bash

unstructured-ingest \

  local \

  --input-path ""discord-output"" \

  --embedding-provider ""langchain-huggingface"" \

  qdrant \

  --collection-name ""<COLLECTION_NAME>"" \

  --api-key ""<QDRANT_API_KEY>"" \

  --location ""<QDRANT_URL>""

```",0.5135135135135135,0.25675675675675674,0.34234234234234234,0.2777777777777778,0.136986301369863,0.18348623853211007,0.3783783783783784,0.1891891891891892,0.2522522522522523,0.4594594594594595,0.22972972972972974,0.3063063063063063,0.7497905492782593,0.647845983505249,0.6144397854804993,0.6307008862495422,0.9256852455437183,0.8827341236174107,35,0.4375,0.45237027611525726
True,,1255.523,0b32bf7a,What are the three ways to use Qdrant?,"According to the documentation, there are three ways to use Qdrant:

1. Run a Docker image: This method is recommended for users who do not have a Python development environment. By running a Docker image, users can quickly set up a local Qdrant server and storage.

2. Get the Python client: Users familiar with Python can install the Qdrant client by using `pip install qdrant-client`. The Python client also supports an in-memory database.

3. Spin up a Qdrant Cloud cluster: This method is recommended for running Qdrant in a production environment. Users can set up their first instance by following the Quickstart guide provided in the documentation.","The three ways to use Qdrant are:
1. Running a Docker image if you don't have a Python development environment.
2. Getting the Python client by using `pip install qdrant-client`.
3. Interacting with Qdrant through an out-of-the-box client library in Python.","## First-Time Users:



There are three ways to use Qdrant:



1. [**Run a Docker image**](quick-start/) if you don't have a Python development environment. Setup a local Qdrant server and storage in a few moments. 

2. [**Get the Python client**](https://github.com/qdrant/qdrant-client) if you're familiar with Python. Just `pip install qdrant-client`. The client also supports an in-memory database.
| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  



## Common Use Cases:
To interact with Qdrant from python, I recommend using an out-of-the-box client library.



To install it, use the following command



```bash

pip install qdrant-client

```



At this point, we should have startup records in file `startups.json`, encoded vectors in file `startup_vectors.npy`, and running Qdrant on a local machine.

Let's write a script to upload all startup data and vectors into the search engine.



First, let's create a client object for Qdrant.



```python

# Import client library
```python

url = ""<---qdrant url here --->""

qdrant = Qdrant.from_documents(

    docs, 

    embeddings, 

    url, 

    prefer_grpc=True, 

    collection_name=""my_documents"",

)

```



## Next steps



If you'd like to know more about running Qdrant in a LangChain-based application, please read our article 

[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information
technology, jump to the section most relevant to your use case.



As you go through the tutorials, please let us know if any questions come up in our 

[Discord channel here](https://qdrant.to/discord). 😎","[""## First-Time Users:\n\n\n\nThere are three ways to use Qdrant:\n\n\n\n1. [**Run a Docker image**](quick-start/) if you don't have a Python development environment. Setup a local Qdrant server and storage in a few moments. \n\n2. [**Get the Python client**](https://github.com/qdrant/qdrant-client) if you're familiar with Python. Just `pip install qdrant-client`. The client also supports an in-memory database."", ""| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  \n\n\n\n## Common Use Cases:"", ""To interact with Qdrant from python, I recommend using an out-of-the-box client library.\n\n\n\nTo install it, use the following command\n\n\n\n```bash\n\npip install qdrant-client\n\n```\n\n\n\nAt this point, we should have startup records in file `startups.json`, encoded vectors in file `startup_vectors.npy`, and running Qdrant on a local machine.\n\nLet's write a script to upload all startup data and vectors into the search engine.\n\n\n\nFirst, let's create a client object for Qdrant.\n\n\n\n```python\n\n# Import client library"", '```python\n\nurl = ""<---qdrant url here --->""\n\nqdrant = Qdrant.from_documents(\n\n    docs, \n\n    embeddings, \n\n    url, \n\n    prefer_grpc=True, \n\n    collection_name=""my_documents"",\n\n)\n\n```\n\n\n\n## Next steps\n\n\n\nIf you\'d like to know more about running Qdrant in a LangChain-based application, please read our article \n\n[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information', 'technology, jump to the section most relevant to your use case.\n\n\n\nAs you go through the tutorials, please let us know if any questions come up in our \n\n[Discord channel here](https://qdrant.to/discord). 😎']","Question: What are the three ways to use Qdrant?\n\nContext: ## First-Time Users:



There are three ways to use Qdrant:



1. [**Run a Docker image**](quick-start/) if you don't have a Python development environment. Setup a local Qdrant server and storage in a few moments. 

2. [**Get the Python client**](https://github.com/qdrant/qdrant-client) if you're familiar with Python. Just `pip install qdrant-client`. The client also supports an in-memory database.
| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  



## Common Use Cases:
To interact with Qdrant from python, I recommend using an out-of-the-box client library.



To install it, use the following command



```bash

pip install qdrant-client

```



At this point, we should have startup records in file `startups.json`, encoded vectors in file `startup_vectors.npy`, and running Qdrant on a local machine.

Let's write a script to upload all startup data and vectors into the search engine.



First, let's create a client object for Qdrant.



```python

# Import client library
```python

url = ""<---qdrant url here --->""

qdrant = Qdrant.from_documents(

docs, 

embeddings, 

url, 

prefer_grpc=True, 

collection_name=""my_documents"",

)

```



## Next steps



If you'd like to know more about running Qdrant in a LangChain-based application, please read our article 

[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information
technology, jump to the section most relevant to your use case.



As you go through the tutorials, please let us know if any questions come up in our 

[Discord channel here](https://qdrant.to/discord). 😎\n\nAnswer:",What are the three ways to use Qdrant?,"According to the documentation, there are three ways to use Qdrant:

1. Run a Docker image: This method is recommended for users who do not have a Python development environment. By running a Docker image, users can quickly set up a local Qdrant server and storage.

2. Get the Python client: Users familiar with Python can install the Qdrant client by using `pip install qdrant-client`. The Python client also supports an in-memory database.

3. Spin up a Qdrant Cloud cluster: This method is recommended for running Qdrant in a production environment. Users can set up their first instance by following the Quickstart guide provided in the documentation.","[""## First-Time Users:\n\n\n\nThere are three ways to use Qdrant:\n\n\n\n1. [**Run a Docker image**](quick-start/) if you don't have a Python development environment. Setup a local Qdrant server and storage in a few moments. \n\n2. [**Get the Python client**](https://github.com/qdrant/qdrant-client) if you're familiar with Python. Just `pip install qdrant-client`. The client also supports an in-memory database."", ""| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  \n\n\n\n## Common Use Cases:"", ""To interact with Qdrant from python, I recommend using an out-of-the-box client library.\n\n\n\nTo install it, use the following command\n\n\n\n```bash\n\npip install qdrant-client\n\n```\n\n\n\nAt this point, we should have startup records in file `startups.json`, encoded vectors in file `startup_vectors.npy`, and running Qdrant on a local machine.\n\nLet's write a script to upload all startup data and vectors into the search engine.\n\n\n\nFirst, let's create a client object for Qdrant.\n\n\n\n```python\n\n# Import client library"", '```python\n\nurl = ""<---qdrant url here --->""\n\nqdrant = Qdrant.from_documents(\n\n    docs, \n\n    embeddings, \n\n    url, \n\n    prefer_grpc=True, \n\n    collection_name=""my_documents"",\n\n)\n\n```\n\n\n\n## Next steps\n\n\n\nIf you\'d like to know more about running Qdrant in a LangChain-based application, please read our article \n\n[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information', 'technology, jump to the section most relevant to your use case.\n\n\n\nAs you go through the tutorials, please let us know if any questions come up in our \n\n[Discord channel here](https://qdrant.to/discord). 😎']","## First-Time Users:



There are three ways to use Qdrant:



1. [**Run a Docker image**](quick-start/) if you don't have a Python development environment. Setup a local Qdrant server and storage in a few moments. 

2. [**Get the Python client**](https://github.com/qdrant/qdrant-client) if you're familiar with Python. Just `pip install qdrant-client`. The client also supports an in-memory database.
| [Optimization](tutorials/optimize/)           | Optimize Qdrant's resource usage. | [Multitenancy](tutorials/multiple-partitions/) | Setup Qdrant for multiple independent users. |                  



## Common Use Cases:
To interact with Qdrant from python, I recommend using an out-of-the-box client library.



To install it, use the following command



```bash

pip install qdrant-client

```



At this point, we should have startup records in file `startups.json`, encoded vectors in file `startup_vectors.npy`, and running Qdrant on a local machine.

Let's write a script to upload all startup data and vectors into the search engine.



First, let's create a client object for Qdrant.



```python

# Import client library
```python

url = ""<---qdrant url here --->""

qdrant = Qdrant.from_documents(

    docs, 

    embeddings, 

    url, 

    prefer_grpc=True, 

    collection_name=""my_documents"",

)

```



## Next steps



If you'd like to know more about running Qdrant in a LangChain-based application, please read our article 

[Question Answering with LangChain and Qdrant without boilerplate](/articles/langchain-integration/). Some more information
technology, jump to the section most relevant to your use case.



As you go through the tutorials, please let us know if any questions come up in our 

[Discord channel here](https://qdrant.to/discord). 😎",0.75,0.3125,0.4411764705882353,0.48717948717948717,0.2,0.2835820895522388,0.65,0.2708333333333333,0.3823529411764705,0.725,0.3020833333333333,0.4264705882352941,0.9268072247505188,0.7647896409034729,0.6537179946899414,0.7049053311347961,0.7997177723639955,0.8330366786879798,42,0.3925233644859813,0.5870570864526545
True,,7084.651,4bfdb343,What are the recommended ways for Qdrant users to provide feedback or contribute to the project?,"Qdrant encourages feedback and contributions from its users, specifically Data Scientists, ML Engineers, and MLOps professionals. The best way for Qdrant users to contribute is by providing feedback on their experience with Qdrant. This can include reporting problems, unexpected behaviors, or suggesting improvements in documentation. Users can share their feedback through various channels such as creating an issue on the Qdrant GitHub repository, starting a discussion on GitHub, or sending a message on Discord. Additionally, users who use Qdrant or Metric Learning in their projects are encouraged to share their stories, articles, and demos with the Qdrant community. For those familiar with Rust, there is a specific contribution guide available on the Qdrant GitHub repository. If users encounter problems with understanding the code or architecture, they can reach out to the Qdrant team at any time for assistance.","Qdrant users can provide feedback or contribute to the project in several ways:

1. Create an issue on the Qdrant GitHub repository: Users can report bugs, suggest new features, or provide feedback by creating an issue on the Qdrant GitHub repository at [https://github.com/qdrant/qdrant/issues](https://github.com/qdrant/qdrant/issues).

2. Start a discussion on the GitHub Discussions platform: Users can engage in discussions, ask questions, or share their experiences on the Qdrant GitHub Discussions platform at [https://github.com/qdrant/qdrant/discussions](https://github.com/qdrant/qdrant/discussions).

3. Drop a message on the Qdrant Discord server: Users can join the Qdrant Discord server at [https://discord.gg/tdtYvXjC4h](https://discord.gg/tdtYvXjC4h) to connect with the community, ask questions, and provide feedback.

4. Share articles and demos: If users use Qdrant or Metric Learning in their projects, they are encouraged to share their stories, articles, and demos with the community to showcase their experiences and use cases.

5. Check out the contribution guide: For those familiar with Rust or interested in contributing to the project, there is a contribution guide available at [https://github.com/qdrant/qdrant/blob/master/CONTRIBUTING.md](https://github.com/qdrant/qdrant/blob/master/CONTRIBUTING.md) that provides guidelines on how to contribute effectively.

Overall, Qdrant users are encouraged to provide feedback, engage with the community, and contribute to the project in various ways to help improve and grow the platform.","You can do it in any convenient way - create an [issue](https://github.com/qdrant/qdrant/issues), start a [discussion](https://github.com/qdrant/qdrant/discussions), or drop up a [message](https://discord.gg/tdtYvXjC4h).

If you use Qdrant or Metric Learning in your projects, we'd love to hear your story! Feel free to share articles and demos in our community.



For those familiar with Rust - check out our [contribution guide](https://github.com/qdrant/qdrant/blob/master/CONTRIBUTING.md).
---

title: Contribution Guidelines

weight: 35

draft: true

---



# How to contribute



If you are a Qdrant user - Data Scientist, ML Engineer, or MLOps, the best contribution would be the feedback on your experience with Qdrant.

Let us know whenever you have a problem, face an unexpected behavior, or see a lack of documentation.
Without a doubt, I'm eager to continue growing alongside this community and contribute to new features and enhancements that elevate the product. I've also become an advocate for Qdrant, introducing this project to numerous coworkers and friends in the tech industry. I'm excited to witness new users and contributors emerge from within my own network!
If you have problems with code or architecture understanding - reach us at any time.

Feeling confident and want to contribute more? - Come to [work with us](https://qdrant.join.com/)!
technology, jump to the section most relevant to your use case.



As you go through the tutorials, please let us know if any questions come up in our 

[Discord channel here](https://qdrant.to/discord). 😎","[""You can do it in any convenient way - create an [issue](https://github.com/qdrant/qdrant/issues), start a [discussion](https://github.com/qdrant/qdrant/discussions), or drop up a [message](https://discord.gg/tdtYvXjC4h).\n\nIf you use Qdrant or Metric Learning in your projects, we'd love to hear your story! Feel free to share articles and demos in our community.\n\n\n\nFor those familiar with Rust - check out our [contribution guide](https://github.com/qdrant/qdrant/blob/master/CONTRIBUTING.md)."", '---\n\ntitle: Contribution Guidelines\n\nweight: 35\n\ndraft: true\n\n---\n\n\n\n# How to contribute\n\n\n\nIf you are a Qdrant user - Data Scientist, ML Engineer, or MLOps, the best contribution would be the feedback on your experience with Qdrant.\n\nLet us know whenever you have a problem, face an unexpected behavior, or see a lack of documentation.', ""Without a doubt, I'm eager to continue growing alongside this community and contribute to new features and enhancements that elevate the product. I've also become an advocate for Qdrant, introducing this project to numerous coworkers and friends in the tech industry. I'm excited to witness new users and contributors emerge from within my own network!"", 'If you have problems with code or architecture understanding - reach us at any time.\n\nFeeling confident and want to contribute more? - Come to [work with us](https://qdrant.join.com/)!', 'technology, jump to the section most relevant to your use case.\n\n\n\nAs you go through the tutorials, please let us know if any questions come up in our \n\n[Discord channel here](https://qdrant.to/discord). 😎']","Question: What are the recommended ways for Qdrant users to provide feedback or contribute to the project?\n\nContext: You can do it in any convenient way - create an [issue](https://github.com/qdrant/qdrant/issues), start a [discussion](https://github.com/qdrant/qdrant/discussions), or drop up a [message](https://discord.gg/tdtYvXjC4h).

If you use Qdrant or Metric Learning in your projects, we'd love to hear your story! Feel free to share articles and demos in our community.



For those familiar with Rust - check out our [contribution guide](https://github.com/qdrant/qdrant/blob/master/CONTRIBUTING.md).
---

title: Contribution Guidelines

weight: 35

draft: true

---



# How to contribute



If you are a Qdrant user - Data Scientist, ML Engineer, or MLOps, the best contribution would be the feedback on your experience with Qdrant.

Let us know whenever you have a problem, face an unexpected behavior, or see a lack of documentation.
Without a doubt, I'm eager to continue growing alongside this community and contribute to new features and enhancements that elevate the product. I've also become an advocate for Qdrant, introducing this project to numerous coworkers and friends in the tech industry. I'm excited to witness new users and contributors emerge from within my own network!
If you have problems with code or architecture understanding - reach us at any time.

Feeling confident and want to contribute more? - Come to [work with us](https://qdrant.join.com/)!
technology, jump to the section most relevant to your use case.



As you go through the tutorials, please let us know if any questions come up in our 

[Discord channel here](https://qdrant.to/discord). 😎\n\nAnswer:",What are the recommended ways for Qdrant users to provide feedback or contribute to the project?,"Qdrant encourages feedback and contributions from its users, specifically Data Scientists, ML Engineers, and MLOps professionals. The best way for Qdrant users to contribute is by providing feedback on their experience with Qdrant. This can include reporting problems, unexpected behaviors, or suggesting improvements in documentation. Users can share their feedback through various channels such as creating an issue on the Qdrant GitHub repository, starting a discussion on GitHub, or sending a message on Discord. Additionally, users who use Qdrant or Metric Learning in their projects are encouraged to share their stories, articles, and demos with the Qdrant community. For those familiar with Rust, there is a specific contribution guide available on the Qdrant GitHub repository. If users encounter problems with understanding the code or architecture, they can reach out to the Qdrant team at any time for assistance.","[""You can do it in any convenient way - create an [issue](https://github.com/qdrant/qdrant/issues), start a [discussion](https://github.com/qdrant/qdrant/discussions), or drop up a [message](https://discord.gg/tdtYvXjC4h).\n\nIf you use Qdrant or Metric Learning in your projects, we'd love to hear your story! Feel free to share articles and demos in our community.\n\n\n\nFor those familiar with Rust - check out our [contribution guide](https://github.com/qdrant/qdrant/blob/master/CONTRIBUTING.md)."", '---\n\ntitle: Contribution Guidelines\n\nweight: 35\n\ndraft: true\n\n---\n\n\n\n# How to contribute\n\n\n\nIf you are a Qdrant user - Data Scientist, ML Engineer, or MLOps, the best contribution would be the feedback on your experience with Qdrant.\n\nLet us know whenever you have a problem, face an unexpected behavior, or see a lack of documentation.', ""Without a doubt, I'm eager to continue growing alongside this community and contribute to new features and enhancements that elevate the product. I've also become an advocate for Qdrant, introducing this project to numerous coworkers and friends in the tech industry. I'm excited to witness new users and contributors emerge from within my own network!"", 'If you have problems with code or architecture understanding - reach us at any time.\n\nFeeling confident and want to contribute more? - Come to [work with us](https://qdrant.join.com/)!', 'technology, jump to the section most relevant to your use case.\n\n\n\nAs you go through the tutorials, please let us know if any questions come up in our \n\n[Discord channel here](https://qdrant.to/discord). 😎']","You can do it in any convenient way - create an [issue](https://github.com/qdrant/qdrant/issues), start a [discussion](https://github.com/qdrant/qdrant/discussions), or drop up a [message](https://discord.gg/tdtYvXjC4h).

If you use Qdrant or Metric Learning in your projects, we'd love to hear your story! Feel free to share articles and demos in our community.



For those familiar with Rust - check out our [contribution guide](https://github.com/qdrant/qdrant/blob/master/CONTRIBUTING.md).
---

title: Contribution Guidelines

weight: 35

draft: true

---



# How to contribute



If you are a Qdrant user - Data Scientist, ML Engineer, or MLOps, the best contribution would be the feedback on your experience with Qdrant.

Let us know whenever you have a problem, face an unexpected behavior, or see a lack of documentation.
Without a doubt, I'm eager to continue growing alongside this community and contribute to new features and enhancements that elevate the product. I've also become an advocate for Qdrant, introducing this project to numerous coworkers and friends in the tech industry. I'm excited to witness new users and contributors emerge from within my own network!
If you have problems with code or architecture understanding - reach us at any time.

Feeling confident and want to contribute more? - Come to [work with us](https://qdrant.join.com/)!
technology, jump to the section most relevant to your use case.



As you go through the tutorials, please let us know if any questions come up in our 

[Discord channel here](https://qdrant.to/discord). 😎",0.4099099099099099,0.7109375,0.52,0.19457013574660634,0.33858267716535434,0.24712643678160917,0.26576576576576577,0.4609375,0.3371428571428572,0.3783783783783784,0.65625,0.48,0.8533909916877747,0.6530997157096863,0.6950536966323853,0.6734239459037781,0.9172185051720589,0.9991733682524812,207,1.5,0.7180261641076984
True,,4169.943,c5ca5d38,What is Aleph Alpha and what capabilities does their API offer?,"Aleph Alpha is a multimodal and multilingual embeddings provider. Their API allows for the creation of embeddings for both text and images, placing them in the same latent space. They offer an official Python client that can be installed using pip. The API provides both synchronous and asynchronous clients. Users can obtain embeddings for images and store them into Qdrant by utilizing the provided Python client and following the specified steps in the documentation.","Aleph Alpha is a multimodal and multilingual embeddings provider. Their API allows users to create embeddings for text and images in the same latent space. The API offers capabilities for vectorizing datasets, building semantic search systems that combine text and image data, and developing recommendation systems.

Users can utilize the official Python client provided by Aleph Alpha to interact with the API. By installing the client with pip and registering for an Aleph Alpha account to obtain an API key, users can access features such as obtaining embeddings for images and storing them into platforms like Qdrant. The API supports both synchronous and asynchronous clients for various use cases.","---

title: Aleph Alpha

weight: 900

aliases: [ ../integrations/aleph-alpha/ ]

---



Aleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both 

in the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be 

installed with pip:



```bash

pip install aleph-alpha-client

```
[Docker is the fastest way to do that](https://qdrant.tech/documentation/quick_start/#installation).

2. You need to have a registered [Aleph Alpha account](https://app.aleph-alpha.com/). 

3. Upon registration, create an API key (see: [API Tokens](https://app.aleph-alpha.com/profile)).



Now you can store the Aleph Alpha API key in a variable and choose the model your are going to use.



```python

aa_token = ""<< your_token >>""

model = ""luminous-base""

```



## Vectorize the dataset
| [Build a Search with Aleph Alpha](tutorials/aleph-alpha-search/)           | Build a simple semantic search that combines text and image data.                  | Qdrant, Aleph Alpha | 

| [Developing Recommendations Systems](https://githubtocolab.com/qdrant/examples/blob/master/qdrant_101_getting_started/getting_started.ipynb)    | Learn how to get started building semantic search and recommendation systems. | Qdrant |
```bash

pip install aleph-alpha-client

```



There is both synchronous and asynchronous client available. Obtaining the embeddings for an image and storing it into Qdrant might 

be done in the following way:



```python

import qdrant_client



from aleph_alpha_client import (

    Prompt,

    AsyncClient,

    SemanticEmbeddingRequest,

    SemanticRepresentation,

    ImagePrompt

)

from qdrant_client.http.models import Batch



aa_token = ""<< your_token >>""

model = ""luminous-base""
---

title: Aleph Alpha Search

weight: 16

---



# Multimodal Semantic Search with Aleph Alpha



| Time: 30 min | Level: Beginner |  |    |

| --- | ----------- | ----------- |----------- |



This tutorial shows you how to run a proper multimodal semantic search system with a few lines of code, without the need to annotate the data or train your networks.","[""---\n\ntitle: Aleph Alpha\n\nweight: 900\n\naliases: [ ../integrations/aleph-alpha/ ]\n\n---\n\n\n\nAleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both \n\nin the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be \n\ninstalled with pip:\n\n\n\n```bash\n\npip install aleph-alpha-client\n\n```"", '[Docker is the fastest way to do that](https://qdrant.tech/documentation/quick_start/#installation).\n\n2. You need to have a registered [Aleph Alpha account](https://app.aleph-alpha.com/). \n\n3. Upon registration, create an API key (see: [API Tokens](https://app.aleph-alpha.com/profile)).\n\n\n\nNow you can store the Aleph Alpha API key in a variable and choose the model your are going to use.\n\n\n\n```python\n\naa_token = ""<< your_token >>""\n\nmodel = ""luminous-base""\n\n```\n\n\n\n## Vectorize the dataset', '| [Build a Search with Aleph Alpha](tutorials/aleph-alpha-search/)           | Build a simple semantic search that combines text and image data.                  | Qdrant, Aleph Alpha | \n\n| [Developing Recommendations Systems](https://githubtocolab.com/qdrant/examples/blob/master/qdrant_101_getting_started/getting_started.ipynb)    | Learn how to get started building semantic search and recommendation systems. | Qdrant |', '```bash\n\npip install aleph-alpha-client\n\n```\n\n\n\nThere is both synchronous and asynchronous client available. Obtaining the embeddings for an image and storing it into Qdrant might \n\nbe done in the following way:\n\n\n\n```python\n\nimport qdrant_client\n\n\n\nfrom aleph_alpha_client import (\n\n    Prompt,\n\n    AsyncClient,\n\n    SemanticEmbeddingRequest,\n\n    SemanticRepresentation,\n\n    ImagePrompt\n\n)\n\nfrom qdrant_client.http.models import Batch\n\n\n\naa_token = ""<< your_token >>""\n\nmodel = ""luminous-base""', '---\n\ntitle: Aleph Alpha Search\n\nweight: 16\n\n---\n\n\n\n# Multimodal Semantic Search with Aleph Alpha\n\n\n\n| Time: 30 min | Level: Beginner |  |    |\n\n| --- | ----------- | ----------- |----------- |\n\n\n\nThis tutorial shows you how to run a proper multimodal semantic search system with a few lines of code, without the need to annotate the data or train your networks.']","Question: What is Aleph Alpha and what capabilities does their API offer?\n\nContext: ---

title: Aleph Alpha

weight: 900

aliases: [ ../integrations/aleph-alpha/ ]

---



Aleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both 

in the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be 

installed with pip:



```bash

pip install aleph-alpha-client

```
[Docker is the fastest way to do that](https://qdrant.tech/documentation/quick_start/#installation).

2. You need to have a registered [Aleph Alpha account](https://app.aleph-alpha.com/). 

3. Upon registration, create an API key (see: [API Tokens](https://app.aleph-alpha.com/profile)).



Now you can store the Aleph Alpha API key in a variable and choose the model your are going to use.



```python

aa_token = ""<< your_token >>""

model = ""luminous-base""

```



## Vectorize the dataset
| [Build a Search with Aleph Alpha](tutorials/aleph-alpha-search/)           | Build a simple semantic search that combines text and image data.                  | Qdrant, Aleph Alpha | 

| [Developing Recommendations Systems](https://githubtocolab.com/qdrant/examples/blob/master/qdrant_101_getting_started/getting_started.ipynb)    | Learn how to get started building semantic search and recommendation systems. | Qdrant |
```bash

pip install aleph-alpha-client

```



There is both synchronous and asynchronous client available. Obtaining the embeddings for an image and storing it into Qdrant might 

be done in the following way:



```python

import qdrant_client



from aleph_alpha_client import (

Prompt,

AsyncClient,

SemanticEmbeddingRequest,

SemanticRepresentation,

ImagePrompt

)

from qdrant_client.http.models import Batch



aa_token = ""<< your_token >>""

model = ""luminous-base""
---

title: Aleph Alpha Search

weight: 16

---



# Multimodal Semantic Search with Aleph Alpha



| Time: 30 min | Level: Beginner |  |    |

| --- | ----------- | ----------- |----------- |



This tutorial shows you how to run a proper multimodal semantic search system with a few lines of code, without the need to annotate the data or train your networks.\n\nAnswer:",What is Aleph Alpha and what capabilities does their API offer?,"Aleph Alpha is a multimodal and multilingual embeddings provider. Their API allows for the creation of embeddings for both text and images, placing them in the same latent space. They offer an official Python client that can be installed using pip. The API provides both synchronous and asynchronous clients. Users can obtain embeddings for images and store them into Qdrant by utilizing the provided Python client and following the specified steps in the documentation.","[""---\n\ntitle: Aleph Alpha\n\nweight: 900\n\naliases: [ ../integrations/aleph-alpha/ ]\n\n---\n\n\n\nAleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both \n\nin the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be \n\ninstalled with pip:\n\n\n\n```bash\n\npip install aleph-alpha-client\n\n```"", '[Docker is the fastest way to do that](https://qdrant.tech/documentation/quick_start/#installation).\n\n2. You need to have a registered [Aleph Alpha account](https://app.aleph-alpha.com/). \n\n3. Upon registration, create an API key (see: [API Tokens](https://app.aleph-alpha.com/profile)).\n\n\n\nNow you can store the Aleph Alpha API key in a variable and choose the model your are going to use.\n\n\n\n```python\n\naa_token = ""<< your_token >>""\n\nmodel = ""luminous-base""\n\n```\n\n\n\n## Vectorize the dataset', '| [Build a Search with Aleph Alpha](tutorials/aleph-alpha-search/)           | Build a simple semantic search that combines text and image data.                  | Qdrant, Aleph Alpha | \n\n| [Developing Recommendations Systems](https://githubtocolab.com/qdrant/examples/blob/master/qdrant_101_getting_started/getting_started.ipynb)    | Learn how to get started building semantic search and recommendation systems. | Qdrant |', '```bash\n\npip install aleph-alpha-client\n\n```\n\n\n\nThere is both synchronous and asynchronous client available. Obtaining the embeddings for an image and storing it into Qdrant might \n\nbe done in the following way:\n\n\n\n```python\n\nimport qdrant_client\n\n\n\nfrom aleph_alpha_client import (\n\n    Prompt,\n\n    AsyncClient,\n\n    SemanticEmbeddingRequest,\n\n    SemanticRepresentation,\n\n    ImagePrompt\n\n)\n\nfrom qdrant_client.http.models import Batch\n\n\n\naa_token = ""<< your_token >>""\n\nmodel = ""luminous-base""', '---\n\ntitle: Aleph Alpha Search\n\nweight: 16\n\n---\n\n\n\n# Multimodal Semantic Search with Aleph Alpha\n\n\n\n| Time: 30 min | Level: Beginner |  |    |\n\n| --- | ----------- | ----------- |----------- |\n\n\n\nThis tutorial shows you how to run a proper multimodal semantic search system with a few lines of code, without the need to annotate the data or train your networks.']","---

title: Aleph Alpha

weight: 900

aliases: [ ../integrations/aleph-alpha/ ]

---



Aleph Alpha is a multimodal and multilingual embeddings' provider. Their API allows creating the embeddings for text and images, both 

in the same latent space. They maintain an [official Python client](https://github.com/Aleph-Alpha/aleph-alpha-client) that might be 

installed with pip:



```bash

pip install aleph-alpha-client

```
[Docker is the fastest way to do that](https://qdrant.tech/documentation/quick_start/#installation).

2. You need to have a registered [Aleph Alpha account](https://app.aleph-alpha.com/). 

3. Upon registration, create an API key (see: [API Tokens](https://app.aleph-alpha.com/profile)).



Now you can store the Aleph Alpha API key in a variable and choose the model your are going to use.



```python

aa_token = ""<< your_token >>""

model = ""luminous-base""

```



## Vectorize the dataset
| [Build a Search with Aleph Alpha](tutorials/aleph-alpha-search/)           | Build a simple semantic search that combines text and image data.                  | Qdrant, Aleph Alpha | 

| [Developing Recommendations Systems](https://githubtocolab.com/qdrant/examples/blob/master/qdrant_101_getting_started/getting_started.ipynb)    | Learn how to get started building semantic search and recommendation systems. | Qdrant |
```bash

pip install aleph-alpha-client

```



There is both synchronous and asynchronous client available. Obtaining the embeddings for an image and storing it into Qdrant might 

be done in the following way:



```python

import qdrant_client



from aleph_alpha_client import (

    Prompt,

    AsyncClient,

    SemanticEmbeddingRequest,

    SemanticRepresentation,

    ImagePrompt

)

from qdrant_client.http.models import Batch



aa_token = ""<< your_token >>""

model = ""luminous-base""
---

title: Aleph Alpha Search

weight: 16

---



# Multimodal Semantic Search with Aleph Alpha



| Time: 30 min | Level: Beginner |  |    |

| --- | ----------- | ----------- |----------- |



This tutorial shows you how to run a proper multimodal semantic search system with a few lines of code, without the need to annotate the data or train your networks.",0.52,0.7878787878787878,0.6265060240963856,0.30303030303030304,0.46153846153846156,0.36585365853658536,0.39,0.5909090909090909,0.46987951807228917,0.48,0.7272727272727273,0.5783132530120482,0.9369840025901794,0.7408549189567566,0.8083258271217346,0.7731211185455322,0.831173401636382,0.8346884541213513,109,1.472972972972973,0.6909456287822361
True,,1745.849,c0fb44f7,What is the purpose of the `task_type` parameter when obtaining Nomic embeddings?,"The `task_type` parameter in the `nomic-embed-text-v1` model is used to define the type of embeddings that are obtained for documents. When obtaining Nomic embeddings for documents, you need to set the `task_type` parameter to `search_document`. This ensures that the embeddings generated are specifically tailored for document search purposes. By specifying the `task_type` as `search_document`, you are instructing the model to encode the text in a way that is optimized for searching and retrieving relevant documents based on the input query. This parameter helps customize the embeddings to suit the specific requirements of document retrieval tasks within the Nomic framework.","The purpose of the `task_type` parameter when obtaining Nomic embeddings is to specify the type of task for which the embeddings will be used. It helps in determining how the embeddings will be generated and utilized. Depending on the task_type specified, the embeddings can be tailored for different purposes such as search and retrieval, semantic similarity, classification, or clustering.","Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
The Embedding Model API supports various task types, outlined as follows:



1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.

2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.

3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.

4. `classification`: Specifies that the given text will be classified.

5. `clustering`: Specifies that the embeddings will be used for clustering.
parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.



- `input_type=""search_document""` - for documents to store in Qdrant

- `input_type=""search_query""` - for search queries to find the most relevant documents

- `input_type=""classification""` - for classification tasks

- `input_type=""clustering""` - for text clustering



While implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the
```python

output = embed.text(

    texts=[""What is the best vector database?""],

    model=""nomic-embed-text-v1"",

    task_type=""search_query"",

)



qdrant_client.search(

    collection_name=""my-collection"",

    query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

    texts=[""Qdrant is the best vector database!""],

    model=""nomic-embed-text-v1"",

    task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

    collection_name=""my-collection"",

    points=models.Batch(

        ids=[1],

        vectors=output[""embeddings""],

    ),

)

```



To query the collection, set the `task_type` to `search_query`:



```python","['Once installed, you can configure it with the official Python client or through direct HTTP requests.\n\n\n\n<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>\n\n\n\nYou can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings\n\nare obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.\n\nFor documents, set the `task_type` to `search_document`:\n\n\n\n```python', 'The Embedding Model API supports various task types, outlined as follows:\n\n\n\n1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.\n\n2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.\n\n3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.\n\n4. `classification`: Specifies that the given text will be classified.\n\n5. `clustering`: Specifies that the embeddings will be used for clustering.', 'parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.\n\n\n\n- `input_type=""search_document""` - for documents to store in Qdrant\n\n- `input_type=""search_query""` - for search queries to find the most relevant documents\n\n- `input_type=""classification""` - for classification tasks\n\n- `input_type=""clustering""` - for text clustering\n\n\n\nWhile implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the', '```python\n\noutput = embed.text(\n\n    texts=[""What is the best vector database?""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_query"",\n\n)\n\n\n\nqdrant_client.search(\n\n    collection_name=""my-collection"",\n\n    query=output[""embeddings""][0],\n\n)\n\n```\n\n\n\nFor more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).', '```python\n\nfrom qdrant_client import QdrantClient, models\n\nfrom nomic import embed\n\n\n\noutput = embed.text(\n\n    texts=[""Qdrant is the best vector database!""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_document"",\n\n)\n\n\n\nqdrant_client = QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""my-collection"",\n\n    points=models.Batch(\n\n        ids=[1],\n\n        vectors=output[""embeddings""],\n\n    ),\n\n)\n\n```\n\n\n\nTo query the collection, set the `task_type` to `search_query`:\n\n\n\n```python']","Question: What is the purpose of the `task_type` parameter when obtaining Nomic embeddings?\n\nContext: Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
The Embedding Model API supports various task types, outlined as follows:



1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.

2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.

3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.

4. `classification`: Specifies that the given text will be classified.

5. `clustering`: Specifies that the embeddings will be used for clustering.
parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.



- `input_type=""search_document""` - for documents to store in Qdrant

- `input_type=""search_query""` - for search queries to find the most relevant documents

- `input_type=""classification""` - for classification tasks

- `input_type=""clustering""` - for text clustering



While implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the
```python

output = embed.text(

texts=[""What is the best vector database?""],

model=""nomic-embed-text-v1"",

task_type=""search_query"",

)



qdrant_client.search(

collection_name=""my-collection"",

query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

texts=[""Qdrant is the best vector database!""],

model=""nomic-embed-text-v1"",

task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

collection_name=""my-collection"",

points=models.Batch(

ids=[1],

vectors=output[""embeddings""],

),

)

```



To query the collection, set the `task_type` to `search_query`:



```python\n\nAnswer:",What is the purpose of the `task_type` parameter when obtaining Nomic embeddings?,"The `task_type` parameter in the `nomic-embed-text-v1` model is used to define the type of embeddings that are obtained for documents. When obtaining Nomic embeddings for documents, you need to set the `task_type` parameter to `search_document`. This ensures that the embeddings generated are specifically tailored for document search purposes. By specifying the `task_type` as `search_document`, you are instructing the model to encode the text in a way that is optimized for searching and retrieving relevant documents based on the input query. This parameter helps customize the embeddings to suit the specific requirements of document retrieval tasks within the Nomic framework.","['Once installed, you can configure it with the official Python client or through direct HTTP requests.\n\n\n\n<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>\n\n\n\nYou can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings\n\nare obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.\n\nFor documents, set the `task_type` to `search_document`:\n\n\n\n```python', 'The Embedding Model API supports various task types, outlined as follows:\n\n\n\n1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.\n\n2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.\n\n3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.\n\n4. `classification`: Specifies that the given text will be classified.\n\n5. `clustering`: Specifies that the embeddings will be used for clustering.', 'parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.\n\n\n\n- `input_type=""search_document""` - for documents to store in Qdrant\n\n- `input_type=""search_query""` - for search queries to find the most relevant documents\n\n- `input_type=""classification""` - for classification tasks\n\n- `input_type=""clustering""` - for text clustering\n\n\n\nWhile implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the', '```python\n\noutput = embed.text(\n\n    texts=[""What is the best vector database?""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_query"",\n\n)\n\n\n\nqdrant_client.search(\n\n    collection_name=""my-collection"",\n\n    query=output[""embeddings""][0],\n\n)\n\n```\n\n\n\nFor more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).', '```python\n\nfrom qdrant_client import QdrantClient, models\n\nfrom nomic import embed\n\n\n\noutput = embed.text(\n\n    texts=[""Qdrant is the best vector database!""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_document"",\n\n)\n\n\n\nqdrant_client = QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""my-collection"",\n\n    points=models.Batch(\n\n        ids=[1],\n\n        vectors=output[""embeddings""],\n\n    ),\n\n)\n\n```\n\n\n\nTo query the collection, set the `task_type` to `search_query`:\n\n\n\n```python']","Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
The Embedding Model API supports various task types, outlined as follows:



1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.

2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.

3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.

4. `classification`: Specifies that the given text will be classified.

5. `clustering`: Specifies that the embeddings will be used for clustering.
parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.



- `input_type=""search_document""` - for documents to store in Qdrant

- `input_type=""search_query""` - for search queries to find the most relevant documents

- `input_type=""classification""` - for classification tasks

- `input_type=""clustering""` - for text clustering



While implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the
```python

output = embed.text(

    texts=[""What is the best vector database?""],

    model=""nomic-embed-text-v1"",

    task_type=""search_query"",

)



qdrant_client.search(

    collection_name=""my-collection"",

    query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

    texts=[""Qdrant is the best vector database!""],

    model=""nomic-embed-text-v1"",

    task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

    collection_name=""my-collection"",

    points=models.Batch(

        ids=[1],

        vectors=output[""embeddings""],

    ),

)

```



To query the collection, set the `task_type` to `search_query`:



```python",0.5925925925925926,0.3404255319148936,0.4324324324324324,0.20754716981132076,0.11827956989247312,0.15068493150684933,0.37037037037037035,0.2127659574468085,0.2702702702702703,0.5185185185185185,0.2978723404255319,0.37837837837837834,0.850432276725769,0.6828280091285706,0.634084939956665,0.6575544476509094,0.9802919051920375,0.9373202413320542,59,0.5959595959595959,0.450750884505794
True,,6233.413,d968e9a0,How can you use the Nomic library to upsert data into a collection and query it using text embeddings?,"To upsert data into a collection and query it using text embeddings with the Nomic library, you first need to use the `embed.text` method with the `task_type` set to `search_document` to generate embeddings for the text data you want to upsert. After obtaining the embeddings, you can use the `QdrantClient` to upsert the data into a specific collection by providing the collection name and the batch of points containing the IDs and corresponding vectors.

To query the collection, you need to set the `task_type` to `search_query` in the `embed.text` method with the query text. Once you have the query embeddings, you can use the `QdrantClient` to search the collection by providing the collection name and the query embedding to retrieve relevant results based on similarity.

For more detailed information and examples on using text , refer to : https://qdrant.tech/documentation/embeddings/nomic/","To use the Nomic library for upserting data into a collection and querying it using text embeddings, you can follow these steps:

1. Import the necessary modules and functions from the libraries:

```python
from qdrant_client import QdrantClient, models
from nomic import embed
```

2. Obtain the text embeddings for the document you want to upsert into the collection. Set the `task_type` to `search_document`:

```python
output = embed.text(
    texts=[""Qdrant is the best vector database!""],
    model=""nomic-embed-text-v1"",
    task_type=""search_document"",
)
```

3. Initialize the Qdrant client and upsert the document into the collection:

```python
qdrant_client = QdrantClient()
qdrant_client.upsert(
    collection_name=""my-collection"",
    points=models.Batch(
        ids=[1],
        vectors=output[""embeddings""],
    ),
)
```

4. To query the collection using text embeddings, set the `task_type` to `search_query`:

```python
output = embed.text(
    texts=[""What is the best vector database?""],
    model=""nomic-embed-text-v1"",
    task_type=""search_query"",
)
```

5. Perform a search on the collection using the query embeddings:

```python
qdrant_client.search(
    collection_name=""my-collection"",
    query=output[""embeddings""][0],
)
```

These steps show how to use the Nomic library to upsert data into a collection and query it using text embeddings. For more detailed information, you can refer to the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).","Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

    texts=[""Qdrant is the best vector database!""],

    model=""nomic-embed-text-v1"",

    task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

    collection_name=""my-collection"",

    points=models.Batch(

        ids=[1],

        vectors=output[""embeddings""],

    ),

)

```



To query the collection, set the `task_type` to `search_query`:



```python
```python

output = embed.text(

    texts=[""What is the best vector database?""],

    model=""nomic-embed-text-v1"",

    task_type=""search_query"",

)



qdrant_client.search(

    collection_name=""my-collection"",

    query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
const body = new TextDecoder().decode(response.body);



    await qdrantClient.upsert(""{collection_name}"", {

        points: [

            {

                id: 1,

                vector: JSON.parse(body).embedding,

            },

        ],

    });

}



main();

```
collection_name=""MyCollection"",

    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),

)





qdrant_client.upsert(

    collection_name=""MyCollection"",

    points=Batch(

        ids=list(range(len(embeddings))),

        vectors=embeddings,

    ),

)



```","['Once installed, you can configure it with the official Python client or through direct HTTP requests.\n\n\n\n<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>\n\n\n\nYou can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings\n\nare obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.\n\nFor documents, set the `task_type` to `search_document`:\n\n\n\n```python', '```python\n\nfrom qdrant_client import QdrantClient, models\n\nfrom nomic import embed\n\n\n\noutput = embed.text(\n\n    texts=[""Qdrant is the best vector database!""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_document"",\n\n)\n\n\n\nqdrant_client = QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""my-collection"",\n\n    points=models.Batch(\n\n        ids=[1],\n\n        vectors=output[""embeddings""],\n\n    ),\n\n)\n\n```\n\n\n\nTo query the collection, set the `task_type` to `search_query`:\n\n\n\n```python', '```python\n\noutput = embed.text(\n\n    texts=[""What is the best vector database?""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_query"",\n\n)\n\n\n\nqdrant_client.search(\n\n    collection_name=""my-collection"",\n\n    query=output[""embeddings""][0],\n\n)\n\n```\n\n\n\nFor more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).', 'const body = new TextDecoder().decode(response.body);\n\n\n\n    await qdrantClient.upsert(""{collection_name}"", {\n\n        points: [\n\n            {\n\n                id: 1,\n\n                vector: JSON.parse(body).embedding,\n\n            },\n\n        ],\n\n    });\n\n}\n\n\n\nmain();\n\n```', 'collection_name=""MyCollection"",\n\n    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),\n\n)\n\n\n\n\n\nqdrant_client.upsert(\n\n    collection_name=""MyCollection"",\n\n    points=Batch(\n\n        ids=list(range(len(embeddings))),\n\n        vectors=embeddings,\n\n    ),\n\n)\n\n\n\n```']","Question: How can you use the Nomic library to upsert data into a collection and query it using text embeddings?\n\nContext: Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

texts=[""Qdrant is the best vector database!""],

model=""nomic-embed-text-v1"",

task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

collection_name=""my-collection"",

points=models.Batch(

ids=[1],

vectors=output[""embeddings""],

),

)

```



To query the collection, set the `task_type` to `search_query`:



```python
```python

output = embed.text(

texts=[""What is the best vector database?""],

model=""nomic-embed-text-v1"",

task_type=""search_query"",

)



qdrant_client.search(

collection_name=""my-collection"",

query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
const body = new TextDecoder().decode(response.body);



await qdrantClient.upsert(""{collection_name}"", {

points: [

{

id: 1,

vector: JSON.parse(body).embedding,

},

],

});

}



main();

```
collection_name=""MyCollection"",

vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),

)





qdrant_client.upsert(

collection_name=""MyCollection"",

points=Batch(

ids=list(range(len(embeddings))),

vectors=embeddings,

),

)



```\n\nAnswer:",How can you use the Nomic library to upsert data into a collection and query it using text embeddings?,"To upsert data into a collection and query it using text embeddings with the Nomic library, you first need to use the `embed.text` method with the `task_type` set to `search_document` to generate embeddings for the text data you want to upsert. After obtaining the embeddings, you can use the `QdrantClient` to upsert the data into a specific collection by providing the collection name and the batch of points containing the IDs and corresponding vectors.

To query the collection, you need to set the `task_type` to `search_query` in the `embed.text` method with the query text. Once you have the query embeddings, you can use the `QdrantClient` to search the collection by providing the collection name and the query embedding to retrieve relevant results based on similarity.

For more detailed information and examples on using text , refer to : https://qdrant.tech/documentation/embeddings/nomic/","['Once installed, you can configure it with the official Python client or through direct HTTP requests.\n\n\n\n<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>\n\n\n\nYou can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings\n\nare obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.\n\nFor documents, set the `task_type` to `search_document`:\n\n\n\n```python', '```python\n\nfrom qdrant_client import QdrantClient, models\n\nfrom nomic import embed\n\n\n\noutput = embed.text(\n\n    texts=[""Qdrant is the best vector database!""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_document"",\n\n)\n\n\n\nqdrant_client = QdrantClient()\n\nqdrant_client.upsert(\n\n    collection_name=""my-collection"",\n\n    points=models.Batch(\n\n        ids=[1],\n\n        vectors=output[""embeddings""],\n\n    ),\n\n)\n\n```\n\n\n\nTo query the collection, set the `task_type` to `search_query`:\n\n\n\n```python', '```python\n\noutput = embed.text(\n\n    texts=[""What is the best vector database?""],\n\n    model=""nomic-embed-text-v1"",\n\n    task_type=""search_query"",\n\n)\n\n\n\nqdrant_client.search(\n\n    collection_name=""my-collection"",\n\n    query=output[""embeddings""][0],\n\n)\n\n```\n\n\n\nFor more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).', 'const body = new TextDecoder().decode(response.body);\n\n\n\n    await qdrantClient.upsert(""{collection_name}"", {\n\n        points: [\n\n            {\n\n                id: 1,\n\n                vector: JSON.parse(body).embedding,\n\n            },\n\n        ],\n\n    });\n\n}\n\n\n\nmain();\n\n```', 'collection_name=""MyCollection"",\n\n    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),\n\n)\n\n\n\n\n\nqdrant_client.upsert(\n\n    collection_name=""MyCollection"",\n\n    points=Batch(\n\n        ids=list(range(len(embeddings))),\n\n        vectors=embeddings,\n\n    ),\n\n)\n\n\n\n```']","Once installed, you can configure it with the official Python client or through direct HTTP requests.



<aside role=""status"">Using Nomic Text Embeddings requires configuring the Nomic API token</aside>



You can use Nomic embeddings directly in Qdrant client calls. There is a difference in the way the embeddings

are obtained for documents and queries. The `task_type` parameter defines the embeddings that you get.

For documents, set the `task_type` to `search_document`:



```python
```python

from qdrant_client import QdrantClient, models

from nomic import embed



output = embed.text(

    texts=[""Qdrant is the best vector database!""],

    model=""nomic-embed-text-v1"",

    task_type=""search_document"",

)



qdrant_client = QdrantClient()

qdrant_client.upsert(

    collection_name=""my-collection"",

    points=models.Batch(

        ids=[1],

        vectors=output[""embeddings""],

    ),

)

```



To query the collection, set the `task_type` to `search_query`:



```python
```python

output = embed.text(

    texts=[""What is the best vector database?""],

    model=""nomic-embed-text-v1"",

    task_type=""search_query"",

)



qdrant_client.search(

    collection_name=""my-collection"",

    query=output[""embeddings""][0],

)

```



For more information, see the Nomic documentation on [Text embeddings](https://docs.nomic.ai/reference/endpoints/nomic-embed-text).
const body = new TextDecoder().decode(response.body);



    await qdrantClient.upsert(""{collection_name}"", {

        points: [

            {

                id: 1,

                vector: JSON.parse(body).embedding,

            },

        ],

    });

}



main();

```
collection_name=""MyCollection"",

    vectors_config=VectorParams(size=EMBEDDING_SIZE, distance=Distance.DOT),

)





qdrant_client.upsert(

    collection_name=""MyCollection"",

    points=Batch(

        ids=list(range(len(embeddings))),

        vectors=embeddings,

    ),

)



```",0.45685279187817257,0.7086614173228346,0.5555555555555556,0.2193877551020408,0.3412698412698413,0.2670807453416149,0.29441624365482233,0.4566929133858268,0.35802469135802467,0.4213197969543147,0.6535433070866141,0.5123456790123456,0.9513633251190186,0.6613909006118774,0.6971641778945923,0.6788065433502197,0.6664500752463937,0.9166463308036328,181,1.3115942028985508,0.3119422060252953
True,,3062.577,b09b7627,What is the purpose of the `task_type` parameter in the Gemini Embedding Model API?,"The `task_type` parameter in the Gemini Embedding Model API serves to designate the intended purpose for the embeddings utilized. It allows users to specify the type of task they want to perform with the given text. The supported task types include `retrieval_query` for search/retrieval queries, `retrieval_document` for documents from the corpus being searched, `semantic_similarity` for Semantic Text Similarity, and `classification` for text classification. This parameter helps in customizing the functionality of the Gemini Embedding Model API based on the specific task requirements.","The `task_type` parameter in the Gemini Embedding Model API serves to designate the intended purpose for the embeddings being utilized. It specifies the type of task for which the given text will be used. The available task types in the Gemini Embedding Model API are as follows:

1. `retrieval_query`: Indicates that the given text is a query in a search/retrieval setting.
2. `retrieval_document`: Indicates that the given text is a document from the corpus being searched.
3. `semantic_similarity`: Specifies that the given text will be used for Semantic Text Similarity.
4. `classification`: Specifies that the given text will be used for classification tasks.
5. `clustering`: Specifies that the embeddings will be used for clustering tasks.","---

title: Gemini

weight: 700

---



# Gemini



Qdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:



Gemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. 



In the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.
The Embedding Model API supports various task types, outlined as follows:



1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.

2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.

3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.

4. `classification`: Specifies that the given text will be classified.

5. `clustering`: Specifies that the embeddings will be used for clustering.
parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.



- `input_type=""search_document""` - for documents to store in Qdrant

- `input_type=""search_query""` - for search queries to find the most relevant documents

- `input_type=""classification""` - for classification tasks

- `input_type=""clustering""` - for text clustering



While implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the
ids=[1],

        vectors=genai.embed_content(

            model=""models/embedding-001"",

            content=""Qdrant is the best vector search engine to use with Gemini"",

            task_type=""retrieval_document"",

            title=""Qdrant x Gemini"",

        )[""embedding""],

    ),

)

```



## Searching for documents with Qdrant



Once the documents are indexed, you can search for the most relevant documents using the same model with the `retrieval_query` task type:



```python
The following example shows how to embed a document with the `models/embedding-001` with the `retrieval_document` task type:



## Embedding a document



```python

import pathlib

import google.generativeai as genai

import qdrant_client



GEMINI_API_KEY = ""YOUR GEMINI API KEY""  # add your key here



genai.configure(api_key=GEMINI_API_KEY)



result = genai.embed_content(

    model=""models/embedding-001"",

    content=""Qdrant is the best vector search engine to use with Gemini"",","['---\n\ntitle: Gemini\n\nweight: 700\n\n---\n\n\n\n# Gemini\n\n\n\nQdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:\n\n\n\nGemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. \n\n\n\nIn the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.', 'The Embedding Model API supports various task types, outlined as follows:\n\n\n\n1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.\n\n2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.\n\n3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.\n\n4. `classification`: Specifies that the given text will be classified.\n\n5. `clustering`: Specifies that the embeddings will be used for clustering.', 'parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.\n\n\n\n- `input_type=""search_document""` - for documents to store in Qdrant\n\n- `input_type=""search_query""` - for search queries to find the most relevant documents\n\n- `input_type=""classification""` - for classification tasks\n\n- `input_type=""clustering""` - for text clustering\n\n\n\nWhile implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the', 'ids=[1],\n\n        vectors=genai.embed_content(\n\n            model=""models/embedding-001"",\n\n            content=""Qdrant is the best vector search engine to use with Gemini"",\n\n            task_type=""retrieval_document"",\n\n            title=""Qdrant x Gemini"",\n\n        )[""embedding""],\n\n    ),\n\n)\n\n```\n\n\n\n## Searching for documents with Qdrant\n\n\n\nOnce the documents are indexed, you can search for the most relevant documents using the same model with the `retrieval_query` task type:\n\n\n\n```python', 'The following example shows how to embed a document with the `models/embedding-001` with the `retrieval_document` task type:\n\n\n\n## Embedding a document\n\n\n\n```python\n\nimport pathlib\n\nimport google.generativeai as genai\n\nimport qdrant_client\n\n\n\nGEMINI_API_KEY = ""YOUR GEMINI API KEY""  # add your key here\n\n\n\ngenai.configure(api_key=GEMINI_API_KEY)\n\n\n\nresult = genai.embed_content(\n\n    model=""models/embedding-001"",\n\n    content=""Qdrant is the best vector search engine to use with Gemini"",']","Question: What is the purpose of the `task_type` parameter in the Gemini Embedding Model API?\n\nContext: ---

title: Gemini

weight: 700

---



# Gemini



Qdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:



Gemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. 



In the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.
The Embedding Model API supports various task types, outlined as follows:



1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.

2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.

3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.

4. `classification`: Specifies that the given text will be classified.

5. `clustering`: Specifies that the embeddings will be used for clustering.
parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.



- `input_type=""search_document""` - for documents to store in Qdrant

- `input_type=""search_query""` - for search queries to find the most relevant documents

- `input_type=""classification""` - for classification tasks

- `input_type=""clustering""` - for text clustering



While implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the
ids=[1],

vectors=genai.embed_content(

model=""models/embedding-001"",

content=""Qdrant is the best vector search engine to use with Gemini"",

task_type=""retrieval_document"",

title=""Qdrant x Gemini"",

)[""embedding""],

),

)

```



## Searching for documents with Qdrant



Once the documents are indexed, you can search for the most relevant documents using the same model with the `retrieval_query` task type:



```python
The following example shows how to embed a document with the `models/embedding-001` with the `retrieval_document` task type:



## Embedding a document



```python

import pathlib

import google.generativeai as genai

import qdrant_client



GEMINI_API_KEY = ""YOUR GEMINI API KEY""  # add your key here



genai.configure(api_key=GEMINI_API_KEY)



result = genai.embed_content(

model=""models/embedding-001"",

content=""Qdrant is the best vector search engine to use with Gemini"",\n\nAnswer:",What is the purpose of the `task_type` parameter in the Gemini Embedding Model API?,"The `task_type` parameter in the Gemini Embedding Model API serves to designate the intended purpose for the embeddings utilized. It allows users to specify the type of task they want to perform with the given text. The supported task types include `retrieval_query` for search/retrieval queries, `retrieval_document` for documents from the corpus being searched, `semantic_similarity` for Semantic Text Similarity, and `classification` for text classification. This parameter helps in customizing the functionality of the Gemini Embedding Model API based on the specific task requirements.","['---\n\ntitle: Gemini\n\nweight: 700\n\n---\n\n\n\n# Gemini\n\n\n\nQdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:\n\n\n\nGemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. \n\n\n\nIn the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.', 'The Embedding Model API supports various task types, outlined as follows:\n\n\n\n1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.\n\n2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.\n\n3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.\n\n4. `classification`: Specifies that the given text will be classified.\n\n5. `clustering`: Specifies that the embeddings will be used for clustering.', 'parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.\n\n\n\n- `input_type=""search_document""` - for documents to store in Qdrant\n\n- `input_type=""search_query""` - for search queries to find the most relevant documents\n\n- `input_type=""classification""` - for classification tasks\n\n- `input_type=""clustering""` - for text clustering\n\n\n\nWhile implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the', 'ids=[1],\n\n        vectors=genai.embed_content(\n\n            model=""models/embedding-001"",\n\n            content=""Qdrant is the best vector search engine to use with Gemini"",\n\n            task_type=""retrieval_document"",\n\n            title=""Qdrant x Gemini"",\n\n        )[""embedding""],\n\n    ),\n\n)\n\n```\n\n\n\n## Searching for documents with Qdrant\n\n\n\nOnce the documents are indexed, you can search for the most relevant documents using the same model with the `retrieval_query` task type:\n\n\n\n```python', 'The following example shows how to embed a document with the `models/embedding-001` with the `retrieval_document` task type:\n\n\n\n## Embedding a document\n\n\n\n```python\n\nimport pathlib\n\nimport google.generativeai as genai\n\nimport qdrant_client\n\n\n\nGEMINI_API_KEY = ""YOUR GEMINI API KEY""  # add your key here\n\n\n\ngenai.configure(api_key=GEMINI_API_KEY)\n\n\n\nresult = genai.embed_content(\n\n    model=""models/embedding-001"",\n\n    content=""Qdrant is the best vector search engine to use with Gemini"",']","---

title: Gemini

weight: 700

---



# Gemini



Qdrant is compatible with Gemini Embedding Model API and its official Python SDK that can be installed as any other package:



Gemini is a new family of Google PaLM models, released in December 2023. The new embedding models succeed the previous Gecko Embedding Model. 



In the latest models, an additional parameter, `task_type`, can be passed to the API call. This parameter serves to designate the intended purpose for the embeddings utilized.
The Embedding Model API supports various task types, outlined as follows:



1. `retrieval_query`: Specifies the given text is a query in a search/retrieval setting.

2. `retrieval_document`: Specifies the given text is a document from the corpus being searched.

3. `semantic_similarity`: Specifies the given text will be used for Semantic Text Similarity.

4. `classification`: Specifies that the given text will be classified.

5. `clustering`: Specifies that the embeddings will be used for clustering.
parameter to the API call: `input_type`. It determines the type of task you want to use the embeddings for.



- `input_type=""search_document""` - for documents to store in Qdrant

- `input_type=""search_query""` - for search queries to find the most relevant documents

- `input_type=""classification""` - for classification tasks

- `input_type=""clustering""` - for text clustering



While implementing semantic search applications, such as RAG, you should use `input_type=""search_document""` for the
ids=[1],

        vectors=genai.embed_content(

            model=""models/embedding-001"",

            content=""Qdrant is the best vector search engine to use with Gemini"",

            task_type=""retrieval_document"",

            title=""Qdrant x Gemini"",

        )[""embedding""],

    ),

)

```



## Searching for documents with Qdrant



Once the documents are indexed, you can search for the most relevant documents using the same model with the `retrieval_query` task type:



```python
The following example shows how to embed a document with the `models/embedding-001` with the `retrieval_document` task type:



## Embedding a document



```python

import pathlib

import google.generativeai as genai

import qdrant_client



GEMINI_API_KEY = ""YOUR GEMINI API KEY""  # add your key here



genai.configure(api_key=GEMINI_API_KEY)



result = genai.embed_content(

    model=""models/embedding-001"",

    content=""Qdrant is the best vector search engine to use with Gemini"",",0.5339805825242718,0.7236842105263158,0.6145251396648046,0.3333333333333333,0.4533333333333333,0.38418079096045193,0.4563106796116505,0.618421052631579,0.5251396648044693,0.4854368932038835,0.6578947368421053,0.558659217877095,0.9614013433456421,0.6911103129386902,0.7491596341133118,0.7189651131629944,0.8533950499161923,0.9947414651978761,115,1.4024390243902438,0.6964673026608396
True,,714.2349999999999,0e225150,What is the maximum number of tokens that Jina embeddings allow for model input lengths?,"Jina embeddings allow for model input lengths of up to 8192 tokens. This means that the models utilizing Jina embeddings can process sequences with a maximum length of 8192 tokens, providing flexibility and capability for handling large amounts of text data efficiently.",The maximum number of tokens that Jina embeddings allow for model input lengths is up to 8192 tokens.,"---

title: Jina Embeddings

weight: 800

aliases: [ ../integrations/jina-embeddings/ ]

---



# Jina Embeddings



Qdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.
The default model and several other models have a context window of maximum 512 tokens. This maximum limit comes from the embedding model training and design itself.If you'd like to embed sequences larger than that, we'd recommend using some pooling strategy to get a single vector out of the sequence. For example, you can use the mean of the embeddings of different chunks of a document. This is also what the [SBERT Paper recommends](https://lilianweng.github.io/posts/2021-05-31-contrastive/#sentence-bert)
. The other caveat is that we pass on the token limits or context windows from the embedding model creators themselves. So in the case of this model, which is BGE base, that is 512 BGE tokens.
This expansion has a direct relationship with what we can control when making a SPLADE model: Sparsity via Regularisation. The number of tokens (BERT wordpieces) we use to represent each document. If we use more tokens, we can represent more terms, but the vectors become denser. This number is typically between 20 to 200 per document. As a reference point, the dense BERT vector is 768 dimensions, OpenAI Embedding is 1536 dimensions, and the sparse vector is 30 dimensions.
""""""

    tokens = tokenizer(text, return_tensors=""pt"")

    output = model(**tokens)

    logits, attention_mask = output.logits, tokens.attention_mask

    relu_log = torch.log(1 + torch.relu(logits))

    weighted_log = relu_log * attention_mask.unsqueeze(-1)

    max_val, _ = torch.max(weighted_log, dim=1)

    vec = max_val.squeeze()



    return vec, tokens





vec, tokens = compute_vector(text)

print(vec.shape)

```","['---\n\ntitle: Jina Embeddings\n\nweight: 800\n\naliases: [ ../integrations/jina-embeddings/ ]\n\n---\n\n\n\n# Jina Embeddings\n\n\n\nQdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.', ""The default model and several other models have a context window of maximum 512 tokens. This maximum limit comes from the embedding model training and design itself.If you'd like to embed sequences larger than that, we'd recommend using some pooling strategy to get a single vector out of the sequence. For example, you can use the mean of the embeddings of different chunks of a document. This is also what the [SBERT Paper recommends](https://lilianweng.github.io/posts/2021-05-31-contrastive/#sentence-bert)"", '. The other caveat is that we pass on the token limits or context windows from the embedding model creators themselves. So in the case of this model, which is BGE base, that is 512 BGE tokens.', 'This expansion has a direct relationship with what we can control when making a SPLADE model: Sparsity via Regularisation. The number of tokens (BERT wordpieces) we use to represent each document. If we use more tokens, we can represent more terms, but the vectors become denser. This number is typically between 20 to 200 per document. As a reference point, the dense BERT vector is 768 dimensions, OpenAI Embedding is 1536 dimensions, and the sparse vector is 30 dimensions.', '""""""\n\n    tokens = tokenizer(text, return_tensors=""pt"")\n\n    output = model(**tokens)\n\n    logits, attention_mask = output.logits, tokens.attention_mask\n\n    relu_log = torch.log(1 + torch.relu(logits))\n\n    weighted_log = relu_log * attention_mask.unsqueeze(-1)\n\n    max_val, _ = torch.max(weighted_log, dim=1)\n\n    vec = max_val.squeeze()\n\n\n\n    return vec, tokens\n\n\n\n\n\nvec, tokens = compute_vector(text)\n\nprint(vec.shape)\n\n```']","Question: What is the maximum number of tokens that Jina embeddings allow for model input lengths?\n\nContext: ---

title: Jina Embeddings

weight: 800

aliases: [ ../integrations/jina-embeddings/ ]

---



# Jina Embeddings



Qdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.
The default model and several other models have a context window of maximum 512 tokens. This maximum limit comes from the embedding model training and design itself.If you'd like to embed sequences larger than that, we'd recommend using some pooling strategy to get a single vector out of the sequence. For example, you can use the mean of the embeddings of different chunks of a document. This is also what the [SBERT Paper recommends](https://lilianweng.github.io/posts/2021-05-31-contrastive/#sentence-bert)
. The other caveat is that we pass on the token limits or context windows from the embedding model creators themselves. So in the case of this model, which is BGE base, that is 512 BGE tokens.
This expansion has a direct relationship with what we can control when making a SPLADE model: Sparsity via Regularisation. The number of tokens (BERT wordpieces) we use to represent each document. If we use more tokens, we can represent more terms, but the vectors become denser. This number is typically between 20 to 200 per document. As a reference point, the dense BERT vector is 768 dimensions, OpenAI Embedding is 1536 dimensions, and the sparse vector is 30 dimensions.
""""""

tokens = tokenizer(text, return_tensors=""pt"")

output = model(**tokens)

logits, attention_mask = output.logits, tokens.attention_mask

relu_log = torch.log(1 + torch.relu(logits))

weighted_log = relu_log * attention_mask.unsqueeze(-1)

max_val, _ = torch.max(weighted_log, dim=1)

vec = max_val.squeeze()



return vec, tokens





vec, tokens = compute_vector(text)

print(vec.shape)

```\n\nAnswer:",What is the maximum number of tokens that Jina embeddings allow for model input lengths?,"Jina embeddings allow for model input lengths of up to 8192 tokens. This means that the models utilizing Jina embeddings can process sequences with a maximum length of 8192 tokens, providing flexibility and capability for handling large amounts of text data efficiently.","['---\n\ntitle: Jina Embeddings\n\nweight: 800\n\naliases: [ ../integrations/jina-embeddings/ ]\n\n---\n\n\n\n# Jina Embeddings\n\n\n\nQdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.', ""The default model and several other models have a context window of maximum 512 tokens. This maximum limit comes from the embedding model training and design itself.If you'd like to embed sequences larger than that, we'd recommend using some pooling strategy to get a single vector out of the sequence. For example, you can use the mean of the embeddings of different chunks of a document. This is also what the [SBERT Paper recommends](https://lilianweng.github.io/posts/2021-05-31-contrastive/#sentence-bert)"", '. The other caveat is that we pass on the token limits or context windows from the embedding model creators themselves. So in the case of this model, which is BGE base, that is 512 BGE tokens.', 'This expansion has a direct relationship with what we can control when making a SPLADE model: Sparsity via Regularisation. The number of tokens (BERT wordpieces) we use to represent each document. If we use more tokens, we can represent more terms, but the vectors become denser. This number is typically between 20 to 200 per document. As a reference point, the dense BERT vector is 768 dimensions, OpenAI Embedding is 1536 dimensions, and the sparse vector is 30 dimensions.', '""""""\n\n    tokens = tokenizer(text, return_tensors=""pt"")\n\n    output = model(**tokens)\n\n    logits, attention_mask = output.logits, tokens.attention_mask\n\n    relu_log = torch.log(1 + torch.relu(logits))\n\n    weighted_log = relu_log * attention_mask.unsqueeze(-1)\n\n    max_val, _ = torch.max(weighted_log, dim=1)\n\n    vec = max_val.squeeze()\n\n\n\n    return vec, tokens\n\n\n\n\n\nvec, tokens = compute_vector(text)\n\nprint(vec.shape)\n\n```']","---

title: Jina Embeddings

weight: 800

aliases: [ ../integrations/jina-embeddings/ ]

---



# Jina Embeddings



Qdrant can also easily work with [Jina embeddings](https://jina.ai/embeddings/) which allow for model input lengths of up to 8192 tokens.
The default model and several other models have a context window of maximum 512 tokens. This maximum limit comes from the embedding model training and design itself.If you'd like to embed sequences larger than that, we'd recommend using some pooling strategy to get a single vector out of the sequence. For example, you can use the mean of the embeddings of different chunks of a document. This is also what the [SBERT Paper recommends](https://lilianweng.github.io/posts/2021-05-31-contrastive/#sentence-bert)
. The other caveat is that we pass on the token limits or context windows from the embedding model creators themselves. So in the case of this model, which is BGE base, that is 512 BGE tokens.
This expansion has a direct relationship with what we can control when making a SPLADE model: Sparsity via Regularisation. The number of tokens (BERT wordpieces) we use to represent each document. If we use more tokens, we can represent more terms, but the vectors become denser. This number is typically between 20 to 200 per document. As a reference point, the dense BERT vector is 768 dimensions, OpenAI Embedding is 1536 dimensions, and the sparse vector is 30 dimensions.
""""""

    tokens = tokenizer(text, return_tensors=""pt"")

    output = model(**tokens)

    logits, attention_mask = output.logits, tokens.attention_mask

    relu_log = torch.log(1 + torch.relu(logits))

    weighted_log = relu_log * attention_mask.unsqueeze(-1)

    max_val, _ = torch.max(weighted_log, dim=1)

    vec = max_val.squeeze()



    return vec, tokens





vec, tokens = compute_vector(text)

print(vec.shape)

```",0.8823529411764706,0.375,0.5263157894736842,0.5625,0.23076923076923078,0.32727272727272727,0.6470588235294118,0.275,0.3859649122807018,0.7647058823529411,0.325,0.456140350877193,0.9264309406280518,0.6830343008041382,0.6052727103233337,0.6418067216873169,0.9992045030812733,0.9976610423764214,18,0.42857142857142855,0.42185196821749665
True,,3712.628,b555f982,Why does Qdrant by default not return vectors in search results?,"By default, Qdrant tries to minimize network traffic and doesn't return vectors in search results. This default behavior is in place to optimize performance and reduce unnecessary data transfer. However, if you specifically require the vectors to be included in the search results, you can override this default setting by setting the `with_vector` parameter of the Search/Scroll to `true`. This allows you to retrieve the vectors along with other search results, providing you with the option to access the vector data when needed.","By default, Qdrant does not return vectors in search results in order to minimize network traffic. However, you can force Qdrant to include vectors in the search results by setting the `with_vector` parameter to `true` in the Search/Scroll method.

If you are seeing `""vector"": null` in your search results, it could be due to the vector not being in the correct format or an issue with how you are calling the upsert method.

If you want to search without a vector, you can use the scroll method, which allows you to retrieve records based on filters or iterate over all the records in the collection.

Qdrant supports operations with vectors and allows you to store additional payload along with vectors for filtering search results. It also provides a Python client and defines an API schema for using it from other languages.

To use Qdrant, you can run a pre-built Docker image. Instructions for starting Qdrant can be found on its homepage at [https://github.com/qdrant/qdrant](https://github.com/qdrant/qdrant).","Please read more about collections, isolation, and multiple users in our [Multitenancy](../../tutorials/multiple-partitions/) tutorial.



### My search results contain vectors with null values. Why?



By default, Qdrant tries to minimize network traffic and doesn't return vectors in search results.

But you can force Qdrant to do so by setting the `with_vector` parameter of the Search/Scroll to `true`.
If you're still seeing `""vector"": null` in your results, it might be that the vector you're passing is not in the correct format, or there's an issue with how you're calling the upsert method.



### How can I search without a vector?



You are likely looking for the [scroll](../../concepts/points/#scroll-points) method. It allows you to retrieve the records based on filters or even iterate over all the records in the collection.



### Does Qdrant support a full-text search or a hybrid search?
. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.
It not only supports all necessary operations with vectors but also allows to store additional payload along with vectors and use it to perform filtering of the search result.

Qdrant has a client for python and also defines the API schema if you need to use it from other languages.



The easiest way to use Qdrant is to run a pre-built image.

So make sure you have Docker installed on your system.



To start Qdrant, use the instructions on its [homepage](https://github.com/qdrant/qdrant).
qdrant_client

        .search_points(&SearchPoints {

            collection_name: COLLECTION_NAME.to_string(),

            vector,

            limit: SEARCH_LIMIT,

            with_payload: Some(true.into()),

            ..Default::default()

        })

        .await

        .map_err(|err| anyhow!(""Failed to search Qdrant: {}"", err))

}

```","[""Please read more about collections, isolation, and multiple users in our [Multitenancy](../../tutorials/multiple-partitions/) tutorial.\n\n\n\n### My search results contain vectors with null values. Why?\n\n\n\nBy default, Qdrant tries to minimize network traffic and doesn't return vectors in search results.\n\nBut you can force Qdrant to do so by setting the `with_vector` parameter of the Search/Scroll to `true`."", 'If you\'re still seeing `""vector"": null` in your results, it might be that the vector you\'re passing is not in the correct format, or there\'s an issue with how you\'re calling the upsert method.\n\n\n\n### How can I search without a vector?\n\n\n\nYou are likely looking for the [scroll](../../concepts/points/#scroll-points) method. It allows you to retrieve the records based on filters or even iterate over all the records in the collection.\n\n\n\n### Does Qdrant support a full-text search or a hybrid search?', '. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.', 'It not only supports all necessary operations with vectors but also allows to store additional payload along with vectors and use it to perform filtering of the search result.\n\nQdrant has a client for python and also defines the API schema if you need to use it from other languages.\n\n\n\nThe easiest way to use Qdrant is to run a pre-built image.\n\nSo make sure you have Docker installed on your system.\n\n\n\nTo start Qdrant, use the instructions on its [homepage](https://github.com/qdrant/qdrant).', 'qdrant_client\n\n        .search_points(&SearchPoints {\n\n            collection_name: COLLECTION_NAME.to_string(),\n\n            vector,\n\n            limit: SEARCH_LIMIT,\n\n            with_payload: Some(true.into()),\n\n            ..Default::default()\n\n        })\n\n        .await\n\n        .map_err(|err| anyhow!(""Failed to search Qdrant: {}"", err))\n\n}\n\n```']","Question: Why does Qdrant by default not return vectors in search results?\n\nContext: Please read more about collections, isolation, and multiple users in our [Multitenancy](../../tutorials/multiple-partitions/) tutorial.



### My search results contain vectors with null values. Why?



By default, Qdrant tries to minimize network traffic and doesn't return vectors in search results.

But you can force Qdrant to do so by setting the `with_vector` parameter of the Search/Scroll to `true`.
If you're still seeing `""vector"": null` in your results, it might be that the vector you're passing is not in the correct format, or there's an issue with how you're calling the upsert method.



### How can I search without a vector?



You are likely looking for the [scroll](../../concepts/points/#scroll-points) method. It allows you to retrieve the records based on filters or even iterate over all the records in the collection.



### Does Qdrant support a full-text search or a hybrid search?
. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.
It not only supports all necessary operations with vectors but also allows to store additional payload along with vectors and use it to perform filtering of the search result.

Qdrant has a client for python and also defines the API schema if you need to use it from other languages.



The easiest way to use Qdrant is to run a pre-built image.

So make sure you have Docker installed on your system.



To start Qdrant, use the instructions on its [homepage](https://github.com/qdrant/qdrant).
qdrant_client

.search_points(&SearchPoints {

collection_name: COLLECTION_NAME.to_string(),

vector,

limit: SEARCH_LIMIT,

with_payload: Some(true.into()),

..Default::default()

})

.await

.map_err(|err| anyhow!(""Failed to search Qdrant: {}"", err))

}

```\n\nAnswer:",Why does Qdrant by default not return vectors in search results?,"By default, Qdrant tries to minimize network traffic and doesn't return vectors in search results. This default behavior is in place to optimize performance and reduce unnecessary data transfer. However, if you specifically require the vectors to be included in the search results, you can override this default setting by setting the `with_vector` parameter of the Search/Scroll to `true`. This allows you to retrieve the vectors along with other search results, providing you with the option to access the vector data when needed.","[""Please read more about collections, isolation, and multiple users in our [Multitenancy](../../tutorials/multiple-partitions/) tutorial.\n\n\n\n### My search results contain vectors with null values. Why?\n\n\n\nBy default, Qdrant tries to minimize network traffic and doesn't return vectors in search results.\n\nBut you can force Qdrant to do so by setting the `with_vector` parameter of the Search/Scroll to `true`."", 'If you\'re still seeing `""vector"": null` in your results, it might be that the vector you\'re passing is not in the correct format, or there\'s an issue with how you\'re calling the upsert method.\n\n\n\n### How can I search without a vector?\n\n\n\nYou are likely looking for the [scroll](../../concepts/points/#scroll-points) method. It allows you to retrieve the records based on filters or even iterate over all the records in the collection.\n\n\n\n### Does Qdrant support a full-text search or a hybrid search?', '. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.', 'It not only supports all necessary operations with vectors but also allows to store additional payload along with vectors and use it to perform filtering of the search result.\n\nQdrant has a client for python and also defines the API schema if you need to use it from other languages.\n\n\n\nThe easiest way to use Qdrant is to run a pre-built image.\n\nSo make sure you have Docker installed on your system.\n\n\n\nTo start Qdrant, use the instructions on its [homepage](https://github.com/qdrant/qdrant).', 'qdrant_client\n\n        .search_points(&SearchPoints {\n\n            collection_name: COLLECTION_NAME.to_string(),\n\n            vector,\n\n            limit: SEARCH_LIMIT,\n\n            with_payload: Some(true.into()),\n\n            ..Default::default()\n\n        })\n\n        .await\n\n        .map_err(|err| anyhow!(""Failed to search Qdrant: {}"", err))\n\n}\n\n```']","Please read more about collections, isolation, and multiple users in our [Multitenancy](../../tutorials/multiple-partitions/) tutorial.



### My search results contain vectors with null values. Why?



By default, Qdrant tries to minimize network traffic and doesn't return vectors in search results.

But you can force Qdrant to do so by setting the `with_vector` parameter of the Search/Scroll to `true`.
If you're still seeing `""vector"": null` in your results, it might be that the vector you're passing is not in the correct format, or there's an issue with how you're calling the upsert method.



### How can I search without a vector?



You are likely looking for the [scroll](../../concepts/points/#scroll-points) method. It allows you to retrieve the records based on filters or even iterate over all the records in the collection.



### Does Qdrant support a full-text search or a hybrid search?
. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.
It not only supports all necessary operations with vectors but also allows to store additional payload along with vectors and use it to perform filtering of the search result.

Qdrant has a client for python and also defines the API schema if you need to use it from other languages.



The easiest way to use Qdrant is to run a pre-built image.

So make sure you have Docker installed on your system.



To start Qdrant, use the instructions on its [homepage](https://github.com/qdrant/qdrant).
qdrant_client

        .search_points(&SearchPoints {

            collection_name: COLLECTION_NAME.to_string(),

            vector,

            limit: SEARCH_LIMIT,

            with_payload: Some(true.into()),

            ..Default::default()

        })

        .await

        .map_err(|err| anyhow!(""Failed to search Qdrant: {}"", err))

}

```",0.32298136645962733,0.6582278481012658,0.43333333333333335,0.15,0.3076923076923077,0.20168067226890757,0.2236024844720497,0.45569620253164556,0.3,0.2857142857142857,0.5822784810126582,0.38333333333333336,0.9212555289268494,0.6202640533447266,0.7157687544822693,0.6646028757095337,0.9884681658440968,0.9828497540147509,166,1.9761904761904763,0.39012563132974243
True,,1019.528,0440f476,What does Qdrant not plan to support in the future?,"In the future, Qdrant does not plan to support BM25 or other non-vector-based retrieval or ranking functions, built-in ontologies or knowledge graphs, and query analyzers and other NLP tools. Qdrant aims to introduce support for sparse vectors, as used in SPLADE or similar models, but does not intend to incorporate the aforementioned features into its functionality. Users can combine Qdrant with specialized tools, including full-text search engines, to complement its capabilities.","Qdrant does not plan to support code interpretation in the future. This is mentioned in the context of the text where it is stated that one type of action they do not support is code interpretation, which they will not be working on.","Other than as expressly set out in these terms, Qdrant does not make any commitments about the Solution and Website’s availability or ability to meet your expectations.





### 15. Liability



In no event shall Qdrant be liable for:





* any indirect damages of any kind, including any potential loss of business;

* any damage or loss which is not caused by a breach of its obligations under the T&Cs;

* disruptions or damage inherent in an electronic communications network;
* unauthorized access to the Solution by a third-party caused by you, including through your negligence;

* your failure to fulfill your obligations under the T&Cs;

* implementation of any software package, software or operating system not compatible with the Solution;

* failure of the electronic communication networks which is not the fault of Qdrant;

* your refusal to collaborate with Qdrant in the resolution of the anomalies and in particular to answer questions and requests for information;
Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.
To keep the human in the loop.

But obviously some stuff that are on our roadmap.

And another thing that we don't support, which is one type of action would.

Be the first we will be working on is obviously code interpretation, which is I think is one of the things that all users ask because they use.

It on Chat GPT.

And so we'll be looking into that as well.





Demetrios:

What made you choose Qdrant?





Stanislas Polu:

So the decision was made, if I.
You acknowledge that you are informed that the unavailability of the Solution may be the result of (a) a maintenance operation, (b) an urgent operation relating in particular to security, (c) a case of “force majeure” or (d) the malfunctioning of computer applications of Qdrant's third-party partners. Qdrant undertakes to restore the availability of the Solution as soon as possible once the problem causing the unavailability has been resolved.","['Other than as expressly set out in these terms, Qdrant does not make any commitments about the Solution and Website’s availability or ability to meet your expectations.\n\n\n\n\n\n### 15. Liability\n\n\n\nIn no event shall Qdrant be liable for:\n\n\n\n\n\n* any indirect damages of any kind, including any potential loss of business;\n\n* any damage or loss which is not caused by a breach of its obligations under the T&Cs;\n\n* disruptions or damage inherent in an electronic communications network;', '* unauthorized access to the Solution by a third-party caused by you, including through your negligence;\n\n* your failure to fulfill your obligations under the T&Cs;\n\n* implementation of any software package, software or operating system not compatible with the Solution;\n\n* failure of the electronic communication networks which is not the fault of Qdrant;\n\n* your refusal to collaborate with Qdrant in the resolution of the anomalies and in particular to answer questions and requests for information;', 'Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.', ""To keep the human in the loop.\n\nBut obviously some stuff that are on our roadmap.\n\nAnd another thing that we don't support, which is one type of action would.\n\nBe the first we will be working on is obviously code interpretation, which is I think is one of the things that all users ask because they use.\n\nIt on Chat GPT.\n\nAnd so we'll be looking into that as well.\n\n\n\n\n\nDemetrios:\n\nWhat made you choose Qdrant?\n\n\n\n\n\nStanislas Polu:\n\nSo the decision was made, if I."", ""You acknowledge that you are informed that the unavailability of the Solution may be the result of (a) a maintenance operation, (b) an urgent operation relating in particular to security, (c) a case of “force majeure” or (d) the malfunctioning of computer applications of Qdrant's third-party partners. Qdrant undertakes to restore the availability of the Solution as soon as possible once the problem causing the unavailability has been resolved.""]","Question: What does Qdrant not plan to support in the future?\n\nContext: Other than as expressly set out in these terms, Qdrant does not make any commitments about the Solution and Website’s availability or ability to meet your expectations.





### 15. Liability



In no event shall Qdrant be liable for:





* any indirect damages of any kind, including any potential loss of business;

* any damage or loss which is not caused by a breach of its obligations under the T&Cs;

* disruptions or damage inherent in an electronic communications network;
* unauthorized access to the Solution by a third-party caused by you, including through your negligence;

* your failure to fulfill your obligations under the T&Cs;

* implementation of any software package, software or operating system not compatible with the Solution;

* failure of the electronic communication networks which is not the fault of Qdrant;

* your refusal to collaborate with Qdrant in the resolution of the anomalies and in particular to answer questions and requests for information;
Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.
To keep the human in the loop.

But obviously some stuff that are on our roadmap.

And another thing that we don't support, which is one type of action would.

Be the first we will be working on is obviously code interpretation, which is I think is one of the things that all users ask because they use.

It on Chat GPT.

And so we'll be looking into that as well.





Demetrios:

What made you choose Qdrant?





Stanislas Polu:

So the decision was made, if I.
You acknowledge that you are informed that the unavailability of the Solution may be the result of (a) a maintenance operation, (b) an urgent operation relating in particular to security, (c) a case of “force majeure” or (d) the malfunctioning of computer applications of Qdrant's third-party partners. Qdrant undertakes to restore the availability of the Solution as soon as possible once the problem causing the unavailability has been resolved.\n\nAnswer:",What does Qdrant not plan to support in the future?,"In the future, Qdrant does not plan to support BM25 or other non-vector-based retrieval or ranking functions, built-in ontologies or knowledge graphs, and query analyzers and other NLP tools. Qdrant aims to introduce support for sparse vectors, as used in SPLADE or similar models, but does not intend to incorporate the aforementioned features into its functionality. Users can combine Qdrant with specialized tools, including full-text search engines, to complement its capabilities.","['Other than as expressly set out in these terms, Qdrant does not make any commitments about the Solution and Website’s availability or ability to meet your expectations.\n\n\n\n\n\n### 15. Liability\n\n\n\nIn no event shall Qdrant be liable for:\n\n\n\n\n\n* any indirect damages of any kind, including any potential loss of business;\n\n* any damage or loss which is not caused by a breach of its obligations under the T&Cs;\n\n* disruptions or damage inherent in an electronic communications network;', '* unauthorized access to the Solution by a third-party caused by you, including through your negligence;\n\n* your failure to fulfill your obligations under the T&Cs;\n\n* implementation of any software package, software or operating system not compatible with the Solution;\n\n* failure of the electronic communication networks which is not the fault of Qdrant;\n\n* your refusal to collaborate with Qdrant in the resolution of the anomalies and in particular to answer questions and requests for information;', 'Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.', ""To keep the human in the loop.\n\nBut obviously some stuff that are on our roadmap.\n\nAnd another thing that we don't support, which is one type of action would.\n\nBe the first we will be working on is obviously code interpretation, which is I think is one of the things that all users ask because they use.\n\nIt on Chat GPT.\n\nAnd so we'll be looking into that as well.\n\n\n\n\n\nDemetrios:\n\nWhat made you choose Qdrant?\n\n\n\n\n\nStanislas Polu:\n\nSo the decision was made, if I."", ""You acknowledge that you are informed that the unavailability of the Solution may be the result of (a) a maintenance operation, (b) an urgent operation relating in particular to security, (c) a case of “force majeure” or (d) the malfunctioning of computer applications of Qdrant's third-party partners. Qdrant undertakes to restore the availability of the Solution as soon as possible once the problem causing the unavailability has been resolved.""]","Other than as expressly set out in these terms, Qdrant does not make any commitments about the Solution and Website’s availability or ability to meet your expectations.





### 15. Liability



In no event shall Qdrant be liable for:





* any indirect damages of any kind, including any potential loss of business;

* any damage or loss which is not caused by a breach of its obligations under the T&Cs;

* disruptions or damage inherent in an electronic communications network;
* unauthorized access to the Solution by a third-party caused by you, including through your negligence;

* your failure to fulfill your obligations under the T&Cs;

* implementation of any software package, software or operating system not compatible with the Solution;

* failure of the electronic communication networks which is not the fault of Qdrant;

* your refusal to collaborate with Qdrant in the resolution of the anomalies and in particular to answer questions and requests for information;
Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.
To keep the human in the loop.

But obviously some stuff that are on our roadmap.

And another thing that we don't support, which is one type of action would.

Be the first we will be working on is obviously code interpretation, which is I think is one of the things that all users ask because they use.

It on Chat GPT.

And so we'll be looking into that as well.





Demetrios:

What made you choose Qdrant?





Stanislas Polu:

So the decision was made, if I.
You acknowledge that you are informed that the unavailability of the Solution may be the result of (a) a maintenance operation, (b) an urgent operation relating in particular to security, (c) a case of “force majeure” or (d) the malfunctioning of computer applications of Qdrant's third-party partners. Qdrant undertakes to restore the availability of the Solution as soon as possible once the problem causing the unavailability has been resolved.",0.3,0.1643835616438356,0.21238938053097345,0.15384615384615385,0.08333333333333333,0.10810810810810811,0.225,0.1232876712328767,0.1592920353982301,0.275,0.1506849315068493,0.19469026548672566,0.37283259630203247,0.5702552795410156,0.5181741118431091,0.5429686307907104,0.2797778844833374,0.3277298708756765,43,0.6056338028169014,0.3634016961575521
True,,4106.432,0f704cbf, What is the recommended approach for configuring multitenancy in terms of the number of collections to create?,"In most cases, the recommended approach for configuring multitenancy is to create only a single collection with payload-based partitioning. This approach, known as multitenancy, is efficient for the majority of users. By using a single collection with payload-based partitioning, each user can access their own vectors while ensuring that they cannot see the vectors of other users. However, it is important to note that this approach requires additional configuration to set up properly.","The recommended approach for configuring multitenancy is to use a single collection with payload-based partitioning. This means that you should create only one collection for your multitenant application and leverage payload-based partitioning to segregate the data of different tenants within that collection. This approach is efficient for most users and simplifies the management of the multitenant data. 

By setting up a single collection with payload-based partitioning, you can achieve efficient multitenancy while minimizing the complexity of managing multiple collections. This approach allows you to segregate tenant data within the same collection based on specific criteria, such as tenant IDs or other identifiers. 

Overall, the recommendation is to create a single collection with payload-based partitioning for multitenancy to streamline data management and optimize performance.","---

title: Multitenancy

weight: 12

aliases:

  - ../tutorials/multiple-partitions

---

# Configure Multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.
## Setting up multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called [multitenancy](https://en.wikipedia.org/wiki/Multitenancy). It is efficient for most of users, but it requires additional configuration. [Learn how to set it up](../../tutorials/multiple-partitions/)
A clear use-case for this feature is managing a multi-tenant collection, where each tenant (let it be a user or organization) is assumed to be segregated, so they can have their data stored in separate shards.



To enable user-defined sharding, set `sharding_method` to `custom` during collection creation:



```http

PUT /collections/{collection_name}

{

    ""shard_number"": 1,

    ""sharding_method"": ""custom""

    // ... other collection parameters

}

```



```python
In addition to metrics and vector size, each collection uses its own set of parameters that controls collection optimization, index construction, and vacuum.

These settings can be changed at any time by a corresponding request.



## Setting up multitenancy
**Figure 3:** Users can query the collection based on two filters: the `group_id` and the individual `shard_key_selector`. This gives your data two additional levels of isolation.

![Qdrant Multitenancy](/articles_data/multitenancy/multitenancy.png)



## Create custom shards for a single collection","['---\n\ntitle: Multitenancy\n\nweight: 12\n\naliases:\n\n  - ../tutorials/multiple-partitions\n\n---\n\n# Configure Multitenancy\n\n\n\n**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.', '## Setting up multitenancy\n\n\n\n**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called [multitenancy](https://en.wikipedia.org/wiki/Multitenancy). It is efficient for most of users, but it requires additional configuration. [Learn how to set it up](../../tutorials/multiple-partitions/)', 'A clear use-case for this feature is managing a multi-tenant collection, where each tenant (let it be a user or organization) is assumed to be segregated, so they can have their data stored in separate shards.\n\n\n\nTo enable user-defined sharding, set `sharding_method` to `custom` during collection creation:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""shard_number"": 1,\n\n    ""sharding_method"": ""custom""\n\n    // ... other collection parameters\n\n}\n\n```\n\n\n\n```python', 'In addition to metrics and vector size, each collection uses its own set of parameters that controls collection optimization, index construction, and vacuum.\n\nThese settings can be changed at any time by a corresponding request.\n\n\n\n## Setting up multitenancy', '**Figure 3:** Users can query the collection based on two filters: the `group_id` and the individual `shard_key_selector`. This gives your data two additional levels of isolation.\n\n![Qdrant Multitenancy](/articles_data/multitenancy/multitenancy.png)\n\n\n\n## Create custom shards for a single collection']","Question:  What is the recommended approach for configuring multitenancy in terms of the number of collections to create?\n\nContext: ---

title: Multitenancy

weight: 12

aliases:

- ../tutorials/multiple-partitions

---

# Configure Multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.
## Setting up multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called [multitenancy](https://en.wikipedia.org/wiki/Multitenancy). It is efficient for most of users, but it requires additional configuration. [Learn how to set it up](../../tutorials/multiple-partitions/)
A clear use-case for this feature is managing a multi-tenant collection, where each tenant (let it be a user or organization) is assumed to be segregated, so they can have their data stored in separate shards.



To enable user-defined sharding, set `sharding_method` to `custom` during collection creation:



```http

PUT /collections/{collection_name}

{

""shard_number"": 1,

""sharding_method"": ""custom""

// ... other collection parameters

}

```



```python
In addition to metrics and vector size, each collection uses its own set of parameters that controls collection optimization, index construction, and vacuum.

These settings can be changed at any time by a corresponding request.



## Setting up multitenancy
**Figure 3:** Users can query the collection based on two filters: the `group_id` and the individual `shard_key_selector`. This gives your data two additional levels of isolation.

![Qdrant Multitenancy](/articles_data/multitenancy/multitenancy.png)



## Create custom shards for a single collection\n\nAnswer:", What is the recommended approach for configuring multitenancy in terms of the number of collections to create?,"In most cases, the recommended approach for configuring multitenancy is to create only a single collection with payload-based partitioning. This approach, known as multitenancy, is efficient for the majority of users. By using a single collection with payload-based partitioning, each user can access their own vectors while ensuring that they cannot see the vectors of other users. However, it is important to note that this approach requires additional configuration to set up properly.","['---\n\ntitle: Multitenancy\n\nweight: 12\n\naliases:\n\n  - ../tutorials/multiple-partitions\n\n---\n\n# Configure Multitenancy\n\n\n\n**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.', '## Setting up multitenancy\n\n\n\n**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called [multitenancy](https://en.wikipedia.org/wiki/Multitenancy). It is efficient for most of users, but it requires additional configuration. [Learn how to set it up](../../tutorials/multiple-partitions/)', 'A clear use-case for this feature is managing a multi-tenant collection, where each tenant (let it be a user or organization) is assumed to be segregated, so they can have their data stored in separate shards.\n\n\n\nTo enable user-defined sharding, set `sharding_method` to `custom` during collection creation:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""shard_number"": 1,\n\n    ""sharding_method"": ""custom""\n\n    // ... other collection parameters\n\n}\n\n```\n\n\n\n```python', 'In addition to metrics and vector size, each collection uses its own set of parameters that controls collection optimization, index construction, and vacuum.\n\nThese settings can be changed at any time by a corresponding request.\n\n\n\n## Setting up multitenancy', '**Figure 3:** Users can query the collection based on two filters: the `group_id` and the individual `shard_key_selector`. This gives your data two additional levels of isolation.\n\n![Qdrant Multitenancy](/articles_data/multitenancy/multitenancy.png)\n\n\n\n## Create custom shards for a single collection']","---

title: Multitenancy

weight: 12

aliases:

  - ../tutorials/multiple-partitions

---

# Configure Multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called multitenancy. It is efficient for most of users, but it requires additional configuration. This document will show you how to set it up.
## Setting up multitenancy



**How many collections should you create?** In most cases, you should only use a single collection with payload-based partitioning. This approach is called [multitenancy](https://en.wikipedia.org/wiki/Multitenancy). It is efficient for most of users, but it requires additional configuration. [Learn how to set it up](../../tutorials/multiple-partitions/)
A clear use-case for this feature is managing a multi-tenant collection, where each tenant (let it be a user or organization) is assumed to be segregated, so they can have their data stored in separate shards.



To enable user-defined sharding, set `sharding_method` to `custom` during collection creation:



```http

PUT /collections/{collection_name}

{

    ""shard_number"": 1,

    ""sharding_method"": ""custom""

    // ... other collection parameters

}

```



```python
In addition to metrics and vector size, each collection uses its own set of parameters that controls collection optimization, index construction, and vacuum.

These settings can be changed at any time by a corresponding request.



## Setting up multitenancy
**Figure 3:** Users can query the collection based on two filters: the `group_id` and the individual `shard_key_selector`. This gives your data two additional levels of isolation.

![Qdrant Multitenancy](/articles_data/multitenancy/multitenancy.png)



## Create custom shards for a single collection",0.39316239316239315,0.6571428571428571,0.49197860962566836,0.21551724137931033,0.36231884057971014,0.2702702702702703,0.28205128205128205,0.4714285714285714,0.35294117647058826,0.38461538461538464,0.6428571428571429,0.4812834224598931,0.7933565378189087,0.6817683577537537,0.6971911191940308,0.6893935203552246,0.9946903622573396,0.9829621726530604,123,1.662162162162162,0.649840740310824
True,,1833.717,907c4808,What is the purpose of the locking feature in Qdrant administration tools?,"The locking feature in Qdrant administration tools serves the purpose of restricting the possible operations on a Qdrant process. It allows users to control the behavior of a Qdrant instance at runtime without manually changing its configuration. When a lock is applied, specific operations such as creating new collections or adding new data to the existing storage can be disabled. It is important to note that the locking configuration is not persistent, so it needs to be reapplied after a restart. Additionally, the locking feature applies to a single node only, requiring the lock to be set on all desired nodes in a distributed deployment setup. The locking API in Qdrant enables users to manage and control the access and operations on the Qdrant instance effectively.","The purpose of the locking feature in Qdrant administration tools is to restrict the possible operations on a Qdrant process. This locking API allows users to control and limit the actions that can be performed on a Qdrant instance at runtime. It is important to note that the configuration is not persistent, so it is necessary to lock the instance again following a restart. This locking mechanism helps in maintaining the security and stability of the Qdrant process by preventing unauthorized or potentially harmful operations.","---

title: Administration

weight: 10

aliases:

  - ../administration

---



# Administration



Qdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.



## Locking



A locking API enables users to restrict the possible operations on a qdrant process.

It is important to mention that:

- The configuration is not persistent therefore it is necessary to lock again following a restart.
Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.
and once the queues are set up, there is no further syscall that would stop

any user thread.



Servers that use this can easily get to over 100k concurrent requests. Today

Linux allows asynchronous IO via io\_uring for network, disk and accessing

other ports, e.g. for printing or recording video.



## And what about Qdrant?



Qdrant can store everything in memory, but not all data sets may fit, which can

require storing  on disk. Before io\_uring, Qdrant used mmap to do its IO. This
To change it, access `config.yaml` and set `storage.temp_path` to another directory location.



## Important changes



The latest release focuses not only on the new features but also introduces some changes making 

Qdrant even more reliable.



### Optimizing group requests
The transition is even easier if you use the official client libraries. For example, the [Python Client](https://github.com/qdrant/qdrant-client) has the support of the API key already built-in, so you only need to provide it once, when the QdrantClient instance is created.



### Cluster configuration



Each instance comes pre-configured with the following tools, features and support services:



- Automatically created with the latest available version of Qdrant.","['---\n\ntitle: Administration\n\nweight: 10\n\naliases:\n\n  - ../administration\n\n---\n\n\n\n# Administration\n\n\n\nQdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.\n\n\n\n## Locking\n\n\n\nA locking API enables users to restrict the possible operations on a qdrant process.\n\nIt is important to mention that:\n\n- The configuration is not persistent therefore it is necessary to lock again following a restart.', 'Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.', 'and once the queues are set up, there is no further syscall that would stop\n\nany user thread.\n\n\n\nServers that use this can easily get to over 100k concurrent requests. Today\n\nLinux allows asynchronous IO via io\\_uring for network, disk and accessing\n\nother ports, e.g. for printing or recording video.\n\n\n\n## And what about Qdrant?\n\n\n\nQdrant can store everything in memory, but not all data sets may fit, which can\n\nrequire storing  on disk. Before io\\_uring, Qdrant used mmap to do its IO. This', 'To change it, access `config.yaml` and set `storage.temp_path` to another directory location.\n\n\n\n## Important changes\n\n\n\nThe latest release focuses not only on the new features but also introduces some changes making \n\nQdrant even more reliable.\n\n\n\n### Optimizing group requests', 'The transition is even easier if you use the official client libraries. For example, the [Python Client](https://github.com/qdrant/qdrant-client) has the support of the API key already built-in, so you only need to provide it once, when the QdrantClient instance is created.\n\n\n\n### Cluster configuration\n\n\n\nEach instance comes pre-configured with the following tools, features and support services:\n\n\n\n- Automatically created with the latest available version of Qdrant.']","Question: What is the purpose of the locking feature in Qdrant administration tools?\n\nContext: ---

title: Administration

weight: 10

aliases:

- ../administration

---



# Administration



Qdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.



## Locking



A locking API enables users to restrict the possible operations on a qdrant process.

It is important to mention that:

- The configuration is not persistent therefore it is necessary to lock again following a restart.
Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.
and once the queues are set up, there is no further syscall that would stop

any user thread.



Servers that use this can easily get to over 100k concurrent requests. Today

Linux allows asynchronous IO via io\_uring for network, disk and accessing

other ports, e.g. for printing or recording video.



## And what about Qdrant?



Qdrant can store everything in memory, but not all data sets may fit, which can

require storing  on disk. Before io\_uring, Qdrant used mmap to do its IO. This
To change it, access `config.yaml` and set `storage.temp_path` to another directory location.



## Important changes



The latest release focuses not only on the new features but also introduces some changes making 

Qdrant even more reliable.



### Optimizing group requests
The transition is even easier if you use the official client libraries. For example, the [Python Client](https://github.com/qdrant/qdrant-client) has the support of the API key already built-in, so you only need to provide it once, when the QdrantClient instance is created.



### Cluster configuration



Each instance comes pre-configured with the following tools, features and support services:



- Automatically created with the latest available version of Qdrant.\n\nAnswer:",What is the purpose of the locking feature in Qdrant administration tools?,"The locking feature in Qdrant administration tools serves the purpose of restricting the possible operations on a Qdrant process. It allows users to control the behavior of a Qdrant instance at runtime without manually changing its configuration. When a lock is applied, specific operations such as creating new collections or adding new data to the existing storage can be disabled. It is important to note that the locking configuration is not persistent, so it needs to be reapplied after a restart. Additionally, the locking feature applies to a single node only, requiring the lock to be set on all desired nodes in a distributed deployment setup. The locking API in Qdrant enables users to manage and control the access and operations on the Qdrant instance effectively.","['---\n\ntitle: Administration\n\nweight: 10\n\naliases:\n\n  - ../administration\n\n---\n\n\n\n# Administration\n\n\n\nQdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.\n\n\n\n## Locking\n\n\n\nA locking API enables users to restrict the possible operations on a qdrant process.\n\nIt is important to mention that:\n\n- The configuration is not persistent therefore it is necessary to lock again following a restart.', 'Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.', 'and once the queues are set up, there is no further syscall that would stop\n\nany user thread.\n\n\n\nServers that use this can easily get to over 100k concurrent requests. Today\n\nLinux allows asynchronous IO via io\\_uring for network, disk and accessing\n\nother ports, e.g. for printing or recording video.\n\n\n\n## And what about Qdrant?\n\n\n\nQdrant can store everything in memory, but not all data sets may fit, which can\n\nrequire storing  on disk. Before io\\_uring, Qdrant used mmap to do its IO. This', 'To change it, access `config.yaml` and set `storage.temp_path` to another directory location.\n\n\n\n## Important changes\n\n\n\nThe latest release focuses not only on the new features but also introduces some changes making \n\nQdrant even more reliable.\n\n\n\n### Optimizing group requests', 'The transition is even easier if you use the official client libraries. For example, the [Python Client](https://github.com/qdrant/qdrant-client) has the support of the API key already built-in, so you only need to provide it once, when the QdrantClient instance is created.\n\n\n\n### Cluster configuration\n\n\n\nEach instance comes pre-configured with the following tools, features and support services:\n\n\n\n- Automatically created with the latest available version of Qdrant.']","---

title: Administration

weight: 10

aliases:

  - ../administration

---



# Administration



Qdrant exposes administration tools which enable to modify at runtime the behavior of a qdrant instance without changing its configuration manually.



## Locking



A locking API enables users to restrict the possible operations on a qdrant process.

It is important to mention that:

- The configuration is not persistent therefore it is necessary to lock again following a restart.
Qdrant undertakes, in particular, to carry out regular checks to verify the operation and accessibility of the Solution. In this regard, Qdrant reserves the right to interrupt access to the Solution momentarily for reasons of maintenance. Similarly, Qdrant may not be held responsible for momentary difficulties or impossibilities in accessing the Solution and/or Website, the origin of which is external to it, “force majeure”, or which are due to disruptions in the telecommunications network.
and once the queues are set up, there is no further syscall that would stop

any user thread.



Servers that use this can easily get to over 100k concurrent requests. Today

Linux allows asynchronous IO via io\_uring for network, disk and accessing

other ports, e.g. for printing or recording video.



## And what about Qdrant?



Qdrant can store everything in memory, but not all data sets may fit, which can

require storing  on disk. Before io\_uring, Qdrant used mmap to do its IO. This
To change it, access `config.yaml` and set `storage.temp_path` to another directory location.



## Important changes



The latest release focuses not only on the new features but also introduces some changes making 

Qdrant even more reliable.



### Optimizing group requests
The transition is even easier if you use the official client libraries. For example, the [Python Client](https://github.com/qdrant/qdrant-client) has the support of the API key already built-in, so you only need to provide it once, when the QdrantClient instance is created.



### Cluster configuration



Each instance comes pre-configured with the following tools, features and support services:



- Automatically created with the latest available version of Qdrant.",0.7162162162162162,0.48623853211009177,0.5792349726775957,0.4246575342465753,0.28703703703703703,0.3425414364640884,0.5135135135135135,0.3486238532110092,0.41530054644808745,0.6756756756756757,0.45871559633027525,0.5464480874316939,0.9325500130653381,0.7596994638442993,0.7032665610313416,0.7303946018218994,0.9992331166140502,0.8308253674961937,85,0.6746031746031746,0.3940316440073073
True,,3107.154,d8897840,What is the significance of the `on_disk_payload` setting?,"The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes.","The significance of the `on_disk_payload` setting is that it determines where the payload data associated with vectors should be stored. When set to `true`, the payload data will be stored on disk only, which can be useful for limiting RAM usage, especially in cases where the payload data is large.

Storing payload data on disk can help conserve RAM resources, as large payloads may require a significant amount of memory if stored in RAM. By utilizing on-disk storage for payloads, the system can manage and access the data more efficiently, particularly when dealing with extensive payload values such as text abstracts or images.

To enable on-disk payload storage, the `on_disk_payload` parameter needs to be set to `true` during the creation or updating of a collection. Additionally, when using the HNSW indexing algorithm, the `hnsw_config.on_disk` parameter should also be set to `true` to take advantage of on-disk storage for indexing data.","* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.

* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.
InMemory payload storage is organized in the same way as in-memory vectors.

The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.

This type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.



In the case of large payload values, it might be better to use OnDisk payload storage.
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
To enable this, you need to set the `hnsw_config.on_disk` parameter to `true` during collection [creation](../collections/#create-a-collection) or [updating](../collections/#update-collection-parameters).



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""hnsw_config"": {

        ""on_disk"": true

    }

}

```



```python
""vectors_count"": 1068786,

        ""indexed_vectors_count"": 1024232,

        ""points_count"": 1068786,

        ""segments_count"": 31,

        ""config"": {

            ""params"": {

                ""vectors"": {

                    ""size"": 384,

                    ""distance"": ""Cosine""

                },

                ""shard_number"": 1,

                ""replication_factor"": 1,

                ""write_consistency_factor"": 1,

                ""on_disk_payload"": false

            },","['* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.\n\n* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\n\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.', 'InMemory payload storage is organized in the same way as in-memory vectors.\n\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\n\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\n\n\n\nIn the case of large payload values, it might be better to use OnDisk payload storage.', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', 'To enable this, you need to set the `hnsw_config.on_disk` parameter to `true` during collection [creation](../collections/#create-a-collection) or [updating](../collections/#update-collection-parameters).\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""hnsw_config"": {\n\n        ""on_disk"": true\n\n    }\n\n}\n\n```\n\n\n\n```python', '""vectors_count"": 1068786,\n\n        ""indexed_vectors_count"": 1024232,\n\n        ""points_count"": 1068786,\n\n        ""segments_count"": 31,\n\n        ""config"": {\n\n            ""params"": {\n\n                ""vectors"": {\n\n                    ""size"": 384,\n\n                    ""distance"": ""Cosine""\n\n                },\n\n                ""shard_number"": 1,\n\n                ""replication_factor"": 1,\n\n                ""write_consistency_factor"": 1,\n\n                ""on_disk_payload"": false\n\n            },']","Question: What is the significance of the `on_disk_payload` setting?\n\nContext: * `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.

* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.
InMemory payload storage is organized in the same way as in-memory vectors.

The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.

This type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.



In the case of large payload values, it might be better to use OnDisk payload storage.
""on_disk_payload"": false

},

""hnsw_config"": {

""m"": 16,

""ef_construct"": 100,

""full_scan_threshold"": 10000,

""max_indexing_threads"": 0

},

""optimizer_config"": {

""deleted_threshold"": 0.2,

""vacuum_min_vector_number"": 1000,

""default_segment_number"": 0,

""max_segment_size"": null,
To enable this, you need to set the `hnsw_config.on_disk` parameter to `true` during collection [creation](../collections/#create-a-collection) or [updating](../collections/#update-collection-parameters).



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""optimizers_config"": {

""memmap_threshold"": 20000

},

""hnsw_config"": {

""on_disk"": true

}

}

```



```python
""vectors_count"": 1068786,

""indexed_vectors_count"": 1024232,

""points_count"": 1068786,

""segments_count"": 31,

""config"": {

""params"": {

""vectors"": {

""size"": 384,

""distance"": ""Cosine""

},

""shard_number"": 1,

""replication_factor"": 1,

""write_consistency_factor"": 1,

""on_disk_payload"": false

},\n\nAnswer:",What is the significance of the `on_disk_payload` setting?,"The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes.","['* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.\n\n* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\n\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.', 'InMemory payload storage is organized in the same way as in-memory vectors.\n\nThe payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\n\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\n\n\n\nIn the case of large payload values, it might be better to use OnDisk payload storage.', '""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,', 'To enable this, you need to set the `hnsw_config.on_disk` parameter to `true` during collection [creation](../collections/#create-a-collection) or [updating](../collections/#update-collection-parameters).\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""hnsw_config"": {\n\n        ""on_disk"": true\n\n    }\n\n}\n\n```\n\n\n\n```python', '""vectors_count"": 1068786,\n\n        ""indexed_vectors_count"": 1024232,\n\n        ""points_count"": 1068786,\n\n        ""segments_count"": 31,\n\n        ""config"": {\n\n            ""params"": {\n\n                ""vectors"": {\n\n                    ""size"": 384,\n\n                    ""distance"": ""Cosine""\n\n                },\n\n                ""shard_number"": 1,\n\n                ""replication_factor"": 1,\n\n                ""write_consistency_factor"": 1,\n\n                ""on_disk_payload"": false\n\n            },']","* `shard_number` - which defines how many shards the collection should have. See [distributed deployment](../../guides/distributed_deployment#sharding) section for details.

* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.

* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.
InMemory payload storage is organized in the same way as in-memory vectors.

The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.

This type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.



In the case of large payload values, it might be better to use OnDisk payload storage.
""on_disk_payload"": false

            },

            ""hnsw_config"": {

                ""m"": 16,

                ""ef_construct"": 100,

                ""full_scan_threshold"": 10000,

                ""max_indexing_threads"": 0

            },

            ""optimizer_config"": {

                ""deleted_threshold"": 0.2,

                ""vacuum_min_vector_number"": 1000,

                ""default_segment_number"": 0,

                ""max_segment_size"": null,
To enable this, you need to set the `hnsw_config.on_disk` parameter to `true` during collection [creation](../collections/#create-a-collection) or [updating](../collections/#update-collection-parameters).



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""optimizers_config"": {

        ""memmap_threshold"": 20000

    },

    ""hnsw_config"": {

        ""on_disk"": true

    }

}

```



```python
""vectors_count"": 1068786,

        ""indexed_vectors_count"": 1024232,

        ""points_count"": 1068786,

        ""segments_count"": 31,

        ""config"": {

            ""params"": {

                ""vectors"": {

                    ""size"": 384,

                    ""distance"": ""Cosine""

                },

                ""shard_number"": 1,

                ""replication_factor"": 1,

                ""write_consistency_factor"": 1,

                ""on_disk_payload"": false

            },",0.32653061224489793,0.5106382978723404,0.3983402489626555,0.10273972602739725,0.16129032258064516,0.12552301255230125,0.19047619047619047,0.2978723404255319,0.23236514522821575,0.2789115646258503,0.43617021276595747,0.3402489626556016,0.8206744194030762,0.6030089855194092,0.6384258270263672,0.620212197303772,0.7645539690759809,0.4322589635848999,150,1.5151515151515151,0.4114894726753151
True,,3568.711,1e6fb4dd,What is the significance of the parameter `max_segment_size_kb` in the context of vector indexation?,"The `max_segment_size_kb` parameter in the context of vector indexation determines the maximum size, in kilobytes, that a segment can have. Segments larger than this specified size might lead to disproportionately long indexation times. Therefore, it is essential to limit the size of segments to optimize the indexation process. The choice of this parameter should be based on the priority between indexation speed and search speed. If indexation speed is more critical, the parameter should be set lower. Conversely, if search speed is more important, the parameter should be set higher. It is important to note that 1Kb is equivalent to 1 vector of size 256. If the `max_segment_size_kb` parameter is not explicitly set, it will be automatically selected based on the number of available CPUs.","In the context of vector indexation, the parameter `max_segment_size_kb` is significant because it determines the maximum size (in kilobytes) of vectors to store in-memory per segment. Segments larger than this threshold will be stored as read-only memmaped files. 

By adjusting the `max_segment_size_kb` parameter, you can control how vectors are stored and accessed during the indexing process. If search speed is a priority, setting a higher value for `max_segment_size_kb` can be beneficial as it allows more vectors to be stored in-memory, potentially improving search performance. 

However, it's important to consider the trade-offs, as storing larger segments in-memory can consume more resources. Additionally, if the threshold is exceeded, vectors will be stored as read-only memmaped files.

Overall, the `max_segment_size_kb` parameter plays a role in optimizing the balance between search speed and resource usage during vector indexation.","# If search speed is more important - make this parameter higher.

    # Note: 1Kb = 1 vector of size 256

    # If not set, will be automatically selected considering the number of available CPUs.

    max_segment_size_kb: null



    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.

    # Segments larger than this threshold will be stored as read-only memmaped file.

    # To enable memmap storage, lower the threshold

    # Note: 1Kb = 1 vector of size 256
# Segments larger than this threshold will be stored as read-only memmaped file.

    # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value.

    # To disable memmap storage, set this to `0`.

    # Note: 1Kb = 1 vector of size 256

    memmap_threshold_kb: 200000



    # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing
The following command enables indexing for segments that have more than 10000 kB of vectors stored:





```http

PATCH /collections/{collection_name}

{

    ""optimizers_config"": {

        ""indexing_threshold"": 10000

    }

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

  -H 'Content-Type: application/json' \

  --data-raw '{

    ""optimizers_config"": {

        ""indexing_threshold"": 10000

    }

  }'

```



```python

client.update_collection(
```text

memory_size = number_of_vectors * vector_dimension * 4 bytes * 1.5

```



Extra 50% is needed for metadata (indexes, point versions, etc.) as well as for temporary segments constructed during the optimization process.



If you need to have payloads along with the vectors, it is recommended to store it on the disc, and only keep [indexed fields](../../concepts/indexing/#payload-index) in RAM.
We're changing the `default_segment_number` to 5. Segment numbers influence the number of graph nodes in the underlying HNSW index, thereby indirectly influencing the memory efficiency. 



#### Next, we upload our vectors to this and then enable indexing: 



```python

batch_size = 10000

client.upload_collection(

    collection_name=collection_name,

    ids=range(len(dataset)),

    vectors=dataset[""openai""],

    payload=[

        {""text"": x} for x in dataset[""text""]

    ],

    parallel=10,

)","['# If search speed is more important - make this parameter higher.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    # If not set, will be automatically selected considering the number of available CPUs.\n\n    max_segment_size_kb: null\n\n\n\n    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.\n\n    # Segments larger than this threshold will be stored as read-only memmaped file.\n\n    # To enable memmap storage, lower the threshold\n\n    # Note: 1Kb = 1 vector of size 256', '# Segments larger than this threshold will be stored as read-only memmaped file.\n\n    # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value.\n\n    # To disable memmap storage, set this to `0`.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    memmap_threshold_kb: 200000\n\n\n\n    # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing', 'The following command enables indexing for segments that have more than 10000 kB of vectors stored:\n\n\n\n\n\n```http\n\nPATCH /collections/{collection_name}\n\n{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 10000\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 10000\n\n    }\n\n  }\'\n\n```\n\n\n\n```python\n\nclient.update_collection(', '```text\n\nmemory_size = number_of_vectors * vector_dimension * 4 bytes * 1.5\n\n```\n\n\n\nExtra 50% is needed for metadata (indexes, point versions, etc.) as well as for temporary segments constructed during the optimization process.\n\n\n\nIf you need to have payloads along with the vectors, it is recommended to store it on the disc, and only keep [indexed fields](../../concepts/indexing/#payload-index) in RAM.', 'We\'re changing the `default_segment_number` to 5. Segment numbers influence the number of graph nodes in the underlying HNSW index, thereby indirectly influencing the memory efficiency. \n\n\n\n#### Next, we upload our vectors to this and then enable indexing: \n\n\n\n```python\n\nbatch_size = 10000\n\nclient.upload_collection(\n\n    collection_name=collection_name,\n\n    ids=range(len(dataset)),\n\n    vectors=dataset[""openai""],\n\n    payload=[\n\n        {""text"": x} for x in dataset[""text""]\n\n    ],\n\n    parallel=10,\n\n)']","Question: What is the significance of the parameter `max_segment_size_kb` in the context of vector indexation?\n\nContext: # If search speed is more important - make this parameter higher.

# Note: 1Kb = 1 vector of size 256

# If not set, will be automatically selected considering the number of available CPUs.

max_segment_size_kb: null



# Maximum size (in KiloBytes) of vectors to store in-memory per segment.

# Segments larger than this threshold will be stored as read-only memmaped file.

# To enable memmap storage, lower the threshold

# Note: 1Kb = 1 vector of size 256
# Segments larger than this threshold will be stored as read-only memmaped file.

# Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value.

# To disable memmap storage, set this to `0`.

# Note: 1Kb = 1 vector of size 256

memmap_threshold_kb: 200000



# Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing
The following command enables indexing for segments that have more than 10000 kB of vectors stored:





```http

PATCH /collections/{collection_name}

{

""optimizers_config"": {

""indexing_threshold"": 10000

}

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

-H 'Content-Type: application/json' \

--data-raw '{

""optimizers_config"": {

""indexing_threshold"": 10000

}

}'

```



```python

client.update_collection(
```text

memory_size = number_of_vectors * vector_dimension * 4 bytes * 1.5

```



Extra 50% is needed for metadata (indexes, point versions, etc.) as well as for temporary segments constructed during the optimization process.



If you need to have payloads along with the vectors, it is recommended to store it on the disc, and only keep [indexed fields](../../concepts/indexing/#payload-index) in RAM.
We're changing the `default_segment_number` to 5. Segment numbers influence the number of graph nodes in the underlying HNSW index, thereby indirectly influencing the memory efficiency. 



#### Next, we upload our vectors to this and then enable indexing: 



```python

batch_size = 10000

client.upload_collection(

collection_name=collection_name,

ids=range(len(dataset)),

vectors=dataset[""openai""],

payload=[

{""text"": x} for x in dataset[""text""]

],

parallel=10,

)\n\nAnswer:",What is the significance of the parameter `max_segment_size_kb` in the context of vector indexation?,"The `max_segment_size_kb` parameter in the context of vector indexation determines the maximum size, in kilobytes, that a segment can have. Segments larger than this specified size might lead to disproportionately long indexation times. Therefore, it is essential to limit the size of segments to optimize the indexation process. The choice of this parameter should be based on the priority between indexation speed and search speed. If indexation speed is more critical, the parameter should be set lower. Conversely, if search speed is more important, the parameter should be set higher. It is important to note that 1Kb is equivalent to 1 vector of size 256. If the `max_segment_size_kb` parameter is not explicitly set, it will be automatically selected based on the number of available CPUs.","['# If search speed is more important - make this parameter higher.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    # If not set, will be automatically selected considering the number of available CPUs.\n\n    max_segment_size_kb: null\n\n\n\n    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.\n\n    # Segments larger than this threshold will be stored as read-only memmaped file.\n\n    # To enable memmap storage, lower the threshold\n\n    # Note: 1Kb = 1 vector of size 256', '# Segments larger than this threshold will be stored as read-only memmaped file.\n\n    # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value.\n\n    # To disable memmap storage, set this to `0`.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    memmap_threshold_kb: 200000\n\n\n\n    # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing', 'The following command enables indexing for segments that have more than 10000 kB of vectors stored:\n\n\n\n\n\n```http\n\nPATCH /collections/{collection_name}\n\n{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 10000\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 10000\n\n    }\n\n  }\'\n\n```\n\n\n\n```python\n\nclient.update_collection(', '```text\n\nmemory_size = number_of_vectors * vector_dimension * 4 bytes * 1.5\n\n```\n\n\n\nExtra 50% is needed for metadata (indexes, point versions, etc.) as well as for temporary segments constructed during the optimization process.\n\n\n\nIf you need to have payloads along with the vectors, it is recommended to store it on the disc, and only keep [indexed fields](../../concepts/indexing/#payload-index) in RAM.', 'We\'re changing the `default_segment_number` to 5. Segment numbers influence the number of graph nodes in the underlying HNSW index, thereby indirectly influencing the memory efficiency. \n\n\n\n#### Next, we upload our vectors to this and then enable indexing: \n\n\n\n```python\n\nbatch_size = 10000\n\nclient.upload_collection(\n\n    collection_name=collection_name,\n\n    ids=range(len(dataset)),\n\n    vectors=dataset[""openai""],\n\n    payload=[\n\n        {""text"": x} for x in dataset[""text""]\n\n    ],\n\n    parallel=10,\n\n)']","# If search speed is more important - make this parameter higher.

    # Note: 1Kb = 1 vector of size 256

    # If not set, will be automatically selected considering the number of available CPUs.

    max_segment_size_kb: null



    # Maximum size (in KiloBytes) of vectors to store in-memory per segment.

    # Segments larger than this threshold will be stored as read-only memmaped file.

    # To enable memmap storage, lower the threshold

    # Note: 1Kb = 1 vector of size 256
# Segments larger than this threshold will be stored as read-only memmaped file.

    # Memmap storage is disabled by default, to enable it, set this threshold to a reasonable value.

    # To disable memmap storage, set this to `0`.

    # Note: 1Kb = 1 vector of size 256

    memmap_threshold_kb: 200000



    # Maximum size (in kilobytes) of vectors allowed for plain index, exceeding this threshold will enable vector indexing
The following command enables indexing for segments that have more than 10000 kB of vectors stored:





```http

PATCH /collections/{collection_name}

{

    ""optimizers_config"": {

        ""indexing_threshold"": 10000

    }

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

  -H 'Content-Type: application/json' \

  --data-raw '{

    ""optimizers_config"": {

        ""indexing_threshold"": 10000

    }

  }'

```



```python

client.update_collection(
```text

memory_size = number_of_vectors * vector_dimension * 4 bytes * 1.5

```



Extra 50% is needed for metadata (indexes, point versions, etc.) as well as for temporary segments constructed during the optimization process.



If you need to have payloads along with the vectors, it is recommended to store it on the disc, and only keep [indexed fields](../../concepts/indexing/#payload-index) in RAM.
We're changing the `default_segment_number` to 5. Segment numbers influence the number of graph nodes in the underlying HNSW index, thereby indirectly influencing the memory efficiency. 



#### Next, we upload our vectors to this and then enable indexing: 



```python

batch_size = 10000

client.upload_collection(

    collection_name=collection_name,

    ids=range(len(dataset)),

    vectors=dataset[""openai""],

    payload=[

        {""text"": x} for x in dataset[""text""]

    ],

    parallel=10,

)",0.45390070921985815,0.5378151260504201,0.49230769230769234,0.2,0.23728813559322035,0.21705426356589147,0.24822695035460993,0.29411764705882354,0.2692307692307693,0.4326241134751773,0.5126050420168067,0.4692307692307692,0.9279860258102417,0.6214562654495239,0.6277743577957153,0.6245993375778198,0.7280963584780693,0.7461437918245792,135,1.08,0.2773286810793351
True,,3457.5330000000004,f1250136,What is the significance of the `indexing_threshold_kb` parameter?,"The `indexing_threshold_kb` parameter specifies the maximum size, in Kilobytes, allowed for vectors for plain index. The default value for this parameter is set to 20000 KB. It is important to note that 1 KB is equivalent to 1 vector of size 256. This parameter plays a crucial role in determining the maximum size of vectors that can be used for indexing. If the `indexing_threshold_kb` parameter is not explicitly set, the default value will be used. Additionally, setting this parameter to `0` will explicitly disable vector indexing. This parameter directly impacts the efficiency and performance of the indexing process for the given vectors.","The `indexing_threshold_kb` parameter is significant in determining when vector indexing should be enabled for segments that have vectors stored above a certain size threshold. In the provided context, the `indexing_threshold_kb` parameter is set to 20,000 KB by default. 

When the `indexing_threshold_kb` is set to a value greater than 0, it means that indexing will be enabled for segments that have more than the specified threshold of vectors stored. In the examples provided, you can see that the threshold is set to 10,000 KB in one case, and then reset to the default value of 20,000 KB in another case.

Setting the `indexing_threshold_kb` to 0 explicitly disables vector indexing, which can be useful for faster uploads of vectors and payloads without indexing overhead. Once the data is loaded, the indexing can be turned back on by setting the `indexing_threshold_kb` to a desired value greater than 0.

In summary, the `indexing_threshold_kb` parameter controls when vector indexing is enabled based on the size of vectors stored in segments, and setting it to 0 disables indexing for faster data uploads.","# To explicitly disable vector indexing, set to `0`.

    # If not set, the default value will be used.

    indexing_threshold_kb: 20000



    # Interval between forced flushes.

    flush_interval_sec: 5



    # Max number of threads, which can be used for optimization per collection.

    # Note: Each optimization thread will also use `max_indexing_threads` for index building.

    # So total number of threads used for optimization will be `max_optimization_threads * max_indexing_threads`
The following command enables indexing for segments that have more than 10000 kB of vectors stored:





```http

PATCH /collections/{collection_name}

{

    ""optimizers_config"": {

        ""indexing_threshold"": 10000

    }

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

  -H 'Content-Type: application/json' \

  --data-raw '{

    ""optimizers_config"": {

        ""indexing_threshold"": 10000

    }

  }'

```



```python

client.update_collection(
# Default value is 20,000, based on <https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md>.

    # To disable vector indexing, set to `0`.

    # Note: 1kB = 1 vector of size 256.

    indexing_threshold_kb: 20000

```



In addition to the configuration file, you can also set optimizer parameters separately for each [collection](../collections).
distance: ""Cosine"",

  },

  optimizers_config: {

    indexing_threshold: 0,

  },

});

```



After upload is done, you can enable indexing by setting `indexing_threshold` to a desired value (default is 20000):



```http

PATCH /collections/{collection_name}

{

    ""optimizers_config"": {

        ""indexing_threshold"": 20000

    }

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.update_collection(
default_segment_number=5,

        indexing_threshold=0,

    ),

    quantization_config=models.BinaryQuantization(

        binary=models.BinaryQuantizationConfig(always_ram=True),

    ),

)

```



#### What is happening in the OptimizerConfig? 



We're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded","['# To explicitly disable vector indexing, set to `0`.\n\n    # If not set, the default value will be used.\n\n    indexing_threshold_kb: 20000\n\n\n\n    # Interval between forced flushes.\n\n    flush_interval_sec: 5\n\n\n\n    # Max number of threads, which can be used for optimization per collection.\n\n    # Note: Each optimization thread will also use `max_indexing_threads` for index building.\n\n    # So total number of threads used for optimization will be `max_optimization_threads * max_indexing_threads`', 'The following command enables indexing for segments that have more than 10000 kB of vectors stored:\n\n\n\n\n\n```http\n\nPATCH /collections/{collection_name}\n\n{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 10000\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 10000\n\n    }\n\n  }\'\n\n```\n\n\n\n```python\n\nclient.update_collection(', '# Default value is 20,000, based on <https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md>.\n\n    # To disable vector indexing, set to `0`.\n\n    # Note: 1kB = 1 vector of size 256.\n\n    indexing_threshold_kb: 20000\n\n```\n\n\n\nIn addition to the configuration file, you can also set optimizer parameters separately for each [collection](../collections).', 'distance: ""Cosine"",\n\n  },\n\n  optimizers_config: {\n\n    indexing_threshold: 0,\n\n  },\n\n});\n\n```\n\n\n\nAfter upload is done, you can enable indexing by setting `indexing_threshold` to a desired value (default is 20000):\n\n\n\n```http\n\nPATCH /collections/{collection_name}\n\n{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 20000\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.update_collection(', ""default_segment_number=5,\n\n        indexing_threshold=0,\n\n    ),\n\n    quantization_config=models.BinaryQuantization(\n\n        binary=models.BinaryQuantizationConfig(always_ram=True),\n\n    ),\n\n)\n\n```\n\n\n\n#### What is happening in the OptimizerConfig? \n\n\n\nWe're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded""]","Question: What is the significance of the `indexing_threshold_kb` parameter?\n\nContext: # To explicitly disable vector indexing, set to `0`.

# If not set, the default value will be used.

indexing_threshold_kb: 20000



# Interval between forced flushes.

flush_interval_sec: 5



# Max number of threads, which can be used for optimization per collection.

# Note: Each optimization thread will also use `max_indexing_threads` for index building.

# So total number of threads used for optimization will be `max_optimization_threads * max_indexing_threads`
The following command enables indexing for segments that have more than 10000 kB of vectors stored:





```http

PATCH /collections/{collection_name}

{

""optimizers_config"": {

""indexing_threshold"": 10000

}

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

-H 'Content-Type: application/json' \

--data-raw '{

""optimizers_config"": {

""indexing_threshold"": 10000

}

}'

```



```python

client.update_collection(
# Default value is 20,000, based on <https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md>.

# To disable vector indexing, set to `0`.

# Note: 1kB = 1 vector of size 256.

indexing_threshold_kb: 20000

```



In addition to the configuration file, you can also set optimizer parameters separately for each [collection](../collections).
distance: ""Cosine"",

},

optimizers_config: {

indexing_threshold: 0,

},

});

```



After upload is done, you can enable indexing by setting `indexing_threshold` to a desired value (default is 20000):



```http

PATCH /collections/{collection_name}

{

""optimizers_config"": {

""indexing_threshold"": 20000

}

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.update_collection(
default_segment_number=5,

indexing_threshold=0,

),

quantization_config=models.BinaryQuantization(

binary=models.BinaryQuantizationConfig(always_ram=True),

),

)

```



#### What is happening in the OptimizerConfig? 



We're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded\n\nAnswer:",What is the significance of the `indexing_threshold_kb` parameter?,"The `indexing_threshold_kb` parameter specifies the maximum size, in Kilobytes, allowed for vectors for plain index. The default value for this parameter is set to 20000 KB. It is important to note that 1 KB is equivalent to 1 vector of size 256. This parameter plays a crucial role in determining the maximum size of vectors that can be used for indexing. If the `indexing_threshold_kb` parameter is not explicitly set, the default value will be used. Additionally, setting this parameter to `0` will explicitly disable vector indexing. This parameter directly impacts the efficiency and performance of the indexing process for the given vectors.","['# To explicitly disable vector indexing, set to `0`.\n\n    # If not set, the default value will be used.\n\n    indexing_threshold_kb: 20000\n\n\n\n    # Interval between forced flushes.\n\n    flush_interval_sec: 5\n\n\n\n    # Max number of threads, which can be used for optimization per collection.\n\n    # Note: Each optimization thread will also use `max_indexing_threads` for index building.\n\n    # So total number of threads used for optimization will be `max_optimization_threads * max_indexing_threads`', 'The following command enables indexing for segments that have more than 10000 kB of vectors stored:\n\n\n\n\n\n```http\n\nPATCH /collections/{collection_name}\n\n{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 10000\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 10000\n\n    }\n\n  }\'\n\n```\n\n\n\n```python\n\nclient.update_collection(', '# Default value is 20,000, based on <https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md>.\n\n    # To disable vector indexing, set to `0`.\n\n    # Note: 1kB = 1 vector of size 256.\n\n    indexing_threshold_kb: 20000\n\n```\n\n\n\nIn addition to the configuration file, you can also set optimizer parameters separately for each [collection](../collections).', 'distance: ""Cosine"",\n\n  },\n\n  optimizers_config: {\n\n    indexing_threshold: 0,\n\n  },\n\n});\n\n```\n\n\n\nAfter upload is done, you can enable indexing by setting `indexing_threshold` to a desired value (default is 20000):\n\n\n\n```http\n\nPATCH /collections/{collection_name}\n\n{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 20000\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.update_collection(', ""default_segment_number=5,\n\n        indexing_threshold=0,\n\n    ),\n\n    quantization_config=models.BinaryQuantization(\n\n        binary=models.BinaryQuantizationConfig(always_ram=True),\n\n    ),\n\n)\n\n```\n\n\n\n#### What is happening in the OptimizerConfig? \n\n\n\nWe're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded""]","# To explicitly disable vector indexing, set to `0`.

    # If not set, the default value will be used.

    indexing_threshold_kb: 20000



    # Interval between forced flushes.

    flush_interval_sec: 5



    # Max number of threads, which can be used for optimization per collection.

    # Note: Each optimization thread will also use `max_indexing_threads` for index building.

    # So total number of threads used for optimization will be `max_optimization_threads * max_indexing_threads`
The following command enables indexing for segments that have more than 10000 kB of vectors stored:





```http

PATCH /collections/{collection_name}

{

    ""optimizers_config"": {

        ""indexing_threshold"": 10000

    }

}

```



```bash

curl -X PATCH http://localhost:6333/collections/test_collection1 \

  -H 'Content-Type: application/json' \

  --data-raw '{

    ""optimizers_config"": {

        ""indexing_threshold"": 10000

    }

  }'

```



```python

client.update_collection(
# Default value is 20,000, based on <https://github.com/google-research/google-research/blob/master/scann/docs/algorithms.md>.

    # To disable vector indexing, set to `0`.

    # Note: 1kB = 1 vector of size 256.

    indexing_threshold_kb: 20000

```



In addition to the configuration file, you can also set optimizer parameters separately for each [collection](../collections).
distance: ""Cosine"",

  },

  optimizers_config: {

    indexing_threshold: 0,

  },

});

```



After upload is done, you can enable indexing by setting `indexing_threshold` to a desired value (default is 20000):



```http

PATCH /collections/{collection_name}

{

    ""optimizers_config"": {

        ""indexing_threshold"": 20000

    }

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.update_collection(
default_segment_number=5,

        indexing_threshold=0,

    ),

    quantization_config=models.BinaryQuantization(

        binary=models.BinaryQuantizationConfig(always_ram=True),

    ),

)

```



#### What is happening in the OptimizerConfig? 



We're setting `indexing_threshold` to 0 i.e. disabling the indexing to zero. This allows faster uploads of vectors and payloads. We will turn it back on down below, once all the data is loaded",0.3448275862068966,0.625,0.4444444444444445,0.1329479768786127,0.24210526315789474,0.17164179104477612,0.21264367816091953,0.3854166666666667,0.2740740740740741,0.3218390804597701,0.5833333333333334,0.4148148148148148,0.8742189407348633,0.6336155533790588,0.6531636118888855,0.6432411074638367,0.8427578522823751,0.8596232967295995,176,1.7254901960784315,0.44322588116729594
True,,6483.5560000000005,18e0e1ec,How can you optimize Qdrant for minimizing latency in search requests?,"To optimize Qdrant for minimizing latency in search requests, you can set up the system to use as many cores as possible for a single request. This can be achieved by setting the number of segments in the collection to be equal to the number of cores in the system. By doing this, each segment will be processed in parallel, leading to a faster final result. This approach allows for the efficient utilization of system resources and can significantly reduce the time taken from the moment a request is submitted to the moment a response is received. By optimizing for latency in this manner, you can enhance the overall speed and responsiveness of the search functionality in Qdrant.","To optimize Qdrant for minimizing latency in search requests, consider implementing the following strategies:

1. Utilize Multiple Cores: Set up Qdrant to use as many cores as possible for a single request by configuring the number of segments in the collection to be equal to the number of cores in the system. This will allow each segment to be processed in parallel, resulting in faster search results.

2. Shard Selection: When performing a search, specify a shard or multiple shards to direct Qdrant to the relevant machines in the cluster. By doing so, unnecessary requests to all machines can be avoided, reducing overhead and enhancing performance.

3. Batch Search Feature: Take advantage of the batch search feature in Qdrant, which enables handling multiple search requests efficiently with a single API call. This feature optimizes performance by minimizing network overhead and improving search speed.

4. ARM Support: Qdrant includes built-in ARM support, allowing for sequential and concurrent flow optimization. This feature ensures that relevant data is efficiently retrieved from cache, minimizing latency in case of cache misses.

5. Disable Rescoring: In systems with high disk latency, consider disabling the rescore feature to improve search speed. This can be achieved by setting the `rescore` parameter to false in the search query.

By implementing these strategies and leveraging the features provided by Qdrant, you can optimize the system for minimizing latency in search requests and improving overall performance.","Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. 



### Common use cases
Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows 

you to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way 

possible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead

or slow performance.



## Built-in ARM support
![sequential vs. concurrent flow](/articles_data/search-as-you-type/sayt_concurrency.png)



While this means more load for the Qdrant vector search engine, this is not the limiting factor. The relevant data is already in cache in many cases, so the overhead stays within acceptable bounds, and the maximum latency in case of prefix cache misses is measurably reduced.



The code is available on the [Qdrant github](https://github.com/qdrant/page-search)
In a system with high disk latency, the re-scoring step may become a bottleneck.



Consider disabling `rescore` to improve the search speed:



```http

POST /collections/{collection_name}/points/search

{

    ""params"": {

        ""quantization"": {

            ""rescore"": false

        }

    },

    ""vector"": [0.2, 0.1, 0.9, 0.7],

    ""limit"": 10

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.search(","['Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.\n\n\n\nTo prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{', 'This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. \n\n\n\n### Common use cases', 'Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows \n\nyou to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way \n\npossible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead\n\nor slow performance.\n\n\n\n## Built-in ARM support', '![sequential vs. concurrent flow](/articles_data/search-as-you-type/sayt_concurrency.png)\n\n\n\nWhile this means more load for the Qdrant vector search engine, this is not the limiting factor. The relevant data is already in cache in many cases, so the overhead stays within acceptable bounds, and the maximum latency in case of prefix cache misses is measurably reduced.\n\n\n\nThe code is available on the [Qdrant github](https://github.com/qdrant/page-search)', 'In a system with high disk latency, the re-scoring step may become a bottleneck.\n\n\n\nConsider disabling `rescore` to improve the search speed:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{\n\n    ""params"": {\n\n        ""quantization"": {\n\n            ""rescore"": false\n\n        }\n\n    },\n\n    ""vector"": [0.2, 0.1, 0.9, 0.7],\n\n    ""limit"": 10\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.search(']","Question: How can you optimize Qdrant for minimizing latency in search requests?\n\nContext: Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. 



### Common use cases
Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows 

you to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way 

possible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead

or slow performance.



## Built-in ARM support
![sequential vs. concurrent flow](/articles_data/search-as-you-type/sayt_concurrency.png)



While this means more load for the Qdrant vector search engine, this is not the limiting factor. The relevant data is already in cache in many cases, so the overhead stays within acceptable bounds, and the maximum latency in case of prefix cache misses is measurably reduced.



The code is available on the [Qdrant github](https://github.com/qdrant/page-search)
In a system with high disk latency, the re-scoring step may become a bottleneck.



Consider disabling `rescore` to improve the search speed:



```http

POST /collections/{collection_name}/points/search

{

""params"": {

""quantization"": {

""rescore"": false

}

},

""vector"": [0.2, 0.1, 0.9, 0.7],

""limit"": 10

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.search(\n\nAnswer:",How can you optimize Qdrant for minimizing latency in search requests?,"To optimize Qdrant for minimizing latency in search requests, you can set up the system to use as many cores as possible for a single request. This can be achieved by setting the number of segments in the collection to be equal to the number of cores in the system. By doing this, each segment will be processed in parallel, leading to a faster final result. This approach allows for the efficient utilization of system resources and can significantly reduce the time taken from the moment a request is submitted to the moment a response is received. By optimizing for latency in this manner, you can enhance the overall speed and responsiveness of the search functionality in Qdrant.","['Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.\n\n\n\nTo prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{', 'This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. \n\n\n\n### Common use cases', 'Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows \n\nyou to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way \n\npossible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead\n\nor slow performance.\n\n\n\n## Built-in ARM support', '![sequential vs. concurrent flow](/articles_data/search-as-you-type/sayt_concurrency.png)\n\n\n\nWhile this means more load for the Qdrant vector search engine, this is not the limiting factor. The relevant data is already in cache in many cases, so the overhead stays within acceptable bounds, and the maximum latency in case of prefix cache misses is measurably reduced.\n\n\n\nThe code is available on the [Qdrant github](https://github.com/qdrant/page-search)', 'In a system with high disk latency, the re-scoring step may become a bottleneck.\n\n\n\nConsider disabling `rescore` to improve the search speed:\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/search\n\n{\n\n    ""params"": {\n\n        ""quantization"": {\n\n            ""rescore"": false\n\n        }\n\n    },\n\n    ""vector"": [0.2, 0.1, 0.9, 0.7],\n\n    ""limit"": 10\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.search(']","Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. 



### Common use cases
Fortunately, the [new batch search feature](https://blog.qdrant.tech/batch-vector-search-with-qdrant-8c4d598179d5) allows 

you to avoid this issue. With just one API call, Qdrant will handle multiple search requests in the most efficient way 

possible. This means that you can perform multiple tasks simultaneously without having to worry about network overhead

or slow performance.



## Built-in ARM support
![sequential vs. concurrent flow](/articles_data/search-as-you-type/sayt_concurrency.png)



While this means more load for the Qdrant vector search engine, this is not the limiting factor. The relevant data is already in cache in many cases, so the overhead stays within acceptable bounds, and the maximum latency in case of prefix cache misses is measurably reduced.



The code is available on the [Qdrant github](https://github.com/qdrant/page-search)
In a system with high disk latency, the re-scoring step may become a bottleneck.



Consider disabling `rescore` to improve the search speed:



```http

POST /collections/{collection_name}/points/search

{

    ""params"": {

        ""quantization"": {

            ""rescore"": false

        }

    },

    ""vector"": [0.2, 0.1, 0.9, 0.7],

    ""limit"": 10

}

```



```python

from qdrant_client import QdrantClient, models



client = QdrantClient(""localhost"", port=6333)



client.search(",0.3881278538812785,0.8252427184466019,0.5279503105590062,0.20642201834862386,0.4411764705882353,0.28125,0.2694063926940639,0.5728155339805825,0.3664596273291925,0.365296803652968,0.7766990291262136,0.4968944099378882,0.9241885542869568,0.6496302485466003,0.7481526136398315,0.6954193115234375,0.9724212862466547,0.9848054482601583,235,1.9915254237288136,0.5227393501536624
True,,4312.523999999999,f6acd45e,How can Qdrant be configured to optimize throughput by utilizing multiple cores for parallel processing?,"To optimize throughput in Qdrant and utilize multiple cores for parallel processing, you can configure the system to use minimal number of segments, typically set to 2. By setting the default_segment_number to 2 in the optimizers_config section of the PUT request for creating a collection, you can instruct Qdrant to use a smaller number of segments. This approach benefits from the size of the index and reduces the overall number of vector comparisons needed to find the nearest neighbors. However, it is important to note that larger segments require more time to build the index. By adjusting this configuration parameter, you can strike a balance between maximizing throughput and optimizing the efficiency of vector search operations in Qdrant.","To configure Qdrant to optimize throughput by utilizing multiple cores for parallel processing, you can follow these steps:

1. Configure Qdrant to use minimal number of segments, usually 2. Large segments benefit from the index size but require more time to build the index.

2. Set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

3. When optimizing for minimizing latency, set the number of segments in the collection to be equal to the number of cores in the system. Each segment will be processed in parallel, leading to faster results.

4. Benchmark internally to understand performance on your specific setup and identify bottlenecks. Collect information about how Qdrant is used to make improvements.

5. Collect timings and counters of various pieces of code to optimize internal heuristics for better performance.

6. Consider batch searching and distributed deployment for further improvements. Batch searching can save time, and distributed deployment reduces inter-cluster requests.

7. Utilize quantization (scalar or product-based) optimizations offered by Qdrant for efficient processing.

By following these steps and fine-tuning the configuration based on your specific use case, hardware, and workload patterns, you can optimize throughput by leveraging multiple cores for parallel processing in Qdrant.",");

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,
Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.



In order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.



Additionally, Qdrant uses a bunch of internal heuristics to optimize the performance.

To better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.

With this information, we can make Qdrant faster for everyone.
The results you may achieve on a specific setup may vary depending on the hardware, however, at the first glance, it seems that batch searching may save you quite a lot of time.



Additional improvements could be achieved in the case of distributed deployment, as Qdrant won’t need to make extensive inter-cluster requests. Moreover, if your requests share the same filtering condition, the query optimizer would be able to reuse it among batch requests.



## Summary
led to some modest overhead in case of disk latency. The kernel may

stop a user thread trying to access a mapped region, which incurs some context

switching overhead plus the wait time until the disk IO is finished. Ultimately,

this works very well with the asynchronous nature of Qdrant's core.



One of the great optimizations Qdrant offers is quantization (either

[scalar](https://qdrant.tech/articles/scalar-quantization/) or 

[product](https://qdrant.tech/articles/product-quantization/)-based).","[');\n\n```\n\n\n\nTo prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.\n\nTo do that, you can configure qdrant to use minimal number of segments, which is usually 2.\n\nLarge segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,', 'Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.\n\n\n\nTo prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{', 'We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.\n\n\n\nIn order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.\n\n\n\nAdditionally, Qdrant uses a bunch of internal heuristics to optimize the performance.\n\nTo better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.\n\nWith this information, we can make Qdrant faster for everyone.', 'The results you may achieve on a specific setup may vary depending on the hardware, however, at the first glance, it seems that batch searching may save you quite a lot of time.\n\n\n\nAdditional improvements could be achieved in the case of distributed deployment, as Qdrant won’t need to make extensive inter-cluster requests. Moreover, if your requests share the same filtering condition, the query optimizer would be able to reuse it among batch requests.\n\n\n\n## Summary', ""led to some modest overhead in case of disk latency. The kernel may\n\nstop a user thread trying to access a mapped region, which incurs some context\n\nswitching overhead plus the wait time until the disk IO is finished. Ultimately,\n\nthis works very well with the asynchronous nature of Qdrant's core.\n\n\n\nOne of the great optimizations Qdrant offers is quantization (either\n\n[scalar](https://qdrant.tech/articles/scalar-quantization/) or \n\n[product](https://qdrant.tech/articles/product-quantization/)-based).""]","Question: How can Qdrant be configured to optimize throughput by utilizing multiple cores for parallel processing?\n\nContext: );

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,
Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.



In order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.



Additionally, Qdrant uses a bunch of internal heuristics to optimize the performance.

To better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.

With this information, we can make Qdrant faster for everyone.
The results you may achieve on a specific setup may vary depending on the hardware, however, at the first glance, it seems that batch searching may save you quite a lot of time.



Additional improvements could be achieved in the case of distributed deployment, as Qdrant won’t need to make extensive inter-cluster requests. Moreover, if your requests share the same filtering condition, the query optimizer would be able to reuse it among batch requests.



## Summary
led to some modest overhead in case of disk latency. The kernel may

stop a user thread trying to access a mapped region, which incurs some context

switching overhead plus the wait time until the disk IO is finished. Ultimately,

this works very well with the asynchronous nature of Qdrant's core.



One of the great optimizations Qdrant offers is quantization (either

[scalar](https://qdrant.tech/articles/scalar-quantization/) or 

[product](https://qdrant.tech/articles/product-quantization/)-based).\n\nAnswer:",How can Qdrant be configured to optimize throughput by utilizing multiple cores for parallel processing?,"To optimize throughput in Qdrant and utilize multiple cores for parallel processing, you can configure the system to use minimal number of segments, typically set to 2. By setting the default_segment_number to 2 in the optimizers_config section of the PUT request for creating a collection, you can instruct Qdrant to use a smaller number of segments. This approach benefits from the size of the index and reduces the overall number of vector comparisons needed to find the nearest neighbors. However, it is important to note that larger segments require more time to build the index. By adjusting this configuration parameter, you can strike a balance between maximizing throughput and optimizing the efficiency of vector search operations in Qdrant.","[');\n\n```\n\n\n\nTo prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.\n\nTo do that, you can configure qdrant to use minimal number of segments, which is usually 2.\n\nLarge segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,', 'Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.\n\n\n\nTo prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\\.\n\nYou can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{', 'We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.\n\n\n\nIn order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.\n\n\n\nAdditionally, Qdrant uses a bunch of internal heuristics to optimize the performance.\n\nTo better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.\n\nWith this information, we can make Qdrant faster for everyone.', 'The results you may achieve on a specific setup may vary depending on the hardware, however, at the first glance, it seems that batch searching may save you quite a lot of time.\n\n\n\nAdditional improvements could be achieved in the case of distributed deployment, as Qdrant won’t need to make extensive inter-cluster requests. Moreover, if your requests share the same filtering condition, the query optimizer would be able to reuse it among batch requests.\n\n\n\n## Summary', ""led to some modest overhead in case of disk latency. The kernel may\n\nstop a user thread trying to access a mapped region, which incurs some context\n\nswitching overhead plus the wait time until the disk IO is finished. Ultimately,\n\nthis works very well with the asynchronous nature of Qdrant's core.\n\n\n\nOne of the great optimizations Qdrant offers is quantization (either\n\n[scalar](https://qdrant.tech/articles/scalar-quantization/) or \n\n[product](https://qdrant.tech/articles/product-quantization/)-based).""]",");

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,
Those approaches are not mutually exclusive, but in some cases it might be preferable to optimize for one or another.



To prefer minimizing latency, you can set up Qdrant to use as many cores as possible for a single request\.

You can do this by setting the number of segments in the collection to be equal to the number of cores in the system. In this case, each segment will be processed in parallel, and the final result will be obtained faster.



```http

PUT /collections/{collection_name}

{
We do a lot of benchmarking internally, but it is impossible to cover all possible use cases, hardware, and configurations.



In order to identify bottlenecks and improve Qdrant, we need to collect information about how it is used.



Additionally, Qdrant uses a bunch of internal heuristics to optimize the performance.

To better set up parameters for these heuristics, we need to collect timings and counters of various pieces of code.

With this information, we can make Qdrant faster for everyone.
The results you may achieve on a specific setup may vary depending on the hardware, however, at the first glance, it seems that batch searching may save you quite a lot of time.



Additional improvements could be achieved in the case of distributed deployment, as Qdrant won’t need to make extensive inter-cluster requests. Moreover, if your requests share the same filtering condition, the query optimizer would be able to reuse it among batch requests.



## Summary
led to some modest overhead in case of disk latency. The kernel may

stop a user thread trying to access a mapped region, which incurs some context

switching overhead plus the wait time until the disk IO is finished. Ultimately,

this works very well with the asynchronous nature of Qdrant's core.



One of the great optimizations Qdrant offers is quantization (either

[scalar](https://qdrant.tech/articles/scalar-quantization/) or 

[product](https://qdrant.tech/articles/product-quantization/)-based).",0.3686868686868687,0.6759259259259259,0.477124183006536,0.13705583756345177,0.2523364485981308,0.1776315789473684,0.21212121212121213,0.3888888888888889,0.27450980392156865,0.32323232323232326,0.5925925925925926,0.41830065359477125,0.8272830843925476,0.5986601710319519,0.6404414772987366,0.6188464164733887,0.6561514867723666,0.9138417972571915,202,1.7118644067796611,0.48362539731824816
True,,3337.792,86b19dcb,How can one enable distributed deployment mode in Qdrant?,"To enable distributed deployment mode in Qdrant, one needs to enable the cluster mode in the configuration or set the ENV variable `QDRANT__CLUSTER__ENABLED=true`. This allows multiple Qdrant services to communicate with each other, distributing data across peers to extend storage capabilities and increase stability. In the configuration file, under the `cluster` section, set `enabled: true` to run Qdrant in distributed deployment mode. Additionally, configure the internal communication port between peers using the `p2p` section with the specified port number. Furthermore, adjust the parameters related to the distributed consensus algorithm as needed for optimal performance.","To enable distributed deployment mode in Qdrant, you can follow these steps:

1. Update the configuration file: Set `enabled: true` under the `cluster` section in the Qdrant configuration file.

2. Alternatively, you can use the environment variable `QDRANT__CLUSTER__ENABLED=true` to enable distributed deployment mode.

3. Ensure that the inter-cluster communication port is properly configured (default port is 6335).

4. After enabling distributed deployment mode, make sure to use performant persistent storage for your data, configure security settings, set up Qdrant on multiple nodes for high availability, deploy a load balancer for the cluster, and establish a backup and disaster recovery strategy for your data.

By following these steps, you can enable distributed deployment mode in Qdrant and leverage its capabilities for horizontally scaling your data storage and improving cluster performance and reliability.","To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.



```yaml

cluster:

  # Use `enabled: true` to run Qdrant in distributed deployment mode

  enabled: true

  # Configuration of the inter-cluster communication

  p2p:

    # Port for internal communication between peers

    port: 6335



  # Configuration related to distributed consensus algorithm

  consensus:
---

title: Distributed Deployment

weight: 100

aliases:

  - ../distributed_deployment

---



# Distributed deployment



Since version v0.8.0 Qdrant supports a distributed deployment mode.

In this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.
setup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale 

both the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle 

large amounts of data without sacrificing performance or reliability.



## Administration API



Another new feature is the administration API, which allows you to disable write operations to the service. This is
#

  # If you enable this you should also enable TLS.

  # (Either above or via an external service like nginx.)

  # Sending an api-key over an unencrypted channel is insecure.

  #

  # Uncomment to enable.

  # read_only_api_key: your_secret_read_only_api_key_here



cluster:

  # Use `enabled: true` to run Qdrant in distributed deployment mode

  enabled: false



  # Configuration of the inter-cluster communication

  p2p:

    # Port for internal communication between peers

    port: 6335
In addition, you have to make sure:



* To use a performant [persistent storage](#storage) for your data

* To configure the [security settings](/documentation/guides/security/) for your deployment

* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)

* To set up a load balancer for your Qdrant cluster

* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data","['To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.\n\n\n\n```yaml\n\ncluster:\n\n  # Use `enabled: true` to run Qdrant in distributed deployment mode\n\n  enabled: true\n\n  # Configuration of the inter-cluster communication\n\n  p2p:\n\n    # Port for internal communication between peers\n\n    port: 6335\n\n\n\n  # Configuration related to distributed consensus algorithm\n\n  consensus:', '---\n\ntitle: Distributed Deployment\n\nweight: 100\n\naliases:\n\n  - ../distributed_deployment\n\n---\n\n\n\n# Distributed deployment\n\n\n\nSince version v0.8.0 Qdrant supports a distributed deployment mode.\n\nIn this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.', 'setup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale \n\nboth the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle \n\nlarge amounts of data without sacrificing performance or reliability.\n\n\n\n## Administration API\n\n\n\nAnother new feature is the administration API, which allows you to disable write operations to the service. This is', '#\n\n  # If you enable this you should also enable TLS.\n\n  # (Either above or via an external service like nginx.)\n\n  # Sending an api-key over an unencrypted channel is insecure.\n\n  #\n\n  # Uncomment to enable.\n\n  # read_only_api_key: your_secret_read_only_api_key_here\n\n\n\ncluster:\n\n  # Use `enabled: true` to run Qdrant in distributed deployment mode\n\n  enabled: false\n\n\n\n  # Configuration of the inter-cluster communication\n\n  p2p:\n\n    # Port for internal communication between peers\n\n    port: 6335', 'In addition, you have to make sure:\n\n\n\n* To use a performant [persistent storage](#storage) for your data\n\n* To configure the [security settings](/documentation/guides/security/) for your deployment\n\n* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)\n\n* To set up a load balancer for your Qdrant cluster\n\n* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data']","Question: How can one enable distributed deployment mode in Qdrant?\n\nContext: To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.



```yaml

cluster:

# Use `enabled: true` to run Qdrant in distributed deployment mode

enabled: true

# Configuration of the inter-cluster communication

p2p:

# Port for internal communication between peers

port: 6335



# Configuration related to distributed consensus algorithm

consensus:
---

title: Distributed Deployment

weight: 100

aliases:

- ../distributed_deployment

---



# Distributed deployment



Since version v0.8.0 Qdrant supports a distributed deployment mode.

In this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.
setup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale 

both the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle 

large amounts of data without sacrificing performance or reliability.



## Administration API



Another new feature is the administration API, which allows you to disable write operations to the service. This is
#

# If you enable this you should also enable TLS.

# (Either above or via an external service like nginx.)

# Sending an api-key over an unencrypted channel is insecure.

#

# Uncomment to enable.

# read_only_api_key: your_secret_read_only_api_key_here



cluster:

# Use `enabled: true` to run Qdrant in distributed deployment mode

enabled: false



# Configuration of the inter-cluster communication

p2p:

# Port for internal communication between peers

port: 6335
In addition, you have to make sure:



* To use a performant [persistent storage](#storage) for your data

* To configure the [security settings](/documentation/guides/security/) for your deployment

* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)

* To set up a load balancer for your Qdrant cluster

* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data\n\nAnswer:",How can one enable distributed deployment mode in Qdrant?,"To enable distributed deployment mode in Qdrant, one needs to enable the cluster mode in the configuration or set the ENV variable `QDRANT__CLUSTER__ENABLED=true`. This allows multiple Qdrant services to communicate with each other, distributing data across peers to extend storage capabilities and increase stability. In the configuration file, under the `cluster` section, set `enabled: true` to run Qdrant in distributed deployment mode. Additionally, configure the internal communication port between peers using the `p2p` section with the specified port number. Furthermore, adjust the parameters related to the distributed consensus algorithm as needed for optimal performance.","['To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.\n\n\n\n```yaml\n\ncluster:\n\n  # Use `enabled: true` to run Qdrant in distributed deployment mode\n\n  enabled: true\n\n  # Configuration of the inter-cluster communication\n\n  p2p:\n\n    # Port for internal communication between peers\n\n    port: 6335\n\n\n\n  # Configuration related to distributed consensus algorithm\n\n  consensus:', '---\n\ntitle: Distributed Deployment\n\nweight: 100\n\naliases:\n\n  - ../distributed_deployment\n\n---\n\n\n\n# Distributed deployment\n\n\n\nSince version v0.8.0 Qdrant supports a distributed deployment mode.\n\nIn this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.', 'setup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale \n\nboth the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle \n\nlarge amounts of data without sacrificing performance or reliability.\n\n\n\n## Administration API\n\n\n\nAnother new feature is the administration API, which allows you to disable write operations to the service. This is', '#\n\n  # If you enable this you should also enable TLS.\n\n  # (Either above or via an external service like nginx.)\n\n  # Sending an api-key over an unencrypted channel is insecure.\n\n  #\n\n  # Uncomment to enable.\n\n  # read_only_api_key: your_secret_read_only_api_key_here\n\n\n\ncluster:\n\n  # Use `enabled: true` to run Qdrant in distributed deployment mode\n\n  enabled: false\n\n\n\n  # Configuration of the inter-cluster communication\n\n  p2p:\n\n    # Port for internal communication between peers\n\n    port: 6335', 'In addition, you have to make sure:\n\n\n\n* To use a performant [persistent storage](#storage) for your data\n\n* To configure the [security settings](/documentation/guides/security/) for your deployment\n\n* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)\n\n* To set up a load balancer for your Qdrant cluster\n\n* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data']","To enable distributed deployment - enable the cluster mode in the [configuration](../configuration) or using the ENV variable: `QDRANT__CLUSTER__ENABLED=true`.



```yaml

cluster:

  # Use `enabled: true` to run Qdrant in distributed deployment mode

  enabled: true

  # Configuration of the inter-cluster communication

  p2p:

    # Port for internal communication between peers

    port: 6335



  # Configuration related to distributed consensus algorithm

  consensus:
---

title: Distributed Deployment

weight: 100

aliases:

  - ../distributed_deployment

---



# Distributed deployment



Since version v0.8.0 Qdrant supports a distributed deployment mode.

In this mode, multiple Qdrant services communicate with each other to distribute the data across the peers to extend the storage capabilities and increase stability.
setup with distributed deployment out of the box. This, combined with sharding, enables you to horizontally scale 

both the size of your collections and the throughput of your cluster. This means that you can use Qdrant to handle 

large amounts of data without sacrificing performance or reliability.



## Administration API



Another new feature is the administration API, which allows you to disable write operations to the service. This is
#

  # If you enable this you should also enable TLS.

  # (Either above or via an external service like nginx.)

  # Sending an api-key over an unencrypted channel is insecure.

  #

  # Uncomment to enable.

  # read_only_api_key: your_secret_read_only_api_key_here



cluster:

  # Use `enabled: true` to run Qdrant in distributed deployment mode

  enabled: false



  # Configuration of the inter-cluster communication

  p2p:

    # Port for internal communication between peers

    port: 6335
In addition, you have to make sure:



* To use a performant [persistent storage](#storage) for your data

* To configure the [security settings](/documentation/guides/security/) for your deployment

* To set up and configure Qdrant on multiple nodes for a highly available [distributed deployment](/documentation/guides/distributed_deployment/)

* To set up a load balancer for your Qdrant cluster

* To create a [backup and disaster recovery strategy](/documentation/concepts/snapshots/) for your data",0.3779527559055118,0.5517241379310345,0.4485981308411215,0.16666666666666666,0.2441860465116279,0.19811320754716982,0.2283464566929134,0.3333333333333333,0.2710280373831776,0.33858267716535434,0.4942528735632184,0.4018691588785047,0.9558295011520386,0.6781136393547058,0.6995812058448792,0.6886801719665527,0.8825057215755805,0.961307498626411,131,1.3936170212765957,0.5857217094985795
True,,4369.927,babfc157,What is the recommended approach for determining the number of shards when creating a collection using Qdrant?,"When creating a collection using Qdrant, it is recommended to set the number of shards to be a multiple of the number of nodes currently running in your cluster. For example, if you have 3 nodes in your cluster, setting the number of shards to 6 could be a good option. Shards are evenly distributed across all existing nodes when a collection is first created. It is important to note that Qdrant does not automatically rebalance shards if your cluster size or replication factor changes, as this operation can be expensive on large clusters. If scaling operations require moving shards, there are specific procedures outlined for this purpose, available as of version 0.9.0 of Qdrant.","The recommended approach for determining the number of shards when creating a collection using Qdrant involves considering the following factors:

1. **Size of the Dataset**: If you have a large dataset, it is beneficial to split the collection into multiple shards to parallelize the upload process. Qdrant suggests having 2 to 4 shards per machine for efficient processing.

2. **Cluster Configuration**: The number of shards should be based on the number of nodes in your cluster. If left unset, the `shard_number` parameter is automatically set to the number of nodes in the cluster.

3. **Custom Sharding**: Qdrant supports custom sharding methods, allowing you to define how the data is distributed across shards. If you have specific requirements for data distribution, you can specify a custom sharding method.

4. **Avoid Excessive Shard Creation**: It is recommended to have a balanced approach when creating shards. Creating too many collections can lead to resource overhead and dependencies, impacting performance and cost.

In summary, consider the dataset size, cluster configuration, and custom sharding needs when determining the number of shards for creating a collection in Qdrant.","```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

	shardNumber: 6,

	replicationFactor: 2

);

```



This code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.
## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""shard_number"": 2

}

```



```python
Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.



When you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 300,
Qdrant is built to excel in a single collection with a vast number of tenants. You should only create multiple collections when your data is not homogenous or if users' vectors are created by different embedding models. Creating too many collections may result in resource overhead and cause dependencies. This can increase costs and affect overall performance. 



## Sharding your database
client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".into(),

        shard_number: Some(1),

        sharding_method: Some(ShardingMethod::Custom),

        // ... other collection parameters

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.ShardingMethod;","['```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\n\tshardNumber: 6,\n\n\treplicationFactor: 2\n\n);\n\n```\n\n\n\nThis code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.', '## Parallel upload into multiple shards\n\n\n\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\n\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python', 'Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.\n\n\n\nWhen you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 300,', ""Qdrant is built to excel in a single collection with a vast number of tenants. You should only create multiple collections when your data is not homogenous or if users' vectors are created by different embedding models. Creating too many collections may result in resource overhead and cause dependencies. This can increase costs and affect overall performance. \n\n\n\n## Sharding your database"", 'client\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".into(),\n\n        shard_number: Some(1),\n\n        sharding_method: Some(ShardingMethod::Custom),\n\n        // ... other collection parameters\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.ShardingMethod;']","Question: What is the recommended approach for determining the number of shards when creating a collection using Qdrant?\n\nContext: ```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

collectionName: ""{collection_name}"",

vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

shardNumber: 6,

replicationFactor: 2

);

```



This code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.
## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""shard_number"": 2

}

```



```python
Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.



When you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 300,
Qdrant is built to excel in a single collection with a vast number of tenants. You should only create multiple collections when your data is not homogenous or if users' vectors are created by different embedding models. Creating too many collections may result in resource overhead and cause dependencies. This can increase costs and affect overall performance. 



## Sharding your database
client

.create_collection(&CreateCollection {

collection_name: ""{collection_name}"".into(),

shard_number: Some(1),

sharding_method: Some(ShardingMethod::Custom),

// ... other collection parameters

..Default::default()

})

.await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.ShardingMethod;\n\nAnswer:",What is the recommended approach for determining the number of shards when creating a collection using Qdrant?,"When creating a collection using Qdrant, it is recommended to set the number of shards to be a multiple of the number of nodes currently running in your cluster. For example, if you have 3 nodes in your cluster, setting the number of shards to 6 could be a good option. Shards are evenly distributed across all existing nodes when a collection is first created. It is important to note that Qdrant does not automatically rebalance shards if your cluster size or replication factor changes, as this operation can be expensive on large clusters. If scaling operations require moving shards, there are specific procedures outlined for this purpose, available as of version 0.9.0 of Qdrant.","['```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreateCollectionAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tvectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },\n\n\tshardNumber: 6,\n\n\treplicationFactor: 2\n\n);\n\n```\n\n\n\nThis code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.', '## Parallel upload into multiple shards\n\n\n\nIn Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.\n\nBy creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 2\n\n}\n\n```\n\n\n\n```python', 'Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.\n\n\n\nWhen you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 300,', ""Qdrant is built to excel in a single collection with a vast number of tenants. You should only create multiple collections when your data is not homogenous or if users' vectors are created by different embedding models. Creating too many collections may result in resource overhead and cause dependencies. This can increase costs and affect overall performance. \n\n\n\n## Sharding your database"", 'client\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".into(),\n\n        shard_number: Some(1),\n\n        sharding_method: Some(ShardingMethod::Custom),\n\n        // ... other collection parameters\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.ShardingMethod;']","```csharp

using Qdrant.Client;

using Qdrant.Client.Grpc;



var client = new QdrantClient(""localhost"", 6334);



await client.CreateCollectionAsync(

	collectionName: ""{collection_name}"",

	vectorsConfig: new VectorParams { Size = 300, Distance = Distance.Cosine },

	shardNumber: 6,

	replicationFactor: 2

);

```



This code sample creates a collection with a total of 6 logical shards backed by a total of 12 physical shards.
## Parallel upload into multiple shards



In Qdrant, each collection is split into shards. Each shard has a separate Write-Ahead-Log (WAL), which is responsible for ordering operations.

By creating multiple shards, you can parallelize upload of a large dataset. From 2 to 4 shards per one machine is a reasonable number.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""shard_number"": 2

}

```



```python
Each node knows where all parts of the collection are stored through the [consensus protocol](./#raft), so when you send a search request to one Qdrant node, it automatically queries all other nodes to obtain the full search result.



When you create a collection, Qdrant splits the collection into `shard_number` shards. If left unset, `shard_number` is set to the number of nodes in your cluster:



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 300,
Qdrant is built to excel in a single collection with a vast number of tenants. You should only create multiple collections when your data is not homogenous or if users' vectors are created by different embedding models. Creating too many collections may result in resource overhead and cause dependencies. This can increase costs and affect overall performance. 



## Sharding your database
client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".into(),

        shard_number: Some(1),

        sharding_method: Some(ShardingMethod::Custom),

        // ... other collection parameters

        ..Default::default()

    })

    .await?;

```



```java

import io.qdrant.client.QdrantClient;

import io.qdrant.client.QdrantGrpcClient;

import io.qdrant.client.grpc.Collections.CreateCollection;

import io.qdrant.client.grpc.Collections.ShardingMethod;",0.3902439024390244,0.5818181818181818,0.46715328467153283,0.1411042944785276,0.21100917431192662,0.1691176470588235,0.21341463414634146,0.3181818181818182,0.2554744525547445,0.34146341463414637,0.509090909090909,0.4087591240875912,0.8562965393066406,0.6006048917770386,0.6375027894973755,0.6185040473937988,0.6736046522049807,0.4886152178049088,182,1.5826086956521739,0.32800838655894515
True,,3239.1290000000004,b939f0ac,How is the discovery score computed in the context provided?,"In the context provided, the discovery score is computed using the formula: 

Discovery Score = sigmoid(s(v_t)) + Σ rank(v_i^+, v_i^-)

Here, s(v_t) represents the similarity function of the target vector, and v_i^+ and v_i^- represent the positive and negative examples, respectively. The sigmoid function is utilized to normalize the score within the range of 0 to 1. The sum of ranks is used to penalize vectors that are closer to negative examples than to positive ones. This means that the sum of individual ranks determines how many positive zones a point is in, while the closeness hierarchy serves as a secondary factor in the computation of the discovery score.","The discovery score is computed by first calculating the rank between positive and negative examples using the given formula:

$$
\text{rank}(v^+, v^-) = \begin{cases}
1, &\quad s(v^+) \geq s(v^-) \\
-1, &\quad s(v^+) < s(v^-)
\end{cases}
$$

where $v^+$ represents a positive example, $v^-$ represents a negative example, and $s(v)$ is the similarity score of a vector $v$ to the target vector.

Then, the discovery score is computed as:

$$
\text{discovery score} = \text{sigmoid}(s(v_t)) + \sum \text{rank}(v_i^+, v_i^-)
$$

In this formula, $\text{sigmoid}(s(v_t))$ represents the similarity score of the target vector, and the summation term involves summing the ranks calculated for each positive-negative pair in the context provided.","$$

\text{rank}(v^+, v^-) = \begin{cases}

    1, &\quad s(v^+) \geq s(v^-) \\\\

    -1, &\quad s(v^+) < s(v^-)

\end{cases}

$$

where $v^+$ represents a positive example, $v^-$ represents a negative example, and $s(v)$ is the similarity score of a vector $v$ to the target vector. The discovery score is then computed as:

$$

 \text{discovery score} = \text{sigmoid}(s(v_t))+ \sum \text{rank}(v_i^+, v_i^-),

$$
The interface for providing context is similar to the recommendation API (ids or raw vectors). Still, in this case, they need to be provided in the form of positive-negative pairs.



Discovery API lets you do two new types of search:

- **Discovery search**: Uses the context (the pairs of positive-negative vectors) and a target to return the points more similar to the target, but constrained by the context.
The Discovery API can be used in two ways - either with or without the target point. The first case is called a **discovery search**, while the second is called a **context search**.



#### Discovery search



*Discovery search* is an operation that uses a target point to find the most relevant points in the collection, while performing the search in the preferred areas only. That is basically a search operation with more control over the search space.
![Discovery search visualization](/articles_data/qdrant-1.7.x/discovery-search.png)



Please refer to the [Discovery API documentation on discovery search](/documentation/concepts/explore/#discovery-search) for more details and the internal mechanics of the operation.



#### Context search
client

    .discover(&DiscoverPoints {

        collection_name: ""{collection_name}"".to_string(),

        context: vec![

            ContextExamplePair {

                positive: Some(VectorExample {

                    example: Some(Example::Id(100.into())),

                }),

                negative: Some(VectorExample {

                    example: Some(Example::Id(718.into())),

                }),

            },

            ContextExamplePair {","['$$\n\n\\text{rank}(v^+, v^-) = \\begin{cases}\n\n    1, &\\quad s(v^+) \\geq s(v^-) \\\\\\\\\n\n    -1, &\\quad s(v^+) < s(v^-)\n\n\\end{cases}\n\n$$\n\nwhere $v^+$ represents a positive example, $v^-$ represents a negative example, and $s(v)$ is the similarity score of a vector $v$ to the target vector. The discovery score is then computed as:\n\n$$\n\n \\text{discovery score} = \\text{sigmoid}(s(v_t))+ \\sum \\text{rank}(v_i^+, v_i^-),\n\n$$', 'The interface for providing context is similar to the recommendation API (ids or raw vectors). Still, in this case, they need to be provided in the form of positive-negative pairs.\n\n\n\nDiscovery API lets you do two new types of search:\n\n- **Discovery search**: Uses the context (the pairs of positive-negative vectors) and a target to return the points more similar to the target, but constrained by the context.', 'The Discovery API can be used in two ways - either with or without the target point. The first case is called a **discovery search**, while the second is called a **context search**.\n\n\n\n#### Discovery search\n\n\n\n*Discovery search* is an operation that uses a target point to find the most relevant points in the collection, while performing the search in the preferred areas only. That is basically a search operation with more control over the search space.', '![Discovery search visualization](/articles_data/qdrant-1.7.x/discovery-search.png)\n\n\n\nPlease refer to the [Discovery API documentation on discovery search](/documentation/concepts/explore/#discovery-search) for more details and the internal mechanics of the operation.\n\n\n\n#### Context search', 'client\n\n    .discover(&DiscoverPoints {\n\n        collection_name: ""{collection_name}"".to_string(),\n\n        context: vec![\n\n            ContextExamplePair {\n\n                positive: Some(VectorExample {\n\n                    example: Some(Example::Id(100.into())),\n\n                }),\n\n                negative: Some(VectorExample {\n\n                    example: Some(Example::Id(718.into())),\n\n                }),\n\n            },\n\n            ContextExamplePair {']","Question: How is the discovery score computed in the context provided?\n\nContext: $$

\text{rank}(v^+, v^-) = \begin{cases}

1, &\quad s(v^+) \geq s(v^-) \\\\

-1, &\quad s(v^+) < s(v^-)

\end{cases}

$$

where $v^+$ represents a positive example, $v^-$ represents a negative example, and $s(v)$ is the similarity score of a vector $v$ to the target vector. The discovery score is then computed as:

$$

\text{discovery score} = \text{sigmoid}(s(v_t))+ \sum \text{rank}(v_i^+, v_i^-),

$$
The interface for providing context is similar to the recommendation API (ids or raw vectors). Still, in this case, they need to be provided in the form of positive-negative pairs.



Discovery API lets you do two new types of search:

- **Discovery search**: Uses the context (the pairs of positive-negative vectors) and a target to return the points more similar to the target, but constrained by the context.
The Discovery API can be used in two ways - either with or without the target point. The first case is called a **discovery search**, while the second is called a **context search**.



#### Discovery search



*Discovery search* is an operation that uses a target point to find the most relevant points in the collection, while performing the search in the preferred areas only. That is basically a search operation with more control over the search space.
![Discovery search visualization](/articles_data/qdrant-1.7.x/discovery-search.png)



Please refer to the [Discovery API documentation on discovery search](/documentation/concepts/explore/#discovery-search) for more details and the internal mechanics of the operation.



#### Context search
client

.discover(&DiscoverPoints {

collection_name: ""{collection_name}"".to_string(),

context: vec![

ContextExamplePair {

positive: Some(VectorExample {

example: Some(Example::Id(100.into())),

}),

negative: Some(VectorExample {

example: Some(Example::Id(718.into())),

}),

},

ContextExamplePair {\n\nAnswer:",How is the discovery score computed in the context provided?,"In the context provided, the discovery score is computed using the formula: 

Discovery Score = sigmoid(s(v_t)) + Σ rank(v_i^+, v_i^-)

Here, s(v_t) represents the similarity function of the target vector, and v_i^+ and v_i^- represent the positive and negative examples, respectively. The sigmoid function is utilized to normalize the score within the range of 0 to 1. The sum of ranks is used to penalize vectors that are closer to negative examples than to positive ones. This means that the sum of individual ranks determines how many positive zones a point is in, while the closeness hierarchy serves as a secondary factor in the computation of the discovery score.","['$$\n\n\\text{rank}(v^+, v^-) = \\begin{cases}\n\n    1, &\\quad s(v^+) \\geq s(v^-) \\\\\\\\\n\n    -1, &\\quad s(v^+) < s(v^-)\n\n\\end{cases}\n\n$$\n\nwhere $v^+$ represents a positive example, $v^-$ represents a negative example, and $s(v)$ is the similarity score of a vector $v$ to the target vector. The discovery score is then computed as:\n\n$$\n\n \\text{discovery score} = \\text{sigmoid}(s(v_t))+ \\sum \\text{rank}(v_i^+, v_i^-),\n\n$$', 'The interface for providing context is similar to the recommendation API (ids or raw vectors). Still, in this case, they need to be provided in the form of positive-negative pairs.\n\n\n\nDiscovery API lets you do two new types of search:\n\n- **Discovery search**: Uses the context (the pairs of positive-negative vectors) and a target to return the points more similar to the target, but constrained by the context.', 'The Discovery API can be used in two ways - either with or without the target point. The first case is called a **discovery search**, while the second is called a **context search**.\n\n\n\n#### Discovery search\n\n\n\n*Discovery search* is an operation that uses a target point to find the most relevant points in the collection, while performing the search in the preferred areas only. That is basically a search operation with more control over the search space.', '![Discovery search visualization](/articles_data/qdrant-1.7.x/discovery-search.png)\n\n\n\nPlease refer to the [Discovery API documentation on discovery search](/documentation/concepts/explore/#discovery-search) for more details and the internal mechanics of the operation.\n\n\n\n#### Context search', 'client\n\n    .discover(&DiscoverPoints {\n\n        collection_name: ""{collection_name}"".to_string(),\n\n        context: vec![\n\n            ContextExamplePair {\n\n                positive: Some(VectorExample {\n\n                    example: Some(Example::Id(100.into())),\n\n                }),\n\n                negative: Some(VectorExample {\n\n                    example: Some(Example::Id(718.into())),\n\n                }),\n\n            },\n\n            ContextExamplePair {']","$$

\text{rank}(v^+, v^-) = \begin{cases}

    1, &\quad s(v^+) \geq s(v^-) \\\\

    -1, &\quad s(v^+) < s(v^-)

\end{cases}

$$

where $v^+$ represents a positive example, $v^-$ represents a negative example, and $s(v)$ is the similarity score of a vector $v$ to the target vector. The discovery score is then computed as:

$$

 \text{discovery score} = \text{sigmoid}(s(v_t))+ \sum \text{rank}(v_i^+, v_i^-),

$$
The interface for providing context is similar to the recommendation API (ids or raw vectors). Still, in this case, they need to be provided in the form of positive-negative pairs.



Discovery API lets you do two new types of search:

- **Discovery search**: Uses the context (the pairs of positive-negative vectors) and a target to return the points more similar to the target, but constrained by the context.
The Discovery API can be used in two ways - either with or without the target point. The first case is called a **discovery search**, while the second is called a **context search**.



#### Discovery search



*Discovery search* is an operation that uses a target point to find the most relevant points in the collection, while performing the search in the preferred areas only. That is basically a search operation with more control over the search space.
![Discovery search visualization](/articles_data/qdrant-1.7.x/discovery-search.png)



Please refer to the [Discovery API documentation on discovery search](/documentation/concepts/explore/#discovery-search) for more details and the internal mechanics of the operation.



#### Context search
client

    .discover(&DiscoverPoints {

        collection_name: ""{collection_name}"".to_string(),

        context: vec![

            ContextExamplePair {

                positive: Some(VectorExample {

                    example: Some(Example::Id(100.into())),

                }),

                negative: Some(VectorExample {

                    example: Some(Example::Id(718.into())),

                }),

            },

            ContextExamplePair {",0.5462962962962963,0.59,0.5673076923076923,0.24299065420560748,0.26262626262626265,0.2524271844660194,0.2777777777777778,0.3,0.2884615384615385,0.39814814814814814,0.43,0.4134615384615385,0.8459107279777527,0.6195341348648071,0.6142948269844055,0.6169033050537109,0.6127664893865585,0.9191809867819151,118,1.063063063063063,0.507652337498715
True,,2949.026,cb9d2c2f,How does Qdrant optimize storage at the segment level?,"Qdrant optimizes storage at the segment level by applying changes in batches rather than individually. When optimization is needed, the segment to be optimized remains readable during the rebuild process. This is achieved by wrapping the segment into a proxy that handles data changes transparently. Changed data is placed in a copy-on-write segment, prioritizing retrieval and subsequent updates. This approach ensures efficiency in storage optimization within Qdrant.","Qdrant optimizes storage at the segment level by implementing mechanisms such as Merge Optimizer to combine small segments into larger ones. This helps in avoiding the creation of too many small segments which can lead to suboptimal search performance.

Additionally, Qdrant offers the feature of Scalar Quantization which is a data compression technique converting floating point values. This optimization not only impacts memory positively but also enhances performance.

Furthermore, for improved disk IO performance, especially when using quantization methods, Qdrant recommends enabling the async scorer with io_uring on Linux systems. This helps in optimizing disk IO operations and aligns well with the asynchronous nature of Qdrant's core.

In order to further optimize the usage of resources, Qdrant allows configuration for using minimal numbers of segments, usually 2, to leverage the benefits of large segments in terms of index size and reduced vector comparisons for finding nearest neighbors.","The service may require the creation of temporary segments.

Such segments, for example, are created as copy-on-write segments during optimization itself.



It is also essential to have at least one small segment that Qdrant will use to store frequently updated data.

On the other hand, too many small segments lead to suboptimal search performance.



There is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.
```



While Qdrant offers various options to store some parts of the data on disk, starting 

from version 1.1.0, you can also optimize your memory by compressing the embeddings. 

We've implemented the mechanism of **Scalar Quantization**! It turns out to have not 

only a positive impact on memory but also on the performance. 



## Scalar Quantization



Scalar quantization is a data compression technique that converts floating point values
led to some modest overhead in case of disk latency. The kernel may

stop a user thread trying to access a mapped region, which incurs some context

switching overhead plus the wait time until the disk IO is finished. Ultimately,

this works very well with the asynchronous nature of Qdrant's core.



One of the great optimizations Qdrant offers is quantization (either

[scalar](https://qdrant.tech/articles/scalar-quantization/) or 

[product](https://qdrant.tech/articles/product-quantization/)-based).
However unless the collection resides fully in memory, this optimization

method generates significant disk IO, so it is a prime candidate for possible

improvements.



If you run Qdrant on Linux, you can enable io\_uring with the following in your

configuration:



```yaml

# within the storage config

storage:

	# enable the async scorer which uses io_uring

	async_scorer: true

```



You can return to the mmap based backend by either deleting the `async_scorer`
);

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,","['The service may require the creation of temporary segments.\n\nSuch segments, for example, are created as copy-on-write segments during optimization itself.\n\n\n\nIt is also essential to have at least one small segment that Qdrant will use to store frequently updated data.\n\nOn the other hand, too many small segments lead to suboptimal search performance.\n\n\n\nThere is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.', ""```\n\n\n\nWhile Qdrant offers various options to store some parts of the data on disk, starting \n\nfrom version 1.1.0, you can also optimize your memory by compressing the embeddings. \n\nWe've implemented the mechanism of **Scalar Quantization**! It turns out to have not \n\nonly a positive impact on memory but also on the performance. \n\n\n\n## Scalar Quantization\n\n\n\nScalar quantization is a data compression technique that converts floating point values"", ""led to some modest overhead in case of disk latency. The kernel may\n\nstop a user thread trying to access a mapped region, which incurs some context\n\nswitching overhead plus the wait time until the disk IO is finished. Ultimately,\n\nthis works very well with the asynchronous nature of Qdrant's core.\n\n\n\nOne of the great optimizations Qdrant offers is quantization (either\n\n[scalar](https://qdrant.tech/articles/scalar-quantization/) or \n\n[product](https://qdrant.tech/articles/product-quantization/)-based)."", 'However unless the collection resides fully in memory, this optimization\n\nmethod generates significant disk IO, so it is a prime candidate for possible\n\nimprovements.\n\n\n\nIf you run Qdrant on Linux, you can enable io\\_uring with the following in your\n\nconfiguration:\n\n\n\n```yaml\n\n# within the storage config\n\nstorage:\n\n\t# enable the async scorer which uses io_uring\n\n\tasync_scorer: true\n\n```\n\n\n\nYou can return to the mmap based backend by either deleting the `async_scorer`', ');\n\n```\n\n\n\nTo prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.\n\nTo do that, you can configure qdrant to use minimal number of segments, which is usually 2.\n\nLarge segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,']","Question: How does Qdrant optimize storage at the segment level?\n\nContext: The service may require the creation of temporary segments.

Such segments, for example, are created as copy-on-write segments during optimization itself.



It is also essential to have at least one small segment that Qdrant will use to store frequently updated data.

On the other hand, too many small segments lead to suboptimal search performance.



There is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.
```



While Qdrant offers various options to store some parts of the data on disk, starting 

from version 1.1.0, you can also optimize your memory by compressing the embeddings. 

We've implemented the mechanism of **Scalar Quantization**! It turns out to have not 

only a positive impact on memory but also on the performance. 



## Scalar Quantization



Scalar quantization is a data compression technique that converts floating point values
led to some modest overhead in case of disk latency. The kernel may

stop a user thread trying to access a mapped region, which incurs some context

switching overhead plus the wait time until the disk IO is finished. Ultimately,

this works very well with the asynchronous nature of Qdrant's core.



One of the great optimizations Qdrant offers is quantization (either

[scalar](https://qdrant.tech/articles/scalar-quantization/) or 

[product](https://qdrant.tech/articles/product-quantization/)-based).
However unless the collection resides fully in memory, this optimization

method generates significant disk IO, so it is a prime candidate for possible

improvements.



If you run Qdrant on Linux, you can enable io\_uring with the following in your

configuration:



```yaml

# within the storage config

storage:

# enable the async scorer which uses io_uring

async_scorer: true

```



You can return to the mmap based backend by either deleting the `async_scorer`
);

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,\n\nAnswer:",How does Qdrant optimize storage at the segment level?,"Qdrant optimizes storage at the segment level by applying changes in batches rather than individually. When optimization is needed, the segment to be optimized remains readable during the rebuild process. This is achieved by wrapping the segment into a proxy that handles data changes transparently. Changed data is placed in a copy-on-write segment, prioritizing retrieval and subsequent updates. This approach ensures efficiency in storage optimization within Qdrant.","['The service may require the creation of temporary segments.\n\nSuch segments, for example, are created as copy-on-write segments during optimization itself.\n\n\n\nIt is also essential to have at least one small segment that Qdrant will use to store frequently updated data.\n\nOn the other hand, too many small segments lead to suboptimal search performance.\n\n\n\nThere is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.', ""```\n\n\n\nWhile Qdrant offers various options to store some parts of the data on disk, starting \n\nfrom version 1.1.0, you can also optimize your memory by compressing the embeddings. \n\nWe've implemented the mechanism of **Scalar Quantization**! It turns out to have not \n\nonly a positive impact on memory but also on the performance. \n\n\n\n## Scalar Quantization\n\n\n\nScalar quantization is a data compression technique that converts floating point values"", ""led to some modest overhead in case of disk latency. The kernel may\n\nstop a user thread trying to access a mapped region, which incurs some context\n\nswitching overhead plus the wait time until the disk IO is finished. Ultimately,\n\nthis works very well with the asynchronous nature of Qdrant's core.\n\n\n\nOne of the great optimizations Qdrant offers is quantization (either\n\n[scalar](https://qdrant.tech/articles/scalar-quantization/) or \n\n[product](https://qdrant.tech/articles/product-quantization/)-based)."", 'However unless the collection resides fully in memory, this optimization\n\nmethod generates significant disk IO, so it is a prime candidate for possible\n\nimprovements.\n\n\n\nIf you run Qdrant on Linux, you can enable io\\_uring with the following in your\n\nconfiguration:\n\n\n\n```yaml\n\n# within the storage config\n\nstorage:\n\n\t# enable the async scorer which uses io_uring\n\n\tasync_scorer: true\n\n```\n\n\n\nYou can return to the mmap based backend by either deleting the `async_scorer`', ');\n\n```\n\n\n\nTo prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.\n\nTo do that, you can configure qdrant to use minimal number of segments, which is usually 2.\n\nLarge segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,']","The service may require the creation of temporary segments.

Such segments, for example, are created as copy-on-write segments during optimization itself.



It is also essential to have at least one small segment that Qdrant will use to store frequently updated data.

On the other hand, too many small segments lead to suboptimal search performance.



There is the Merge Optimizer, which combines the smallest segments into one large segment. It is used if too many segments are created.
```



While Qdrant offers various options to store some parts of the data on disk, starting 

from version 1.1.0, you can also optimize your memory by compressing the embeddings. 

We've implemented the mechanism of **Scalar Quantization**! It turns out to have not 

only a positive impact on memory but also on the performance. 



## Scalar Quantization



Scalar quantization is a data compression technique that converts floating point values
led to some modest overhead in case of disk latency. The kernel may

stop a user thread trying to access a mapped region, which incurs some context

switching overhead plus the wait time until the disk IO is finished. Ultimately,

this works very well with the asynchronous nature of Qdrant's core.



One of the great optimizations Qdrant offers is quantization (either

[scalar](https://qdrant.tech/articles/scalar-quantization/) or 

[product](https://qdrant.tech/articles/product-quantization/)-based).
However unless the collection resides fully in memory, this optimization

method generates significant disk IO, so it is a prime candidate for possible

improvements.



If you run Qdrant on Linux, you can enable io\_uring with the following in your

configuration:



```yaml

# within the storage config

storage:

	# enable the async scorer which uses io_uring

	async_scorer: true

```



You can return to the mmap based backend by either deleting the `async_scorer`
);

```



To prefer throughput, you can set up Qdrant to use as many cores as possible for processing multiple requests in parallel.

To do that, you can configure qdrant to use minimal number of segments, which is usually 2.

Large segments benefit from the size of the index and overall smaller number of vector comparisons required to find the nearest neighbors. But at the same time require more time to build index.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,",0.18439716312056736,0.4126984126984127,0.2549019607843137,0.05,0.11290322580645161,0.06930693069306931,0.12056737588652482,0.2698412698412698,0.16666666666666666,0.1773049645390071,0.3968253968253968,0.2450980392156863,0.777032196521759,0.5542261004447937,0.6222453117370605,0.5862693786621094,0.923542220211987,0.5154499396681785,148,2.208955223880597,0.28133515275682675
True,,2462.0769999999998,273efa53,What is the purpose of the Vacuum Optimizer in the Qdrant database system?,"The Vacuum Optimizer in the Qdrant database system is utilized to address the issue of accumulated deleted records within segments. When records are marked as deleted instead of being immediately removed, they can accumulate over time, occupying memory and slowing down the system. The Vacuum Optimizer is triggered when a segment has accumulated a significant number of deleted records, as defined by the criteria set in the configuration file. This optimizer helps in optimizing segments by removing these accumulated deleted records, thereby improving system performance and efficiency. The configuration file specifies parameters such as the minimal fraction of deleted vectors and the minimal number of vectors in a segment required to trigger the Vacuum Optimizer. By running the Vacuum Optimizer, the system can effectively manage and optimize segments to prevent performance degradation due to accumulated deleted records.","The purpose of the Vacuum Optimizer in the Qdrant database system is to manage and optimize the database storage by removing accumulated deleted records. As mentioned, the strategy of marking records as deleted rather than immediately deleting them helps in minimizing disk access but can lead to the accumulation of deleted records over time, which can slow down the system and occupy memory.

The Vacuum Optimizer is used to address this issue by identifying segments that have accumulated too many deleted records and optimizing the storage by removing them. By doing so, the system can improve performance, reduce memory usage, and maintain efficient query processing capabilities.","Like many other databases, Qdrant does not delete entries immediately after a query.

Instead, it marks records as deleted and ignores them for future queries.



This strategy allows us to minimize disk access - one of the slowest operations.

However, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.



To avoid these adverse effects, Vacuum Optimizer is used.

It is used if the segment has accumulated too many deleted records.
---

title: Optimizer

weight: 70

aliases:

  - ../optimizer

---



# Optimizer



It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.



Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).
The results you may achieve on a specific setup may vary depending on the hardware, however, at the first glance, it seems that batch searching may save you quite a lot of time.



Additional improvements could be achieved in the case of distributed deployment, as Qdrant won’t need to make extensive inter-cluster requests. Moreover, if your requests share the same filtering condition, the query optimizer would be able to reuse it among batch requests.



## Summary
This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. 



### Common use cases
Systems like Qdrant exemplify scalability in vector databases. It leverages Rust's efficiency in **memory management** and **performance**, which allows handling of large-scale data with optimized resource usage.





### Efficient Query Processing","['Like many other databases, Qdrant does not delete entries immediately after a query.\n\nInstead, it marks records as deleted and ignores them for future queries.\n\n\n\nThis strategy allows us to minimize disk access - one of the slowest operations.\n\nHowever, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.\n\n\n\nTo avoid these adverse effects, Vacuum Optimizer is used.\n\nIt is used if the segment has accumulated too many deleted records.', '---\n\ntitle: Optimizer\n\nweight: 70\n\naliases:\n\n  - ../optimizer\n\n---\n\n\n\n# Optimizer\n\n\n\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\n\n\n\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).', 'The results you may achieve on a specific setup may vary depending on the hardware, however, at the first glance, it seems that batch searching may save you quite a lot of time.\n\n\n\nAdditional improvements could be achieved in the case of distributed deployment, as Qdrant won’t need to make extensive inter-cluster requests. Moreover, if your requests share the same filtering condition, the query optimizer would be able to reuse it among batch requests.\n\n\n\n## Summary', 'This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. \n\n\n\n### Common use cases', ""Systems like Qdrant exemplify scalability in vector databases. It leverages Rust's efficiency in **memory management** and **performance**, which allows handling of large-scale data with optimized resource usage.\n\n\n\n\n\n### Efficient Query Processing""]","Question: What is the purpose of the Vacuum Optimizer in the Qdrant database system?\n\nContext: Like many other databases, Qdrant does not delete entries immediately after a query.

Instead, it marks records as deleted and ignores them for future queries.



This strategy allows us to minimize disk access - one of the slowest operations.

However, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.



To avoid these adverse effects, Vacuum Optimizer is used.

It is used if the segment has accumulated too many deleted records.
---

title: Optimizer

weight: 70

aliases:

- ../optimizer

---



# Optimizer



It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.



Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).
The results you may achieve on a specific setup may vary depending on the hardware, however, at the first glance, it seems that batch searching may save you quite a lot of time.



Additional improvements could be achieved in the case of distributed deployment, as Qdrant won’t need to make extensive inter-cluster requests. Moreover, if your requests share the same filtering condition, the query optimizer would be able to reuse it among batch requests.



## Summary
This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. 



### Common use cases
Systems like Qdrant exemplify scalability in vector databases. It leverages Rust's efficiency in **memory management** and **performance**, which allows handling of large-scale data with optimized resource usage.





### Efficient Query Processing\n\nAnswer:",What is the purpose of the Vacuum Optimizer in the Qdrant database system?,"The Vacuum Optimizer in the Qdrant database system is utilized to address the issue of accumulated deleted records within segments. When records are marked as deleted instead of being immediately removed, they can accumulate over time, occupying memory and slowing down the system. The Vacuum Optimizer is triggered when a segment has accumulated a significant number of deleted records, as defined by the criteria set in the configuration file. This optimizer helps in optimizing segments by removing these accumulated deleted records, thereby improving system performance and efficiency. The configuration file specifies parameters such as the minimal fraction of deleted vectors and the minimal number of vectors in a segment required to trigger the Vacuum Optimizer. By running the Vacuum Optimizer, the system can effectively manage and optimize segments to prevent performance degradation due to accumulated deleted records.","['Like many other databases, Qdrant does not delete entries immediately after a query.\n\nInstead, it marks records as deleted and ignores them for future queries.\n\n\n\nThis strategy allows us to minimize disk access - one of the slowest operations.\n\nHowever, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.\n\n\n\nTo avoid these adverse effects, Vacuum Optimizer is used.\n\nIt is used if the segment has accumulated too many deleted records.', '---\n\ntitle: Optimizer\n\nweight: 70\n\naliases:\n\n  - ../optimizer\n\n---\n\n\n\n# Optimizer\n\n\n\nIt is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.\n\n\n\nStorage optimization in Qdrant occurs at the segment level (see [storage](../storage)).', 'The results you may achieve on a specific setup may vary depending on the hardware, however, at the first glance, it seems that batch searching may save you quite a lot of time.\n\n\n\nAdditional improvements could be achieved in the case of distributed deployment, as Qdrant won’t need to make extensive inter-cluster requests. Moreover, if your requests share the same filtering condition, the query optimizer would be able to reuse it among batch requests.\n\n\n\n## Summary', 'This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. \n\n\n\n### Common use cases', ""Systems like Qdrant exemplify scalability in vector databases. It leverages Rust's efficiency in **memory management** and **performance**, which allows handling of large-scale data with optimized resource usage.\n\n\n\n\n\n### Efficient Query Processing""]","Like many other databases, Qdrant does not delete entries immediately after a query.

Instead, it marks records as deleted and ignores them for future queries.



This strategy allows us to minimize disk access - one of the slowest operations.

However, a side effect of this strategy is that, over time, deleted records accumulate, occupy memory and slow down the system.



To avoid these adverse effects, Vacuum Optimizer is used.

It is used if the segment has accumulated too many deleted records.
---

title: Optimizer

weight: 70

aliases:

  - ../optimizer

---



# Optimizer



It is much more efficient to apply changes in batches than perform each change individually, as many other databases do. Qdrant here is no exception. Since Qdrant operates with data structures that are not always easy to change, it is sometimes necessary to rebuild those structures completely.



Storage optimization in Qdrant occurs at the segment level (see [storage](../storage)).
The results you may achieve on a specific setup may vary depending on the hardware, however, at the first glance, it seems that batch searching may save you quite a lot of time.



Additional improvements could be achieved in the case of distributed deployment, as Qdrant won’t need to make extensive inter-cluster requests. Moreover, if your requests share the same filtering condition, the query optimizer would be able to reuse it among batch requests.



## Summary
This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. 



### Common use cases
Systems like Qdrant exemplify scalability in vector databases. It leverages Rust's efficiency in **memory management** and **performance**, which allows handling of large-scale data with optimized resource usage.





### Efficient Query Processing",0.6666666666666666,0.5289256198347108,0.5898617511520738,0.2736842105263158,0.21666666666666667,0.24186046511627907,0.375,0.2975206611570248,0.3317972350230415,0.6458333333333334,0.512396694214876,0.5714285714285714,0.9280412197113037,0.7238211631774902,0.6946554183959961,0.708938479423523,0.9957877780689159,0.8632565685087078,106,0.7737226277372263,0.4873648263340393
True,,1450.11,32da8e35,What is the purpose of the `payload` field in the data points?,"The `payload` field in the data points being upserted using the QdrantClient serves as a container for additional metadata or information related to the vector data being stored. It allows users to associate supplementary details with each data point, such as the city name, price, or any other custom attributes that provide context or additional insights into the vector data. This metadata can be used for filtering, searching, or categorizing the vectors during retrieval or query operations, enabling more efficient and targeted data analysis and retrieval processes. In the examples given, the `payload` field includes information like city names, prices, and other relevant details specific to each data point, enhancing the overall utility and relevance of the stored vector data.","The purpose of the `payload` field in the data points is to store additional information or metadata related to the vectors. This additional information can be used for various purposes such as filtering, categorizing, or providing context to the vectors. The payload allows users to associate key-value pairs or structured data with the vectors, enabling more efficient and effective search, exploration, and manipulation of the data points.","## Payload



A [Payload](/documentation/concepts/payload/) describes information that you can store with vectors.



## Points



[Points](/documentation/concepts/points/) are a record which consists of a vector and an optional payload. 



## Search



[Search](/documentation/concepts/search/) describes _similarity search_, which set up related objects close to each other in vector space.



## Explore
```http

POST /collections/{collection_name}/points/payload

{

    ""payload"": {

        ""property1"": ""string"",

        ""property2"": ""string""

    },

    ""filter"": {

        ""must"": [

            {

                ""key"": ""color"",

                ""match"": {

                    ""value"": ""red""

                }

            }

        ]

    }

}

```



```python

client.set_payload(

    collection_name=""{collection_name}"",

    payload={

        ""property1"": ""string"",
and



```http

PUT /collections/{collection_name}/points

{

    ""points"": [

        {

            ""id"": 1,

            ""payload"": {""color"": ""red""},

            ""vector"": [0.9, 0.1, 0.1]

        }

    ]

}

```



```python

client.upsert(

    collection_name=""{collection_name}"",

    points=[

        models.PointStruct(

            id=1,

            payload={

                ""color"": ""red"",

            },

            vector=[0.9, 0.1, 0.1],

        ),

    ],

)

```



```typescript
""payloads"": [

            {""color"": ""red""},

            {""color"": ""green""},

            {""color"": ""blue""}

        ],

        ""vectors"": [

            [0.9, 0.1, 0.1],

            [0.1, 0.9, 0.1],

            [0.1, 0.1, 0.9]

        ]

    }

}

```



```python

client.upsert(

    collection_name=""{collection_name}"",

    points=models.Batch(

        ids=[1, 2, 3],

        payloads=[

            {""color"": ""red""},

            {""color"": ""green""},

            {""color"": ""blue""},
""with_payload"": [""title"", ""text""],



        // Options for specifying what to bring from the vector(s) 

        // of the looked up point, true by default

        ""with_vectors: false

    }

}

```



```python

client.search_groups(

    collection_name=""chunks"",

    # Same as in the regular search() API

    query_vector=[1.1],

    # Grouping parameters

    group_by=""document_id"",  # Path of the field to group by

    limit=2,  # Max amount of groups","['## Payload\n\n\n\nA [Payload](/documentation/concepts/payload/) describes information that you can store with vectors.\n\n\n\n## Points\n\n\n\n[Points](/documentation/concepts/points/) are a record which consists of a vector and an optional payload. \n\n\n\n## Search\n\n\n\n[Search](/documentation/concepts/search/) describes _similarity search_, which set up related objects close to each other in vector space.\n\n\n\n## Explore', '```http\n\nPOST /collections/{collection_name}/points/payload\n\n{\n\n    ""payload"": {\n\n        ""property1"": ""string"",\n\n        ""property2"": ""string""\n\n    },\n\n    ""filter"": {\n\n        ""must"": [\n\n            {\n\n                ""key"": ""color"",\n\n                ""match"": {\n\n                    ""value"": ""red""\n\n                }\n\n            }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.set_payload(\n\n    collection_name=""{collection_name}"",\n\n    payload={\n\n        ""property1"": ""string"",', 'and\n\n\n\n```http\n\nPUT /collections/{collection_name}/points\n\n{\n\n    ""points"": [\n\n        {\n\n            ""id"": 1,\n\n            ""payload"": {""color"": ""red""},\n\n            ""vector"": [0.9, 0.1, 0.1]\n\n        }\n\n    ]\n\n}\n\n```\n\n\n\n```python\n\nclient.upsert(\n\n    collection_name=""{collection_name}"",\n\n    points=[\n\n        models.PointStruct(\n\n            id=1,\n\n            payload={\n\n                ""color"": ""red"",\n\n            },\n\n            vector=[0.9, 0.1, 0.1],\n\n        ),\n\n    ],\n\n)\n\n```\n\n\n\n```typescript', '""payloads"": [\n\n            {""color"": ""red""},\n\n            {""color"": ""green""},\n\n            {""color"": ""blue""}\n\n        ],\n\n        ""vectors"": [\n\n            [0.9, 0.1, 0.1],\n\n            [0.1, 0.9, 0.1],\n\n            [0.1, 0.1, 0.9]\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.upsert(\n\n    collection_name=""{collection_name}"",\n\n    points=models.Batch(\n\n        ids=[1, 2, 3],\n\n        payloads=[\n\n            {""color"": ""red""},\n\n            {""color"": ""green""},\n\n            {""color"": ""blue""},', '""with_payload"": [""title"", ""text""],\n\n\n\n        // Options for specifying what to bring from the vector(s) \n\n        // of the looked up point, true by default\n\n        ""with_vectors: false\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.search_groups(\n\n    collection_name=""chunks"",\n\n    # Same as in the regular search() API\n\n    query_vector=[1.1],\n\n    # Grouping parameters\n\n    group_by=""document_id"",  # Path of the field to group by\n\n    limit=2,  # Max amount of groups']","Question: What is the purpose of the `payload` field in the data points?\n\nContext: ## Payload



A [Payload](/documentation/concepts/payload/) describes information that you can store with vectors.



## Points



[Points](/documentation/concepts/points/) are a record which consists of a vector and an optional payload. 



## Search



[Search](/documentation/concepts/search/) describes _similarity search_, which set up related objects close to each other in vector space.



## Explore
```http

POST /collections/{collection_name}/points/payload

{

""payload"": {

""property1"": ""string"",

""property2"": ""string""

},

""filter"": {

""must"": [

{

""key"": ""color"",

""match"": {

""value"": ""red""

}

}

]

}

}

```



```python

client.set_payload(

collection_name=""{collection_name}"",

payload={

""property1"": ""string"",
and



```http

PUT /collections/{collection_name}/points

{

""points"": [

{

""id"": 1,

""payload"": {""color"": ""red""},

""vector"": [0.9, 0.1, 0.1]

}

]

}

```



```python

client.upsert(

collection_name=""{collection_name}"",

points=[

models.PointStruct(

id=1,

payload={

""color"": ""red"",

},

vector=[0.9, 0.1, 0.1],

),

],

)

```



```typescript
""payloads"": [

{""color"": ""red""},

{""color"": ""green""},

{""color"": ""blue""}

],

""vectors"": [

[0.9, 0.1, 0.1],

[0.1, 0.9, 0.1],

[0.1, 0.1, 0.9]

]

}

}

```



```python

client.upsert(

collection_name=""{collection_name}"",

points=models.Batch(

ids=[1, 2, 3],

payloads=[

{""color"": ""red""},

{""color"": ""green""},

{""color"": ""blue""},
""with_payload"": [""title"", ""text""],



// Options for specifying what to bring from the vector(s) 

// of the looked up point, true by default

""with_vectors: false

}

}

```



```python

client.search_groups(

collection_name=""chunks"",

# Same as in the regular search() API

query_vector=[1.1],

# Grouping parameters

group_by=""document_id"",  # Path of the field to group by

limit=2,  # Max amount of groups\n\nAnswer:",What is the purpose of the `payload` field in the data points?,"The `payload` field in the data points being upserted using the QdrantClient serves as a container for additional metadata or information related to the vector data being stored. It allows users to associate supplementary details with each data point, such as the city name, price, or any other custom attributes that provide context or additional insights into the vector data. This metadata can be used for filtering, searching, or categorizing the vectors during retrieval or query operations, enabling more efficient and targeted data analysis and retrieval processes. In the examples given, the `payload` field includes information like city names, prices, and other relevant details specific to each data point, enhancing the overall utility and relevance of the stored vector data.","['## Payload\n\n\n\nA [Payload](/documentation/concepts/payload/) describes information that you can store with vectors.\n\n\n\n## Points\n\n\n\n[Points](/documentation/concepts/points/) are a record which consists of a vector and an optional payload. \n\n\n\n## Search\n\n\n\n[Search](/documentation/concepts/search/) describes _similarity search_, which set up related objects close to each other in vector space.\n\n\n\n## Explore', '```http\n\nPOST /collections/{collection_name}/points/payload\n\n{\n\n    ""payload"": {\n\n        ""property1"": ""string"",\n\n        ""property2"": ""string""\n\n    },\n\n    ""filter"": {\n\n        ""must"": [\n\n            {\n\n                ""key"": ""color"",\n\n                ""match"": {\n\n                    ""value"": ""red""\n\n                }\n\n            }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.set_payload(\n\n    collection_name=""{collection_name}"",\n\n    payload={\n\n        ""property1"": ""string"",', 'and\n\n\n\n```http\n\nPUT /collections/{collection_name}/points\n\n{\n\n    ""points"": [\n\n        {\n\n            ""id"": 1,\n\n            ""payload"": {""color"": ""red""},\n\n            ""vector"": [0.9, 0.1, 0.1]\n\n        }\n\n    ]\n\n}\n\n```\n\n\n\n```python\n\nclient.upsert(\n\n    collection_name=""{collection_name}"",\n\n    points=[\n\n        models.PointStruct(\n\n            id=1,\n\n            payload={\n\n                ""color"": ""red"",\n\n            },\n\n            vector=[0.9, 0.1, 0.1],\n\n        ),\n\n    ],\n\n)\n\n```\n\n\n\n```typescript', '""payloads"": [\n\n            {""color"": ""red""},\n\n            {""color"": ""green""},\n\n            {""color"": ""blue""}\n\n        ],\n\n        ""vectors"": [\n\n            [0.9, 0.1, 0.1],\n\n            [0.1, 0.9, 0.1],\n\n            [0.1, 0.1, 0.9]\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.upsert(\n\n    collection_name=""{collection_name}"",\n\n    points=models.Batch(\n\n        ids=[1, 2, 3],\n\n        payloads=[\n\n            {""color"": ""red""},\n\n            {""color"": ""green""},\n\n            {""color"": ""blue""},', '""with_payload"": [""title"", ""text""],\n\n\n\n        // Options for specifying what to bring from the vector(s) \n\n        // of the looked up point, true by default\n\n        ""with_vectors: false\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.search_groups(\n\n    collection_name=""chunks"",\n\n    # Same as in the regular search() API\n\n    query_vector=[1.1],\n\n    # Grouping parameters\n\n    group_by=""document_id"",  # Path of the field to group by\n\n    limit=2,  # Max amount of groups']","## Payload



A [Payload](/documentation/concepts/payload/) describes information that you can store with vectors.



## Points



[Points](/documentation/concepts/points/) are a record which consists of a vector and an optional payload. 



## Search



[Search](/documentation/concepts/search/) describes _similarity search_, which set up related objects close to each other in vector space.



## Explore
```http

POST /collections/{collection_name}/points/payload

{

    ""payload"": {

        ""property1"": ""string"",

        ""property2"": ""string""

    },

    ""filter"": {

        ""must"": [

            {

                ""key"": ""color"",

                ""match"": {

                    ""value"": ""red""

                }

            }

        ]

    }

}

```



```python

client.set_payload(

    collection_name=""{collection_name}"",

    payload={

        ""property1"": ""string"",
and



```http

PUT /collections/{collection_name}/points

{

    ""points"": [

        {

            ""id"": 1,

            ""payload"": {""color"": ""red""},

            ""vector"": [0.9, 0.1, 0.1]

        }

    ]

}

```



```python

client.upsert(

    collection_name=""{collection_name}"",

    points=[

        models.PointStruct(

            id=1,

            payload={

                ""color"": ""red"",

            },

            vector=[0.9, 0.1, 0.1],

        ),

    ],

)

```



```typescript
""payloads"": [

            {""color"": ""red""},

            {""color"": ""green""},

            {""color"": ""blue""}

        ],

        ""vectors"": [

            [0.9, 0.1, 0.1],

            [0.1, 0.9, 0.1],

            [0.1, 0.1, 0.9]

        ]

    }

}

```



```python

client.upsert(

    collection_name=""{collection_name}"",

    points=models.Batch(

        ids=[1, 2, 3],

        payloads=[

            {""color"": ""red""},

            {""color"": ""green""},

            {""color"": ""blue""},
""with_payload"": [""title"", ""text""],



        // Options for specifying what to bring from the vector(s) 

        // of the looked up point, true by default

        ""with_vectors: false

    }

}

```



```python

client.search_groups(

    collection_name=""chunks"",

    # Same as in the regular search() API

    query_vector=[1.1],

    # Grouping parameters

    group_by=""document_id"",  # Path of the field to group by

    limit=2,  # Max amount of groups",0.7833333333333333,0.4351851851851852,0.5595238095238095,0.3050847457627119,0.16822429906542055,0.21686746987951808,0.43333333333333335,0.24074074074074073,0.30952380952380953,0.7166666666666667,0.39814814814814814,0.511904761904762,0.7678458094596863,0.7680554389953613,0.6888661980628967,0.7263087034225464,0.6954164107640584,0.403654009103775,67,0.5583333333333333,0.6505271684128976
True,,3673.7690000000002,5f8b3de2,What is the purpose of shard replication in Qdrant?,"Shard replication in Qdrant increases the reliability of the cluster by keeping several copies of a shard spread across the cluster. This ensures the availability of the data in case of node failures, except if all replicas are lost.","The purpose of shard replication in Qdrant is to increase the reliability of the cluster by keeping multiple copies of a shard spread across the cluster. This replication process ensures that there are backups of the shard data in case of failures, thereby improving data availability and consistency.

There are two key benefits of shard replication in Qdrant:

1. Transferring index and quantization data: When a shard is replicated, its index and quantization data are transferred to the target shard. This eliminates the need to optimize the shard again on the target node, making the data immediately available. This process helps maintain performance levels and prevents degradation, especially for large shards.

2. Ensuring consistency and data integrity: By replicating shards across nodes, Qdrant ensures that the data remains consistent and up-to-date across the cluster. Replication helps in maintaining data integrity and preventing inconsistencies in the cluster.

It is important to consider the storage implications of shard replication, as maintaining multiple copies of shards will require additional storage space. Therefore, it is recommended to ensure that the hardware can accommodate the extra shard replicas before configuring the replication factor.","ordering guarantees can be `strong`[^ordered], required for some applications.



The `stream_records` method is currently used as default. This may change in the

future.



## Replication



*Available as of v0.11.0*



Qdrant allows you to replicate shards between nodes in the cluster.



Shard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.
order to the target shard to bring it into the same state as the source. There

are two important benefits: 1. It transfers index and quantization data, so that

the shard does not have to be optimized again on the target node, making them

immediately available. This way, Qdrant ensures that there will be no

degradation in performance at the end of the transfer. Especially on large

shards, this can give a huge performance improvement. 2. The consistency and
Since a replication factor of ""2"" would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.



### Creating new shard replicas



It is possible to create or delete replicas manually on an existing collection using the [Update collection cluster setup API](https://qdrant.github.io/qdrant/redoc/index.html?v=v0.11.0#tag/cluster/operation/update_collection_cluster).
Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.

If there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.



### Recover from a URL or local file



*Available as of v0.11.3*
Currently, the replication factor of a collection can only be configured at creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",","['ordering guarantees can be `strong`[^ordered], required for some applications.\n\n\n\nThe `stream_records` method is currently used as default. This may change in the\n\nfuture.\n\n\n\n## Replication\n\n\n\n*Available as of v0.11.0*\n\n\n\nQdrant allows you to replicate shards between nodes in the cluster.\n\n\n\nShard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.', 'order to the target shard to bring it into the same state as the source. There\n\nare two important benefits: 1. It transfers index and quantization data, so that\n\nthe shard does not have to be optimized again on the target node, making them\n\nimmediately available. This way, Qdrant ensures that there will be no\n\ndegradation in performance at the end of the transfer. Especially on large\n\nshards, this can give a huge performance improvement. 2. The consistency and', 'Since a replication factor of ""2"" would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.\n\n\n\n### Creating new shard replicas\n\n\n\nIt is possible to create or delete replicas manually on an existing collection using the [Update collection cluster setup API](https://qdrant.github.io/qdrant/redoc/index.html?v=v0.11.0#tag/cluster/operation/update_collection_cluster).', 'Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.\n\nIf there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.\n\n\n\n### Recover from a URL or local file\n\n\n\n*Available as of v0.11.3*', 'Currently, the replication factor of a collection can only be configured at creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",']","Question: What is the purpose of shard replication in Qdrant?\n\nContext: ordering guarantees can be `strong`[^ordered], required for some applications.



The `stream_records` method is currently used as default. This may change in the

future.



## Replication



*Available as of v0.11.0*



Qdrant allows you to replicate shards between nodes in the cluster.



Shard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.
order to the target shard to bring it into the same state as the source. There

are two important benefits: 1. It transfers index and quantization data, so that

the shard does not have to be optimized again on the target node, making them

immediately available. This way, Qdrant ensures that there will be no

degradation in performance at the end of the transfer. Especially on large

shards, this can give a huge performance improvement. 2. The consistency and
Since a replication factor of ""2"" would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.



### Creating new shard replicas



It is possible to create or delete replicas manually on an existing collection using the [Update collection cluster setup API](https://qdrant.github.io/qdrant/redoc/index.html?v=v0.11.0#tag/cluster/operation/update_collection_cluster).
Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.

If there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.



### Recover from a URL or local file



*Available as of v0.11.3*
Currently, the replication factor of a collection can only be configured at creation time.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 300,

""distance"": ""Cosine""

},

""shard_number"": 6,

""replication_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",\n\nAnswer:",What is the purpose of shard replication in Qdrant?,"Shard replication in Qdrant increases the reliability of the cluster by keeping several copies of a shard spread across the cluster. This ensures the availability of the data in case of node failures, except if all replicas are lost.","['ordering guarantees can be `strong`[^ordered], required for some applications.\n\n\n\nThe `stream_records` method is currently used as default. This may change in the\n\nfuture.\n\n\n\n## Replication\n\n\n\n*Available as of v0.11.0*\n\n\n\nQdrant allows you to replicate shards between nodes in the cluster.\n\n\n\nShard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.', 'order to the target shard to bring it into the same state as the source. There\n\nare two important benefits: 1. It transfers index and quantization data, so that\n\nthe shard does not have to be optimized again on the target node, making them\n\nimmediately available. This way, Qdrant ensures that there will be no\n\ndegradation in performance at the end of the transfer. Especially on large\n\nshards, this can give a huge performance improvement. 2. The consistency and', 'Since a replication factor of ""2"" would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.\n\n\n\n### Creating new shard replicas\n\n\n\nIt is possible to create or delete replicas manually on an existing collection using the [Update collection cluster setup API](https://qdrant.github.io/qdrant/redoc/index.html?v=v0.11.0#tag/cluster/operation/update_collection_cluster).', 'Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.\n\nIf there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.\n\n\n\n### Recover from a URL or local file\n\n\n\n*Available as of v0.11.3*', 'Currently, the replication factor of a collection can only be configured at creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",']","ordering guarantees can be `strong`[^ordered], required for some applications.



The `stream_records` method is currently used as default. This may change in the

future.



## Replication



*Available as of v0.11.0*



Qdrant allows you to replicate shards between nodes in the cluster.



Shard replication increases the reliability of the cluster by keeping several copies of a shard spread across the cluster.
order to the target shard to bring it into the same state as the source. There

are two important benefits: 1. It transfers index and quantization data, so that

the shard does not have to be optimized again on the target node, making them

immediately available. This way, Qdrant ensures that there will be no

degradation in performance at the end of the transfer. Especially on large

shards, this can give a huge performance improvement. 2. The consistency and
Since a replication factor of ""2"" would require twice as much storage space, it is advised to make sure the hardware can host the additional shard replicas beforehand.



### Creating new shard replicas



It is possible to create or delete replicas manually on an existing collection using the [Update collection cluster setup API](https://qdrant.github.io/qdrant/redoc/index.html?v=v0.11.0#tag/cluster/operation/update_collection_cluster).
Regardless of the method used, Qdrant will extract the shard data from the snapshot and properly register shards in the cluster.

If there are other active replicas of the recovered shards in the cluster, Qdrant will replicate them to the newly recovered node by default to maintain data consistency.



### Recover from a URL or local file



*Available as of v0.11.3*
Currently, the replication factor of a collection can only be configured at creation time.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

        ""size"": 300,

        ""distance"": ""Cosine""

    },

    ""shard_number"": 6,

    ""replication_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",",0.16374269005847952,0.8484848484848485,0.27450980392156865,0.1,0.53125,0.16831683168316833,0.14619883040935672,0.7575757575757576,0.2450980392156863,0.16374269005847952,0.8484848484848485,0.27450980392156865,0.9404008984565735,0.6048450469970703,0.7377281785011292,0.664710521697998,0.9678637646009318,0.612861268222332,188,4.82051282051282,0.37972149234476155
True,,3143.674,3db723c4,What are the steps to set up product quantization in QdrantClient?,"To set up product quantization in QdrantClient, you need to specify the quantization parameters in the quantization_config section of the collection configuration. The compression ratio can be set to x16 and the always_ram parameter can be set to true to store quantized vectors in RAM. The vectors_config section should also be specified with the desired vector size and distance metric. Finally, you can use the create_collection method of the QdrantClient to create the collection with the specified configurations.","To set up product quantization in QdrantClient, you need to follow these steps:

1. Specify the quantization parameters in the `quantization_config` section of the collection configuration.
2. Use the provided code snippets in the programming language of your choice to create a QdrantClient and set up the collection with product quantization.
3. Ensure that you specify the compression ratio and whether to always keep the quantized vectors in RAM in the `product` section of the `quantization_config`.

By following these steps and providing the necessary configuration parameters, you can successfully set up product quantization in QdrantClient for your collection.","### Setting up Product Quantization



To enable product quantization, you need to specify the quantization parameters in the `quantization_config` section of the collection configuration.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""quantization_config"": {

        ""product"": {

            ""compression"": ""x16"",

            ""always_ram"": true

        }

    }

}

```



```python
import io.qdrant.client.grpc.Collections.ProductQuantization;

import io.qdrant.client.grpc.Collections.QuantizationConfig;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(
use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, CompressionRatio,

        CreateCollection, Distance, ProductQuantization, QuantizationConfig, VectorParams,

        VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".to_string(),

        vectors_config: Some(VectorsConfig {
use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,

        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,

        VectorParams, VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".to_string(),
use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,

        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,

        VectorParams, VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".to_string(),","['### Setting up Product Quantization\n\n\n\nTo enable product quantization, you need to specify the quantization parameters in the `quantization_config` section of the collection configuration.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""quantization_config"": {\n\n        ""product"": {\n\n            ""compression"": ""x16"",\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python', 'import io.qdrant.client.grpc.Collections.ProductQuantization;\n\nimport io.qdrant.client.grpc.Collections.QuantizationConfig;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(', 'use qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, CompressionRatio,\n\n        CreateCollection, Distance, ProductQuantization, QuantizationConfig, VectorParams,\n\n        VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".to_string(),\n\n        vectors_config: Some(VectorsConfig {', 'use qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,\n\n        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,\n\n        VectorParams, VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".to_string(),', 'use qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,\n\n        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,\n\n        VectorParams, VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".to_string(),']","Question: What are the steps to set up product quantization in QdrantClient?\n\nContext: ### Setting up Product Quantization



To enable product quantization, you need to specify the quantization parameters in the `quantization_config` section of the collection configuration.



```http

PUT /collections/{collection_name}

{

""vectors"": {

""size"": 768,

""distance"": ""Cosine""

},

""quantization_config"": {

""product"": {

""compression"": ""x16"",

""always_ram"": true

}

}

}

```



```python
import io.qdrant.client.grpc.Collections.ProductQuantization;

import io.qdrant.client.grpc.Collections.QuantizationConfig;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

.createCollectionAsync(

CreateCollection.newBuilder()

.setCollectionName(""{collection_name}"")

.setVectorsConfig(
use qdrant_client::{

client::QdrantClient,

qdrant::{

quantization_config::Quantization, vectors_config::Config, CompressionRatio,

CreateCollection, Distance, ProductQuantization, QuantizationConfig, VectorParams,

VectorsConfig,

},

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

.create_collection(&CreateCollection {

collection_name: ""{collection_name}"".to_string(),

vectors_config: Some(VectorsConfig {
use qdrant_client::{

client::QdrantClient,

qdrant::{

quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,

OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,

VectorParams, VectorsConfig,

},

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

.create_collection(&CreateCollection {

collection_name: ""{collection_name}"".to_string(),
use qdrant_client::{

client::QdrantClient,

qdrant::{

quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,

OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,

VectorParams, VectorsConfig,

},

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

.create_collection(&CreateCollection {

collection_name: ""{collection_name}"".to_string(),\n\nAnswer:",What are the steps to set up product quantization in QdrantClient?,"To set up product quantization in QdrantClient, you need to specify the quantization parameters in the quantization_config section of the collection configuration. The compression ratio can be set to x16 and the always_ram parameter can be set to true to store quantized vectors in RAM. The vectors_config section should also be specified with the desired vector size and distance metric. Finally, you can use the create_collection method of the QdrantClient to create the collection with the specified configurations.","['### Setting up Product Quantization\n\n\n\nTo enable product quantization, you need to specify the quantization parameters in the `quantization_config` section of the collection configuration.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""quantization_config"": {\n\n        ""product"": {\n\n            ""compression"": ""x16"",\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python', 'import io.qdrant.client.grpc.Collections.ProductQuantization;\n\nimport io.qdrant.client.grpc.Collections.QuantizationConfig;\n\nimport io.qdrant.client.grpc.Collections.VectorParams;\n\nimport io.qdrant.client.grpc.Collections.VectorsConfig;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createCollectionAsync(\n\n        CreateCollection.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .setVectorsConfig(', 'use qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, CompressionRatio,\n\n        CreateCollection, Distance, ProductQuantization, QuantizationConfig, VectorParams,\n\n        VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".to_string(),\n\n        vectors_config: Some(VectorsConfig {', 'use qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,\n\n        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,\n\n        VectorParams, VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".to_string(),', 'use qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,\n\n        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,\n\n        VectorParams, VectorsConfig,\n\n    },\n\n};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_collection(&CreateCollection {\n\n        collection_name: ""{collection_name}"".to_string(),']","### Setting up Product Quantization



To enable product quantization, you need to specify the quantization parameters in the `quantization_config` section of the collection configuration.



```http

PUT /collections/{collection_name}

{

    ""vectors"": {

      ""size"": 768,

      ""distance"": ""Cosine""

    },

    ""quantization_config"": {

        ""product"": {

            ""compression"": ""x16"",

            ""always_ram"": true

        }

    }

}

```



```python
import io.qdrant.client.grpc.Collections.ProductQuantization;

import io.qdrant.client.grpc.Collections.QuantizationConfig;

import io.qdrant.client.grpc.Collections.VectorParams;

import io.qdrant.client.grpc.Collections.VectorsConfig;



QdrantClient client =

    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());



client

    .createCollectionAsync(

        CreateCollection.newBuilder()

            .setCollectionName(""{collection_name}"")

            .setVectorsConfig(
use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, CompressionRatio,

        CreateCollection, Distance, ProductQuantization, QuantizationConfig, VectorParams,

        VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".to_string(),

        vectors_config: Some(VectorsConfig {
use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,

        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,

        VectorParams, VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".to_string(),
use qdrant_client::{

    client::QdrantClient,

    qdrant::{

        quantization_config::Quantization, vectors_config::Config, CreateCollection, Distance,

        OptimizersConfigDiff, QuantizationConfig, QuantizationType, ScalarQuantization,

        VectorParams, VectorsConfig,

    },

};



let client = QdrantClient::from_url(""http://localhost:6334"").build()?;



client

    .create_collection(&CreateCollection {

        collection_name: ""{collection_name}"".to_string(),",0.5340909090909091,0.6619718309859155,0.5911949685534591,0.28735632183908044,0.35714285714285715,0.3184713375796178,0.38636363636363635,0.4788732394366197,0.4276729559748428,0.5,0.6197183098591549,0.5534591194968552,0.970524787902832,0.6686571836471558,0.6905943155288696,0.6794487237930298,0.9499944243580103,0.9200099133886397,98,1.2564102564102564,0.5443337530528335
True,,2951.6330000000003,cc26c998,What are the different write ordering options available in Qdrant?,"Qdrant provides three different write ordering options: weak, medium, and strong. Weak ordering does not provide any additional guarantees and allows write operations to be freely reordered. Medium ordering serializes write operations through a dynamically elected leader, which may cause minor inconsistencies in case of leader change. Strong ordering serializes write operations through the permanent leader, providing strong consistency but may result in write operations being unavailable if the leader is down.","In Qdrant, the different write ordering options available are:

1. **Sequential Ordering**: This option ensures that update and delete operations are executed in the same order on all replicas. When using this option, Qdrant routes the operation to the leader replica of the shard and waits for the response before responding to the client. This is useful to prevent data inconsistency in case of concurrent updates to the same documents.

By setting the `write_consistency_factor` to a value greater than 1, you can specify the number of replicas that must acknowledge the write operation before it is considered successful. This can help ensure data consistency across replicas.

Overall, these write ordering options help maintain data consistency and reliability in distributed environments when working with Qdrant.","- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

    shard_number=6,

    replication_factor=2,

    write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
To change it, access `config.yaml` and set `storage.temp_path` to another directory location.



## Important changes



The latest release focuses not only on the new features but also introduces some changes making 

Qdrant even more reliable.



### Optimizing group requests
location=""http://localhost:6333"",

            collection_name=""test"",

        ),

        write_config=QdrantWriteConfig(batch_size=80),

    )



if __name__ == ""__main__"":

    writer = get_writer()

    runner = LocalRunner(

        processor_config=ProcessorConfig(

            verbose=True,

            output_dir=""local-output-to-qdrant"",

            num_processes=2,

        ),

        connector_config=SimpleLocalConfig(","['- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", '""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n\n    shard_number=6,\n\n    replication_factor=2,\n\n    write_consistency_factor=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', 'To change it, access `config.yaml` and set `storage.temp_path` to another directory location.\n\n\n\n## Important changes\n\n\n\nThe latest release focuses not only on the new features but also introduces some changes making \n\nQdrant even more reliable.\n\n\n\n### Optimizing group requests', 'location=""http://localhost:6333"",\n\n            collection_name=""test"",\n\n        ),\n\n        write_config=QdrantWriteConfig(batch_size=80),\n\n    )\n\n\n\nif __name__ == ""__main__"":\n\n    writer = get_writer()\n\n    runner = LocalRunner(\n\n        processor_config=ProcessorConfig(\n\n            verbose=True,\n\n            output_dir=""local-output-to-qdrant"",\n\n            num_processes=2,\n\n        ),\n\n        connector_config=SimpleLocalConfig(']","Question: What are the different write ordering options available in Qdrant?\n\nContext: - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

collection_name=""{collection_name}"",

vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

shard_number=6,

replication_factor=2,

write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
To change it, access `config.yaml` and set `storage.temp_path` to another directory location.



## Important changes



The latest release focuses not only on the new features but also introduces some changes making 

Qdrant even more reliable.



### Optimizing group requests
location=""http://localhost:6333"",

collection_name=""test"",

),

write_config=QdrantWriteConfig(batch_size=80),

)



if __name__ == ""__main__"":

writer = get_writer()

runner = LocalRunner(

processor_config=ProcessorConfig(

verbose=True,

output_dir=""local-output-to-qdrant"",

num_processes=2,

),

connector_config=SimpleLocalConfig(\n\nAnswer:",What are the different write ordering options available in Qdrant?,"Qdrant provides three different write ordering options: weak, medium, and strong. Weak ordering does not provide any additional guarantees and allows write operations to be freely reordered. Medium ordering serializes write operations through a dynamically elected leader, which may cause minor inconsistencies in case of leader change. Strong ordering serializes write operations through the permanent leader, providing strong consistency but may result in write operations being unavailable if the leader is down.","['- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", '""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),\n\n    shard_number=6,\n\n    replication_factor=2,\n\n    write_consistency_factor=2,\n\n)\n\n```\n\n\n\n```typescript\n\nimport { QdrantClient } from ""@qdrant/js-client-rest"";', 'To change it, access `config.yaml` and set `storage.temp_path` to another directory location.\n\n\n\n## Important changes\n\n\n\nThe latest release focuses not only on the new features but also introduces some changes making \n\nQdrant even more reliable.\n\n\n\n### Optimizing group requests', 'location=""http://localhost:6333"",\n\n            collection_name=""test"",\n\n        ),\n\n        write_config=QdrantWriteConfig(batch_size=80),\n\n    )\n\n\n\nif __name__ == ""__main__"":\n\n    writer = get_writer()\n\n    runner = LocalRunner(\n\n        processor_config=ProcessorConfig(\n\n            verbose=True,\n\n            output_dir=""local-output-to-qdrant"",\n\n            num_processes=2,\n\n        ),\n\n        connector_config=SimpleLocalConfig(']","- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
""write_consistency_factor"": 2,

}

```



```python

from qdrant_client import QdrantClient

from qdrant_client.http import models



client = QdrantClient(""localhost"", port=6333)



client.create_collection(

    collection_name=""{collection_name}"",

    vectors_config=models.VectorParams(size=300, distance=models.Distance.COSINE),

    shard_number=6,

    replication_factor=2,

    write_consistency_factor=2,

)

```



```typescript

import { QdrantClient } from ""@qdrant/js-client-rest"";
To change it, access `config.yaml` and set `storage.temp_path` to another directory location.



## Important changes



The latest release focuses not only on the new features but also introduces some changes making 

Qdrant even more reliable.



### Optimizing group requests
location=""http://localhost:6333"",

            collection_name=""test"",

        ),

        write_config=QdrantWriteConfig(batch_size=80),

    )



if __name__ == ""__main__"":

    writer = get_writer()

    runner = LocalRunner(

        processor_config=ProcessorConfig(

            verbose=True,

            output_dir=""local-output-to-qdrant"",

            num_processes=2,

        ),

        connector_config=SimpleLocalConfig(",0.21929824561403508,0.36231884057971014,0.273224043715847,0.07079646017699115,0.11764705882352941,0.08839779005524862,0.17543859649122806,0.2898550724637681,0.2185792349726776,0.21929824561403508,0.36231884057971014,0.273224043715847,0.735222578048706,0.5420031547546387,0.5666874051094055,0.5540705323219299,0.8995415674455997,0.27832625806331635,124,1.7222222222222223,0.5421300420933037
True,,4893.041,c755361c,How can Qdrant be optimized for low memory footprint and high speed search?,"To optimize Qdrant for low memory footprint and high speed search, you can use vector quantization. This involves converting vectors into a more compact representation that can be stored in memory and used for search. By using smaller vectors, you can cache more in RAM and reduce the number of disk reads. To configure in-memory quantization with on-disk original vectors, you need to create a collection with the specified configuration.","To optimize Qdrant for low memory footprint and high-speed search, you can consider the following strategies:

1. **Utilize Scalar Quantization:** Reduce the memory footprint by a factor of 4 by implementing scalar quantization. This not only saves memory but also speeds up the search process. Qdrant uses a special SIMD CPU instruction for fast vector comparison, which works with 8-bit integers. Converting vectors to `uint8` allows Qdrant to perform comparisons faster.

2. **Configurable Trade-offs:** Qdrant offers configurable trade-offs between RAM usage and search speed. You can adjust the settings according to your specific requirements to find the right balance between memory footprint and search performance.

3. **Shard Selection:** When searching for data, specify the shard or shards where the data resides. This enables Qdrant to directly access the relevant shards, avoiding unnecessary queries to all machines in the cluster. This minimizes overhead and maximizes search performance.

4. **Optimal Hardware Configuration:** Consider using local SSDs or production-grade disks to enhance search speed. Local SSDs can significantly increase search speed, with some configurations reaching up to 1M IOPS. This can be particularly beneficial for serving large datasets with minimal search latency in Qdrant.

By implementing these strategies, you can optimize Qdrant for efficient use of resources, achieving a balance between low memory footprint and high-speed search capabilities.","---

title: Optimize Resources

weight: 11

aliases:

  - ../tutorials/optimize

---



# Optimize Qdrant



Different use cases have different requirements for balancing between memory, speed, and precision.

Qdrant is designed to be flexible and customizable so you can tune it to your needs.



![Trafeoff](/docs/tradeoff.png)



Let's look deeper into each of those possible optimization scenarios.



## Prefer low memory footprint with high speed search
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
It provides configurable trade-offs between RAM usage and search speed.



We are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.

Please feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!
This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. 



### Common use cases
With a local SSD, we were able to increase the search speed by 10x!



With the production-grade disk, the search speed could be even higher. 

Some configurations of the SSDs can reach 1M IOPS and more.



Which might be an interesting option to serve large datasets with low search latency in Qdrant.





## Conclusion



In this article, we showed that Qdrant have flexibility in terms of RAM usage and can be used to serve large datasets.","[""---\n\ntitle: Optimize Resources\n\nweight: 11\n\naliases:\n\n  - ../tutorials/optimize\n\n---\n\n\n\n# Optimize Qdrant\n\n\n\nDifferent use cases have different requirements for balancing between memory, speed, and precision.\n\nQdrant is designed to be flexible and customizable so you can tune it to your needs.\n\n\n\n![Trafeoff](/docs/tradeoff.png)\n\n\n\nLet's look deeper into each of those possible optimization scenarios.\n\n\n\n## Prefer low memory footprint with high speed search"", 'Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.\n\n\n\nIn addition to reducing the memory footprint, scalar quantization also speeds up the search process.\n\nQdrant uses a special SIMD CPU instruction to perform fast vector comparison.\n\nThis instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.', 'It provides configurable trade-offs between RAM usage and search speed.\n\n\n\nWe are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.\n\nPlease feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!', 'This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. \n\n\n\n### Common use cases', 'With a local SSD, we were able to increase the search speed by 10x!\n\n\n\nWith the production-grade disk, the search speed could be even higher. \n\nSome configurations of the SSDs can reach 1M IOPS and more.\n\n\n\nWhich might be an interesting option to serve large datasets with low search latency in Qdrant.\n\n\n\n\n\n## Conclusion\n\n\n\nIn this article, we showed that Qdrant have flexibility in terms of RAM usage and can be used to serve large datasets.']","Question: How can Qdrant be optimized for low memory footprint and high speed search?\n\nContext: ---

title: Optimize Resources

weight: 11

aliases:

- ../tutorials/optimize

---



# Optimize Qdrant



Different use cases have different requirements for balancing between memory, speed, and precision.

Qdrant is designed to be flexible and customizable so you can tune it to your needs.



![Trafeoff](/docs/tradeoff.png)



Let's look deeper into each of those possible optimization scenarios.



## Prefer low memory footprint with high speed search
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
It provides configurable trade-offs between RAM usage and search speed.



We are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.

Please feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!
This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. 



### Common use cases
With a local SSD, we were able to increase the search speed by 10x!



With the production-grade disk, the search speed could be even higher. 

Some configurations of the SSDs can reach 1M IOPS and more.



Which might be an interesting option to serve large datasets with low search latency in Qdrant.





## Conclusion



In this article, we showed that Qdrant have flexibility in terms of RAM usage and can be used to serve large datasets.\n\nAnswer:",How can Qdrant be optimized for low memory footprint and high speed search?,"To optimize Qdrant for low memory footprint and high speed search, you can use vector quantization. This involves converting vectors into a more compact representation that can be stored in memory and used for search. By using smaller vectors, you can cache more in RAM and reduce the number of disk reads. To configure in-memory quantization with on-disk original vectors, you need to create a collection with the specified configuration.","[""---\n\ntitle: Optimize Resources\n\nweight: 11\n\naliases:\n\n  - ../tutorials/optimize\n\n---\n\n\n\n# Optimize Qdrant\n\n\n\nDifferent use cases have different requirements for balancing between memory, speed, and precision.\n\nQdrant is designed to be flexible and customizable so you can tune it to your needs.\n\n\n\n![Trafeoff](/docs/tradeoff.png)\n\n\n\nLet's look deeper into each of those possible optimization scenarios.\n\n\n\n## Prefer low memory footprint with high speed search"", 'Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.\n\n\n\nIn addition to reducing the memory footprint, scalar quantization also speeds up the search process.\n\nQdrant uses a special SIMD CPU instruction to perform fast vector comparison.\n\nThis instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.', 'It provides configurable trade-offs between RAM usage and search speed.\n\n\n\nWe are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.\n\nPlease feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!', 'This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. \n\n\n\n### Common use cases', 'With a local SSD, we were able to increase the search speed by 10x!\n\n\n\nWith the production-grade disk, the search speed could be even higher. \n\nSome configurations of the SSDs can reach 1M IOPS and more.\n\n\n\nWhich might be an interesting option to serve large datasets with low search latency in Qdrant.\n\n\n\n\n\n## Conclusion\n\n\n\nIn this article, we showed that Qdrant have flexibility in terms of RAM usage and can be used to serve large datasets.']","---

title: Optimize Resources

weight: 11

aliases:

  - ../tutorials/optimize

---



# Optimize Qdrant



Different use cases have different requirements for balancing between memory, speed, and precision.

Qdrant is designed to be flexible and customizable so you can tune it to your needs.



![Trafeoff](/docs/tradeoff.png)



Let's look deeper into each of those possible optimization scenarios.



## Prefer low memory footprint with high speed search
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
It provides configurable trade-offs between RAM usage and search speed.



We are eager to learn more about how you use Qdrant in your projects, what challenges you face, and how we can help you solve them.

Please feel free to join our [Discord](https://qdrant.to/discord) and share your experience with us!
This works in the other direction as well. Whenever you search for something, you can specify a shard or several shards and Qdrant will know where to find them. It will avoid asking all machines in your cluster for results. This will minimize overhead and maximize performance. 



### Common use cases
With a local SSD, we were able to increase the search speed by 10x!



With the production-grade disk, the search speed could be even higher. 

Some configurations of the SSDs can reach 1M IOPS and more.



Which might be an interesting option to serve large datasets with low search latency in Qdrant.





## Conclusion



In this article, we showed that Qdrant have flexibility in terms of RAM usage and can be used to serve large datasets.",0.22488038277511962,0.6911764705882353,0.33935018050541516,0.07692307692307693,0.23880597014925373,0.11636363636363638,0.1339712918660287,0.4117647058823529,0.20216606498194947,0.21052631578947367,0.6470588235294118,0.31768953068592054,0.9198856353759766,0.550482988357544,0.6327323317527771,0.5887489318847656,0.8210040383633254,0.8667224100790918,215,3.0714285714285716,0.4934941623511575
True,,4068.1839999999997,e0139796,How does Qdrant optimize memory and search speed for sparse vectors?,"Qdrant optimizes memory and search speed for sparse vectors by utilizing an inverted index structure to store vectors for each non-zero dimension. This approach allows Qdrant to efficiently represent sparse vectors, which are characterized by a high proportion of zeroes. By only storing information about non-zero dimensions, Qdrant reduces the memory footprint required to store sparse vectors and also speeds up search operations by focusing only on relevant dimensions during indexing and querying processes. This optimization ensures that Qdrant can handle sparse vectors effectively while maintaining efficient memory usage and search performance.","Qdrant optimizes memory and search speed for sparse vectors through the use of efficient indexing and quantization techniques. 

1. **Efficient Indexing:** Qdrant utilizes an inverted index structure to store vectors for each non-zero dimension. This approach optimizes memory usage by only storing information about the non-zero values, reducing the overall memory footprint.

2. **Quantization Techniques:** Qdrant employs scalar quantization to further reduce the memory footprint and speed up the search process. Scalar quantization reduces the amount of memory required to store a vector by a factor of 4, making it more memory-efficient. Additionally, scalar quantization speeds up the search process by using a special SIMD CPU instruction for fast vector comparison. By converting data to `uint8`, Qdrant can leverage this instruction to perform comparisons faster.

By combining efficient indexing with quantization techniques like scalar quantization, Qdrant is able to optimize memory usage and search speed for sparse vectors, making billion-scale vector search affordable and efficient.","performance.



## Sparse Vector Index



*Available as of v1.7.0*



### Key Features of Sparse Vector Index

- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.

- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.



### Search Mechanism

- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.
Our plan for the current [open-source roadmap](https://github.com/qdrant/qdrant/blob/master/docs/roadmap/README.md) is to make billion-scale vector search affordable. Our recent release of the [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/) improves both memory usage (x4) as well as speed (x2). Upcoming [Product Quantization](https://www.irisa.fr/texmex/people/jegou/papers/jegou_searching_with_quantization.pdf) will introduce even another option with more memory saving. Stay tuned
- **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.





#### Memory and speed tuning



In this section, we will discuss how to tune the memory and speed of the search process with quantization.



There are 3 possible modes to place storage of vectors within the qdrant collection:
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
---

title: ""Sparse Vectors in Qdrant: Pure Vector-based Hybrid Search""

short_description: ""Combining the precision of exact keyword search with NN-based ranking""

description: ""Sparse vectors are the generalization of TF-IDF and BM25, that allows to leverage the power of neural networks for text retrieval.""

social_preview_image: /articles_data/sparse-vectors/social_preview.png

small_preview_image: /articles_data/sparse-vectors/sparse-vectors-icon.svg

preview_dir: /articles_data/sparse-vectors/preview","['performance.\n\n\n\n## Sparse Vector Index\n\n\n\n*Available as of v1.7.0*\n\n\n\n### Key Features of Sparse Vector Index\n\n- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.\n\n- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.\n\n\n\n### Search Mechanism\n\n- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.', 'Our plan for the current [open-source roadmap](https://github.com/qdrant/qdrant/blob/master/docs/roadmap/README.md) is to make billion-scale vector search affordable. Our recent release of the [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/) improves both memory usage (x4) as well as speed (x2). Upcoming [Product Quantization](https://www.irisa.fr/texmex/people/jegou/papers/jegou_searching_with_quantization.pdf) will introduce even another option with more memory saving. Stay tuned', '- **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.\n\n\n\n\n\n#### Memory and speed tuning\n\n\n\nIn this section, we will discuss how to tune the memory and speed of the search process with quantization.\n\n\n\nThere are 3 possible modes to place storage of vectors within the qdrant collection:', 'Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.\n\n\n\nIn addition to reducing the memory footprint, scalar quantization also speeds up the search process.\n\nQdrant uses a special SIMD CPU instruction to perform fast vector comparison.\n\nThis instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.', '---\n\ntitle: ""Sparse Vectors in Qdrant: Pure Vector-based Hybrid Search""\n\nshort_description: ""Combining the precision of exact keyword search with NN-based ranking""\n\ndescription: ""Sparse vectors are the generalization of TF-IDF and BM25, that allows to leverage the power of neural networks for text retrieval.""\n\nsocial_preview_image: /articles_data/sparse-vectors/social_preview.png\n\nsmall_preview_image: /articles_data/sparse-vectors/sparse-vectors-icon.svg\n\npreview_dir: /articles_data/sparse-vectors/preview']","Question: How does Qdrant optimize memory and search speed for sparse vectors?\n\nContext: performance.



## Sparse Vector Index



*Available as of v1.7.0*



### Key Features of Sparse Vector Index

- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.

- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.



### Search Mechanism

- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.
Our plan for the current [open-source roadmap](https://github.com/qdrant/qdrant/blob/master/docs/roadmap/README.md) is to make billion-scale vector search affordable. Our recent release of the [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/) improves both memory usage (x4) as well as speed (x2). Upcoming [Product Quantization](https://www.irisa.fr/texmex/people/jegou/papers/jegou_searching_with_quantization.pdf) will introduce even another option with more memory saving. Stay tuned
- **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.





#### Memory and speed tuning



In this section, we will discuss how to tune the memory and speed of the search process with quantization.



There are 3 possible modes to place storage of vectors within the qdrant collection:
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
---

title: ""Sparse Vectors in Qdrant: Pure Vector-based Hybrid Search""

short_description: ""Combining the precision of exact keyword search with NN-based ranking""

description: ""Sparse vectors are the generalization of TF-IDF and BM25, that allows to leverage the power of neural networks for text retrieval.""

social_preview_image: /articles_data/sparse-vectors/social_preview.png

small_preview_image: /articles_data/sparse-vectors/sparse-vectors-icon.svg

preview_dir: /articles_data/sparse-vectors/preview\n\nAnswer:",How does Qdrant optimize memory and search speed for sparse vectors?,"Qdrant optimizes memory and search speed for sparse vectors by utilizing an inverted index structure to store vectors for each non-zero dimension. This approach allows Qdrant to efficiently represent sparse vectors, which are characterized by a high proportion of zeroes. By only storing information about non-zero dimensions, Qdrant reduces the memory footprint required to store sparse vectors and also speeds up search operations by focusing only on relevant dimensions during indexing and querying processes. This optimization ensures that Qdrant can handle sparse vectors effectively while maintaining efficient memory usage and search performance.","['performance.\n\n\n\n## Sparse Vector Index\n\n\n\n*Available as of v1.7.0*\n\n\n\n### Key Features of Sparse Vector Index\n\n- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.\n\n- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.\n\n\n\n### Search Mechanism\n\n- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.', 'Our plan for the current [open-source roadmap](https://github.com/qdrant/qdrant/blob/master/docs/roadmap/README.md) is to make billion-scale vector search affordable. Our recent release of the [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/) improves both memory usage (x4) as well as speed (x2). Upcoming [Product Quantization](https://www.irisa.fr/texmex/people/jegou/papers/jegou_searching_with_quantization.pdf) will introduce even another option with more memory saving. Stay tuned', '- **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.\n\n\n\n\n\n#### Memory and speed tuning\n\n\n\nIn this section, we will discuss how to tune the memory and speed of the search process with quantization.\n\n\n\nThere are 3 possible modes to place storage of vectors within the qdrant collection:', 'Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.\n\n\n\nIn addition to reducing the memory footprint, scalar quantization also speeds up the search process.\n\nQdrant uses a special SIMD CPU instruction to perform fast vector comparison.\n\nThis instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.', '---\n\ntitle: ""Sparse Vectors in Qdrant: Pure Vector-based Hybrid Search""\n\nshort_description: ""Combining the precision of exact keyword search with NN-based ranking""\n\ndescription: ""Sparse vectors are the generalization of TF-IDF and BM25, that allows to leverage the power of neural networks for text retrieval.""\n\nsocial_preview_image: /articles_data/sparse-vectors/social_preview.png\n\nsmall_preview_image: /articles_data/sparse-vectors/sparse-vectors-icon.svg\n\npreview_dir: /articles_data/sparse-vectors/preview']","performance.



## Sparse Vector Index



*Available as of v1.7.0*



### Key Features of Sparse Vector Index

- **Support for Sparse Vectors:** Qdrant supports sparse vectors, characterized by a high proportion of zeroes.

- **Efficient Indexing:** Utilizes an inverted index structure to store vectors for each non-zero dimension, optimizing memory and search speed.



### Search Mechanism

- **Index Usage:** The index identifies vectors with non-zero values in query dimensions during a search.
Our plan for the current [open-source roadmap](https://github.com/qdrant/qdrant/blob/master/docs/roadmap/README.md) is to make billion-scale vector search affordable. Our recent release of the [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/) improves both memory usage (x4) as well as speed (x2). Upcoming [Product Quantization](https://www.irisa.fr/texmex/people/jegou/papers/jegou_searching_with_quantization.pdf) will introduce even another option with more memory saving. Stay tuned
- **Enable rescore**: Having the original vectors available, Qdrant can re-evaluate top-k search results using the original vectors. On large collections, this can improve the search quality, with just minor performance impact.





#### Memory and speed tuning



In this section, we will discuss how to tune the memory and speed of the search process with quantization.



There are 3 possible modes to place storage of vectors within the qdrant collection:
Effectively, this means that the amount of memory required to store a vector is reduced by a factor of 4.



In addition to reducing the memory footprint, scalar quantization also speeds up the search process.

Qdrant uses a special SIMD CPU instruction to perform fast vector comparison.

This instruction works with 8-bit integers, so the conversion to `uint8` allows Qdrant to perform the comparison faster.
---

title: ""Sparse Vectors in Qdrant: Pure Vector-based Hybrid Search""

short_description: ""Combining the precision of exact keyword search with NN-based ranking""

description: ""Sparse vectors are the generalization of TF-IDF and BM25, that allows to leverage the power of neural networks for text retrieval.""

social_preview_image: /articles_data/sparse-vectors/social_preview.png

small_preview_image: /articles_data/sparse-vectors/sparse-vectors-icon.svg

preview_dir: /articles_data/sparse-vectors/preview",0.4391891891891892,0.7142857142857143,0.5439330543933055,0.272108843537415,0.4444444444444444,0.33755274261603374,0.3310810810810811,0.5384615384615384,0.4100418410041841,0.43243243243243246,0.7032967032967034,0.5355648535564853,0.8945528268814087,0.6727601289749146,0.7497711181640625,0.7091810703277588,0.7537476697936654,0.9378350083716214,155,1.684782608695652,0.3976946996282029
True,,3291.7999999999997,f2b794a5,How does Qdrant address the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters?,"Qdrant addresses the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters by extending the HNSW graph with additional edges based on the stored payload values. This extension allows for more efficient searching of nearby vectors using the HNSW index while applying filters during the search in the graph. By adding extra edges, Qdrant minimizes the overhead on condition checks since the conditions only need to be calculated for a small fraction of the points involved in the search. This approach improves the performance of the HNSW index in cases where the traditional methods of weak filters or complete rescore are not optimal, providing a more balanced solution for scenarios that fall in between.","Qdrant addresses the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters by implementing an advanced query planning strategy. This strategy allows Qdrant to efficiently apply filters without compromising on the speed and accuracy of the search results. By incorporating additional filters during the search process, Qdrant can check conditions over the values that match the type of the filtering condition, ensuring that only relevant data is considered. In cases where the stored value type does not fit the filtering condition, Qdrant will treat it as not satisfied, thus preventing mismatches and empty outputs. This approach optimizes the performance of the solution and ensures that the search results are both fast and accurate, even in scenarios with varying levels of filter stringency.","Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements
All right, keep going. I like it.



Rishabh Bhardwaj:

Yeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.



Demetrios:

Right.



Rishabh Bhardwaj:
Qdrant avoids all these problems and also benefits from the speed boost, as it implements an advanced [query planning strategy](/documentation/search/#query-planning).



<aside role=""status"">The Filtering Benchmark is all about changes in performance between filter and un-filtered queries. Please refer to the search benchmark for absolute speed comparison.</aside>
2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.

3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.

4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.

5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.
This feature is implemented as additional filters during the search and will enable you to incorporate custom logic on top of semantic similarity.



During the filtering, Qdrant will check the conditions over those values that match the type of the filtering condition. If the stored value type does not fit the filtering condition - it will be considered not satisfied.



For example, you will get an empty output if you apply the [range condition](../filtering/#range) on the string data.","['Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements', 'All right, keep going. I like it.\n\n\n\nRishabh Bhardwaj:\n\nYeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.\n\n\n\nDemetrios:\n\nRight.\n\n\n\nRishabh Bhardwaj:', 'Qdrant avoids all these problems and also benefits from the speed boost, as it implements an advanced [query planning strategy](/documentation/search/#query-planning).\n\n\n\n<aside role=""status"">The Filtering Benchmark is all about changes in performance between filter and un-filtered queries. Please refer to the search benchmark for absolute speed comparison.</aside>', '2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.\n\n3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.\n\n4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.\n\n5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.', 'This feature is implemented as additional filters during the search and will enable you to incorporate custom logic on top of semantic similarity.\n\n\n\nDuring the filtering, Qdrant will check the conditions over those values that match the type of the filtering condition. If the stored value type does not fit the filtering condition - it will be considered not satisfied.\n\n\n\nFor example, you will get an empty output if you apply the [range condition](../filtering/#range) on the string data.']","Question: How does Qdrant address the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters?\n\nContext: Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements
All right, keep going. I like it.



Rishabh Bhardwaj:

Yeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.



Demetrios:

Right.



Rishabh Bhardwaj:
Qdrant avoids all these problems and also benefits from the speed boost, as it implements an advanced [query planning strategy](/documentation/search/#query-planning).



<aside role=""status"">The Filtering Benchmark is all about changes in performance between filter and un-filtered queries. Please refer to the search benchmark for absolute speed comparison.</aside>
2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.

3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.

4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.

5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.
This feature is implemented as additional filters during the search and will enable you to incorporate custom logic on top of semantic similarity.



During the filtering, Qdrant will check the conditions over those values that match the type of the filtering condition. If the stored value type does not fit the filtering condition - it will be considered not satisfied.



For example, you will get an empty output if you apply the [range condition](../filtering/#range) on the string data.\n\nAnswer:",How does Qdrant address the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters?,"Qdrant addresses the limitations faced by the HNSW index when dealing with cases in the middle of weak and stringent filters by extending the HNSW graph with additional edges based on the stored payload values. This extension allows for more efficient searching of nearby vectors using the HNSW index while applying filters during the search in the graph. By adding extra edges, Qdrant minimizes the overhead on condition checks since the conditions only need to be calculated for a small fraction of the points involved in the search. This approach improves the performance of the HNSW index in cases where the traditional methods of weak filters or complete rescore are not optimal, providing a more balanced solution for scenarios that fall in between.","['Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements', 'All right, keep going. I like it.\n\n\n\nRishabh Bhardwaj:\n\nYeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.\n\n\n\nDemetrios:\n\nRight.\n\n\n\nRishabh Bhardwaj:', 'Qdrant avoids all these problems and also benefits from the speed boost, as it implements an advanced [query planning strategy](/documentation/search/#query-planning).\n\n\n\n<aside role=""status"">The Filtering Benchmark is all about changes in performance between filter and un-filtered queries. Please refer to the search benchmark for absolute speed comparison.</aside>', '2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.\n\n3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.\n\n4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.\n\n5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.', 'This feature is implemented as additional filters during the search and will enable you to incorporate custom logic on top of semantic similarity.\n\n\n\nDuring the filtering, Qdrant will check the conditions over those values that match the type of the filtering condition. If the stored value type does not fit the filtering condition - it will be considered not satisfied.\n\n\n\nFor example, you will get an empty output if you apply the [range condition](../filtering/#range) on the string data.']","Our unique custom modification of the [HNSW algorithm](https://qdrant.tech/articles/filtrable-hnsw/) for Approximate Nearest Neighbor Search (ANN) allows querying the result with a state-of-the-art speed and applying filters without compromising on results. Cloud-native support for distributed deployment and replications makes the engine suitable for high-throughput applications with real-time latency requirements
All right, keep going. I like it.



Rishabh Bhardwaj:

Yeah. So initially, during the experimentations, we begin with the default values for the HNSW algorithm that Qdrant ships with. And these benchmarks that I just told you about, it was based on those parameters. But as our use cases evolved, we also experimented on multiple values of basically M and EF construct that Qdrant allow us to specify in the indexing algorithm.



Demetrios:

Right.



Rishabh Bhardwaj:
Qdrant avoids all these problems and also benefits from the speed boost, as it implements an advanced [query planning strategy](/documentation/search/#query-planning).



<aside role=""status"">The Filtering Benchmark is all about changes in performance between filter and un-filtered queries. Please refer to the search benchmark for absolute speed comparison.</aside>
2. Learn how Qdrant, an open-source vector database, outperformed other solutions and provided an efficient solution for high-speed matching.

3. Explore the unique modification of the HNSW algorithm in Qdrant and how it optimized the performance of the solution.

4. Dive into the crucial role of geofiltering and how it ensures accurate matching based on hotel locations.

5. Gain insights into the considerations surrounding GDPR compliance and the secure handling of hotel data.
This feature is implemented as additional filters during the search and will enable you to incorporate custom logic on top of semantic similarity.



During the filtering, Qdrant will check the conditions over those values that match the type of the filtering condition. If the stored value type does not fit the filtering condition - it will be considered not satisfied.



For example, you will get an empty output if you apply the [range condition](../filtering/#range) on the string data.",0.5,0.5471698113207547,0.5225225225225225,0.21739130434782608,0.23809523809523808,0.22727272727272724,0.33620689655172414,0.36792452830188677,0.3513513513513513,0.4224137931034483,0.46226415094339623,0.44144144144144143,0.790346622467041,0.6797856092453003,0.6839027404785156,0.6818379163742065,0.9969213952077552,0.9429576636757702,131,1.065040650406504,0.34247145252960703
True,,3859.45,9d8fa92c,How is metric learning utilized in addressing the challenge of detecting anomalies in coffee beans?,"Metric learning is utilized in addressing the challenge of detecting anomalies in coffee beans by encoding images in an n-dimensional vector space and using learned similarities to label images during the inference process. This approach involves representing the images in a vector space where similar images are closer together in the space. By doing so, the model can effectively classify and detect anomalies in coffee beans based on the similarities learned during training. The KNN (K-Nearest Neighbors) classification method is commonly used in this approach as it simplifies the process of determining the class label of an image based on the labels of its nearest neighbors in the vector space. This method allows for the adaptation to new types of defects and changing shooting conditions, making the model more robust and accurate in detecting anomalies in coffee beans.","Metric learning is utilized in addressing the challenge of detecting anomalies in coffee beans by leveraging the collected and labeled images of coffee beans with various defects. The data is used to train a classifier that evaluates crop quality and highlights possible problems. However, as anomalies in coffee beans are diverse and new types of defects may appear over time, a one-time labeled dataset becomes insufficient.

To address this challenge, the approach of metric learning is used. By using metric learning, the evaluation metrics can be improved considerably to match the results of a supervised classification approach. This allows for more efficient anomaly detection in coffee beans with just a fraction of the labeled data (~0.6%).

Some advantages of using metric learning in this context include the ability to benefit from unlabeled data, tuning relevant metrics according to changing requirements during inference without re-training, and adding queries labeled with a high score to the KNN classifier on the fly as new data points. This approach helps in enhancing the quality assessment of coffee beans and improving anomaly detection in a cost-effective and efficient manner.","---

title: Metric Learning for Anomaly Detection

short_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""

description: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.

social_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg

preview_dir: /articles_data/detecting-coffee-anomalies/preview
## Coffee Beans



[Agrivero.ai](https://agrivero.ai/) - is a company making AI-enabled solution for quality control & traceability of green coffee for producers, traders, and roasters.

They have collected and labeled more than **30 thousand** images of coffee beans with various defects - wet, broken, chipped, or bug-infested samples.

This data is used to train a classifier that evaluates crop quality and highlights possible problems.
{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}



We should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it's own.

In the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.



Let's find out how metric learning might help to address this challenge.



## Metric Learning Approach
We will describe our approach in the further articles.



This time it converged smoothly, and our evaluation metrics also improved considerably to match the supervised classification approach.



{{< figure src=/articles_data/detecting-coffee-anomalies/ae_report_knn.png caption=""Metrics for the autoencoder model with KNN classifier"" >}}



{{< figure src=/articles_data/detecting-coffee-anomalies/ft_report_knn.png caption=""Metrics for the finetuned model with KNN classifier"" >}}
{{< figure src=/articles_data/detecting-coffee-anomalies/anomalies_detection.png caption=""Production deployment"" >}}



This approach has the following advantages:



* We can benefit from unlabeled data, considering labeling is time-consuming and expensive.

* The relevant metric, e.g., precision or recall, can be tuned according to changing requirements during the inference without re-training.

* Queries labeled with a high score can be added to the KNN classifier on the fly as new data points.","['---\n\ntitle: Metric Learning for Anomaly Detection\n\nshort_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""\n\ndescription: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.\n\nsocial_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg\n\npreview_dir: /articles_data/detecting-coffee-anomalies/preview', '## Coffee Beans\n\n\n\n[Agrivero.ai](https://agrivero.ai/) - is a company making AI-enabled solution for quality control & traceability of green coffee for producers, traders, and roasters.\n\nThey have collected and labeled more than **30 thousand** images of coffee beans with various defects - wet, broken, chipped, or bug-infested samples.\n\nThis data is used to train a classifier that evaluates crop quality and highlights possible problems.', '{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}\n\n\n\nWe should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it\'s own.\n\nIn the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.\n\n\n\nLet\'s find out how metric learning might help to address this challenge.\n\n\n\n## Metric Learning Approach', 'We will describe our approach in the further articles.\n\n\n\nThis time it converged smoothly, and our evaluation metrics also improved considerably to match the supervised classification approach.\n\n\n\n{{< figure src=/articles_data/detecting-coffee-anomalies/ae_report_knn.png caption=""Metrics for the autoencoder model with KNN classifier"" >}}\n\n\n\n{{< figure src=/articles_data/detecting-coffee-anomalies/ft_report_knn.png caption=""Metrics for the finetuned model with KNN classifier"" >}}', '{{< figure src=/articles_data/detecting-coffee-anomalies/anomalies_detection.png caption=""Production deployment"" >}}\n\n\n\nThis approach has the following advantages:\n\n\n\n* We can benefit from unlabeled data, considering labeling is time-consuming and expensive.\n\n* The relevant metric, e.g., precision or recall, can be tuned according to changing requirements during the inference without re-training.\n\n* Queries labeled with a high score can be added to the KNN classifier on the fly as new data points.']","Question: How is metric learning utilized in addressing the challenge of detecting anomalies in coffee beans?\n\nContext: ---

title: Metric Learning for Anomaly Detection

short_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""

description: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.

social_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg

preview_dir: /articles_data/detecting-coffee-anomalies/preview
## Coffee Beans



[Agrivero.ai](https://agrivero.ai/) - is a company making AI-enabled solution for quality control & traceability of green coffee for producers, traders, and roasters.

They have collected and labeled more than **30 thousand** images of coffee beans with various defects - wet, broken, chipped, or bug-infested samples.

This data is used to train a classifier that evaluates crop quality and highlights possible problems.
{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}



We should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it's own.

In the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.



Let's find out how metric learning might help to address this challenge.



## Metric Learning Approach
We will describe our approach in the further articles.



This time it converged smoothly, and our evaluation metrics also improved considerably to match the supervised classification approach.



{{< figure src=/articles_data/detecting-coffee-anomalies/ae_report_knn.png caption=""Metrics for the autoencoder model with KNN classifier"" >}}



{{< figure src=/articles_data/detecting-coffee-anomalies/ft_report_knn.png caption=""Metrics for the finetuned model with KNN classifier"" >}}
{{< figure src=/articles_data/detecting-coffee-anomalies/anomalies_detection.png caption=""Production deployment"" >}}



This approach has the following advantages:



* We can benefit from unlabeled data, considering labeling is time-consuming and expensive.

* The relevant metric, e.g., precision or recall, can be tuned according to changing requirements during the inference without re-training.

* Queries labeled with a high score can be added to the KNN classifier on the fly as new data points.\n\nAnswer:",How is metric learning utilized in addressing the challenge of detecting anomalies in coffee beans?,"Metric learning is utilized in addressing the challenge of detecting anomalies in coffee beans by encoding images in an n-dimensional vector space and using learned similarities to label images during the inference process. This approach involves representing the images in a vector space where similar images are closer together in the space. By doing so, the model can effectively classify and detect anomalies in coffee beans based on the similarities learned during training. The KNN (K-Nearest Neighbors) classification method is commonly used in this approach as it simplifies the process of determining the class label of an image based on the labels of its nearest neighbors in the vector space. This method allows for the adaptation to new types of defects and changing shooting conditions, making the model more robust and accurate in detecting anomalies in coffee beans.","['---\n\ntitle: Metric Learning for Anomaly Detection\n\nshort_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""\n\ndescription: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.\n\nsocial_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg\n\npreview_dir: /articles_data/detecting-coffee-anomalies/preview', '## Coffee Beans\n\n\n\n[Agrivero.ai](https://agrivero.ai/) - is a company making AI-enabled solution for quality control & traceability of green coffee for producers, traders, and roasters.\n\nThey have collected and labeled more than **30 thousand** images of coffee beans with various defects - wet, broken, chipped, or bug-infested samples.\n\nThis data is used to train a classifier that evaluates crop quality and highlights possible problems.', '{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}\n\n\n\nWe should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it\'s own.\n\nIn the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.\n\n\n\nLet\'s find out how metric learning might help to address this challenge.\n\n\n\n## Metric Learning Approach', 'We will describe our approach in the further articles.\n\n\n\nThis time it converged smoothly, and our evaluation metrics also improved considerably to match the supervised classification approach.\n\n\n\n{{< figure src=/articles_data/detecting-coffee-anomalies/ae_report_knn.png caption=""Metrics for the autoencoder model with KNN classifier"" >}}\n\n\n\n{{< figure src=/articles_data/detecting-coffee-anomalies/ft_report_knn.png caption=""Metrics for the finetuned model with KNN classifier"" >}}', '{{< figure src=/articles_data/detecting-coffee-anomalies/anomalies_detection.png caption=""Production deployment"" >}}\n\n\n\nThis approach has the following advantages:\n\n\n\n* We can benefit from unlabeled data, considering labeling is time-consuming and expensive.\n\n* The relevant metric, e.g., precision or recall, can be tuned according to changing requirements during the inference without re-training.\n\n* Queries labeled with a high score can be added to the KNN classifier on the fly as new data points.']","---

title: Metric Learning for Anomaly Detection

short_description: ""How to use metric learning to detect anomalies: quality assessment of coffee beans with just 200 labelled samples""

description: Practical use of metric learning for anomaly detection. A way to match the results of a classification-based approach with only ~0.6% of the labeled data.

social_preview_image: /articles_data/detecting-coffee-anomalies/preview/social_preview.jpg

preview_dir: /articles_data/detecting-coffee-anomalies/preview
## Coffee Beans



[Agrivero.ai](https://agrivero.ai/) - is a company making AI-enabled solution for quality control & traceability of green coffee for producers, traders, and roasters.

They have collected and labeled more than **30 thousand** images of coffee beans with various defects - wet, broken, chipped, or bug-infested samples.

This data is used to train a classifier that evaluates crop quality and highlights possible problems.
{{< figure src=/articles_data/detecting-coffee-anomalies/detection.gif caption=""Anomalies in coffee"" width=""400px"" >}}



We should note that anomalies are very diverse, so the enumeration of all possible anomalies is a challenging task on it's own.

In the course of work, new types of defects appear, and shooting conditions change. Thus, a one-time labeled dataset becomes insufficient.



Let's find out how metric learning might help to address this challenge.



## Metric Learning Approach
We will describe our approach in the further articles.



This time it converged smoothly, and our evaluation metrics also improved considerably to match the supervised classification approach.



{{< figure src=/articles_data/detecting-coffee-anomalies/ae_report_knn.png caption=""Metrics for the autoencoder model with KNN classifier"" >}}



{{< figure src=/articles_data/detecting-coffee-anomalies/ft_report_knn.png caption=""Metrics for the finetuned model with KNN classifier"" >}}
{{< figure src=/articles_data/detecting-coffee-anomalies/anomalies_detection.png caption=""Production deployment"" >}}



This approach has the following advantages:



* We can benefit from unlabeled data, considering labeling is time-consuming and expensive.

* The relevant metric, e.g., precision or recall, can be tuned according to changing requirements during the inference without re-training.

* Queries labeled with a high score can be added to the KNN classifier on the fly as new data points.",0.4093567251461988,0.5645161290322581,0.4745762711864407,0.15294117647058825,0.21138211382113822,0.17747440273037543,0.2222222222222222,0.3064516129032258,0.2576271186440678,0.38011695906432746,0.5241935483870968,0.44067796610169496,0.852584958076477,0.6496639251708984,0.6706928014755249,0.6600108742713928,0.9879608913033735,0.7931729137897492,184,1.3333333333333333,0.4892245004102568
True,,2671.315,652f61ca,How can one reproduce the benchmark for Open Source vector databases?,"To reproduce the benchmark for Open Source vector databases, one can access the source code available on Github at https://github.com/qdrant/vector-db-benchmark. The repository contains a `README.md` file that provides detailed instructions on how to run the benchmark for a specific engine. By following the guidelines outlined in the README file, individuals can easily replicate the benchmarks conducted on the Open Source vector databases. Additionally, the document emphasizes the importance of transparency in the benchmarking process and invites contributions from individuals who may be able to provide insights, identify misconfigurations, or suggest improvements. Interested parties can contribute to the benchmarking efforts by accessing the benchmark repository at https://github.com/qdrant/vector-db-benchmark.","To reproduce the benchmark for Open Source vector databases, you can follow these steps:

1. Visit the Github repository at https://github.com/qdrant/vector-db-benchmark.

2. Read the `README.md` file in the repository to understand the process of running the benchmark for a specific engine.

3. Set up the necessary environment and dependencies as described in the repository.

4. Run the benchmark using the provided scripts or instructions.

5. Analyze the benchmark results and compare the performance of different vector databases.

6. If you have any improvements or contributions to make, you can do so by participating in the development of the benchmarking framework.

By following these steps, you can reproduce the benchmark for Open Source vector databases and contribute to improving the performance of vector databases.","That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.



This is not the final list, and we’ll continue benchmarking as many different engines as possible.



## How to reproduce the benchmark?



The source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.
If you're interested in testing the benchmark yourself or want to contribute to its development, head over to our [benchmark repository](https://github.com/qdrant/vector-db-benchmark). We appreciate your support and involvement in improving the performance of vector databases.
---

title: Vector Database Benchmarks

description: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.

keywords:

  - vector databases comparative benchmark

  - ANN Benchmark

  - Qdrant vs Milvus

  - Qdrant vs Weaviate

  - Qdrant vs Redis

  - Qdrant vs ElasticSearch

  - benchmark

  - performance

  - latency

  - RPS

  - comparison

  - vector search

  - embedding

preview_image: /benchmarks/benchmark-1.png
However, we tried our best, kept scrolling the docs up and down, experimented with combinations of different configurations, and gave all of them an equal chance to stand out. If you believe you can do it better than us, our **benchmarks are fully [open-sourced](https://github.com/qdrant/vector-db-benchmark), and contributions are welcome**!





## What do we measure?



There are several factors considered while deciding on which database to use.
There is one important thing - **the speed of the vector databases should to be compared only if they achieve the same precision**. Otherwise, they could maximize the speed factors by providing inaccurate results, which everybody would rather avoid. Thus, our benchmark results are compared only at a specific search precision threshold.



## How we select hardware?



In our experiments, we are not focusing on the absolute values of the metrics but rather on a relative comparison of different engines.","['That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.\n\n\n\nThis is not the final list, and we’ll continue benchmarking as many different engines as possible.\n\n\n\n## How to reproduce the benchmark?\n\n\n\nThe source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.', ""If you're interested in testing the benchmark yourself or want to contribute to its development, head over to our [benchmark repository](https://github.com/qdrant/vector-db-benchmark). We appreciate your support and involvement in improving the performance of vector databases."", '---\n\ntitle: Vector Database Benchmarks\n\ndescription: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.\n\nkeywords:\n\n  - vector databases comparative benchmark\n\n  - ANN Benchmark\n\n  - Qdrant vs Milvus\n\n  - Qdrant vs Weaviate\n\n  - Qdrant vs Redis\n\n  - Qdrant vs ElasticSearch\n\n  - benchmark\n\n  - performance\n\n  - latency\n\n  - RPS\n\n  - comparison\n\n  - vector search\n\n  - embedding\n\npreview_image: /benchmarks/benchmark-1.png', 'However, we tried our best, kept scrolling the docs up and down, experimented with combinations of different configurations, and gave all of them an equal chance to stand out. If you believe you can do it better than us, our **benchmarks are fully [open-sourced](https://github.com/qdrant/vector-db-benchmark), and contributions are welcome**!\n\n\n\n\n\n## What do we measure?\n\n\n\nThere are several factors considered while deciding on which database to use.', 'There is one important thing - **the speed of the vector databases should to be compared only if they achieve the same precision**. Otherwise, they could maximize the speed factors by providing inaccurate results, which everybody would rather avoid. Thus, our benchmark results are compared only at a specific search precision threshold.\n\n\n\n## How we select hardware?\n\n\n\nIn our experiments, we are not focusing on the absolute values of the metrics but rather on a relative comparison of different engines.']","Question: How can one reproduce the benchmark for Open Source vector databases?\n\nContext: That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.



This is not the final list, and we’ll continue benchmarking as many different engines as possible.



## How to reproduce the benchmark?



The source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.
If you're interested in testing the benchmark yourself or want to contribute to its development, head over to our [benchmark repository](https://github.com/qdrant/vector-db-benchmark). We appreciate your support and involvement in improving the performance of vector databases.
---

title: Vector Database Benchmarks

description: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.

keywords:

- vector databases comparative benchmark

- ANN Benchmark

- Qdrant vs Milvus

- Qdrant vs Weaviate

- Qdrant vs Redis

- Qdrant vs ElasticSearch

- benchmark

- performance

- latency

- RPS

- comparison

- vector search

- embedding

preview_image: /benchmarks/benchmark-1.png
However, we tried our best, kept scrolling the docs up and down, experimented with combinations of different configurations, and gave all of them an equal chance to stand out. If you believe you can do it better than us, our **benchmarks are fully [open-sourced](https://github.com/qdrant/vector-db-benchmark), and contributions are welcome**!





## What do we measure?



There are several factors considered while deciding on which database to use.
There is one important thing - **the speed of the vector databases should to be compared only if they achieve the same precision**. Otherwise, they could maximize the speed factors by providing inaccurate results, which everybody would rather avoid. Thus, our benchmark results are compared only at a specific search precision threshold.



## How we select hardware?



In our experiments, we are not focusing on the absolute values of the metrics but rather on a relative comparison of different engines.\n\nAnswer:",How can one reproduce the benchmark for Open Source vector databases?,"To reproduce the benchmark for Open Source vector databases, one can access the source code available on Github at https://github.com/qdrant/vector-db-benchmark. The repository contains a `README.md` file that provides detailed instructions on how to run the benchmark for a specific engine. By following the guidelines outlined in the README file, individuals can easily replicate the benchmarks conducted on the Open Source vector databases. Additionally, the document emphasizes the importance of transparency in the benchmarking process and invites contributions from individuals who may be able to provide insights, identify misconfigurations, or suggest improvements. Interested parties can contribute to the benchmarking efforts by accessing the benchmark repository at https://github.com/qdrant/vector-db-benchmark.","['That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.\n\n\n\nThis is not the final list, and we’ll continue benchmarking as many different engines as possible.\n\n\n\n## How to reproduce the benchmark?\n\n\n\nThe source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.', ""If you're interested in testing the benchmark yourself or want to contribute to its development, head over to our [benchmark repository](https://github.com/qdrant/vector-db-benchmark). We appreciate your support and involvement in improving the performance of vector databases."", '---\n\ntitle: Vector Database Benchmarks\n\ndescription: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.\n\nkeywords:\n\n  - vector databases comparative benchmark\n\n  - ANN Benchmark\n\n  - Qdrant vs Milvus\n\n  - Qdrant vs Weaviate\n\n  - Qdrant vs Redis\n\n  - Qdrant vs ElasticSearch\n\n  - benchmark\n\n  - performance\n\n  - latency\n\n  - RPS\n\n  - comparison\n\n  - vector search\n\n  - embedding\n\npreview_image: /benchmarks/benchmark-1.png', 'However, we tried our best, kept scrolling the docs up and down, experimented with combinations of different configurations, and gave all of them an equal chance to stand out. If you believe you can do it better than us, our **benchmarks are fully [open-sourced](https://github.com/qdrant/vector-db-benchmark), and contributions are welcome**!\n\n\n\n\n\n## What do we measure?\n\n\n\nThere are several factors considered while deciding on which database to use.', 'There is one important thing - **the speed of the vector databases should to be compared only if they achieve the same precision**. Otherwise, they could maximize the speed factors by providing inaccurate results, which everybody would rather avoid. Thus, our benchmark results are compared only at a specific search precision threshold.\n\n\n\n## How we select hardware?\n\n\n\nIn our experiments, we are not focusing on the absolute values of the metrics but rather on a relative comparison of different engines.']","That makes the comparison unfair. That’s why we purely focused on testing the Open Source vector databases, so everybody may reproduce the benchmarks easily.



This is not the final list, and we’ll continue benchmarking as many different engines as possible.



## How to reproduce the benchmark?



The source code is available on [Github](https://github.com/qdrant/vector-db-benchmark) and has a `README.md` file describing the process of running the benchmark for a specific engine.
If you're interested in testing the benchmark yourself or want to contribute to its development, head over to our [benchmark repository](https://github.com/qdrant/vector-db-benchmark). We appreciate your support and involvement in improving the performance of vector databases.
---

title: Vector Database Benchmarks

description: The first comparative benchmark and benchmarking framework for vector search engines and vector databases.

keywords:

  - vector databases comparative benchmark

  - ANN Benchmark

  - Qdrant vs Milvus

  - Qdrant vs Weaviate

  - Qdrant vs Redis

  - Qdrant vs ElasticSearch

  - benchmark

  - performance

  - latency

  - RPS

  - comparison

  - vector search

  - embedding

preview_image: /benchmarks/benchmark-1.png
However, we tried our best, kept scrolling the docs up and down, experimented with combinations of different configurations, and gave all of them an equal chance to stand out. If you believe you can do it better than us, our **benchmarks are fully [open-sourced](https://github.com/qdrant/vector-db-benchmark), and contributions are welcome**!





## What do we measure?



There are several factors considered while deciding on which database to use.
There is one important thing - **the speed of the vector databases should to be compared only if they achieve the same precision**. Otherwise, they could maximize the speed factors by providing inaccurate results, which everybody would rather avoid. Thus, our benchmark results are compared only at a specific search precision threshold.



## How we select hardware?



In our experiments, we are not focusing on the absolute values of the metrics but rather on a relative comparison of different engines.",0.49557522123893805,0.5384615384615384,0.5161290322580645,0.23214285714285715,0.2524271844660194,0.24186046511627907,0.35398230088495575,0.38461538461538464,0.3686635944700461,0.48672566371681414,0.5288461538461539,0.5069124423963134,0.94533371925354,0.6767383217811584,0.6902607679367065,0.6834326386451721,0.6202099328261933,0.971256835758686,124,1.1481481481481481,0.5866135200635582
True,,3425.326,6e00b8cd,What is the purpose of the `ordering` parameter in Qdrant and when should it be used?,"The `ordering` parameter in Qdrant is used with update and delete operations to ensure that the operations are executed in the same order on all replicas. When this option is enabled, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This is beneficial in preventing data inconsistency that may arise from concurrent updates of the same documents. The `ordering` parameter is particularly recommended when read operations are more frequent than updates and when search performance is critical. By enforcing a specific order of operations across replicas, the `ordering` parameter helps maintain data consistency and integrity in distributed environments.","The `ordering` parameter in Qdrant is used with update and delete operations to ensure that the operations are executed in the same order on all replicas. By setting the `ordering` parameter to `WriteOrderingType.Strong`, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This helps in avoiding data inconsistency in case of concurrent updates of the same documents.

The `ordering` parameter is useful in scenarios where maintaining consistency across replicas is crucial, especially when dealing with concurrent updates. By enforcing strong ordering, Qdrant ensures that the updates are applied in a consistent manner across all replicas, preventing any potential conflicts or inconsistencies in the data.

Overall, the `ordering` parameter provides a mechanism to maintain data integrity and consistency in distributed environments by controlling the order in which updates are applied to replicas.","- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
Payload = { [""city""] = ""blue"" }

		}

	},

	ordering: WriteOrderingType.Strong

);

```



## Listener mode



<aside role=""alert"">This is an experimental feature, its behavior may change in the future.</aside>



In some cases it might be useful to have a Qdrant node that only accumulates data and does not participate in search operations.

There are several scenarios where this can be useful:
```json

{

  ""indices"": [6, 7],

  ""values"": [1.0, 2.0]

}

```



The `indices` and `values` arrays must have the same length.

And the `indices` must be unique.



If the `indices` are not sorted, Qdrant will sort them internally so you may not rely on the order of the elements.



Sparse vectors must be named and can be uploaded in the same way as dense vectors.



```http

PUT /collections/{collection_name}/points

{

    ""points"": [

        {

            ""id"": 1,

            ""vector"": {
. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.","['- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", 'Payload = { [""city""] = ""blue"" }\n\n\t\t}\n\n\t},\n\n\tordering: WriteOrderingType.Strong\n\n);\n\n```\n\n\n\n## Listener mode\n\n\n\n<aside role=""alert"">This is an experimental feature, its behavior may change in the future.</aside>\n\n\n\nIn some cases it might be useful to have a Qdrant node that only accumulates data and does not participate in search operations.\n\nThere are several scenarios where this can be useful:', '```json\n\n{\n\n  ""indices"": [6, 7],\n\n  ""values"": [1.0, 2.0]\n\n}\n\n```\n\n\n\nThe `indices` and `values` arrays must have the same length.\n\nAnd the `indices` must be unique.\n\n\n\nIf the `indices` are not sorted, Qdrant will sort them internally so you may not rely on the order of the elements.\n\n\n\nSparse vectors must be named and can be uploaded in the same way as dense vectors.\n\n\n\n```http\n\nPUT /collections/{collection_name}/points\n\n{\n\n    ""points"": [\n\n        {\n\n            ""id"": 1,\n\n            ""vector"": {', '. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.']","Question: What is the purpose of the `ordering` parameter in Qdrant and when should it be used?\n\nContext: - Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
Payload = { [""city""] = ""blue"" }

}

},

ordering: WriteOrderingType.Strong

);

```



## Listener mode



<aside role=""alert"">This is an experimental feature, its behavior may change in the future.</aside>



In some cases it might be useful to have a Qdrant node that only accumulates data and does not participate in search operations.

There are several scenarios where this can be useful:
```json

{

""indices"": [6, 7],

""values"": [1.0, 2.0]

}

```



The `indices` and `values` arrays must have the same length.

And the `indices` must be unique.



If the `indices` are not sorted, Qdrant will sort them internally so you may not rely on the order of the elements.



Sparse vectors must be named and can be uploaded in the same way as dense vectors.



```http

PUT /collections/{collection_name}/points

{

""points"": [

{

""id"": 1,

""vector"": {
. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.\n\nAnswer:",What is the purpose of the `ordering` parameter in Qdrant and when should it be used?,"The `ordering` parameter in Qdrant is used with update and delete operations to ensure that the operations are executed in the same order on all replicas. When this option is enabled, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This is beneficial in preventing data inconsistency that may arise from concurrent updates of the same documents. The `ordering` parameter is particularly recommended when read operations are more frequent than updates and when search performance is critical. By enforcing a specific order of operations across replicas, the `ordering` parameter helps maintain data consistency and integrity in distributed environments.","['- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents', ""Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk."", 'Payload = { [""city""] = ""blue"" }\n\n\t\t}\n\n\t},\n\n\tordering: WriteOrderingType.Strong\n\n);\n\n```\n\n\n\n## Listener mode\n\n\n\n<aside role=""alert"">This is an experimental feature, its behavior may change in the future.</aside>\n\n\n\nIn some cases it might be useful to have a Qdrant node that only accumulates data and does not participate in search operations.\n\nThere are several scenarios where this can be useful:', '```json\n\n{\n\n  ""indices"": [6, 7],\n\n  ""values"": [1.0, 2.0]\n\n}\n\n```\n\n\n\nThe `indices` and `values` arrays must have the same length.\n\nAnd the `indices` must be unique.\n\n\n\nIf the `indices` are not sorted, Qdrant will sort them internally so you may not rely on the order of the elements.\n\n\n\nSparse vectors must be named and can be uploaded in the same way as dense vectors.\n\n\n\n```http\n\nPUT /collections/{collection_name}/points\n\n{\n\n    ""points"": [\n\n        {\n\n            ""id"": 1,\n\n            ""vector"": {', '. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.']","- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents
Those familiar with the Qdrant API will notice that the extra care taken to be consistent with the existing named vectors API -- this is to make it easier to use sparse vectors in existing codebases. As always, you're able to **apply payload filters**, shard keys, and other advanced features you've come to expect from Qdrant. To make things easier for you, the indices and values don't have to be sorted before upsert. Qdrant will sort them when the index is persisted e.g. on disk.
Payload = { [""city""] = ""blue"" }

		}

	},

	ordering: WriteOrderingType.Strong

);

```



## Listener mode



<aside role=""alert"">This is an experimental feature, its behavior may change in the future.</aside>



In some cases it might be useful to have a Qdrant node that only accumulates data and does not participate in search operations.

There are several scenarios where this can be useful:
```json

{

  ""indices"": [6, 7],

  ""values"": [1.0, 2.0]

}

```



The `indices` and `values` arrays must have the same length.

And the `indices` must be unique.



If the `indices` are not sorted, Qdrant will sort them internally so you may not rely on the order of the elements.



Sparse vectors must be named and can be uploaded in the same way as dense vectors.



```http

PUT /collections/{collection_name}/points

{

    ""points"": [

        {

            ""id"": 1,

            ""vector"": {
. In that case, the Python client for Qdrant also provides a parameter called exact, and if we specify it as true, then it basically does not use indexing and it makes a full search on the whole vector collection, basically.",0.5859375,0.7575757575757576,0.6607929515418502,0.41732283464566927,0.5408163265306123,0.47111111111111115,0.5234375,0.6767676767676768,0.5903083700440528,0.5625,0.7272727272727273,0.6343612334801763,0.9608546495437622,0.7908414602279663,0.8339189887046814,0.811809241771698,0.9906588038429618,0.8178404870617669,144,1.2972972972972974,0.5358142762575366
True,,3277.1400000000003,eee43d7b,How can you enable API key based authentication in Qdrant instances for security purposes?,"To enable API key based authentication in Qdrant instances for security purposes, you must specify a key in the configuration. This can be done by adding the following code snippet to your configuration file:

```yaml
service:
  # Set an api-key.
  # If set, all requests must include a header with the api-key.
  # example header: `api-key: <API-KEY>`
  #
  # If you enable this you should also enable TLS.
  # (Either above or via an external service like nginx.)
  # Sending an api-key over an unencrypted channel is insecure.
  api_key: your_secret_api_key_here
```

By adding this configuration, all requests to your Qdrant instance must include a header with the specified API key. This simple form of client authentication helps secure your instance and is available starting from version 1.2.0","To enable API key based authentication in Qdrant instances for security purposes, you can follow these steps:

1. Specify the API key in the configuration file:

```yaml
service:
  api_key: your_secret_api_key_here
```

2. Alternatively, you can set the API key using an environment variable:

```bash
export QDRANT__SERVICE__API_KEY=your_secret_api_key_here
```

3. Ensure that TLS (Transport Layer Security) is used to prevent leaking the API key over an unencrypted connection.

4. When making requests to the Qdrant instance, include the API key in the request header every time you make a request via Rest or gRPC interface.

5. You can also refer to the official Qdrant clients for Python, TypeScript, Go, Rust, .NET, and Java, which support the API key parameter for authentication.

By following these steps, you can enhance the security of your Qdrant instance by enabling API key based authentication.","This can be used to secure your instance.



To enable API key based authentication in your own Qdrant instance you must

specify a key in the configuration:



```yaml

service:

  # Set an api-key.

  # If set, all requests must include a header with the api-key.

  # example header: `api-key: <API-KEY>`

  #

  # If you enable this you should also enable TLS.

  # (Either above or via an external service like nginx.)

  # Sending an api-key over an unencrypted channel is insecure.
api_key: your_secret_api_key_here

```



Or alternatively, you can use the environment variable:



```bash

export QDRANT__SERVICE__API_KEY=your_secret_api_key_here

```



<aside role=""alert""><a href=""#tls"">TLS</a> must be used to prevent leaking the API key over an unencrypted connection.</aside>



For using API key based authentication in Qdrant cloud see the cloud

[Authentication](https://qdrant.tech/documentation/cloud/authentication)

section.
```

> **Note:** The API key needs to be present in the request header every time you make a request via Rest or gRPC interface.



## Step 3: Authenticate via SDK



Now that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.

Our official Qdrant clients for Python, TypeScript, Go, Rust, and .NET all support the API key parameter. 



```python

from qdrant_client import QdrantClient



qdrant_client = QdrantClient(
The transition is even easier if you use the official client libraries. For example, the [Python Client](https://github.com/qdrant/qdrant-client) has the support of the API key already built-in, so you only need to provide it once, when the QdrantClient instance is created.



### Cluster configuration



Each instance comes pre-configured with the following tools, features and support services:



- Automatically created with the latest available version of Qdrant.
## Authenticate via SDK



Now that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.

Our official Qdrant clients for Python, TypeScript, Go, Rust, .NET and Java all support the API key parameter. 



```bash

curl \

  -X GET https://xyz-example.eu-central.aws.cloud.qdrant.io:6333 \

  --header 'api-key: <provide-your-own-key>'



# Alternatively, you can use the `Authorization` header with the `Bearer` prefix

curl \","['This can be used to secure your instance.\n\n\n\nTo enable API key based authentication in your own Qdrant instance you must\n\nspecify a key in the configuration:\n\n\n\n```yaml\n\nservice:\n\n  # Set an api-key.\n\n  # If set, all requests must include a header with the api-key.\n\n  # example header: `api-key: <API-KEY>`\n\n  #\n\n  # If you enable this you should also enable TLS.\n\n  # (Either above or via an external service like nginx.)\n\n  # Sending an api-key over an unencrypted channel is insecure.', 'api_key: your_secret_api_key_here\n\n```\n\n\n\nOr alternatively, you can use the environment variable:\n\n\n\n```bash\n\nexport QDRANT__SERVICE__API_KEY=your_secret_api_key_here\n\n```\n\n\n\n<aside role=""alert""><a href=""#tls"">TLS</a> must be used to prevent leaking the API key over an unencrypted connection.</aside>\n\n\n\nFor using API key based authentication in Qdrant cloud see the cloud\n\n[Authentication](https://qdrant.tech/documentation/cloud/authentication)\n\nsection.', '```\n\n> **Note:** The API key needs to be present in the request header every time you make a request via Rest or gRPC interface.\n\n\n\n## Step 3: Authenticate via SDK\n\n\n\nNow that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.\n\nOur official Qdrant clients for Python, TypeScript, Go, Rust, and .NET all support the API key parameter. \n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nqdrant_client = QdrantClient(', 'The transition is even easier if you use the official client libraries. For example, the [Python Client](https://github.com/qdrant/qdrant-client) has the support of the API key already built-in, so you only need to provide it once, when the QdrantClient instance is created.\n\n\n\n### Cluster configuration\n\n\n\nEach instance comes pre-configured with the following tools, features and support services:\n\n\n\n- Automatically created with the latest available version of Qdrant.', ""## Authenticate via SDK\n\n\n\nNow that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.\n\nOur official Qdrant clients for Python, TypeScript, Go, Rust, .NET and Java all support the API key parameter. \n\n\n\n```bash\n\ncurl \\\n\n  -X GET https://xyz-example.eu-central.aws.cloud.qdrant.io:6333 \\\n\n  --header 'api-key: <provide-your-own-key>'\n\n\n\n# Alternatively, you can use the `Authorization` header with the `Bearer` prefix\n\ncurl \\""]","Question: How can you enable API key based authentication in Qdrant instances for security purposes?\n\nContext: This can be used to secure your instance.



To enable API key based authentication in your own Qdrant instance you must

specify a key in the configuration:



```yaml

service:

# Set an api-key.

# If set, all requests must include a header with the api-key.

# example header: `api-key: <API-KEY>`

#

# If you enable this you should also enable TLS.

# (Either above or via an external service like nginx.)

# Sending an api-key over an unencrypted channel is insecure.
api_key: your_secret_api_key_here

```



Or alternatively, you can use the environment variable:



```bash

export QDRANT__SERVICE__API_KEY=your_secret_api_key_here

```



<aside role=""alert""><a href=""#tls"">TLS</a> must be used to prevent leaking the API key over an unencrypted connection.</aside>



For using API key based authentication in Qdrant cloud see the cloud

[Authentication](https://qdrant.tech/documentation/cloud/authentication)

section.
```

> **Note:** The API key needs to be present in the request header every time you make a request via Rest or gRPC interface.



## Step 3: Authenticate via SDK



Now that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.

Our official Qdrant clients for Python, TypeScript, Go, Rust, and .NET all support the API key parameter. 



```python

from qdrant_client import QdrantClient



qdrant_client = QdrantClient(
The transition is even easier if you use the official client libraries. For example, the [Python Client](https://github.com/qdrant/qdrant-client) has the support of the API key already built-in, so you only need to provide it once, when the QdrantClient instance is created.



### Cluster configuration



Each instance comes pre-configured with the following tools, features and support services:



- Automatically created with the latest available version of Qdrant.
## Authenticate via SDK



Now that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.

Our official Qdrant clients for Python, TypeScript, Go, Rust, .NET and Java all support the API key parameter. 



```bash

curl \

-X GET https://xyz-example.eu-central.aws.cloud.qdrant.io:6333 \

--header 'api-key: <provide-your-own-key>'



# Alternatively, you can use the `Authorization` header with the `Bearer` prefix

curl \\n\nAnswer:",How can you enable API key based authentication in Qdrant instances for security purposes?,"To enable API key based authentication in Qdrant instances for security purposes, you must specify a key in the configuration. This can be done by adding the following code snippet to your configuration file:

```yaml
service:
  # Set an api-key.
  # If set, all requests must include a header with the api-key.
  # example header: `api-key: <API-KEY>`
  #
  # If you enable this you should also enable TLS.
  # (Either above or via an external service like nginx.)
  # Sending an api-key over an unencrypted channel is insecure.
  api_key: your_secret_api_key_here
```

By adding this configuration, all requests to your Qdrant instance must include a header with the specified API key. This simple form of client authentication helps secure your instance and is available starting from version 1.2.0","['This can be used to secure your instance.\n\n\n\nTo enable API key based authentication in your own Qdrant instance you must\n\nspecify a key in the configuration:\n\n\n\n```yaml\n\nservice:\n\n  # Set an api-key.\n\n  # If set, all requests must include a header with the api-key.\n\n  # example header: `api-key: <API-KEY>`\n\n  #\n\n  # If you enable this you should also enable TLS.\n\n  # (Either above or via an external service like nginx.)\n\n  # Sending an api-key over an unencrypted channel is insecure.', 'api_key: your_secret_api_key_here\n\n```\n\n\n\nOr alternatively, you can use the environment variable:\n\n\n\n```bash\n\nexport QDRANT__SERVICE__API_KEY=your_secret_api_key_here\n\n```\n\n\n\n<aside role=""alert""><a href=""#tls"">TLS</a> must be used to prevent leaking the API key over an unencrypted connection.</aside>\n\n\n\nFor using API key based authentication in Qdrant cloud see the cloud\n\n[Authentication](https://qdrant.tech/documentation/cloud/authentication)\n\nsection.', '```\n\n> **Note:** The API key needs to be present in the request header every time you make a request via Rest or gRPC interface.\n\n\n\n## Step 3: Authenticate via SDK\n\n\n\nNow that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.\n\nOur official Qdrant clients for Python, TypeScript, Go, Rust, and .NET all support the API key parameter. \n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\n\n\nqdrant_client = QdrantClient(', 'The transition is even easier if you use the official client libraries. For example, the [Python Client](https://github.com/qdrant/qdrant-client) has the support of the API key already built-in, so you only need to provide it once, when the QdrantClient instance is created.\n\n\n\n### Cluster configuration\n\n\n\nEach instance comes pre-configured with the following tools, features and support services:\n\n\n\n- Automatically created with the latest available version of Qdrant.', ""## Authenticate via SDK\n\n\n\nNow that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.\n\nOur official Qdrant clients for Python, TypeScript, Go, Rust, .NET and Java all support the API key parameter. \n\n\n\n```bash\n\ncurl \\\n\n  -X GET https://xyz-example.eu-central.aws.cloud.qdrant.io:6333 \\\n\n  --header 'api-key: <provide-your-own-key>'\n\n\n\n# Alternatively, you can use the `Authorization` header with the `Bearer` prefix\n\ncurl \\""]","This can be used to secure your instance.



To enable API key based authentication in your own Qdrant instance you must

specify a key in the configuration:



```yaml

service:

  # Set an api-key.

  # If set, all requests must include a header with the api-key.

  # example header: `api-key: <API-KEY>`

  #

  # If you enable this you should also enable TLS.

  # (Either above or via an external service like nginx.)

  # Sending an api-key over an unencrypted channel is insecure.
api_key: your_secret_api_key_here

```



Or alternatively, you can use the environment variable:



```bash

export QDRANT__SERVICE__API_KEY=your_secret_api_key_here

```



<aside role=""alert""><a href=""#tls"">TLS</a> must be used to prevent leaking the API key over an unencrypted connection.</aside>



For using API key based authentication in Qdrant cloud see the cloud

[Authentication](https://qdrant.tech/documentation/cloud/authentication)

section.
```

> **Note:** The API key needs to be present in the request header every time you make a request via Rest or gRPC interface.



## Step 3: Authenticate via SDK



Now that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.

Our official Qdrant clients for Python, TypeScript, Go, Rust, and .NET all support the API key parameter. 



```python

from qdrant_client import QdrantClient



qdrant_client = QdrantClient(
The transition is even easier if you use the official client libraries. For example, the [Python Client](https://github.com/qdrant/qdrant-client) has the support of the API key already built-in, so you only need to provide it once, when the QdrantClient instance is created.



### Cluster configuration



Each instance comes pre-configured with the following tools, features and support services:



- Automatically created with the latest available version of Qdrant.
## Authenticate via SDK



Now that you have created your first cluster and key, you might want to access Qdrant Cloud from within your application.

Our official Qdrant clients for Python, TypeScript, Go, Rust, .NET and Java all support the API key parameter. 



```bash

curl \

  -X GET https://xyz-example.eu-central.aws.cloud.qdrant.io:6333 \

  --header 'api-key: <provide-your-own-key>'



# Alternatively, you can use the `Authorization` header with the `Bearer` prefix

curl \",0.5367647058823529,0.6083333333333333,0.5703125,0.26666666666666666,0.3025210084033613,0.28346456692913385,0.3382352941176471,0.38333333333333336,0.359375,0.47794117647058826,0.5416666666666666,0.5078125,0.9611688852310181,0.6672329306602478,0.6877096891403198,0.6773166060447693,0.5630870194290765,0.9337539181578904,136,1.1428571428571428,0.7710065023537673
