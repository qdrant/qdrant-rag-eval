question,answer,contexts,ground_truth
What is vaccum optimizer ?," The vacuum optimizer is a feature in Qdrant that constantly transforms in-memory segments into memmap segments on disk. This process can be slower, but it helps optimize performance and resource usage when ingesting a large amount of data. You can configure the vacuum optimizer in the configuration file by setting parameters such as the deleted threshold and vacuum min vector number. For more information on configuring the vacuum optimizer, refer to the documentation on configuring memmap storage.","['document:return optimizer\n\n```\n\n\n\nCaching in Quaterion is used for avoiding calculation of outputs of a frozen pretrained `Encoder` in every epoch.\n\nWhen it is configured, outputs will be computed once and cached in the preferred device for direct usage later on.\n\nIt provides both a considerable speedup and less memory footprint.\n\nHowever, it is quite a bit versatile and has several knobs to tune.,source:articles/cars-recognition.md'
 'document:The criteria for starting the optimizer are defined in the configuration file.\n\n\n\nHere is an example of parameter values:\n\n\n\n```yaml\n\nstorage:\n\n  optimizers:\n\n    # The minimal fraction of deleted vectors in a segment, required to perform segment optimization\n\n    deleted_threshold: 0.2\n\n    # The minimal number of vectors in a segment, required to perform segment optimization\n\n    vacuum_min_vector_number: 1000\n\n```\n\n\n\n## Merge Optimizer\n\n\n\nThe service may require the creation of temporary segments.,source:documentation/concepts/optimizer.md'
 ""document:## **Top takeaways:**\n\n\n\nDiscover how reimagined data infrastructures revolutionize AI-agent workflows as Diptanu delves into Indexify, transforming raw data into real-time knowledge bases, and shares expert insights on optimizing rag-based applications, all amidst the ever-evolving landscape of Spark.\n\n\n\nHere's What You'll Discover:,source:blog/indexify-unveiled-diptanu-gon-choudhury-vector-space-talk-009.md""
 'document:the [optimizer](../../concepts/optimizer/) to constantly\n\ntransform in-memory segments into memmap segments on disk. This process is\n\nslower, and the optimizer can be a bottleneck when ingesting a large amount of\n\ndata.\n\n\n\nRead more about this in\n\n[Configuring Memmap Storage](../../concepts/storage/#configuring-memmap-storage).\n\n\n\n## Parallel upload into multiple shards,source:documentation/tutorials/bulk-upload.md'
 'document:Qdrant supports up to 65,535 dimensions by default, but this can be configured to support higher dimensions.\n\n\n\n### What is the maximum size of vector metadata that can be stored?\n\n\n\nThere is no inherent limitation on metadata size, but it should be [optimized for performance and resource usage](/documentation/guides/optimize/). Users can set upper limits in the configuration.\n\n\n\n### Can the same similarity search query yield different results on different machines?,source:documentation/faq/qdrant-fundamentals.md'
 'document:},\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,\n\n                ""memmap_threshold"": null,\n\n                ""indexing_threshold"": 20000,\n\n                ""flush_interval_sec"": 5,\n\n                ""max_optimization_threads"": 1\n\n            },\n\n            ""wal_config"": {\n\n                ""wal_capacity_mb"": 32,,source:documentation/concepts/collections.md'
 'document:Yes, due to differences in hardware configurations and parallel processing, results may vary slightly.\n\n\n\n### What to do with documents with small chunks using a fixed chunk strategy?\n\n\n\nFor documents with small chunks, consider merging chunks or using variable chunk sizes to optimize vector representation and search performance.\n\n\n\n### How do I choose the right vector embeddings for my use case?,source:documentation/faq/qdrant-fundamentals.md']","The Vacuum Optimizer is a feature in the Qdrant database designed to manage and improve system performance by removing obsolete data. In Qdrant, when records are deleted, they are not immediately removed from the database. Instead, they are marked as deleted and subsequently ignored in future queries. This approach helps minimize disk access but results in accumulated deleted records that can occupy valuable memory and degrade performance over time.
The Vacuum Optimizer addresses this issue by systematically clearing out these marked-as-deleted records once they reach a certain threshold. This threshold is configurable and includes conditions such as the minimum fraction of deleted vectors in a segment and the minimum number of vectors present in a segment. "
Tell me about ‘always_ram’ parameter?," The 'always_ram' parameter is used in the quantization config to determine whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, in some setups, it might be beneficial to keep quantized vectors in RAM to speed up the search process. In such cases, the 'always_ram' parameter can be set to 'true'. This information can be found in the documentation under the 'Quantization' section of the 'Guides' page.","['document:Suppose you have a ‘users’ collection and have defined specific roles for each user, such as ‘developer’, ‘manager’, ‘admin’, ‘analyst’, and ‘revoked’. In such a scenario, you can use a combination of **exp** and **value_exists**.\n\n```json\n\n{\n\n  ""exp"":  1690995200, \n\n  ""value_exists"": {\n\n    ""collection"": ""users"",\n\n    ""matches"": [\n\n      { ""key"": ""username"", ""value"": ""john"" },\n\n      { ""key"": ""role"", ""value"": ""developer"" }\n\n    ],\n\n  },\n\n}\n\n\n\n```,source:articles/data-privacy.md'
 'document:""quantile"": 0.8,\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\'\n\n```\n\n\n\n```python\n\nclient.update_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config={\n\n        ""my_vector"": models.VectorParamsDiff(\n\n            hnsw_config=models.HnswConfigDiff(\n\n                m=32,\n\n                ef_construct=123,\n\n            ),\n\n            quantization_config=models.ProductQuantization(\n\n                product=models.ProductQuantizationConfig(,source:documentation/concepts/collections.md'
 'document:This mode is enabled by setting `always_ram` to `true` in the quantization config while using memmap storage:\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine"",\n\n      ""on_disk"": true\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models,source:documentation/guides/quantization.md'
 'document:On an ecommerce platform, a user can, for instance, try to find ‘clothing for a trek’, when they actually want results around ‘waterproof jackets’, or ‘winter socks’. Keyword, or full-text, or even synonym search would fail to provide any response to such a query. Similarly, on a music app, a user might be looking for songs that sound similar to an audio clip they have heard. Or, they might want to look up furniture that has a similar look as the one they saw on a trip.,source:blog/what-is-vector-similarity.md'
 'document:`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\n\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\n\n\n\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\n\n\n\n### Setting up Binary Quantization,source:documentation/guides/quantization.md'
 'document:}),\n\n})\n\n```\n\n\n\n`on_disk` will ensure that vectors will be stored on disk, while `always_ram` will ensure that quantized vectors will be stored in RAM.\n\n\n\nOptionally, you can disable rescoring with search `params`, which will reduce the number of disk reads even further, but potentially slightly decrease the precision.\n\n\n\n```http\n\nPOST /collections/{collection_name}/points/query\n\n{\n\n    ""query"": [0.2, 0.1, 0.9, 0.7],\n\n    ""params"": {\n\n        ""quantization"": {\n\n            ""rescore"": false\n\n        },source:documentation/guides/optimize.md'
 'document:In this case, the quantized vector will be 16 times smaller than the original vector.\n\n\n\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\n\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process. Then set `always_ram` to `true`.\n\n\n\n### Searching with Quantization,source:documentation/guides/quantization.md'
 'document:)\n\n        )\n\n    ),\n\n)\n\n```\n\n\n\nWe are going to use the `contracts` collection to store the vectors of the documents. The `always_ram` flag is set to\n\n`True` to keep the quantized vectors in RAM, which will speed up the search process. We also wanted to restrict access \n\nto the individual documents, so only users with the proper permissions can see them. In Qdrant that should be solved by,source:documentation/examples/rag-contract-management-stackit-aleph-alpha.md'
 'document:These components are pulled together into ‘chains’ that are constructed using [LangChain Expression Language](https://python.langchain.com/v0.1/docs/expression_language/) (LCEL). We’ill first look at the various building blocks, and then see how they can be combined using LCEL.\n\n\n\n### **LLM Model I/O**,source:blog/dspy-vs-langchain.md']","In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times."
What is difference between scalar and product quantization?," Scalar quantization and product quantization are two different methods of compressing vector embeddings. While scalar quantization converts floating point numbers to integers, product quantization converts them into binary or boolean values. Product quantization offers a higher compression rate compared to scalar quantization, but it also comes with trade-offs in accuracy and search speed. Additionally, product quantization is more customizable and allows for finding a balance between memory usage and search precision. For more information on the differences between the two methods, please refer to the relevant sections in the documentation.","['document:## Product Quantization vs Scalar Quantization\n\n\n\nCompared to [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/), Product Quantization offers a higher compression rate. However, this comes with considerable trade-offs in accuracy, and at times, in-RAM search speed.\n\n\n\nProduct Quantization tends to be favored in certain specific scenarios:\n\n\n\n- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself,source:articles/product-quantization.md'
 'document:But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\n\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\n\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n## How to choose the right quantization method,source:documentation/guides/quantization.md'
 'document:## What is Binary Quantization?\n\nBinary quantization (BQ) converts any vector embedding of floating point numbers into a vector of binary or boolean values. This feature is an extension of our past work on [scalar quantization](/articles/scalar-quantization/) where we convert `float32` to `uint8` and then leverage a specific SIMD CPU instruction to perform fast vector comparison. \n\n\n\n![What is binary quantization](/articles_data/binary-quantization/bq-2.png),source:articles/binary-quantization.md'
 'document:<td>921 s</td>\n\n         <td>597 s</td>\n\n         <td>481 s</td>\n\n         <td>474 s</td>\n\n      </tr>\n\n   </tbody>\n\n</table>\n\n\n\nIt turns out that in some cases, Product Quantization may not only reduce the memory usage, \n\nbut also the search time.\n\n\n\n## Product Quantization vs Scalar Quantization,source:articles/product-quantization.md'
 'document:. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.,source:blog/binary-quantization-andrey-vasnetsov-vector-space-talk-001.md'
 ""document:method. However, the process is slightly more complicated than [Scalar Quantization](https://qdrant.tech/articles/scalar-quantization/) and is more customizable, so you can find the sweet spot between memory usage and search precision. This article \n\ncovers all the steps required to perform Product Quantization and the way it's implemented in Qdrant.\n\n\n\n## How Does Product Quantization Work?\n\n\n\nLet’s assume we have a few vectors being added to the collection and that our optimizer decided,source:articles/product-quantization.md""]","While both methods aim to reduce the memory footprint and potentially speed up operations, scalar quantization offers faster processing with SIMD-friendly operations and minimal accuracy loss, making it suitable for scenarios where speed is critical. Product quantization achieves higher compression rates and is better suited for handling very high-dimensional vectors where memory space is more of a concern than computation speed."
What is ‘best_score’ strategy?," The ‘best_score’ strategy is a new recommendation strategy introduced in v1.6 of the recommendation API. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one. This strategy uses a more sophisticated algorithm and takes into account multiple positive and negative examples, resulting in a richer variety of results. ","['document:This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.\n\n\n\n### Best score strategy\n\n\n\n*Available as of v1.6.0*\n\n\n\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.,source:documentation/concepts/explore.md'
 'document:default) or `best_score`. Moreover, we can pass both IDs (`718`) and embeddings (`[0.2, 0.3, 0.4, 0.5]`) as both positive and \n\nnegative examples. \n\n\n\n## HNSW ANN example and strategy\n\n\n\nLet’s start with an example to help you understand the [HNSW graph](/articles/filtrable-hnsw/). Assume you want \n\nto travel to a small city on another continent:\n\n\n\n1. You start from your hometown and take a bus to the local airport.\n\n2. Then, take a flight to one of the closest hubs.,source:articles/new-recommendation-api.md'
 'document:The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\n\n\n\n```rust\n\nlet score = if best_positive_score > best_negative_score {\n\n    best_positive_score\n\n} else {\n\n    -(best_negative_score * best_negative_score)\n\n};\n\n```\n\n\n\n<aside role=""alert"">\n\nThe performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.\n\n</aside>,source:documentation/concepts/explore.md'
 'document:#### Using only negative examples\n\n\n\nA beneficial side-effect of `best_score` strategy is that you can use it with only negative examples. This will allow you to find the most dissimilar vectors to the ones you provide. This can be useful for finding outliers in your data, or for finding the most dissimilar vectors to a given one.\n\n\n\nCombining negative-only examples with filtering can be a powerful tool for data exploration and cleaning.\n\n\n\n### Multiple vectors\n\n\n\n*Available as of v0.10.0*,source:documentation/concepts/explore.md'
 'document:Suppose you have a ‘users’ collection and have defined specific roles for each user, such as ‘developer’, ‘manager’, ‘admin’, ‘analyst’, and ‘revoked’. In such a scenario, you can use a combination of **exp** and **value_exists**.\n\n```json\n\n{\n\n  ""exp"":  1690995200, \n\n  ""value_exists"": {\n\n    ""collection"": ""users"",\n\n    ""matches"": [\n\n      { ""key"": ""username"", ""value"": ""john"" },\n\n      { ""key"": ""role"", ""value"": ""developer"" }\n\n    ],\n\n  },\n\n}\n\n\n\n```,source:articles/data-privacy.md'
 'document:On an ecommerce platform, a user can, for instance, try to find ‘clothing for a trek’, when they actually want results around ‘waterproof jackets’, or ‘winter socks’. Keyword, or full-text, or even synonym search would fail to provide any response to such a query. Similarly, on a music app, a user might be looking for songs that sound similar to an audio clip they have heard. Or, they might want to look up furniture that has a similar look as the one they saw on a trip.,source:blog/what-is-vector-similarity.md'
 'document:samples and uses a slightly more sophisticated algorithm under the hood.\n\n\n\nThe best score is chosen at every step of HNSW graph traversal. We separately calculate the distance between a traversed point \n\nand every positive and negative example. In the case of the best score strategy, **there is no single query vector anymore, but a \n\nbunch of positive and negative queries**. As a result, for each sample in the query, we have a set of distances, one for each,source:articles/new-recommendation-api.md'
 'document:##### One positive and one negative example\n\n\n\n<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>\n\n\n\nThe more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there \n\nis just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making \n\nthe variety richer.,source:articles/new-recommendation-api.md']","The `best_score` strategy is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided."
How does oversampling helps?," Oversampling is a technique used to improve the accuracy and performance of similarity search algorithms. It works by compressing high-dimensional vectors in memory and then compensating for the accuracy loss by re-scoring additional points with the original vectors. This method is discussed in detail in the documentation under the section ""Oversampling for quantization"" (source: articles/qdrant-1.3.x.md). It is also mentioned in other articles such as ""Binary Quantization for OpenAI"" (source: articles/binary-quantization-openai.md) and ""Binary Quantization and Vector Space Talk"" (source: blog/binary-quantization-andrey-vasnetsov-vector-space-talk-001.md). According to these sources, oversampling can significantly improve the accuracy of searches, especially when one class significantly outnumbers others. It is recommended to use an oversampling factor of 3 for the best balance between accuracy and efficiency (source: articles/binary-quantization-openai.md).","['document:### Oversampling for quantization\n\n\n\nWe are introducing [oversampling](/documentation/guides/quantization/#oversampling) as a new way to help you improve the accuracy and performance of similarity search algorithms. With this method, you are able to significantly compress high-dimensional vectors in memory and then compensate the accuracy loss by re-scoring additional points with the original vectors.,source:articles/qdrant-1.3.x.md'
 'document:- **Oversampling**: By oversampling, we can limit the loss of information inherent in quantization. This also helps to preserve the semantic richness of your OpenAI embeddings. We experimented with different oversampling factors, and identified the impact on the accuracy and efficiency of search. Spoiler: higher oversampling factors tend to improve the accuracy of searches. However, they usually require more computational resources.,source:articles/binary-quantization-openai.md'
 'document:It works well when one class significantly outnumbers others. This imbalance\n\ncan skew the performance of models, which favors the majority class at the\n\nexpense of others. By creating additional samples from the minority classes,\n\noversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.,source:articles/binary-quantization-openai.md'
 'document:Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user,source:blog/binary-quantization-andrey-vasnetsov-vector-space-talk-001.md'
 'document:3. Oversampling: Use an oversampling factor of 3 for the best balance between accuracy and efficiency. This factor is suitable for a wide range of applications.\n\n4. Rescoring: Enable rescoring to improve the accuracy of search results.,source:articles/binary-quantization-openai.md']","Defines how many extra vectors should be pre-selected using quantized index, and then re-scored using original vectors. For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring. Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time."
What is the purpose of ‘CreatePayloadIndexAsync’?," The 'CreatePayloadIndexAsync' function is used to create a new payload index for a specified collection. This allows for the storage structure to be organized in a way that co-locates vectors of the same tenant together, which can improve performance and optimization. This information can be found in the documentation under the 'source' field of the context provided.","['document:await client.CreatePayloadIndexAsync(\n\n collectionName: ""{collection_name}"",\n\n fieldName: ""group_id"",\n\n schemaType: PayloadSchemaType.Keyword,\n\n indexParams: new PayloadIndexParams\n\n {\n\n  KeywordIndexParams = new KeywordIndexParams\n\n  {\n\n   IsTenant = true\n\n  }\n\n }\n\n);\n\n\n\n```\n\n\n\nAs a result, the storage structure will be organized in a way to co-locate vectors of the same tenant together at the next optimization.,source:blog/qdrant-1.11.x.md'
 'document:client\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"",\n\n        ""group_id"",\n\n        PayloadSchemaType.Keyword,\n\n        PayloadIndexParams.newBuilder()\n\n            .setKeywordIndexParams(\n\n                KeywordIndexParams.newBuilder()\n\n                    .setIsTenant(true)\n\n                    .build())\n\n            .build(),\n\n        null,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);,source:documentation/guides/multiple-partitions.md'
 'document:Suppose you have a ‘users’ collection and have defined specific roles for each user, such as ‘developer’, ‘manager’, ‘admin’, ‘analyst’, and ‘revoked’. In such a scenario, you can use a combination of **exp** and **value_exists**.\n\n```json\n\n{\n\n  ""exp"":  1690995200, \n\n  ""value_exists"": {\n\n    ""collection"": ""users"",\n\n    ""matches"": [\n\n      { ""key"": ""username"", ""value"": ""john"" },\n\n      { ""key"": ""role"", ""value"": ""developer"" }\n\n    ],\n\n  },\n\n}\n\n\n\n```,source:articles/data-privacy.md'
 'document:await client.CreatePayloadIndexAsync(\n\n collectionName: ""{collection_name}"",\n\n fieldName: ""payload_field_name"",\n\n schemaType: PayloadSchemaType.Keyword,\n\n indexParams: new PayloadIndexParams\n\n {\n\n  KeywordIndexParams = new KeywordIndexParams\n\n  {\n\n   OnDisk   = true\n\n  }\n\n }\n\n);\n\n```\n\n\n\n```go\n\nimport (\n\n\t""context""\n\n\n\n\t""github.com/qdrant/go-client/qdrant""\n\n)\n\n\n\nclient, err := qdrant.NewClient(&qdrant.Config{\n\n\tHost: ""localhost"",\n\n\tPort: 6334,\n\n}),source:documentation/concepts/indexing.md'
 'document:On an ecommerce platform, a user can, for instance, try to find ‘clothing for a trek’, when they actually want results around ‘waterproof jackets’, or ‘winter socks’. Keyword, or full-text, or even synonym search would fail to provide any response to such a query. Similarly, on a music app, a user might be looking for songs that sound similar to an audio clip they have heard. Or, they might want to look up furniture that has a similar look as the one they saw on a trip.,source:blog/what-is-vector-similarity.md'
 'document:These components are pulled together into ‘chains’ that are constructed using [LangChain Expression Language](https://python.langchain.com/v0.1/docs/expression_language/) (LCEL). We’ill first look at the various building blocks, and then see how they can be combined using LCEL.\n\n\n\n### **LLM Model I/O**,source:blog/dspy-vs-langchain.md']",CreatePayloadIndexAsync is a method in the Qdrant library that enables the creation of a payload index in Qdrant. A payload index is a data structure designed to store supplemental information about the data stored in Qdrant. This method requires a Qdrant instance and a payload index name as input parameters.
What is the purpose of ef_construct in HNSW ?," The ef_construct parameter in HNSW is used to specify the number of neighbors to consider during the index building process. Increasing this value can improve the precision of the search, but it may also increase the indexing time. This information can be found in the documentation under the 'HNSW Index' section of the 'Indexing' page (source: documentation/concepts/indexing.md). Additionally, the default value for ef_construct is 100, but it can be configured in the configuration file for each collection or named vector individually (source: documentation/tutorials/retrieval-quality.md).","['document:In order to improve performance, HNSW limits the maximum degree of nodes on each layer of the graph to `m`. In addition, you can use `ef_construct` (when building index) or `ef` (when searching targets) to specify a search range.\n\n\n\nThe corresponding parameters could be configured in the configuration file:\n\n\n\n```yaml\n\nstorage:\n\n  # Default parameters of HNSW Index. Could be overridden for each collection or named vector individually\n\n  hnsw_index:\n\n    # Number of edges per node in the index graph.,source:documentation/concepts/indexing.md'
 'document:# in this case full-scan search should be preferred by query planner and additional indexing is not required.\n\n    # Note: 1Kb = 1 vector of size 256\n\n    full_scan_threshold: 10000\n\n\n\n```\n\n\n\nAnd so in the process of creating a [collection](../collections/). The `ef` parameter is configured during [the search](../search/) and by default is equal to `ef_construct`.\n\n\n\nHNSW is chosen for several reasons.,source:documentation/concepts/indexing.md'
 ""document:The larger the value of it, the higher the precision of the search, but more space required. The `ef_construct` parameter is the number of \n\nneighbours to consider during the index building. Again, the larger the value, the higher the precision, but the longer the indexing time.\n\nThe default values of these parameters are `m=16` and `ef_construct=100`. Let's try to increase them to `m=32` and `ef_construct=200` and,source:documentation/tutorials/retrieval-quality.md""
 'document:Currently, it could be:\n\n\n\n* `hnsw_ef` - value that specifies `ef` parameter of the HNSW algorithm.\n\n* `exact` - option to not use the approximate search (ANN). If set to true, the search may run for a long as it performs a full scan to retrieve exact results.,source:documentation/concepts/search.md'
 'document:}),\n\n\tHnswConfig: &qdrant.HnswConfigDiff{\n\n\t\tOnDisk: qdrant.PtrOf(true),\n\n\t},\n\n})\n\n```\n\n\n\nIn this scenario you can increase the precision of the search by increasing the `ef` and `m` parameters of the HNSW index, even with limited RAM.\n\n\n\n```json\n\n...\n\n""hnsw_config"": {\n\n    ""m"": 64,\n\n    ""ef_construct"": 512,\n\n    ""on_disk"": true\n\n}\n\n...\n\n```\n\n\n\nThe disk IOPS is a critical factor in this scenario, it will determine how fast you can perform search.,source:documentation/guides/optimize.md'
 ""document:Rishabh Bhardwaj:\n\nSo should I go with the slide first or should I answer your question first?\n\n\n\nDemetrios:\n\nProbably answer the question so we don't get too far off track, and then we can hit up your slides. And the slides, I'm sure, will prompt many other questions from my side and the audience's side.\n\n\n\nRishabh Bhardwaj:\n\nSo, for HNSW configuration, we have specified the value of M, which is, I think, basically the layers as 64, and the value for EF construct is 256.\n\n\n\nDemetrios:,source:blog/building-a-high-performance-entity-matching-solution-with-qdrant-rishabh-bhardwaj-vector-space-talks-005.md""]","In HNSW algorithm the ef_construct parameter is the number of neighbours to consider during the index building. The larger the value, the higher the precision, but the longer the indexing time. The default values of this parameters 100"
How do you use ‘ordering’ parameter?," To use the 'ordering' parameter, you need to specify the payload key using the 'order_by' key parameter. You can also add other fields such as 'direction' and 'start_from' to control the ordering. This parameter can be used with update and delete operations to ensure that they are executed in the same order on all replicas. This helps to avoid data inconsistency in case of concurrent updates of the same documents. For more information, please refer to the 'ordering' parameter in the 'Distributed Deployment' guide in the documentation.","['document:- Write `ordering` param, can be used with update and delete operations to ensure that the operations are executed in the same order on all replicas. If this option is used, Qdrant will route the operation to the leader replica of the shard and wait for the response before responding to the client. This option is useful to avoid data inconsistency in case of concurrent updates of the same documents,source:documentation/guides/distributed_deployment.md'
 'document:```http\n\n""order_by"": {\n\n    ""key"": ""timestamp"",\n\n    ""direction"": ""desc"" // default is ""asc""\n\n    ""start_from"": 123, // start from this value\n\n}\n\n```\n\n\n\n```python\n\norder_by=models.OrderBy(\n\n    key=""timestamp"",\n\n    direction=""desc"",  # default is ""asc""\n\n    start_from=123,  # start from this value\n\n)\n\n```\n\n\n\n```typescript\n\norder_by: {\n\n    key: ""timestamp"",\n\n    direction: ""desc"", // default is ""asc""\n\n    start_from: 123, // start from this value\n\n}\n\n```\n\n\n\n```rust,source:documentation/concepts/points.md'
 'document:## How Does Vector Similarity Work?\n\nSo, how does an algorithm capture the essence of a user’s query, and then unearth results that are relevant?\n\n\n\nAt a high level, here’s how:\n\n\n\n- Unstructured data is first converted into a numerical representation, known as vectors, using a deep-learning model. The goal here is to capture the ‘semantics’ or the key features of this data.\n\n- The vectors are then stored in a vector database, along with references to their original data.,source:blog/what-is-vector-similarity.md'
 'document:""github.com/qdrant/go-client/qdrant""\n\n)\n\n\n\nclient, err := qdrant.NewClient(&qdrant.Config{\n\n\tHost: ""localhost"",\n\n\tPort: 6334,\n\n})\n\n\n\nclient.Scroll(context.Background(), &qdrant.ScrollPoints{\n\n\tCollectionName: ""{collection_name}"",\n\n\tLimit:          qdrant.PtrOf(uint32(15)),\n\n\tOrderBy: &qdrant.OrderBy{\n\n\t\tKey: ""timestamp"",\n\n\t},\n\n})\n\n```\n\n\n\nYou need to use the `order_by` `key` parameter to specify the payload key. Then you can add other fields to control the ordering, such as `direction` and `start_from`:,source:documentation/concepts/points.md'
 'document:Human memory is unreliable. Thus, as long as we have been trying to collect ‘knowledge’ in written form, we had to figure out how to search for relevant content without rereading the same books repeatedly. That’s why some brilliant minds introduced the inverted index. In the simplest form, it’s an appendix to a book, typically put at its end, with a list of the essential terms-and links to pages they occur at. Terms are put in alphabetical order,source:documentation/overview/vector-search.md'
 'document:.limit(10u64)\n\n        )\n\n        .query(Query::new_order_by(""price""))\n\n).await?;\n\n```\n\n\n\n```java\n\nimport static io.qdrant.client.ConditionFactory.matchKeyword;\n\nimport static io.qdrant.client.QueryFactory.nearest;\n\nimport static io.qdrant.client.QueryFactory.orderBy;\n\n\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Points.Filter;\n\nimport io.qdrant.client.grpc.Points.PrefetchQuery;,source:documentation/concepts/hybrid-queries.md'
 'document:}\n\n```\n\n\n\n<aside role=""alert"">When you use the <code>order_by</code> parameter, pagination is disabled.</aside>\n\n\n\nWhen sorting is based on a non-unique value, it is not possible to rely on an ID offset. Thus, next_page_offset is not returned within the response. However, you can still do pagination by combining `""order_by"": { ""start_from"": ... }` with a `{ ""must_not"": [{ ""has_id"": [...] }] }` filter.\n\n\n\n## Counting points\n\n\n\n_Available as of v0.8.4_,source:documentation/concepts/points.md'
 'document:Suppose you have a ‘users’ collection and have defined specific roles for each user, such as ‘developer’, ‘manager’, ‘admin’, ‘analyst’, and ‘revoked’. In such a scenario, you can use a combination of **exp** and **value_exists**.\n\n```json\n\n{\n\n  ""exp"":  1690995200, \n\n  ""value_exists"": {\n\n    ""collection"": ""users"",\n\n    ""matches"": [\n\n      { ""key"": ""username"", ""value"": ""john"" },\n\n      { ""key"": ""role"", ""value"": ""developer"" }\n\n    ],\n\n  },\n\n}\n\n\n\n```,source:articles/data-privacy.md']","Write ordering can be specified for any write request to serialize it through a single “leader” node, which ensures that all write operations (issued with the same ordering) are performed and observed sequentially. It is of 3 types weak , medium and strong and is used in python with additional param ordering=models.WriteOrdering.STRONG to upsert request."
What is significance of ‘on_disk_payload’ setting?," The 'on_disk_payload' setting is used to define where to store payload data. If set to 'true', the payload will be stored on disk only. This can be useful for limiting RAM usage, especially when working with large payloads. This feature is explained in more detail in the [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml) and can be configured when creating an index for a specific workspace. However, it is important to note that using on-disk payload index may affect cold request latency due to additional disk I/O operations. For more information on this feature, please refer to the [documentation](../../guides/quantization/#setting-up-quantization-in-qdrant) on quantization.","['document:* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\n\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\n\n\n\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).,source:documentation/concepts/collections.md'
 'document:*This feature can help you manage a high number of different payload indexes, which is beneficial if you are working with large varied datasets.*\n\n\n\n**Figure 2:** By moving the data from Workspace 2 to disk, the system can free up valuable memory resources for Workspaces 1, 3 and 4, which are accessed more frequently.\n\n\n\n![on-disk-payload](/blog/qdrant-1.11.x/on-disk-payload.png)\n\n\n\n**Example:** As you create an index for Workspace 2, set the `on_disk` parameter.\n\n\n\n```http,source:blog/qdrant-1.11.x.md'
 'document:<aside role=""alert"">\n\n    On-disk payload index might affect cold requests latency, as it requires additional disk I/O operations.\n\n</aside>\n\n\n\nTo configure on-disk payload index, you can use the following index parameters:\n\n\n\n```http\n\nPUT /collections/{collection_name}/index\n\n{\n\n    ""field_name"": ""payload_field_name"",\n\n    ""field_schema"": {\n\n        ""type"": ""keyword"",\n\n        ""on_disk"": true\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.create_payload_index(\n\n    collection_name=""{collection_name}"",,source:documentation/concepts/indexing.md'
 'document:Suppose you have a ‘users’ collection and have defined specific roles for each user, such as ‘developer’, ‘manager’, ‘admin’, ‘analyst’, and ‘revoked’. In such a scenario, you can use a combination of **exp** and **value_exists**.\n\n```json\n\n{\n\n  ""exp"":  1690995200, \n\n  ""value_exists"": {\n\n    ""collection"": ""users"",\n\n    ""matches"": [\n\n      { ""key"": ""username"", ""value"": ""john"" },\n\n      { ""key"": ""role"", ""value"": ""developer"" }\n\n    ],\n\n  },\n\n}\n\n\n\n```,source:articles/data-privacy.md'
 'document:1. **Activate Hybrid Cloud**: First, log into your [Qdrant Cloud account](https://cloud.qdrant.io/login) and select ‘Hybrid Cloud’ to activate.\n\n\n\n2. **Integrate Your Clusters**: Navigate to the Hybrid Cloud settings and add your Scaleway Kubernetes clusters as a Hybrid Cloud Environment.\n\n\n\n3. **Simplified Management**: Use the Qdrant Management Console for easy creation and oversight of your Qdrant clusters on Scaleway.,source:blog/hybrid-cloud-scaleway.md'
 'document:On an ecommerce platform, a user can, for instance, try to find ‘clothing for a trek’, when they actually want results around ‘waterproof jackets’, or ‘winter socks’. Keyword, or full-text, or even synonym search would fail to provide any response to such a query. Similarly, on a music app, a user might be looking for songs that sound similar to an audio clip they have heard. Or, they might want to look up furniture that has a similar look as the one they saw on a trip.,source:blog/what-is-vector-similarity.md']","The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes."
What is the impact of ‘write_consistency_factor’ ?," The `write_consistency_factor` determines the number of replicas that must acknowledge a write operation before responding to the client. This value can be increased to make write operations more tolerant to network partitions in the cluster, but it will require a higher number of replicas to be active in order to perform write operations. This information can be found in the documentation under the 'Write consistency factor' section of the 'Distributed Deployment' guide.","['document:- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.,source:documentation/guides/distributed_deployment.md'
 'document:### Write consistency factor\n\n\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\n\nIt can be configured at the collection\'s creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python,source:documentation/guides/distributed_deployment.md'
 'document:CollectionName: ""{collection_name}"",\n\n\tVectorsConfig: qdrant.NewVectorsConfig(&qdrant.VectorParams{\n\n\t\tSize:     300,\n\n\t\tDistance: qdrant.Distance_Cosine,\n\n\t}),\n\n\tShardNumber:            qdrant.PtrOf(uint32(6)),\n\n\tReplicationFactor:      qdrant.PtrOf(uint32(2)),\n\n\tWriteConsistencyFactor: qdrant.PtrOf(uint32(2)),\n\n})\n\n```\n\n\n\nWrite operations will fail if the number of active replicas is less than the `write_consistency_factor`.\n\n\n\n### Read consistency,source:documentation/guides/distributed_deployment.md'
 'document:Suppose you have a ‘users’ collection and have defined specific roles for each user, such as ‘developer’, ‘manager’, ‘admin’, ‘analyst’, and ‘revoked’. In such a scenario, you can use a combination of **exp** and **value_exists**.\n\n```json\n\n{\n\n  ""exp"":  1690995200, \n\n  ""value_exists"": {\n\n    ""collection"": ""users"",\n\n    ""matches"": [\n\n      { ""key"": ""username"", ""value"": ""john"" },\n\n      { ""key"": ""role"", ""value"": ""developer"" }\n\n    ],\n\n  },\n\n}\n\n\n\n```,source:articles/data-privacy.md'
 'document:On an ecommerce platform, a user can, for instance, try to find ‘clothing for a trek’, when they actually want results around ‘waterproof jackets’, or ‘winter socks’. Keyword, or full-text, or even synonym search would fail to provide any response to such a query. Similarly, on a music app, a user might be looking for songs that sound similar to an audio clip they have heard. Or, they might want to look up furniture that has a similar look as the one they saw on a trip.,source:blog/what-is-vector-similarity.md'
 'document:These components are pulled together into ‘chains’ that are constructed using [LangChain Expression Language](https://python.langchain.com/v0.1/docs/expression_language/) (LCEL). We’ill first look at the various building blocks, and then see how they can be combined using LCEL.\n\n\n\n### **LLM Model I/O**,source:blog/dspy-vs-langchain.md']","The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully."
