question,answer,contexts,ground_truth
What is vaccum optimizer ?," The vacuum optimizer is a component of the Milvus service that is responsible for optimizing the storage of data in a collection. It works by identifying segments that have a high number of deleted vectors or a low number of vectors and merging them into a new, optimized segment. This helps to improve the performance of queries and reduce the storage space used by the collection. The parameters for the vacuum optimizer can be configured in the Milvus configuration file. For example, the ""deleted_threshold"" parameter specifies the minimum fraction of deleted vectors in a segment that is required for the optimizer to perform its task. ","['document:optimizersConfig: new OptimizersConfigDiff { DefaultSegmentNumber = 2 }\n\n);\n\n```,source:documentation/guides/optimize.md'
 'document:#### Optimization with Web Workers,source:articles/web-ui-gsoc.md'
 'document:},\n\n            ""optimizer_config"": {\n\n                ""deleted_threshold"": 0.2,\n\n                ""vacuum_min_vector_number"": 1000,\n\n                ""default_segment_number"": 0,\n\n                ""max_segment_size"": null,\n\n                ""memmap_threshold"": null,\n\n                ""indexing_threshold"": 20000,\n\n                ""flush_interval_sec"": 5,\n\n                ""max_optimization_threads"": 1\n\n            },\n\n            ""wal_config"": {\n\n                ""wal_capacity_mb"": 32,,source:documentation/concepts/collections.md'
 'document:```http\n\nPATCH /collections/{collection_name}\n\n{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 10000\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""optimizers_config"": {\n\n        ""indexing_threshold"": 10000\n\n    }\n\n  }\'\n\n```\n\n\n\n```python\n\nclient.update_collection(\n\n    collection_name=""{collection_name}"",\n\n    optimizer_config=models.OptimizersConfigDiff(indexing_threshold=10000),\n\n),source:documentation/concepts/collections.md'
 'document:The criteria for starting the optimizer are defined in the configuration file.\n\n\n\nHere is an example of parameter values:\n\n\n\n```yaml\n\nstorage:\n\n  optimizers:\n\n    # The minimal fraction of deleted vectors in a segment, required to perform segment optimization\n\n    deleted_threshold: 0.2\n\n    # The minimal number of vectors in a segment, required to perform segment optimization\n\n    vacuum_min_vector_number: 1000\n\n```\n\n\n\n## Merge Optimizer\n\n\n\nThe service may require the creation of temporary segments.,source:documentation/concepts/optimizer.md']","The Vacuum Optimizer is a feature in the Qdrant database designed to manage and improve system performance by removing obsolete data. In Qdrant, when records are deleted, they are not immediately removed from the database. Instead, they are marked as deleted and subsequently ignored in future queries. This approach helps minimize disk access but results in accumulated deleted records that can occupy valuable memory and degrade performance over time.
The Vacuum Optimizer addresses this issue by systematically clearing out these marked-as-deleted records once they reach a certain threshold. This threshold is configurable and includes conditions such as the minimum fraction of deleted vectors in a segment and the minimum number of vectors present in a segment. "
Tell me about ‘always_ram’ parameter?," The `always_ram` parameter is used to determine whether quantized vectors should be kept in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors. However, in some setups, it might be beneficial to keep quantized vectors in RAM to speed up the search process. In such cases, you can set `always_ram` to `true` to store quantized vectors in RAM. This parameter can be found in the `quantization_config` section of the collection configuration. For more information, please refer to the 'Setting up Binary Quantization' guide in the documentation at source:documentation/guides/quantization.md.","['document:""compression"": ""x32"",\n\n                    ""always_ram"": true\n\n                }\n\n            },\n\n            ""on_disk"": true\n\n        }\n\n    },\n\n    ""hnsw_config"": {\n\n        ""ef_construct"": 123\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""quantile"": 0.8,\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\,source:documentation/concepts/collections.md'
 'document:It might be worth tuning this parameter if you experience a significant decrease in search quality.\n\n\n\n`always_ram` - whether to keep quantized vectors always cached in RAM or not. By default, quantized vectors are loaded in the same way as the original vectors.\n\nHowever, in some setups you might want to keep quantized vectors in RAM to speed up the search process.\n\n\n\nIn this case, you can set `always_ram` to `true` to store quantized vectors in RAM.\n\n\n\n### Setting up Binary Quantization,source:documentation/guides/quantization.md'
 'document:""vectors"": {\n\n                    ""size"": 384,\n\n                    ""distance"": ""Cosine""\n\n                },\n\n                ""shard_number"": 1,\n\n                ""replication_factor"": 1,\n\n                ""write_consistency_factor"": 1,\n\n                ""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },,source:documentation/concepts/collections.md'
 'document:{\n\n    ""vectors"": {\n\n      ""size"": 768,\n\n      ""distance"": ""Cosine""\n\n    },\n\n    ""optimizers_config"": {\n\n        ""memmap_threshold"": 20000\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient, models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.create_collection(\n\n    collection_name=""{collection_name}"",,source:documentation/guides/quantization.md'
 'document:compression: CompressionRatio::X16.into(),\n\n                always_ram: Some(true),\n\n            })),\n\n        }),\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.CompressionRatio;\n\nimport io.qdrant.client.grpc.Collections.CreateCollection;\n\nimport io.qdrant.client.grpc.Collections.Distance;,source:documentation/guides/quantization.md']","In the context of vector quantization in Qdrant, the 'always_ram' parameter determines whether quantized vectors should be kept always cached in RAM or not. By default, quantized vectors are loaded in the same manner as the original vectors. Setting 'always_ram' to true ensures that the quantized vectors are consistently cached in RAM, providing faster access times."
What is difference between scalar and product quantization?," Scalar quantization is a compression technique that reduces the number of bits used to represent each vector component, while product quantization is a technique that compresses vectors by dividing them into smaller subvectors and compressing each subvector separately. The main difference between the two is that scalar quantization is faster but less accurate, while product quantization is slower but more accurate. For more information on how to choose the right quantization method for your use case, please refer to the [Quantization Tips](/documentation/guides/quantization/#quantization-tips) section of the documentation.","['document:But there are some tradeoffs. Product quantization distance calculations are not SIMD-friendly, so it is slower than scalar quantization.\n\nAlso, product quantization has a loss of accuracy, so it is recommended to use it only for high-dimensional vectors.\n\n\n\nPlease refer to the [Quantization Tips](#quantization-tips) section for more information on how to optimize the quantization parameters for your use case.\n\n\n\n## How to choose the right quantization method,source:documentation/guides/quantization.md'
 'document:The main drawback of scalar quantization is the loss of accuracy. The `float32 -> uint8` conversion introduces an error that can lead to a slight decrease in search quality.\n\nHowever, this error is usually negligible, and tends to be less significant for high-dimensional vectors.\n\nIn our experiments, we found that the error introduced by scalar quantization is usually less than 1%. \n\n\n\nHowever, this value depends on the data and the quantization parameters.,source:documentation/guides/quantization.md'
 'document:. For example, on one of the previous releases we have scalar quantization and product quantization, which can compress up to 64 times the size of the vector. And we only keep in fast storage these compressed vectors.,source:blog/binary-quantization-andrey-vasnetsov-vector-space-talk-001.md'
 'document:*Available as of v1.1.0*\n\n\n\nScalar quantization, in the context of vector search engines, is a compression technique that compresses vectors by reducing the number of bits used to represent each vector component.\n\n\n\n\n\nFor instance, Qdrant uses 32-bit floating numbers to represent the original vector components. Scalar quantization allows you to reduce the number of bits used to 8.\n\nIn other words, Qdrant performs `float32 -> uint8` conversion for each vector component.,source:documentation/guides/quantization.md'
 'document:- Deployment in a low-RAM environment where the limiting factor is the number of disk reads rather than the vector comparison itself\n\n- Situations where the dimensionality of the original vectors is sufficiently high\n\n- Cases where indexing speed is not a critical factor\n\n\n\nIn circumstances that do not align with the above, Scalar Quantization should be the preferred choice.\n\n\n\nQdrant documentation on [Product Quantization](/documentation/guides/quantization/#setting-up-product-quantization),source:articles/product-quantization.md']","While both methods aim to reduce the memory footprint and potentially speed up operations, scalar quantization offers faster processing with SIMD-friendly operations and minimal accuracy loss, making it suitable for scenarios where speed is critical. Product quantization achieves higher compression rates and is better suited for handling very high-dimensional vectors where memory space is more of a concern than computation speed."
What is ‘best_score’ strategy?," The ‘best_score’ strategy is a new recommendation strategy introduced in version 1.6 of the Qdrant software. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one. This strategy takes into account all the examples separately, making the variety of results richer. To use this strategy, set ""strategy"": ""best_score"" in the recommendation request. For more information on this and other strategies, refer to the documentation on the ‘Explore’ concept in the Qdrant documentation.","['document:#   [ScoredPoint(id=3, version=0, score=0.09, \n\n#                payload=None, vector=[0.0, 0.0, 0.0, 0.1]),\n\n#    ScoredPoint(id=1, version=0, score=0.10049876, \n\n#                payload=None, vector=[0.0, 0.1, 0.0, 0.0])]\n\n# ]\n\n\n\n```,source:blog/batch-vector-search-with-qdrant.md'
 'document:The way it works is that each candidate is measured against every example, then we select the best positive and best negative scores. The final score is chosen with this step formula:\n\n\n\n```rust\n\nlet score = if best_positive_score > best_negative_score {\n\n    best_positive_score;\n\n} else {\n\n    -(best_negative_score * best_negative_score);\n\n};\n\n```\n\n\n\n<aside role=""alert"">\n\nThe performance of <code>best_score</code> strategy will be linearly impacted by the amount of examples.\n\n</aside>,source:documentation/concepts/explore.md'
 'document:This is the default strategy that\'s going to be set implicitly, but you can explicitly define it by setting `""strategy"": ""average_vector""` in the recommendation request.\n\n\n\n### Best score strategy\n\n\n\n*Available as of v1.6.0*\n\n\n\nA new strategy introduced in v1.6, is called `best_score`. It is based on the idea that the best way to find similar vectors is to find the ones that are closer to a positive example, while avoiding the ones that are closer to a negative one.,source:documentation/concepts/explore.md'
 'document:##### One positive and one negative example\n\n\n\n<video autoplay=""true"" loop=""true"" width=""100%"" controls><source src=""/articles_data/new-recommendation-api/one-positive-one-negative.mp4"" type=""video/mp4""></video>\n\n\n\nThe more likes and dislikes we add, the more diverse the results of the `best_score` strategy will be. In the old strategy, there \n\nis just a single vector, so all the examples are similar to it. The new one takes into account all the examples separately, making \n\nthe variety richer.,source:articles/new-recommendation-api.md'
 'document:""status"": ""ok"",\n\n  ""time"": 0.001\n\n}\n\n```\n\n\n\nThe algorithm used to get the recommendations is selected from the available `strategy` options. Each of them has its own strengths and weaknesses, so experiment and choose the one that works best for your case.\n\n\n\n### Average vector strategy,source:documentation/concepts/explore.md']","The `best_score` strategy is based on the idea of finding similar vectors by identifying those that are closer to a positive example while avoiding those that are closer to a negative one. This strategy measures each candidate against every example and selects the best positive and best negative scores. The final score is determined using a specific formula: if the best positive score is greater than the best negative score, the final score is set as the best positive score. Otherwise, the final score is calculated as the negative of the square of the best negative score. It is important to note that the performance of the `best_score` strategy is linearly impacted by the number of examples provided."
How does oversampling helps?," Oversampling helps by equalizing the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios. This is explained in the article ""Binary Quantization"" in the section ""Oversampling"" (https://www.example.com/articles/binary-quantization.md). Additionally, in the blog post ""Binary Quantization and Andrey Vasnetsov's Vector Space Talk"" (https://www.example.com/blog/binary-quantization-andrey-vasnetsov-vector-space-talk-001.md), it is mentioned that oversampling is a special technique used to control precision in real-time search.","['document:oversampling helps equalize the representation of classes in the training dataset, thus enabling more fair and accurate modeling of real-world scenarios.,source:articles/binary-quantization-openai.md'
 'document:These two parameters are how you are going to balance speed versus accuracy. The larger the size of your oversample, the more items you need to read from disk and the more elements you have to search with the relatively slower full vector index. On the other hand, doing this will produce more accurate results.,source:articles/binary-quantization.md'
 'document:Yeah, so oversampling is a special technique we use to control precision of the search in real time, in query time. And the thing is, we can internally retrieve from quantized storage a bit more vectors than we actually need. And when we do rescoring with original vectors, we assign more precise score. And therefore from this overselection, we can pick only those vectors which are actually good for the user,source:blog/binary-quantization-andrey-vasnetsov-vector-space-talk-001.md'
 'document:We see that ""correct"" results i.e. recall increases as the number of potential ""candidates"" increase (limit x oversampling). To highlight the impact of changing the `limit`, different limit values are broken apart into different curves. For example, we see that the lowest recall for limit 50 is around 94 correct, with 100 candidates. This also implies we used an oversampling of 2.0\n\n\n\nAs oversampling increases, we see a general improvement in results – but that does not hold in every case.,source:articles/binary-quantization.md'
 'document:3. Oversampling: Use an oversampling factor of 3 for the best balance between accuracy and efficiency. This factor is suitable for a wide range of applications.\n\n4. Rescoring: Enable rescoring to improve the accuracy of search results.,source:articles/binary-quantization-openai.md']","Defines how many extra vectors should be pre-selected using quantized index, and then re-scored using original vectors. For example, if oversampling is 2.4 and limit is 100, then 240 vectors will be pre-selected using quantized index, and then top-100 will be returned after re-scoring. Oversampling is useful if you want to tune the tradeoff between search speed and search quality in the query time."
What is the purpose of ‘CreatePayloadIndexAsync’?," The 'CreatePayloadIndexAsync' function is used to create a new payload index for a specific collection in Qdrant. This allows for efficient searching and retrieval of data based on the specified field and its schema type. This function is referenced in the documentation under the 'Multiple Partitions' guide, specifically in the 'Limitations' section.","['document:```java\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Collections.PayloadSchemaType;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient\n\n    .createPayloadIndexAsync(\n\n        ""{collection_name}"", ""group_id"", PayloadSchsemaType.Keyword, null, null, null, null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);,source:documentation/guides/multiple-partitions.md'
 'document:client.createPayloadIndex(""{collection_name}"", {\n\n  field_name: ""name_of_the_field_to_index"",\n\n  field_schema: {\n\n    type: ""text"",\n\n    tokenizer: ""word"",\n\n    min_token_len: 2,\n\n    max_token_len: 15,\n\n    lowercase: true,\n\n  },\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{\n\n    client::QdrantClient,\n\n    qdrant::{\n\n        payload_index_params::IndexParams, FieldType, PayloadIndexParams, TextIndexParams,\n\n        TokenizerType,\n\n    },\n\n};,source:documentation/concepts/indexing.md'
 'document:await client.CreatePayloadIndexAsync(collectionName: ""{collection_name}"", fieldName: ""group_id"");\n\n```\n\n\n\n## Limitations\n\n\n\nOne downside to this approach is that global requests (without the `group_id` filter) will be slower since they will necessitate scanning all groups to identify the nearest neighbors.,source:documentation/guides/multiple-partitions.md'
 'document:.setMaxTokenLen(10)\n\n                    .setLowercase(true)\n\n                    .build())\n\n            .build(),\n\n        null,\n\n        null,\n\n        null)\n\n    .get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\nusing Qdrant.Client.Grpc;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.CreatePayloadIndexAsync(\n\n\tcollectionName: ""{collection_name}"",\n\n\tfieldName: ""name_of_the_field_to_index"",\n\n\tschemaType: PayloadSchemaType.Text,,source:documentation/concepts/indexing.md'
 'document:client.createPayloadIndex(""{collection_name}"", {\n\n  field_name: ""group_id"",\n\n  field_schema: ""keyword"",\n\n});\n\n```\n\n\n\n```rust\n\nuse qdrant_client::{client::QdrantClient, qdrant::FieldType};\n\n\n\nlet client = QdrantClient::from_url(""http://localhost:6334"").build()?;\n\n\n\nclient\n\n    .create_field_index(\n\n        ""{collection_name}"",\n\n        ""group_id"",\n\n        FieldType::Keyword,\n\n        None,\n\n        None,\n\n    )\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport io.qdrant.client.QdrantClient;,source:documentation/guides/multiple-partitions.md']",CreatePayloadIndexAsync is a method in the Qdrant library that enables the creation of a payload index in Qdrant. A payload index is a data structure designed to store supplemental information about the data stored in Qdrant. This method requires a Qdrant instance and a payload index name as input parameters.
What is the purpose of ef_construct in HNSW ?," The `ef_construct` parameter in HNSW (Hierarchical Navigable Small World) algorithm is used to control the trade-off between the index build time and the index quality. It specifies the number of neighbors that are visited during the construction of the index. A higher value of `ef_construct` results in a better index quality but also increases the build time. This parameter can be set in the `hnsw_config` field of the `vectors_config` parameter when creating or updating a collection. For more information, refer to the 'HNSW Config' section in the 'Collections' concept page of the Qdrant documentation.","['document:(""my_vector"".into()),\n\n                        VectorParamsDiff {\n\n                            hnsw_config: Some(HnswConfigDiff {\n\n                                m: Some(32),\n\n                                ef_construct: Some(123),\n\n                                ..Default::default()\n\n                            }),\n\n                            ..Default::default()\n\n                        },\n\n                    )]),\n\n                },\n\n            )),\n\n        }),,source:documentation/concepts/collections.md'
 'document:hnsw_ef: Some(128),\n\n            exact: Some(false),\n\n            ..Default::default()\n\n        }),\n\n        limit: 3,\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport java.util.List;\n\n\n\nimport io.qdrant.client.QdrantClient;\n\nimport io.qdrant.client.QdrantGrpcClient;\n\nimport io.qdrant.client.grpc.Points.SearchParams;\n\nimport io.qdrant.client.grpc.Points.SearchPoints;\n\n\n\nQdrantClient client =,source:documentation/guides/optimize.md'
 'document:""ef_construct"": 123\n\n    },\n\n    ""quantization_config"": {\n\n        ""scalar"": {\n\n            ""type"": ""int8"",\n\n            ""quantile"": 0.8,\n\n            ""always_ram"": false\n\n        }\n\n    }\n\n}\'\n\n```\n\n\n\n```python\n\nclient.update_collection(\n\n    collection_name=""{collection_name}"",\n\n    vectors_config={\n\n        ""my_vector"": models.VectorParamsDiff(\n\n            hnsw_config=models.HnswConfigDiff(\n\n                m=32,\n\n                ef_construct=123,\n\n            ),,source:documentation/concepts/collections.md'
 'document:""hnsw_ef"": 128,\n\n        ""exact"": false\n\n    },\n\n    ""vector"": [0.2, 0.1, 0.9, 0.7],\n\n    ""limit"": 3\n\n}\n\n```\n\n\n\n```python\n\nfrom qdrant_client import QdrantClient\n\nfrom qdrant_client.http import models\n\n\n\nclient = QdrantClient(""localhost"", port=6333)\n\n\n\nclient.search(\n\n    collection_name=""{collection_name}"",\n\n    query_filter=models.Filter(\n\n        must=[\n\n            models.FieldCondition(\n\n                key=""city"",\n\n                match=models.MatchValue(\n\n                    value=""London"",,source:documentation/concepts/search.md'
 'document:Values under the key `params` specify custom parameters for the search.\n\nCurrently, it could be:\n\n\n\n* `hnsw_ef` - value that specifies `ef` parameter of the HNSW algorithm.\n\n* `exact` - option to not use the approximate search (ANN). If set to true, the search may run for a long as it performs a full scan to retrieve exact results.,source:documentation/concepts/search.md']","In HNSW algorithm the ef_construct parameter is the number of neighbours to consider during the index building. The larger the value, the higher the precision, but the longer the indexing time. The default values of this parameters 100"
How do you use ‘ordering’ parameter?," To use the 'ordering' parameter, you can include it in the PUT request when creating a new collection. This parameter allows you to specify the ordering method for the points in the collection. You can choose between 'strong' and 'weak' ordering, which determine the level of similarity between points. For more information on how to use this parameter, please refer to the 'PUT /collections/{collection_name}/points' section in the documentation [here](https://github.com/qdrant/qdrant/blob/master/docs/api.md#put-collectionscollection_namepoints).","['document:```http\n\nPUT /collections/{collection_name}/points?ordering=strong\n\n{\n\n    ""batch"": {\n\n        ""ids"": [1, 2, 3],\n\n        ""payloads"": [\n\n            {""color"": ""red""},\n\n            {""color"": ""green""},\n\n            {""color"": ""blue""}\n\n        ],\n\n        ""vectors"": [\n\n            [0.9, 0.1, 0.1],\n\n            [0.1, 0.9, 0.1],\n\n            [0.1, 0.1, 0.9]\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.upsert(\n\n    collection_name=""{collection_name}"",\n\n    points=models.Batch(\n\n        ids=[1, 2, 3],,source:documentation/guides/distributed_deployment.md'
 'document:limit: 10,\n\n        ..Default::default()\n\n    })\n\n    .await?;\n\n```\n\n\n\n```java\n\nimport java.util.List;\n\n\n\nimport static io.qdrant.client.PointIdFactory.id;\n\n\n\nimport io.qdrant.client.grpc.Points.RecommendPoints;\n\n\n\nclient\n\n    .recommendAsync(\n\n        RecommendPoints.newBuilder()\n\n            .setCollectionName(""{collection_name}"")\n\n            .addAllPositive(List.of(id(100), id(231)))\n\n            .addAllNegative(List.of(id(718)))\n\n            .setUsing(""image"")\n\n            .setLimit(10),source:documentation/concepts/explore.md'
 'document:""must"": [\n\n            {\n\n                ""key"": ""diet[].food"",\n\n                  ""match"": {\n\n                    ""value"": ""meat""\n\n                }\n\n            },\n\n            {\n\n                ""key"": ""diet[].likes"",\n\n                  ""match"": {\n\n                    ""value"": true\n\n                }\n\n            }\n\n        ]\n\n    }\n\n}\n\n```\n\n\n\n```python\n\nclient.scroll(\n\n    collection_name=""{collection_name}"",\n\n    scroll_filter=models.Filter(\n\n        must=[\n\n            models.FieldCondition(,source:documentation/concepts/filtering.md'
 'document:WITH ENGINE = ""qdrant"",\n\nPARAMETERS = {\n\n    ""location"": "":memory:"",\n\n    ""collection_config"": {\n\n        ""size"": 386,\n\n        ""distance"": ""Cosine""\n\n    }\n\n}\n\n```\n\n\n\nThe available arguments for instantiating Qdrant can be found [here](https://github.com/mindsdb/mindsdb/blob/23a509cb26bacae9cc22475497b8644e3f3e23c3/mindsdb/integrations/handlers/qdrant_handler/qdrant_handler.py#L408-L468).\n\n\n\n## Creating a new table,source:documentation/frameworks/mindsdb.md'
 'document:import io.qdrant.client.QdrantGrpcClient;\n\n\n\nQdrantClient client =\n\n    new QdrantClient(QdrantGrpcClient.newBuilder(""localhost"", 6334, false).build());\n\n\n\nclient.deleteCollectionAsync(""{collection_name}"").get();\n\n```\n\n\n\n```csharp\n\nusing Qdrant.Client;\n\n\n\nvar client = new QdrantClient(""localhost"", 6334);\n\n\n\nawait client.DeleteCollectionAsync(""{collection_name}"");\n\n```\n\n\n\n### Update collection parameters,source:documentation/concepts/collections.md']","Write ordering can be specified for any write request to serialize it through a single “leader” node, which ensures that all write operations (issued with the same ordering) are performed and observed sequentially. It is of 3 types weak , medium and strong and is used in python with additional param ordering=models.WriteOrdering.STRONG to upsert request."
What is significance of ‘on_disk_payload’ setting?," The 'on_disk_payload' setting in the 'on_disk_payload' field of the collection configuration allows for the storage of payload data on disk only. This can be useful for limiting RAM usage, especially for collections with large payload values. This setting is defined in the configuration file and can also be modified using the API. For more information on this setting and how to use it, please refer to the 'on_disk_payload' section in the 'collections' documentation page (https://github.com/qdrant/qdrant/blob/master/config/config.yaml).","['document:* `on_disk_payload` - defines where to store payload data. If `true` - payload will be stored on disk only. Might be useful for limiting the RAM usage in case of large payload.\n\n* `quantization_config` - see [quantization](../../guides/quantization/#setting-up-quantization-in-qdrant) for details.\n\n\n\nDefault parameters for the optional collection parameters are defined in [configuration file](https://github.com/qdrant/qdrant/blob/master/config/config.yaml).,source:documentation/concepts/collections.md'
 'document:""vectors"": {\n\n                    ""size"": 384,\n\n                    ""distance"": ""Cosine""\n\n                },\n\n                ""shard_number"": 1,\n\n                ""replication_factor"": 1,\n\n                ""write_consistency_factor"": 1,\n\n                ""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },,source:documentation/concepts/collections.md'
 'document:The payload data is loaded into RAM at service startup while disk and [RocksDB](https://rocksdb.org/) are used for persistence only.\n\nThis type of storage works quite fast, but it may require a lot of space to keep all the data in RAM, especially if the payload has large values attached - abstracts of text or even images.\n\n\n\nIn the case of large payload values, it might be better to use OnDisk payload storage.,source:documentation/concepts/storage.md'
 'document:{\n\n    ""vectors"": {\n\n        """": {\n\n            ""on_disk"": true\n\n        }\n\n    }\n\n}\n\n```\n\n\n\n```bash\n\ncurl -X PATCH http://localhost:6333/collections/test_collection1 \\\n\n  -H \'Content-Type: application/json\' \\\n\n  --data-raw \'{\n\n    ""vectors"": {\n\n        """": { \n\n            ""on_disk"": true \n\n      }\n\n    }\n\n  }\'\n\n```\n\n\n\n\n\nTo put vector data on disk for a collection that **does have** named vectors:,source:documentation/concepts/collections.md'
 'document:""payload_schema"": {\n\n        ""property1"": {\n\n            ""data_type"": ""keyword""\n\n        },\n\n        ""property2"": {\n\n            ""data_type"": ""integer""\n\n        }\n\n    }\n\n}\n\n```,source:documentation/concepts/payload.md']","The `on_disk_payload` setting in the storage configuration determines whether a point's payload will be stored in memory or read from disk every time it is requested. When set to `true`, the point's payload will not be stored in memory, saving RAM but slightly increasing the response time as the data needs to be retrieved from disk. It is important to note that payload values involved in filtering and indexed values will still remain in RAM for efficient access. This setting allows for a balance between RAM usage and response time in handling data storage and retrieval processes."
What is the impact of ‘write_consistency_factor’ ?, The `write_consistency_factor` determines the number of replicas that must acknowledge a write operation before responding to the client. This value can be configured at the collection's creation time and increasing it can make write operations more tolerant to network partitions in the cluster. This information can be found in the documentation under the 'write_consistency_factor' section of the 'distributed_deployment' guide.,"['document:### Write consistency factor\n\n\n\nThe `write_consistency_factor` represents the number of replicas that must acknowledge a write operation before responding to the client. It is set to one by default.\n\nIt can be configured at the collection\'s creation time.\n\n\n\n```http\n\nPUT /collections/{collection_name}\n\n{\n\n    ""vectors"": {\n\n        ""size"": 300,\n\n        ""distance"": ""Cosine""\n\n    },\n\n    ""shard_number"": 6,\n\n    ""replication_factor"": 2,\n\n    ""write_consistency_factor"": 2,\n\n}\n\n```\n\n\n\n```python,source:documentation/guides/distributed_deployment.md'
 'document:- `write_consistency_factor` - defines the number of replicas that must acknowledge a write operation before responding to the client. Increasing this value will make write operations tolerant to network partitions in the cluster, but will require a higher number of replicas to be active to perform write operations.,source:documentation/guides/distributed_deployment.md'
 'document:""vectors"": {\n\n                    ""size"": 384,\n\n                    ""distance"": ""Cosine""\n\n                },\n\n                ""shard_number"": 1,\n\n                ""replication_factor"": 1,\n\n                ""write_consistency_factor"": 1,\n\n                ""on_disk_payload"": false\n\n            },\n\n            ""hnsw_config"": {\n\n                ""m"": 16,\n\n                ""ef_construct"": 100,\n\n                ""full_scan_threshold"": 10000,\n\n                ""max_indexing_threads"": 0\n\n            },,source:documentation/concepts/collections.md'
 ""document:expect. It's therefore important **not** to rely on them.\n\n\n\nMore specifically, these numbers represent the count of points and vectors in\n\nQdrant's internal storage. Internally, Qdrant may temporarily duplicate points\n\nas part of automatic optimizations. It may keep changed or deleted points for a\n\nbit. And it may delay indexing of new points. All of that is for optimization\n\nreasons.\n\n\n\nUpdates you do are therefore not directly reflected in these numbers. If you see,source:documentation/concepts/collections.md""
 'document:**Python**, being just a bit younger than Java, is ubiquitous in ML projects, mostly owing to its tooling (notably jupyter notebooks), being easy to learn and integration in most ML stacks. It doesn’t have a traditional garbage collector, opting for ubiquitous reference counting instead, which somewhat helps memory consumption. With that said, unless you only use it as glue code over high-perf modules, you may find yourself waiting for results,source:articles/why-rust.md']","The `write_consistency_factor` parameter in a distributed deployment using Qdrant defines the number of replicas that must acknowledge a write operation before responding to the client. By increasing this value, the write operations become more tolerant to network partitions within the cluster. However, this also means that a higher number of replicas need to be active in order to perform write operations successfully."
